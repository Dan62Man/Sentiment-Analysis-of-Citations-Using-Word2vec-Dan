id,sentiment,review
1,0,However_comma_ another approach is to train a separate out-of-domain parser_comma_ and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al. _comma_ 2006).
2,0,Recent work by McClosky et al.(2006) and Blitzer et al.(2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation.
3,0,c2007 Association for Computational Linguistics Structural Correspondence Learning for Dependency Parsing Nobuyuki Shimizu Information Technology Center University of Tokyo Tokyo_comma_ Japan shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center University of Tokyo Tokyo_comma_ Japan nakagawa@dl.itc.u-tokyo.ac.jp Abstract Following (Blitzer et al. _comma_ 2006)_comma_ we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al. _comma_ 2005).
4,0,In this paper_comma_ we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al. _comma_ 2006) in the domain adaptation task given by the CoNLL 2007.
5,0,They hypothesize that a model trained in the source domain using this common feature representation will generalize better to the target domain_comma_ and focus on using unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains.
6,0,3 Domain Adaptation Following (Blitzer et al. _comma_ 2006)_comma_ we present an application of structural correspondence learning (SCL) to non-projective dependency parsing (McDonald et al. _comma_ 2005).
7,0,SCL is a method for adapting a classifier learned in a source domain to a target domain.
8,0,SCL works as follows: 1.
9,0,WSJ represents the application of the parser without SCL to the source domain test set_comma_ and WSJ-SCL the parser with SCL to the same test set.
10,0,Similarily 1168 Domain LAS UAS Label Accuracy WSJ 83.01%  83.43% 86.43%  86.81% 88.77%  88.99% WSJ-SCL 83.43%  83.59% 86.87%  86.93% 88.75%  89.01% Chem 74.75%  75.18% 80.74%  81.24% 82.34%  82.70% Chem-SCL 75.04%  74.91% 81.02%  80.82% 82.18%  82.18% Table 3: Labeled Attachment Score_comma_ Unlabeled Attachment Score and Label Accuracy Chem and Chem-SCL represents the application of the parser without SCL and with SCL to the source domain test set respectively.
11,0,Contrary to our expectations_comma_ we seem to see SCL overfitting to the source domain WSJ in this experiment.
12,0,Effectiveness of SCL for domain adaptation is mixed in this experiment perhaps due to the mismatch between feature sets.
13,0,Furthermore_comma_ I plan to apply my parsers in other domains (e.g. _comma_ biomedical data) (Blitzer et al. _comma_ 2006) besides treebank data_comma_ to investigate the effectiveness and generality of my approaches.
14,0,Following (Blitzer et al. _comma_ 2006)_comma_ we call the first the source domain_comma_ and the second the target domain.
15,0,Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani_comma_ 2003; Chelba and Acero_comma_ 2004; Florian et al. _comma_ 2004; Daume III and Marcu_comma_ 2006; Blitzer et al. _comma_ 2006).
16,0,The POS data set and the CTS data set have previously been used for testing other adaptation methods (Daume III and Marcu_comma_ 2006; Blitzer et al. _comma_ 2006)_comma_ though the setup there is different from ours.
17,0,Our performance using instance weighting is comparable to their best performance (slightly worse for POS and better for CTS).
18,0,Blitzer et al.(2006) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation_comma_ which can be regarded as weighting the features.
19,0,First_comma_ we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm_comma_ reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline.
20,0,440 respondence learning (SCL) domain adaptation algorithm (Blitzer et al. _comma_ 2006) for use in sentiment classification.
21,0,A key step in SCL is the selection of pivot features thatare usedtolink thesourceandtarget domains.
22,0,For data as diverse as product reviews_comma_ SCL can sometimes misalign features_comma_ resulting in degradation when we adapt between domains.
23,0,In the next section we briefly review SCL and introduce our new pivot selection method.
24,0,Section 4 gives results for SCL and the mutual information method for selecting pivot features.
25,0,2 Structural Correspondence Learning Before reviewing SCL_comma_ we give a brief illustrative example.
26,0,2.1 Algorithm Overview Given labeled data from a source domain and unlabeled data from both source and target domains_comma_ SCLfirstchoosesasetofmpivotfeatureswhichoccur frequently in both domains.
27,0,Then_comma_ it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang_comma_ 2005; Blitzer et al. _comma_ 2006).
28,0,2.2 Selecting Pivots with Mutual Information The efficacy of SCL depends on the choice of pivot features.
29,0,For the part of speech tagging problem studied by Blitzer et al.(2006)_comma_ frequently-occurring words in both domains were good choices_comma_ since they often correspond to function words such as prepositions and determiners_comma_ which are good indicators of parts of speech.
30,0,Table 1 shows the set-symmetric 441 SCL_comma_ not SCL-MI SCL-MI_comma_ not SCL book one <num> so all a must a wonderful loved it very about they like weak dont waste awful good when highly recommended and easy Table 1: Top pivots selected by SCL_comma_ but not SCLMI (left) and vice-versa (right) differencesbetweenthetwomethodsforpivotselectionwhenadaptingaclassifierfrombookstokitchen appliances.
31,0,When we reportresultswithSCLandSCL-MI_comma_werequirethat pivots occur in more than five documents in each domain.
32,0,4 Experiments with SCL and SCL-MI Each labeled dataset was split into a training set of 1600 instances and a test set of 400 instances.
33,0,The second is 442 65 70 75 80 85 90 D->BE->BK->BB->DE->DK->D baselineSCLSCL-MIbooks 72.8 76.8 79.7 70.7 75.475.4 70.9 66.1 68.6 80.4 82.4 77.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9 dvd 65 70 75 80 85 90 B->ED->EK->EB->KD->KE->K electronics kitchen 70.8 77.5 75.9 73.0 74.1 74.1 82.7 83.7 86.8 84.4 87.7 74.5 78.778.9 74.0 79.4 81.484.0 84.4 85.9 Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI.
34,0,domain\polarity negative positive books plot <num> pages predictable reader grisham engaging reading this page <num> must read fascinating kitchen the plastic poorly designed excellent product espresso leaking awkward to defective are perfect years now a breeze Table 2: Correspondences discovered by SCL for books and kitchen appliances.
35,0,To show how SCL deals with those domain mismatches_comma_ we look at the adaptation from book reviews to reviews of kitchen appliances.
36,1,SCL addresses both of these issues simultaneously by aligning features from the two domains.
37,0,In contrast_comma_ SCL assigns weight to these features indirectly through the projection matrix.
38,0,While some rows of the projection matrix  are 443 useful for classification_comma_ SCL can also misalign features.
39,0,Ando and Zhang (2005) and Blitzeretal.(2006)suggest = 104_comma_ = 0_comma_which we have used in our results so far.
40,0,We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al. _comma_ 2004; Blitzer et al. _comma_ 2006).
41,0,As a baseline_comma_ we used the label of the sourcedomainclassifierasafeatureinthetarget_comma_ but did not use any SCL features.
42,0,Here we show how to select source domains using only unlabeled data and the SCL representation.
43,0,6.1 The A-distance We propose to measure domain adaptability by using the divergence of two domains after the SCL projection.
44,0,Our procedure is as follows: Given two domains_comma_ we compute the SCL representation.
45,0,As we noted in Section 5_comma_ we are able to significantly outperform basic structural correspondence learning (Blitzer et al. _comma_ 2006).
46,0,We also note that while Florian et al.(2004) and Blitzer et al.(2006) observe that including the label of a source classifier asa featureon smallamounts of target data tends to improve over using either the source alone or the target alone_comma_ we did not observe that for our data.
47,0,Finally we note that while Blitzer et al.(2006) did combine SCL with labeled target domain data_comma_ they only compared using the label of SCL or non-SCL source classifiers as features_comma_ following the work of Florian et al.(2004).
48,0,By only adapting the SCLrelated part of the weight vector v_comma_ we are able to make better use of our small amount of unlabeled data than these previous techniques.
49,0,First_comma_ we showed that for a given source and target domain_comma_ we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al.(2006).
50,0,In the field of classifications_comma_ Blitzer et al.(2006) utilized unlabelled corpora to extract features of structural correspondences_comma_ and then adapted a POS-tagger to a biomedical domain.
51,0,Although unsupervised methods can exploit large in-domain data_comma_ the above studies could not obtain the accuracy as high as that for an original domain_comma_ even with the sufficient size of the unlabelled corpora.
52,0,Most of this prior work deals with supervised transfer learning_comma_ and thus requires labeled source domain data_comma_ though there are examples of unsupervised (Arnold et al._comma_ 2007)_comma_ semi-supervised (Grandvalet and Bengio_comma_ 2005; Blitzer et al._comma_ 2006)_comma_ and transductive approaches (Taskar et al._comma_ 2003).
53,0,Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al._comma_ 2006)_comma_ which is an extension of the method described by (Ando and Zhang_comma_ 558 2005).
54,0,In particular_comma_ both methods use a two-stage approach; They first train generative models or auxiliary problems from unlabeled data_comma_ and then_comma_ they incorporate these trained models into a supervised learning algorithm as real valued features.
55,0,Moreover_comma_ both methods make direct use of existing feature-vector definitions f(x_comma_y) in inducing representations from unlabeled data.
56,0,Previous work in domain adaptation can be classified into two categories: [S+T+]_comma_ where a small_comma_ labeled target domain data is available_comma_ e.g.(Blitzer et al._comma_ 2006; Jiang and Zhai_comma_ 2007; Daume III_comma_ 2007; Finkel and Manning_comma_ 2009)_comma_ or [S+T-]_comma_ where no labeled target domain data is available_comma_ e.g.(Blitzer et al._comma_ 2006; Jiang and Zhai_comma_ 2007).
57,0,In both cases_comma_ and especially for [S+T-]_comma_ domain adaptation can leverage on large amounts of unlabeled data in the target domain.
58,1,Thus although [S+T+] is easier to handle_comma_ [S+T-] is of higher practical importance.
59,0,In this paper_comma_ we propose a domain adaptive bootstrapping (DAB) approach to tackle the domain adaptation problem under the setting [S+T-].
60,0,Bickel et al.(Bickel et al._comma_ 2007) discriminatively learns a scaling factor for source domain training data_comma_ so as to adapt the source domain data distribution to resemble the target domain data distribution_comma_ under the [S+T-] setting.
61,0,They learn the underlying mixture distribution using the conditional expectation maximization algorithm_comma_ under the [S+T+] setting.
62,0,Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T-] settings.
63,1,For [S+T-]_comma_ the resulting algorithm is a balanced bootstrapping algorithm_comma_ which was shown to outperform the standard bootstrapping algorithm.
64,1,In this paper_comma_ we assume the [S+T-] settings_comma_ and we show that the approach proposed in this paper_comma_ domain adaptive bootstrapping (DAB)_comma_ outperforms the balanced bootstrapping algorithm on NER.
65,0,He assumed the existence of training data in the target-domain (under the setting [S+T+])_comma_ so that the three classes of features can be jointly trained using source and target domain labeled data.
66,0,This cannot be done in the setting [S+T-]_comma_ where no training data is available in the target domain.
67,0,Using a different approach_comma_ Blitzer et al.(2006) induces correspondences between feature spaces in different domains_comma_ by detecting pivot features.
68,0,3 Bootstrapping for domain adaptation We first define the notations used for domain adaptation in the [S+T-] setting.
69,0,For [S+T-]_comma_ the resulting algorithm is a balanced bootstrapping algorithm_comma_ which we describe below.
70,0,In general_comma_ applying semi-supervised learning methods directly to [S+T-] type domain adaptation problems do not work and appropriate modifications need to be made to the methods.
71,0,c2009 Association for Computational Linguistics Structural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen_comma_ The Netherlands b.plank@rug.nl Abstract The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG).
72,1,So far_comma_ SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007).
73,0,An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa_comma_ 2007)_comma_ however_comma_ without any clear conclusions.
74,1,We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains.
75,0,The problem itself has started to get attention only recently (Roark and Bacchiani_comma_ 2003; Hara et al._comma_ 2005; Daume III and Marcu_comma_ 2006; Daume III_comma_ 2007; Blitzer et al._comma_ 2006; McClosky et al._comma_ 2006; Dredze et al._comma_ 2007).
76,0,In contrast_comma_ semi-supervised domain adaptation (Blitzer et al._comma_ 2006; McClosky et al._comma_ 2006; Dredze et al._comma_ 2007) is the scenario in which_comma_ in addition to the labeled source data_comma_ we only have unlabeled and no labeled target domain data.
77,0,2 Motivation and Prior Work While several authors have looked at the supervised adaptation case_comma_ there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al._comma_ 2006; Blitzer et al._comma_ 2006; Dredze et al._comma_ 2007).
78,1,Similarly_comma_ Structural Correspondence Learning (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007; Blitzer_comma_ 2008) has proven to be successful for the two tasks examined_comma_ PoS tagging and Sentiment Classification.
79,0,In the same shared task_comma_ an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa_comma_ 2007).
80,0,However_comma_ based on annotation differences in the datasets (Dredze et al._comma_ 2007) and a bug in their system (Shimizu and Nakagawa_comma_ 2007)_comma_ their results are inconclusive.1 Thus_comma_ the effectiveness of SCL is rather unexplored for parsing.
81,0,Parse selection constitutes an important part of many parsing systems (Johnson et al._comma_ 1999; Hara et al._comma_ 2005; van Noord and Malouf_comma_ 2005; McClosky et al._comma_ 2006).
82,1,We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) for this task_comma_ a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis.
83,0,Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection_comma_ including all our design choices.
84,0,4 Structural Correspondence Learning SCL (Structural Correspondence Learning) (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007; Blitzer_comma_ 2008) is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains.
85,0,Before describing the algorithm in detail_comma_ let us illustrate the intuition behind SCL with an example_comma_ borrowed from Blitzer et al.(2007).
86,0,Therefore_comma_ the key idea of SCL is to identify automatically correspondences among features from different domains by modeling their correlations with pivot features.
87,0,Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al._comma_ 2006).
88,0,Intuitively_comma_ if we are able to find good correspondences among features_comma_ then the augmented labeled source domain data should transfer better to a target domain (where no labeled data is available) (Blitzer et al._comma_ 2006).
89,0,The outline of the algorithm is given in Figure 1.
90,0,Figure 1: SCL algorithm (Blitzer et al._comma_ 2006).
91,0,Applying the projection WTx (where x is a training instance) would give us m new features_comma_ however_comma_ for both computational and statistical reasons (Blitzer et al._comma_ 2006; Ando and Zhang_comma_ 2005) a low-dimensional approximation of the original feature space is computed by applying Singular Value Decomposition (SVD) on W (step 4).
92,0,The resulting  is a projection onto a lower dimensional space Rh_comma_ parameterized by h. The final step of SCL is to train a linear predictor on the augmented labeled source data x_comma_x.
93,0,4.1 SCL for Parse Disambiguation A property of the pivot predictors is that they can be trained from unlabeled data_comma_ as they represent properties of the input.
94,0,So far_comma_ pivot features on the word level were used (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007; Blitzer_comma_ 2008)_comma_ e.g. Does the bigram not buy occur in this document? (Blitzer_comma_ 2008).
95,0,Pivot features are the key ingredient for SCL_comma_ and they should align well with the NLP task.
96,0,Predictive features As pointed out by Blitzer et al.(2006)_comma_ each instance will actually contain features which are totally predictive of the pivot features (i.e. the pivot itself).
97,0,Matrix and SVD Following Blitzer et al.(2006) (which follow Ando and Zhang (2005))_comma_ we only use positive entries in the pivot predictors weight vectors to compute the SVD.
98,0,4.2 Further practical issues of SCL In practice_comma_ there are more free parameters and model choices (Ando and Zhang_comma_ 2005; Ando_comma_ 2006; Blitzer et al._comma_ 2006; Blitzer_comma_ 2008) besides the ones discussed above.
99,0,Blitzer et al.(2006) found it necessary to normalize and scale the new features obtained by the projection _comma_ in order to allow them to receive more weight from a regularized discriminative learner.
100,0,For each of the features_comma_ they centered them by subtracting out the mean and normalized them to unit variance (i.e. x  mean/sd).
101,0,They then rescaled the features by a factor  found on heldout data: x. Restricted Regularization.
102,0,When training the supervised model on the augmented feature space x_comma_x_comma_ Blitzer et al.(2006) only regularize the weight vector of the original features_comma_ but not the one for the new low-dimensional features.
103,0,This was done to encourage the model to use the new low-dimensional representation rather than the higher-dimensional original representation (Blitzer_comma_ 2008).
104,0,Due to the positive results in Ando (2006)_comma_ Blitzer et al.(2006) include this in their standard setting of SCL and report results using block SVDs only.
105,0,Optionally_comma_ filter out certain pages In our empirical setup_comma_ we followed Blitzer et al.(2006) and tried to balance the size of source and target data.
106,0,SCL results Table 4 shows the results of our instantiation of SCL for parse disambiguation_comma_ with varying h parameter (dimensionality parameter; 42 h = 25 means that applying the projection x resulted in adding 25 new features to every source domain instance).
107,0,baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-]_comma_ h = 25 85.12 85.46 78.64 2.64 SCL[+/-]_comma_ h = 50 85.29 85.63 79.66 7.29 SCL[+/-]_comma_ h = 100 85.19 85.53 79.04 4.47 SCL[+/-]_comma_ h = 200 85.21 85.54 79.18 5.10 baseline Paus 85.72 86.32 77.23 0.00 SCL[+/-]_comma_ h = 25 85.87 86.48 78.26 4.52 SCL[+/-]_comma_ h = 50 85.82 86.43 77.87 2.81 SCL[+/-]_comma_ h = 100 85.87 86.49 78.26 4.52 SCL[+/-]_comma_ h = 200 85.87 86.48 78.26 4.52 baseline DeMorgan 80.09 80.61 74.44 0.00 SCL[+/-]_comma_ h = 25 80.15 80.67 74.92 1.88 SCL[+/-]_comma_ h = 50 80.12 80.64 74.68 0.94 SCL[+/-]_comma_ h = 100 80.12 80.64 74.68 0.94 SCL[+/-]_comma_ h = 200 80.15 80.67 74.91 1.88 Table 4: Results of our instantiation of SCL (with varying h parameter and no feature normalization).
108,1,Thus_comma_ our first instantiation of SCL for parse disambiguation indeed shows promising results.
109,0,We can confirm that changing the dimensionality parameter h has rather little effect (Table 4)_comma_ which is in line with previous findings (Ando and Zhang_comma_ 2005; Blitzer et al._comma_ 2006).
110,0,This means our current instantiation of SCL is an actually simplified version of the original SCL algorithm_comma_ applied to parse disambiguation.
111,0,Of course_comma_ our results are preliminary and_comma_ rather than warranting many definite conclusions_comma_ encourage further exploration of SCL and related semi-supervised adaptation techniques.
112,0,5.4 Additional Empirical Results In the following_comma_ we describe additional results obtained by extensions and/or refinements of our current SCL instantiation.
113,0,While Blitzer et al.(2006) found it necessary to normalize (and scale) the projection features_comma_ we did not observe any improvement by normalizing them (actually_comma_ it slightly degraded performance in our case).
114,0,A look at  To gain some insight of which kind of correspondences SCL learned in our case_comma_ we started to examine the rows of .
115,0,Figure 4 shows example of correspondences that SCL found in the Prince dataset.
116,0,SCL clustered information about Chaka Khan_comma_ an Afro-Amerikaanse zangeres (afro-american singer) whose real name is Yvette Marie Stevens.
117,0,Moreover_comma_ SCL finds that tournee_comma_ regisseur and pop rock in the Prince domain behave like voorsprong (advance)_comma_ wetenschap (research) and plan as possible heads in a subject relation in the newspaper domain.
118,0,Table 5 shows the effect of using this larger dataset for SCL with h = 25.
119,0,baseline Prince 85.03 85.38 78.06 0.00 SCL[+/-]_comma_ h = 25_comma_ all 85.25 85.58 79.42 6.20 Table 5: First result on increasing unlabeled data.
120,0,Applying SCL with dimensionality reduction by feature type (SCL block) results in a model that performs better (CA 85.27_comma_  79.52_comma_ rel.er.
121,0,6 Conclusions and Future Work The paper presents an application of Structural Correspondence Learning (SCL) to parse disamFigure 5: Results of dimensionality reduction by feature type_comma_ h = 25; block SVD included all 9 feature types; the right part shows the accuracy when one feature type was removed.
122,0,While SCL has been successfully applied to PoS tagging and Sentiment Analysis (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007)_comma_ its effectiveness for parsing was rather unexplored.
123,1,The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results_comma_ even without the many additional extensions on the feature level as done in Blitzer et al.(2006).
124,1,On the three examined datasets_comma_ SCL slightly but constantly outperformed the baseline.
125,0,Applying SCL involves many design choices and practical issues_comma_ which we tried to depict here in detail.
126,0,(1) further explore/refine SCL (block SVDs_comma_ varying amount of target domain data_comma_ other testsets_comma_ etc.)_comma_ and (2) examine selftraining.
127,1,c 2009 Association for Computational Linguistics Improving SCL Model for Sentiment-Transfer Learning  Songbo Tan Institute of Computing Technology Beijing_comma_ China tansongbo@software.ict.ac.cn Xueqi Cheng Institute of Computing Technology Beijing_comma_ China cxq@ict.ac.cn  ABSTRACT In recent years_comma_ Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning.
128,0,However_comma_ SCL model treats each feature as well as each instance by an equivalent-weight strategy.
129,1,Among these techniques_comma_ SCL (Structural Correspondence Learning) (Blitzer et al._comma_ 2006) is regarded as a promising method to tackle transfer-learning problem.
130,0,The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features).
131,0,However_comma_ SCL model treats each feature as well as each instance by an equivalent-weight strategy.
132,0,From the perspective of feature_comma_ this strategy fails to overcome the adverse influence of highfrequency domain-specific (HFDS) features.
133,0,From the other perspective_comma_ the equivalentweight strategy of SCL model ignores the labels (positive or negative) of labeled instances.
134,0,Obviously_comma_ this is not a good idea.
135,0,2   Structural Correspondence Learning In the section_comma_ we provide the detailed procedures for SCL model.
136,0,The pivot predictors are the key job_comma_ because they directly decide the performance of SCL.
137,0,For each pivot feature k_comma_ we use a loss function L k _comma_ () 2 1)( wxwxpL i i T ikk +=         (1) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i _comma_ otherwise xif xp ik ik 0 1 1 )( >     = _comma_ where the weight vector w encodes the correspondence of the non-pivot features with the pivot feature k (Blitzer et al._comma_ 2006).
138,0,When  =0_comma_ it indicates that no HFDS features are used to build the correspondence vectors; while  =1 indicates that all features are equally used to build the correspondence vectors_comma_ that is to say_comma_ proposed FW-SCL algorithm is simplified as traditional SCL algorithm.
139,0,Consequently_comma_ proposed FW-SCL algorithm could be regarded as a generalized version of traditional SCL algorithm.
140,0,4 Instance-Weighted SCL Model The traditional SCL model does not take into account the labels (positive or negative) of instances on the source domain and pivot features.
141,0,For transfer-learning baseline_comma_ we implement traditional SCL model (T-SCL) (Blitzer et al._comma_ 2006).
142,0,Like TSVM_comma_ it makes use of the sourcedomain labeled data as well as the target-domain unlabeled data.
143,0,Note that we use 100 manualannotated pivot features for T-SCL_comma_ FW-SCL and W-SCL in the following experiments.
144,0,For T-SCL_comma_ FWSCL and W-SCL_comma_ we use prototype classifier (Sebastiani_comma_ 2002) to train the final model.
145,0,Table 2 shows the results of experiments comparing proposed method with supervised learning_comma_ transductive learning and T-SCL.
146,0,As expected_comma_ proposed method FW-SCL does indeed provide much better performance than supervised baselines_comma_ TSVM and T-SCL model.
147,0,For example_comma_ the average accuracy of FW-SCL beats supervised baselines by about 12 percents_comma_ beats TSVM by about 11 percents and beats TSCL by about 10 percents.
148,0,Table 2: Accuracy of different methods  NB T-SCL FW-SCL W-SCL Edu->Sto 0.6704 0.7965 0.7917 0.8108 Edu->Comp 0.5085 0.8019 0.8993 0.9025 Sto->Edu 0.6824 0.7712 0.9072 0.9368 Sto->Comp 0.5053 0.8126 0.8126 0.8693 Comp->Sto 0.6580 0.6523 0.7010 0.7717 Comp->Edu 0.6114 0.5976 0.9112 0.9408 Average 0.6060 0.7387 0.8372 0.8720 Although SCL is a method designed for transfer learning_comma_ but it cannot provide better performance than TSVM.
149,0,In another word_comma_ very few top-frequency words degrade the representative ability of SCL model for sentiment classification.
150,0,Various machine learning strategies have been proposed to address this problem_comma_ including semi-supervised learning (Zhu_comma_ 2007)_comma_ domain adaptation (Wu and Dietterich_comma_ 2004; Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007; Arnold et al._comma_ 2007; Chan and Ng_comma_ 2007; Daume_comma_ 2007; Jiang and Zhai_comma_ 2007; Reichart and Rappoport_comma_ 2007; Andreevskaia and Bergler_comma_ 2008)_comma_ multi-task learning (Caruana_comma_ 1997; Reichart et al._comma_ 2008; Arnold et al._comma_ 2008)_comma_ self-taught learning (Raina et al._comma_ 2007)_comma_ etc. A commonality among these methods is that they all require the training data and test data to be in the same feature space.
151,0,In addition_comma_ most of them are designed for supervised learning.
152,0,Performance also degrades when the domain of the test set differs from the domain of the training set_comma_ in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth_comma_ rare words) (Blitzer et al._comma_ 2006; Daume III and Marcu_comma_ 2006; Chelba and Acero_comma_ 2004).
153,0,For our POS tagging experiments_comma_ we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE_comma_ 2005)_comma_ a test set previously used by Blitzer et al.(2006).
154,0,We use the same experimental setup as Blitzer et al.: 40_comma_000 manually tagged sentences from the Penn Treebank for our labeled training data_comma_ and all of the unlabeled text from the Penn Treebank plus their MEDLINE corpus of 71_comma_306 sentences to train our HMM.
155,0,This table also includes results for two previous systems as reported by Blitzer et al.(2006): the semi-supervised Alternating Structural Optimization (ASO) technique and the Structural Correspondence Learning (SCL) technique for domain adaptation.
156,0,All Unknown Model words words Baseline 88.3 67.3 ASO 88.4 70.9 SCL 88.9 72.0 HMM 90.5 75.2 Table 5: On biomedical data from the Penn BioIE project_comma_ our HMM-smoothed tagger outperforms the SCL tagger by 3% (accuracy) on OOV words_comma_ and by 1.6% (accuracy) on all words.
157,0,Differences between the smoothed tagger and the SCL tagger are significant at p < .001 for all words and for OOV words_comma_ using the Chi-squared test with 1 degree of freedom.
158,0,The performance improvements for both the smoothed NP chunker and tagger are again impressive: there is a 12% improvement on OOV words_comma_ and a 10% overall improvement on rare words for chunking; the tagger shows an 8% improvement on OOV words compared to out baseline and a 3% improvement on OOV words compared to the SCL model.
159,0,HMM-smoothing improves on the most closely related work_comma_ the Structural Correspondence Learning technique for domain adaptation (Blitzer et al._comma_ 2006)_comma_ in experiments.
160,0,While transfer learning was proposed more than a decade ago (Thrun_comma_ 1996; Caruana_comma_ 1997)_comma_ its application in natural language processing is still a relatively new territory (Blitzer et al._comma_ 2006; Daume III_comma_ 2007; Jiang and Zhai_comma_ 2007a; Arnold et al._comma_ 2008; Dredze and Crammer_comma_ 2008)_comma_ and its application in relation extraction is still unexplored.
161,0,Blitzer et al.(2006) proposed a structural correspondence learning method for domain adaptation and applied it to part-of-speech tagging.
162,0,The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) and Self-training (Abney_comma_ 2007; McClosky et al._comma_ 2006).
163,1,A preliminary evaluation favors the use of SCL over the simpler self-training techniques.
164,0,We examine Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) for this task_comma_ and compare it to several variants of Self-training (Abney_comma_ 2007; McClosky et al._comma_ 2006).
165,1,2 Previous Work So far_comma_ Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007).
166,0,An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa_comma_ 2007).
167,0,However_comma_ the system just ended up at rank 7 out of 8 teams.
168,1,A recent attempt (Plank_comma_ 2009) shows promising results on applying SCL to parse disambiguation.
169,0,In this paper_comma_ we extend that line of work and compare SCL to bootstrapping approaches such as self-training.
170,0,While they self-trained a generative model_comma_ we examine self-training and SCL for semi-supervised adaptation of a discriminative parse selection system.
171,0,37 3 Semi-supervised Domain Adaptation 3.1 Structural Correspondence Learning Structural Correspondence Learning (Blitzer et al._comma_ 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains.
172,0,The outline of SCL is given in Algorithm 1.
173,0,The key to SCL is to exploit pivot features to automatically identify feature correspondences.
174,0,Pivots are features occurring frequently and behaving similarly in both domains (Blitzer et al._comma_ 2006).
175,0,Intuitively_comma_ if we are able to find good correspondences through linking pivots_comma_ then the augmented source data should transfer better to a target domain (Blitzer et al._comma_ 2006).
176,0,Algorithm 1 SCL (Blitzer et al._comma_ 2006) 1: Select m pivot features.
177,0,SCL for Discriminative Parse Selection So far_comma_ pivot features on the word level were used (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007).
178,0,However_comma_ for parse disambiguation based on a conditional model they are irrelevant.
179,0,In our empirical setup_comma_ we follow Blitzer et al.(2006) and balance the size of source and target data.
180,0,We report on results of SCL with dimensionality parameter set to h = 25_comma_ and remaining settings identical to Plank (2009) (i.e._comma_ no feature-specific regularization and no feature normalization and rescaling).
181,1,SCL and Self-training results The results for SCL (Table 3) show a small_comma_ but consistent increase in absolute performance on all testsets over the baselines (up to +0.27 absolute CA or 7.34% relative error reduction_comma_ which is significant at p < 0.05 according to sign test).
182,1,In contrast_comma_ basic self-training (Table 3) achieves roughly only baseline accuracy and lower performance than SCL_comma_ with one exception.
183,0,On the DeMorgan testset_comma_ self-training scores slightly higher than SCL.
184,1,However_comma_ the improvements of both SCL and self-training are not significant on this rather 39 small testset.
185,0,CA  Rel.ER Prince baseline 85.03 78.06 0.00 SCL  85.30 79.67 7.34 Self-train (all-at-once) 85.08 78.38 1.46 Paus baseline 85.72 77.23 0.00 SCL 85.82 77.87 2.81 Self-train (all-at-once) 85.78 77.62 1.71 DeMorgan baseline 80.09 74.44 0.00 SCL 80.15 74.92 1.88 Self-train (all-at-once) 80.24 75.63 4.65 Table 3: Results of SCL and self-training (single iteration_comma_ no selection).
186,0,Figure 2 compares several self-training variants with the supervised baseline and SCL.
187,1,The main conclusion is that in contrast to SCL_comma_ none of the self-training instantiations achieves a significant improvement over the baseline.
188,0,5 Conclusions and Future Work The paper compares Structural Correspondence Learning (Blitzer et al._comma_ 2006) with (various instances of) self-training (Abney_comma_ 2007; McClosky et al._comma_ 2006) for the adaptation of a parse selection model to Wikipedia domains.
189,1,The more indirect exploitation of unlabeled data through SCL is more fruitful than pure self-training.
190,0,Of course_comma_ our results are preliminary and_comma_ rather than warranting yet many definite conclusions_comma_ encourage further investigation of SCL (varying size of target data_comma_ pivots selection_comma_ bigger testsets as well as other domains etc.) as well as related semisupervised adaptation techniques.
191,0,40 0 50 100 150 200 85.00 85.05 85.10 85.15 85.20 85.25 85.30 number of iterations accuracy Indelibility versus delibility baseline SCL Indelible SelfTrain Delible SelfTrain EM Figure 1: Delible versus Indelible self-training and EM.
192,0,0 50 100 150 200 85.00 85.05 85.10 85.15 85.20 85.25 85.30 number of iterations accuracy shorter sent entropy fewer parses / no selection baseline SCL Indelibility with different selection techniques select shorter sent no selection Figure 2: Self-training variants compared to supervised baseline and SCL.
193,1,In all cases SCL still performs best.
194,0,Some of this work focuses on classifying the semantic orientation of individual words or phrases_comma_ using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown_comma_ 1997; Turney and Littman_comma_ 2002).
195,0,Turneys (2002) work on classiflcation of reviews is perhaps the closest to ours.2 He applied a speciflc unsupervised learning technique based on the mutual information between document phrases and the words \excellent' and \poor'_comma_ where the mutual information is computed using statistics gathered by a search engine.
196,0,We also note that Turney (2002) found movie reviews to be the most 2Indeed_comma_ although our choice of title was completely independent of his_comma_ our selections were eerily similar.
197,0,Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe_comma_ 2000; Turney_comma_ 2002)13_comma_ we looked at the performance of using adjectives alone.
198,0,In terms of relative performance_comma_ Naive Bayes tends to do the worst and SVMs tend to do the best_comma_ although the 12http://www.english.bham.ac.uk/stafi/oliver/software/tagger/index.htm 13Turneys (2002) unsupervised algorithm uses bigrams containing an adjective or an adverb.
199,0,(Turney (2002) makes a similar point_comma_ noting that for reviews_comma_ \the whole is not necessarily the sum of the parts').
200,0,Such techniques are currently being applied in many areas_comma_ including language identification_comma_ authorship attribution (Stamatatos et al. _comma_ 2000)_comma_ text genre classification (Kesseler et al. _comma_ 1997; Stamatatos et al. _comma_ 2000)_comma_ topic identification (Dumais et al. _comma_ 1998; Lewis_comma_ 1992; McCallum_comma_ 1998; Yang_comma_ 1999)_comma_ and subjective sentiment classification (Turney_comma_ 2002).
201,0,We are currently investigating more challenging problems like multiple category classification using the Reuters-21578 data set (Lewis_comma_ 1992) and subjective sentiment classification (Turney_comma_ 2002).
202,0,Some work identifies inflammatory texts (e.g. _comma_ (Spertus_comma_ 1997)) or classifies reviews as positive or negative ((Turney_comma_ 2002; Pang et al. _comma_ 2002)).
203,0,Researchers have focused on learning adjectives or adjectival phrases (Turney_comma_ 2002; Hatzivassiloglou and McKeown_comma_ 1997; Wiebe_comma_ 2000) and verbs (Wiebe et al. _comma_ 2001)_comma_ but no previous work has focused on learning nouns.
204,0,(Turney_comma_ 2002) used patterns representing part-of-speech sequences_comma_ (Hatzivassiloglou and McKeown_comma_ 1997) recognized adjectival phrases_comma_ and (Wiebe et al. _comma_ 2001) learned N-grams.
205,0,Some existing resources contain lists of subjective words (e.g. _comma_ Levins desire verbs (1993))_comma_ and some empirical methods in NLP have automatically identified adjectives_comma_ verbs_comma_ and N-grams that are statistically associated with subjective language (e.g. _comma_ (Turney_comma_ 2002; Hatzivassiloglou and McKeown_comma_ 1997; Wiebe_comma_ 2000; Wiebe et al. _comma_ 2001)).
206,0,2 Background 2.1 Subjectivity Analysis Much previous work on subjectivity recognition has focused on document-level classification.
207,0,For example_comma_ (Spertus_comma_ 1997) developed a system to identify inflammatory texts and (Turney_comma_ 2002; Pang et al. _comma_ 2002) developed methods for classifying reviews as positive or negative.
208,0,Turney (2002) showed that it is possible to use only a few of those semantically oriented words (namely_comma_ excellent and poor) to label other phrases co-occuring with them as positive or negative.
209,0,He then used these phrases to automatically separate positive and negative movie and product reviews_comma_ with accuracy of 6684%.
210,0,For determining whether an opinion sentence is positive or negative_comma_ we have used seed words similar to those produced by (Hatzivassiloglou and McKeown_comma_ 1997) and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by (Turney_comma_ 2002).
211,0,Our focus is on the sentence level_comma_ unlike (Pang et al. _comma_ 2002) and (Turney_comma_ 2002); we employ a significantly larger set of seed words_comma_ and we explore as indicators of orientation words from syntactic classes other than adjectives (nouns_comma_ verbs_comma_ and adverbs).
212,0,The approach is based on the hypothesis that positive words co-occur more than expected by chance_comma_ and so do negative words; this hypothesis was validated_comma_ at least for strong positive/negative words_comma_ in (Turney_comma_ 2002).
213,0,In earlier work (Turney_comma_ 2002) only singletons were used as seed words; varying their number allows us to test whether multiple seed words have a positive effect in detection performance.
214,0,2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Turney_comma_ 2002; Pang et al. _comma_ 2002) where a document is assumed to have only a single sentiment_comma_ thus these studies are not applicable to our goal.
215,0,Much research is also being directed at acquiring affect lexica automatically (Turney 2002_comma_ Turney and Littman 2002).
216,0,SO can be used to classify reviews (e.g. _comma_ movie reviews) as positive or negative (Turney_comma_ 2002)_comma_ and applied to subjectivity analysis such as recognizing hostile messages_comma_ classifying emails_comma_ mining reviews (Wiebe et al. _comma_ 2001).
217,0,The first step of those applications is to recognize that the text is subjective and then the second step_comma_ naturally_comma_ is to determine the SO of the subjective text.
218,0,Recent computational work either focuses on sentence subjectivity (Wiebe et al. 2002; Riloff et al. 2003)_comma_ concentrates just on explicit statements of evaluation_comma_ such as of films (Turney 2002; Pang et al. 2002)_comma_ or focuses on just one aspect of opinion_comma_ e.g._comma_ (Hatzivassiloglou and McKeown 1997) on adjectives.
219,0,A contrasting approach (Turney_comma_ 2002) relies only upon documents whose labels are unknown.
220,0,This makes it possible to use a large underlying corpus  in this case_comma_ the entire Internet as seen through the AltaVista search engine.
221,0,As a result_comma_ estimates for model parameters are subject to a relatively small amount of random variation.
222,0,The corresponding drawback to such an approach is that its predictions are not validated on actual documents.
223,0,Turneys model introduces the further consideration of incorporating human-provided knowledge about language.
224,0,The basic concept behind Turneys model is quite simple.
225,0,In the following section_comma_ we relate this model with Naive Bayes classification_comma_ showing that Turneys classifier is a pseudo-supervised approach: it effectively generates a new corpus of labeled documents_comma_ upon which it fits a Naive Bayes classifier.
226,0,Turneys classifier for the i-th documents sentiment si can now be written: ^si = sign Pp j=1 ^ jdij jdij !
227,0,(3) Using a carefully chosen collection of features_comma_ this classifier produces correct results on 65.8% of a collection of 120 movie reviews_comma_ where 60 are labeled positive and 60 negative.
228,0,Although this is not a particularly encouraging result_comma_ movie reviews tend to be a difficult domain.
229,0,Accuracy on sentiment classification in other domains exceeds 80% (Turney_comma_ 2002).
230,0,In Turney (2002)_comma_ features are selected according to part-of-speech labels.
231,0,2.3 Turneys Classifier as Naive Bayes Although Naive Bayes classification requires a labeled corpus of documents_comma_ we show in this section that Turneys approach corresponds to a Naive Bayes model.
232,0,In Propositions 1 and 2 below_comma_ we show that Turneys estimates of sentiment orientation ^ are closely related to ^_comma_ and that both estimates produce identical classifiers.
233,0,Proposition 1 ^ = C1^ (12) where C1 = Nexc:= P i:si=1 jdij Npoor=Pi:si= 1 jdij (13) Proof: Because a feature is restricted to at most one occurrence in a document_comma_ X i:si=k dij = N(w;ak) (14) Then from Equations 6 and 11: ^ j = log ^q1j^q 1j (15) = log N(w;exc:)= P i:si=1 jdij N(w;poor)=Pi:si= 1 jdij (16) = C1^ j (17) 2 2If both anchors occur on a site_comma_ then there will actually be two documents_comma_ one for each sentiment Proposition 2 Turneys classifier is identical to a Naive Bayes classifier fit on this corpus_comma_ with 1 = 1 = 0:5.
234,0,Thus the estimated logit is dlogit(sjd) = pX j=1 ^ jdj (18) = C1 pX j=1 ^ jdj (19) This is a positive multiple of Turneys classifier (Equation 3)_comma_ so they clearly match in sign.
235,0,2 3 A More Versatile Model 3.1 Desired Extensions By understanding Turneys model within a Naive Bayes framework_comma_ we are able to interpret its output as a probability model for document classes.
236,0,Turney Naive Bayes Coefs.
237,0,For each feature wj_comma_ we estimate j by using Turneys procedure_comma_ and by fitting a traditional Naive Bayes model to the labeled documents.
238,0,Second_comma_ movie reviews are apparently harder to classify than reviews of other products (Turney_comma_ 2002; Dave_comma_ Lawrence_comma_ and Pennock_comma_ 2003).
239,0,2 Motivation In the past_comma_ work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman_comma_ 2003; Turney_comma_ 2002; Kamps et al. _comma_ 2002; Hatzivassiloglou and Wiebe_comma_ 2000; Hatzivassiloglou and McKeown_comma_ 2002; Wiebe_comma_ 2000)_comma_ but in many domains of text_comma_ the values of individual phrases may bear little relation to the overall sentiment expressed by the text.
240,0,In the present work_comma_ the approach taken by Turney (2002) is used to derive such values for selected phrases in the text.
241,0,For the purposes of this paper_comma_ these phrases will be referred to as value phrases_comma_ since they will be the sources of SO values.
242,0,A SO value of zero would indicate a completely neutral semantic orientation.
243,0,The classes looked at in this work are as follows: Turney Value The average value of all value phrases SO values for the text.
244,0,Classification by this feature alone is not the equivalent of Turneys approach_comma_ since the present approach involves retraining in a supervised model.
245,0,Features used include word unigrams and lemmatized unigrams3 as well as the features described in 3.3 which make use of topic information_comma_ namely the broader PMI derived SO values and the topic-sentence Osgood values.
246,0,2http://www.pitchforkmedia.com 3We employ the Conexor FDG parser (Tapanainen and Jarvinen_comma_ 1997) for POS tagging and lemmatization 4http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM Model 5 folds 10 folds 20 folds 100 folds All (THIS WORK and THIS ARTIST) PMI 70% 70% 68% 69% THIS WORK PMI 72% 69% 70% 71% All Osgood 64% 64% 65% 64% All PMI and Osgood 74% 71% 74% 72% Unigrams 79% 80% 78% 82% Unigrams_comma_ PMI_comma_ Osgood 81% 80% 82% 82% Lemmas 83% 85% 84% 84% Lemmas and Osgood 83% 84% 84% 84% Lemmas and Turney 84% 85% 84% 84% Lemmas_comma_ Turney_comma_ text-wide Osgood 84% 85% 84% 84% Lemmas_comma_ PMI_comma_ Osgood 84% 85% 84% 86% Lemmas and PMI 84% 85% 85% 86% Hybrid SVM (PMI/Osgood and Lemmas) 86% 87% 84% 89% Figure 1: Accuracy results (percent of texts correctly classed) for 5_comma_ 10_comma_ 20 and 100-fold cross-validation tests with Pitchforkmedia.com record review data_comma_ hand-annotated for topic.
247,0,As Turney notes_comma_ unpredictable_comma_ is generally positive when describing a movie plot_comma_ and negative when describing an automobile or a politician.
248,1,Again_comma_ cursory investigations have thus far supported Turneys conclusion that the former are the appropriate terms to use for this task.
249,0,As a result_comma_ the problem of opinion mining has seen increasing attention over the last three years from (Turney_comma_ 2002; Hu and Liu_comma_ 2004) and many others.
250,0,While other systems_comma_ such as (Hu and Liu_comma_ 2004; Turney_comma_ 2002)_comma_ have addressed these tasks to some degree_comma_ OPINE is the first to report results.
251,0,PMI++ is an extended version of (Turney_comma_ 2002)s method for finding the SO label of a phrase (as an attempt to deal with context-sensitive words).
252,0,Subjective phrases are used by (Turney_comma_ 2002; Pang and Vaithyanathan_comma_ 2002; Kushal et al. _comma_ 2003; Kim and Hovy_comma_ 2004) and others in order to classify reviews or sentences as positive or negative.
253,0,7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment_comma_ for example determining whether a review is positive or negative (e.g. _comma_ (Turney_comma_ 2002; Dave et al. _comma_ 2003; Pang and Lee_comma_ 2004; Beineke et al. _comma_ 2004)).
254,0,A number of researchers have explored learning words and phrases with prior positive or negative polarity (another term is semantic orientation) (e.g. _comma_ (Hatzivassiloglou and McKeown_comma_ 1997; Kamps and Marx_comma_ 2002; Turney_comma_ 2002)).
255,0,To make the relationship between that task and ours clearer_comma_ note that some word lists used to evaluate methods for recognizing prior polarity are included in our prior-polarity lexicon (General Inquirer lists (General-Inquirer_comma_ 2000) used for evaluation by Turney_comma_ and lists of manually identified positive and negative adjectives_comma_ used for evaluation by Hatzivassiloglou and McKeown).
256,0,Much of this research explores sentiment classification_comma_ a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g. _comma_ Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
257,0,6 Conclusions and Future Directions In previous work_comma_ statistical NLP computation over large corpora has been a slow_comma_ of ine process_comma_ as in KNOWITALL (Etzioni et al. _comma_ 2005) and also in PMI-IR applications such as sentiment classi cation (Turney_comma_ 2002).
258,0,Many NLP problems address attitudinal meaning distinctions in text_comma_ e.g. detecting subjective opinion documents or expressions_comma_ e.g.(Wiebe et al_comma_ 2004)_comma_ measuring strength of subjective clauses (Wilson_comma_ Wiebe and Hwa_comma_ 2004)_comma_ determining word polarity (Hatzivassiloglou and McKeown_comma_ 1997) or texts attitudinal valence_comma_ e.g.(Turney_comma_ 2002)_comma_ (Bai_comma_ Padman and Airoldi_comma_ 2004)_comma_ (Beineke_comma_ Hastie and Vaithyanathan_comma_ 2003)_comma_ (Mullen and Collier_comma_ 2003)_comma_ (Pang and Lee_comma_ 2003).
259,0,Much of the relevant research explores sentiment classification_comma_ a text categorization task in which the goal is to assign to a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
260,0,Several other approaches have been applied for learning words and phrases that signal subjectivity.
261,0,Turney (2002) and Wiebe (2000) focused on learning adjectives and adjectival phrases and Wiebe et al.(2001) focused on nouns.
262,0,Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach.
263,0,In the document classification approach_comma_ researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g.(Dave et al. _comma_ 2003; Pang and Lee_comma_ 2004; Turney_comma_ 2002)).
264,0,Most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney_comma_ 2002; Pang_comma_ Lee_comma_ and Vaithyanathan_comma_ 2002; Dave_comma_ Lawrence_comma_ and Pennock_comma_ 2003; Yu and Hatzivassiloglou_comma_ 2003).
265,0,Also_comma_ even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classi cation techniques (Pang_comma_ Lee_comma_ and Vaithyanathan_comma_ 2002; Turney_comma_ 2002).
266,0,(Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen_comma_ 2001; Tong_comma_ 2001; Turney_comma_ 2002)).
267,0,But counterexamples are easy to construct: reviews can contain off-topic opinions_comma_ or recount many positive aspects before describing a fatal aw.
268,0,Turney (2002) noted that the unigram unpredictable might have a positive sentiment in a movie review (e.g. unpredictable plot)_comma_ but could be negative in the review of an automobile (e.g. unpredictable steering).
269,0,c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002).
270,0,We show that our approach outperforms Turneys original approach.
271,0,1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002_comma_ Pang et al. 2004_comma_ Turney 2002_comma_ Turney and Littman 2002_comma_ Wiebe et al. 2001_comma_ Bai et al. 2004_comma_ Yu and Hatzivassiloglou 2003 and many others).
272,0,Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002_comma_ Pang et al. 2004_comma_ Turney 2002_comma_ Turney and Littman 2002).
273,0,Turney (2002) and Turney and Littman (2002) exploit the first two generalizations for unsupervised sentiment classification of movie reviews.
274,0,They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms.
275,0,These seed terms can be viewed as proxies for the class labels positive and negative_comma_ allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation_comma_ and vice versa for poor.
276,0,Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and poor).
277,0,Given a set of terms with unknown sentiment orientation_comma_ Turney (2002) then uses the PMI-IR algorithm (Turney 2001) to issue queries to the web and determine_comma_ for each of these terms_comma_ its pointwise mutual information (PMI) with the two seed words across a large set of documents.
278,0,Turney et al.s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents.
279,0,We can then use this newly identified set to: (1) use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier_comma_ and (2) use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney (2002)_comma_ we do not use the web as a resource to find associations_comma_ rather we apply the method directly to in-domain data.
280,0,This has the disadvantage of not being able to apply the classification to any arbitrary domain.
281,0,It is worth noting_comma_ however_comma_ that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews.
282,0,We then used these seed terms in two basic ways: (1) We used them as seeds for a Turneystyle determination of the semantic orientation of words in the corpus (semantic orientation_comma_ or SO method).
283,0,As mentioned above_comma_ this process is based on the assumption that terms of similar orientation tend to co-occur.
284,0,accuracy Training data Turney (2002) 66% unsupervised Pang & Lee (2004) 87.15% supervised Aue & Gamon (2005) 91.4% supervised SO 73.95% unsupervised SM+SO to increase seed words_comma_ then SO 74.85% weakly supervised Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web.
285,0,Using the Turney SO method on in-domain data instead of web data achieves 73.95% accuracy (using the same two seed words that Turney does).
286,0,determining document orientation (or polarity)_comma_ as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee_comma_ 2004; Turney_comma_ 2002); 3.
287,0,The conceptually simplest approach to this latter problem is probably Turneys (2002)_comma_ who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches arealsopossible (Hatzivassiloglou and Wiebe_comma_ 2000; Riloff et al. _comma_ 2003; Wilson et al. _comma_ 2004).
288,0,Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases_comma_ whichhadoriginallybeendevelopedforwordsentiment classification.
289,0,In their method_comma_ the number of hits returned by a search-engine_comma_ with a query consisting of a phrase and a seed word (e.g. _comma_ phrase NEAR good) is used to determine the orientation.
290,0,Their method is similar to Turneys in the sense that cooccurrence with seed words is used.
291,0,The three methods above are based on context information.
292,0,This problem will be solved by incorporating other resources such as thesaurus or a dictionary_comma_orcombiningourmethodwithothermethods using external wider contexts (Suzuki et al. _comma_ 2006; Turney_comma_ 2002; Baron and Hirst_comma_ 2004).
293,0,Measures of attributional similarity have been studied extensively_comma_ due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997)_comma_ information retrieval (Deerwester et al. 1990)_comma_ determining semantic orientation (Turney 2002)_comma_ grading student essays (Rehder et al. 1998)_comma_ measuring textual cohesion (Morris and Hirst 1991)_comma_ and word sense disambiguation (Lesk 1986).
294,0,Many 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking (Lakoff and Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner et al. 2001; French 2002).
295,0,Although in its infancy_comma_ many researchers have worked in various facets of opinion analysis.
296,0,Pang et al.(2002) and Turney (2002) classified sentiment polarity of reviews at the document level.
297,0,These techniques were applied and examined in different domains_comma_ such as customer reviews (Hu and Liu 2004) and news articles 1.
298,0,These researchers use lists of opinion-bearing clue words and phrases_comma_ and then apply various additional techniques and refinements.
299,0,In addition_comma_ our method places the adjectival phrases (APs) in the common patterns on a more fine-grained scale of 1 to 5_comma_ similar to the strengthclassificationsin (Wilsonet al. _comma_ 2004)_comma_ in contrast to other automatic methods that classify expressions into a binary positive or negative polarity (e.g.(Turney_comma_ 2002)).
300,0,The first is identifying words and phrases that are associated with subjectivity_comma_ for example_comma_ that think is associated with private states and that beautiful is associated with positive sentiments (e.g. _comma_ (Hatzivassiloglou and McKeown_comma_ 1997; Wiebe_comma_ 2000; Kamps and Marx_comma_ 2002; Turney_comma_ 2002; Esuli and Sebastiani_comma_ 2005)).
301,0,The third exploits automatic subjectivity analysis in applications such as review classification (e.g. _comma_ (Turney_comma_ 2002; Pang and Lee_comma_ 2004))_comma_ mining texts for product reviews (e.g. _comma_ (Yi et al. _comma_ 2003; Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005))_comma_ summarization (e.g. _comma_ (Kim and Hovy_comma_ 2004))_comma_ information extraction (e.g. _comma_ (Riloff et al. _comma_ 2005))_comma_ 1Note that sentiment_comma_ the focus of much recent work in the area_comma_ is a type of subjectivity_comma_ specifically involving positive or negative opinion_comma_ emotion_comma_ or evaluation.
302,1,Turney also reported good result without domain customization (Turney_comma_ 2002).
303,0,We think these results can be further improved by domain adaptation technique_comma_ and it is one future work.
304,0,6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney_comma_ 2002)_comma_ and similar method is utilized by many other researchers (Yu and Hatzivassiloglou_comma_ 2003).
305,0,The concept behind Turneys model is that positive/negative phrases co-occur with words like excellent/poor.
306,0,Since his method relies on search engine_comma_ it is difficult to use rich linguistic information such as dependencies.
307,0,Identifying subjectivity helps separate opinions from fact_comma_ which may be useful in question answering_comma_ summarization_comma_ etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown_comma_ 1997; Turney_comma_ 2002; Esuli and Sebastiani_comma_ 2005).
308,0,Document level sentiment classification is mostly applied to reviews_comma_ where systems assign a positive or negative sentiment for a whole review document (Pang et al. _comma_ 2002; Turney_comma_ 2002).
309,0,Building on this work_comma_ more sophisticated problems in the opinion domain have been studied by many researchers.
310,0,Semantic orientation applied to unsupervised classification of reviews_comma_ Proceedings of ACL-02_comma_ Philadelphia_comma_ Pennsylvania_comma_ 417-424 Wiebe_comma_ Janyce M. _comma_ Bruce_comma_ Rebecca F. _comma_ and O'Hara_comma_ Thomas P. 1999.
311,0,2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g. _comma_ Pang et al.(2002)_comma_ Turney (2002))_comma_ a sentence (e.g. _comma_ Liu et al.(2003)_comma_ Yu and Hatzivassiloglou (2003)_comma_ Kim and Hovy (2004)_comma_ Gamon et al.(2005))_comma_ a phrase (e.g. _comma_ Wilson et al.(2005))_comma_ and a specific object (such as a product) mentioned in a document (e.g. _comma_ Morinaga et al.(2002)_comma_ Yi et al.(2003)_comma_ Popescu and Etzioni (2005)).
312,0,Much work has been performed on learning to identify and classify polarity terms (i.e. _comma_ terms expressing a positive sentiment (e.g. _comma_ happy) or a negative sentiment (e.g. _comma_ terrible)) and exploiting them to do polarity classification (e.g. _comma_ Hatzivassiloglou and McKeown (1997)_comma_ Turney (2002)_comma_ Kim and Hovy (2004)_comma_ Whitelaw et al.(2005)_comma_ Esuli and Sebastiani (2005)).
313,0,Though reasonably successful_comma_ these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision.
314,0,For instance_comma_ instead of representing the polarity of a term using a binary value_comma_ Mullen and Collier (2004) use Turneys (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration.
315,0,Work focusses on analysing subjective features of text or speech_comma_ such as sentiment_comma_ opinion_comma_ emotion or point of view (Pang et al. _comma_ 2002; Turney_comma_ 2002; Dave et al. _comma_ 2003; Liu et al. _comma_ 2003; Pang and Lee_comma_ 2005; Shanahan et al. _comma_ 2005).
316,0,For instance_comma_ both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative?
317,0,Identifying subjectivity helps separate opinions from fact_comma_ which may be useful in question answering_comma_ summarization_comma_ etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown_comma_ 1997; Turney_comma_ 2002; Esuli and Sebastiani_comma_ 2005)_comma_ phrases and sentences (Kim and Hovy_comma_ 2004; Wilson et al. _comma_ 2005)_comma_ or documents (Pang et al. _comma_ 2002; Turney_comma_ 2002).
318,0,Building on this work_comma_ more sophisticated problems such as opinion holder identification have also been studied.
319,0,The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document eitherpositive(thumbsup)ornegative(thumbs down) polarity (e.g. Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
320,0,Most of the annotation approaches tackling these issues_comma_ however_comma_ are aimed at performing classifications at either the document level (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ or the sentence or word level (Wiebe et al. _comma_ 2004; Yu and Hatzivassiloglou_comma_ 2003).
321,0,In addition_comma_ these approaches focus primarily on sentiment classification_comma_ and use the same for getting at the classification of facts vs. opinions.
322,0,In contrast to these approaches_comma_ the focus here is on marking attribution on more analytic semantic units_comma_ namely the Abstract Objects (AOs) associated with predicate-argument discourse relations annotated in the PDTB_comma_ with the aim of providing a compositional classification of the factuality of AOs.
323,0,In analyzing opinions (Cardie et al. _comma_ 2003; Wilson et al. _comma_ 2004)_comma_ judging document-level subjectivity (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ and answering opinion questions (Cardie et al. _comma_ 2003; Yu and Hatzivassiloglou_comma_ 2003)_comma_ the output of a sentence-level subjectivity classification can be used without modification.
324,0,The ve part-ofspeech (POS) patterns from (Turney_comma_ 2002) were used for the extraction of indicators_comma_ all involving at least one adjective or adverb.
325,0,One possible view on this annotation scheme could consider the rst two sets of categories as negative and the third set of categories positive_comma_ in the sense of Pang et al.(2002) and Turney (2002).
326,0,In particular_comma_ since we treat each individual speech within a debate as a single document_comma_ we are considering a version of document-level sentiment-polarity classification_comma_ namely_comma_ automatically distinguishing between positive and negative documents (Das and Chen_comma_ 2001; Pang et al. _comma_ 2002; Turney_comma_ 2002; Dave et al. _comma_ 2003).
327,0,Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently.
328,0,a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)).
329,0,For example_comma_ the adjective unpredictable may have a negative orientation in an automotive review_comma_ in a phrase such as unpredictable steering_comma_ but it could have a positive orientation in a movie review_comma_ in a phrase such as unpredictable plot_comma_ as mentioned in (Turney_comma_ 2002) in the context of his sentiment word detection.
330,0,C3BTC5 and CCCDCA were used in (Kamps and Marx_comma_ 2002) and (Turney and Littman_comma_ 2003)_comma_ respectively.
331,0,Turney (2002) used collocation with excellent or poor to obtain positive and negative clues for document classification.
332,0,The token precision is higher than 90% in all of the corpora_comma_ including the movie domain_comma_ which is considered to be difficult for SA (Turney_comma_ 2002).
333,0,In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005)_comma_ little attention has been paid to the important task studied here  assessing review helpfulness.
334,0,Many researchers have worked in various facets of opinion analysis.
335,0,Pang et al.(2002) and Turney (2002) classified sentiment polarity of reviews at the document level.
336,0,These techniques were applied and examined in different domains_comma_ such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004).
337,0,Lexical cues of differing complexities have been used_comma_ including single words and Ngrams (e.g. _comma_ (Mullen and Collier_comma_ 2004; Pang et al. _comma_ 2002; Turney_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Wiebe et al. _comma_ 2004))_comma_ as well as phrases and lexico-syntactic patterns (e.g_comma_ (Kim and Hovy_comma_ 2004; Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005; Riloff and Wiebe_comma_ 2003; Whitelaw et al. _comma_ 2005)).
338,0,While many of these studies investigate combinations of features and feature selection_comma_ this is the rst work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classi cation.
339,0,Also_comma_ PMI-IR is useful for calculating semantic orientation and rating reviews (Turney_comma_ 2002).
340,0,For example_comma_ researchers (Turney 2002; Yu and Hatzivassiloglou 2003) have identified semantic correlation between words and views: positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have a positive semantic orientation and vice versa for negative reviews or sentences with a negative semantic orientation.
341,0,1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al. _comma_ 2005; Turney_comma_ 2002; Dave et al. _comma_ 2003; Hu and Liu_comma_ 2004; Chaovalit and Zhou_comma_ 2005).
342,0,Most research on opinion analysis in computational linguistics has focused on sentiment analysis_comma_ subjectivity detection_comma_ and review mining.
343,0,Pang et al.(2002) and Turney (2002) classified sentiment polarity of reviews at the document level.
344,0,This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al. _comma_ 2002; Turney_comma_ 2002_comma_ etc.).
345,0,Typically_comma_ a small set of seed polar phrases are prepared_comma_ and new polar phrases are detected based on the strength of co-occurrence with the seeds (Hatzivassiloglous and McKeown_comma_ 1997; Turney_comma_ 2002; Kanayama and Nasukawa_comma_ 2006).
346,0,In Turneys work_comma_ the co-occurrence is considered as the appearance in the same window (Turney_comma_ 2002).
347,0,Although this idea is simple and feasible_comma_ there is a room for improvement.
348,0,This idea is the same as (Turney_comma_ 2002).
349,1,(Turney_comma_ 2002) is one of the most famous work that discussed learning polarity from corpus.
350,0,Turney determined polarity value 5 based on co-occurrence with seed words (excellent and poor).
351,0,The cooccurrence is measured by the number of hits returned by a search engine.
352,0,The polarity value proposed by (Turney_comma_ 2002) is as follows.
353,0,In summary_comma_ the strength of our approach is to exploit extremely precise structural clues_comma_ and to use 5 Semantic Orientation in (Turney_comma_ 2002).
354,0,massive collection of HTML documents to compensate for the low recall.
355,0,Although Turneys method also uses massive collection of HTML documents_comma_ his method does not make much of precision compared with our method.
356,0,As we will see in Section 5_comma_ our experimental result revealed that our method overwhelms Turneys method.
357,0,For example_comma_ if the lexicon contains an adjective excellent_comma_ it matches every adjective phrase that includes excellent such as view-excellent etc. As a baseline_comma_ we built lexicon similarly by using polarity value of (Turney_comma_ 2002).
358,0,Although Turneys method may be improved with minor configurations (e.g. using other seeds etc.)_comma_ we think this results indicate the feasibility of the proposed method.
359,0,Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases_comma_ which had originally been developed for word sentiment classification.
360,0,Their method is similar to Turneys in the sense that cooccurrence with seed words is used.
361,0,The four methods above are based on context information.
362,0,1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al. _comma_ 2002; Turney_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003).
363,0,However_comma_ multiple opinions on related matters are often intertwined throughout a text.
364,0,2 Related Work Sentiment Classi cation Traditionally_comma_ categorization of opinion texts has been cast as a binary classication task (Pang et al. _comma_ 2002; Turney_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Dave et al. _comma_ 2003).
365,0,Sentiment analysis includes a variety of different problems_comma_ including: sentiment classification techniques to classify reviews as positive or negative_comma_ based on bag of words (Pang et al. _comma_ 2002) or positive and negative words (Turney_comma_ 2002; Mullen and Collier_comma_ 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe_comma_ 2003; Pang and Lee_comma_ 2004); identifying or classifying appraisal targets (Nigam and Hurst_comma_ 2004); identifying the source of an opinion in a text (Choi et al. _comma_ 2005)_comma_ whether the author is expressing the opinion_comma_ or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. _comma_ 2005; Popescu and Etzioni_comma_ 2005).
366,0,Much of this work has utilized the fundamental concept of semantic orientation_comma_ (Turney_comma_ 2002); however_comma_ sentiment analysis still lacks a unified field theory.
367,1,SO-PMI is an unsupervised approach proposed by Turney that has been shown to work well for English.
368,0,We first used the SO-PMI algorithm on Japanese in a way very similar to Turneys original idea.
369,0,This shows that our proposed approach not only adapted the SO-PMI for Japanese_comma_ but also modified it to analyze Japanese opinions more effectively.
370,0,Turney (2002) has presented an unsupervised opinion classification algorithm called SO-PMI (Semantic Orientation Using Pointwise Mutual Information).
371,0,The main use of SO-PMI is to estimate the semantic orientation (i.e. positive or negative) of a phrase by measuring the hits returned from a search engine of pairs of words or phrases_comma_ based on the mutual information theory.
372,1,This approach has previously been successfully used on English.
373,0,However_comma_ according to our preliminary experiment_comma_ directly translating Turneys original idea into Japanese gave a very slanted result_comma_ with a positive accuracy of 95% and a negative accuracy of only 8%.
374,0,In related work (Chaovalit_comma_ 2005; Turney_comma_ 2002)_comma_ both supervised and unsupervised approaches have been shown to have their pros and cons.
375,0,To evaluate the polarity and strength of opinions_comma_ most of the existing approaches rely either on training from human-annotated data (Hatzivassiloglou and McKeown_comma_ 1997)_comma_ or use linguistic resources (Hu and Liu_comma_ 2004; Kim and Hovy_comma_ 2004) like WordNet_comma_ or rely on co-occurrence statistics (Turney_comma_ 2002) between words that are unambiguously positive (e.g. _comma_ excellent) and unambiguously negative (e.g. _comma_ horrible).
376,0,We follow the approach by Turney (2002)_comma_ who note that the semantic orientation of an adjective depends on the noun that it modifies and suggest using adjective-noun or adverb-verb pairs to extract semantic orientation.
377,0,However_comma_ we do not rely on linguistic resources (Kamps and Marx_comma_ 2002) or on search engines (Turney and Littman_comma_ 2003) to determine the semantic orientation_comma_ but rather rely on econometrics for this task.
378,0,Previous workonsentimentanalysishascoveredawiderange of tasks_comma_ including polarity classification (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ opinion extraction (Pang and Lee_comma_ 2004)_comma_ and opinion source assignment (Choi et al. _comma_ 2005; Choi et al. _comma_ 2006).
379,0,Furthermore_comma_ these systems have tackled the problem at different levels of granularity_comma_ from the document level (Pang et al. _comma_ 2002)_comma_ sentence level (Pang and Lee_comma_ 2004; Mao and Lebanon_comma_ 2006)_comma_ phrase level (Turney_comma_ 2002; Choi et al. _comma_ 2005)_comma_ as well as the speaker level in debates (Thomas et al. _comma_ 2006).
380,0,1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al. _comma_ 2002; Turney_comma_ 2002; Goldberg and Zhu_comma_ 2004).
381,0,The work most similar in spirit to ours that of Turney (2002).
382,0,He used the difference in mutual information with two human-selected features (the words excellent and poor) to score features in a completely unsupervised manner.
383,0,Then he classified documents according to various functions of these mutual information scores.
384,0,While we do not have a direct comparison_comma_ we note that Turney (2002) performs worse on movie reviews than on his other datasets_comma_ the same type of data as the polarity dataset.
385,0,2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications_comma_ such as tracking sentiment timelines in online forums and news (Lloyd et al. _comma_ 2005; Balog et al. _comma_ 2006)_comma_ review classification (Turney_comma_ 2002; Pang et al. _comma_ 2002)_comma_ mining opinions from product reviews (Hu and Liu_comma_ 2004)_comma_ automatic expressive text-to-speech synthesis (Alm et al. _comma_ 2005)_comma_ text semantic analysis (Wiebe and Mihalcea_comma_ 2006; Esuli and Sebastiani_comma_ 2006)_comma_ and question answering (Yu and Hatzivassiloglou_comma_ 2003).
386,0,Others_comma_ such as Turney (2002)_comma_ Pang and Vaithyanathan (2002)_comma_ have examined the positive or negative polarity_comma_ rather than presence or absence_comma_ of affective content in text.
387,0,Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments_comma_ such as sentences (Wilson et al. _comma_ 2005)_comma_ or on longer documents with an explicit polarity orientation like movie or product reviews (Turney_comma_ 2002).
388,0,Most of the authors traditionally use a classification-based approach for sentiment extraction and sentiment polarity detection (for example_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Kim and Hovy (2004) and others)_comma_ however_comma_ the research described in this paper uses the information retrieval (IR) paradigm which has also been used by some researchers.
389,0,TheauthorsapplySO-PMI-IR(Turney_comma_ 2002) to extract and determine the polarity of adjectives.
390,0,They then use a variant of SO-PMI-IR to determine a potential value for affect_comma_ judgement and appreciation_comma_ calculating the mutual information between the adjective and three pronoun-copular pairs: I was (affect); he was (judgement) and it was (appreciation).
391,0,As comparison_comma_ Turney and Littman (2003) used seed sets consisting of 7 words in their word valence annotation experiments_comma_ while Turney (2002) used minimal seed sets consisting of only one positive and one negative word (excellent and poor) in his experiments on review classification.
392,0,Such minimal seed sets of antonym pairs are not possible to use in the present experiment because they are often nearest neighbors to each other in the word space.
393,0,Also_comma_ it is difficult to find such clear paradigm words for the newswire domain.
394,0,2 Related work Our approach for emotion classification is based on the idea of (Hatzivassiloglou and McKeown_comma_ 1997) and is similar to those of (Turney_comma_ 2002) and (Turney and Littman_comma_ 2003).
395,0,The idea of tracing polarity through adjective cooccurrence is adopted by Turney (2002) for the binary (positive and negative) classification of text reviews.
396,0,They take two adjectives_comma_ for instance ?excellent??and ?poor??in a way that the first adjective expresses positive meaning_comma_ meanwhile the second one expresses negative.
397,0,Then_comma_ they extract all adjectives from the review text and combine them with ?excellent??and ?poor??
398,0,Following Hatzivassiloglou and McKeown (1997) and Turney (2002)_comma_ we decided to observe how often the words from the headline co-occur with each one of the six emotions.
399,0,This study helped us deduce information according to which ?birthday??appears more often with ?joy??
400,0,Some of the differences between our approach and those of Turney (2002) are mentioned below: ??objectives: Turney (2002) aims at binary text classification_comma_ while our objective is six class classification of one-liner headlines.
401,0,Moreover_comma_ we have to provide a score between 0 and 100 indicating the presence of an emotion_comma_ and not simply to identify what the emotion in the text is. Apart from the difficulty introduced by the multi-category classification_comma_ we have to deal with a small number of content words while Turney works with large list of adjectives.
402,0,??word class: Turney (2002) measures polarity using only adjectives_comma_ however in our approach we consider the noun_comma_ the verb_comma_ the adverb and the adjective content words.
403,0,??search engines: Turney (2002) uses the Altavista web browser_comma_ while we consider and combine the frequency information acquired from three web search engines.
404,0,??word proximity: For the web searches_comma_ Turney (2002) uses the NEAR operator and considers only those documents that contain the adjectives within a specific proximity.
405,0,??queries: The queries of Turney (2002) are made up of a pair of adjectives_comma_ and in our approach the query contains the content words of the headline and an emotion.
406,0,Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney_comma_ 2002; Takamura et al._comma_ 2006; Kaji and Kitsuregawa_comma_ 2006).
407,0,One major focus is sentiment classification and opinion mining (e.g._comma_ Pang et al 2002; Turney 2002; Hu and Liu 2004; Wilson et al 2004; Kim and Hovy 2004; Popescu and Etzioni 2005)   2008.
408,0,However_comma_ these studies mainly center on direct opinions or sentiments expressed on entities.
409,0,Sentiment classification at the document level investigates ways to classify each evaluative document (e.g._comma_ product review) as positive or negative (Pang et al 2002; Turney 2002).
410,0,These works are different from ours as we study comparatives.
411,0,Point-wise mutual information (PMI) is commonly used for computing the association of two terms (e.g._comma_ Turney 2002)_comma_ which is defined as: nullnullnull null null_comma_null null nullnullnull nullnullnullnull_comma_nullnull nullnull null null null nullnullnullnullnull . However_comma_ we argue that PMI is not a suitable measure for our purpose.
412,0,The acquisition of clues is a key technology in these research efforts_comma_ as seen in learning methods for document-level SA (Hatzivassiloglou and McKeown_comma_ 1997; Turney_comma_ 2002) and for phraselevel SA (Wilson et al._comma_ 2005; Kanayama and Nasukawa_comma_ 2006).
413,0,In contrast to the problem of identifying subjectivity or sentiment at the document level (e.g. Pang et al.(2002)_comma_ Turney (2002))_comma_ we are interested in fine-grained subjectivity analysis_comma_ which is concerned with subjectivity at the phrase or clause level.
414,0,Regarding the statistical approach_comma_ Kozareva et al.(2007) apply the theory of (Hatzivassiloglou and McKeown_comma_ 1997) and (Turney_comma_ 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy_comma_ fear).
415,0,80 8.0% Positive child education Positive cost Negative SUBJECT increase Figure 3: An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification_comma_ ranging from the use of co-occurrence with typical positive and negative words (Turney_comma_ 2002) to bag of words (Pang et al._comma_ 2002) and dependency structure (Kudo and Matsumoto_comma_ 2004).
416,0,Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words (as measured by pointwise mutual information).
417,0,The sentiment of a document is calculated as the average semantic orientation of all such phrases.
418,0,Another possible comparison could be with a version of Turney's (2002) sentiment classification method applied to Chinese.
419,0,However_comma_ the results would not be comparable since Turney's method would require the additional use of very large text corpus and the manual selection of positive and negative seed words.
420,0,Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs_comma_ which is denoted as the semantic oriented method.
421,0,1 Introduction Automatic classification of sentiment has been a focus of a number of recent research efforts (e.g.(Turney_comma_ 2002; Pang et al._comma_ 2002; Dave at al._comma_ 2003).
422,1,An important potential application of such work is in business intelligence: brands and company image are valuable property_comma_so organizations want to know how they are viewed by the media (what the 'spin' is on news stories_comma_ and editorials)_comma_ business analysts (as expressed in stock market reports)_comma_ customers (for example on product review sites) and their own employees.
423,1,Another important application is to help people find out others' views about products they have purchased(e.g. consumer electronics)_comma_ services and entertainment (e.g. movies)_comma_ stocks and shares (from investor bulletin boards)_comma_ and so on.
424,0,2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment (Turney_comma_ 2002; Pang et al._comma_ 2002; Dave at al._comma_ 2003).
425,0,But it is close to the paradigm described by Yarowsky (1995) and Turney (2002) as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples.
426,0,291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon_comma_ 2005; Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Riloff et al._comma_ 2006; Turney_comma_ 2002; Turney and Littman_comma_ 2003) or at the sentence levels (Gamon and Aue_comma_ 2005; Hu and Liu_comma_ 2004; Kim and Hovy_comma_ 2005; Riloff et al._comma_ 2006).
427,0,For example_comma_ it has been observed that texts often contain multiple opinions on different topics (Turney_comma_ 2002; Wiebe et al._comma_ 2001)_comma_ which makes assignment of the overall sentiment to the whole document problematic.
428,0,Sentiment classification is a well studied problem (Wiebe_comma_ 2000; Pang et al._comma_ 2002; Turney_comma_ 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007).
429,0,Sentiment summarization has been well studied in the past decade (Turney_comma_ 2002; Pang et al._comma_ 2002; Dave et al._comma_ 2003; Hu and Liu_comma_ 2004a_comma_ 2004b; Carenini et al._comma_ 2006; Liu et al._comma_ 2007).
430,0,There are many research directions_comma_ e.g._comma_ sentiment classification (classifying an opinion document as positive or negative) (e.g._comma_ Pang_comma_ Lee and Vaithyanathan_comma_ 2002; Turney_comma_ 2002)_comma_ subjectivity classification (determining whether a sentence is subjective or objective_comma_ and its associated opinion) (Wiebe and Wilson_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Wilson et al_comma_ 2004; Kim and Hovy_comma_ 2004; Riloff and Wiebe_comma_ 2005)_comma_ feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni_comma_ 2005; Carenini et al._comma_ 2005; Ku et al._comma_ 2006; Kobayashi_comma_ Inui and Matsumoto_comma_ 2007; Titov and McDonald.
431,0,One of the main directions is sentiment classification_comma_ which classifies the whole opinion document (e.g._comma_ a product review) as positive or negative (e.g._comma_ Pang et al_comma_ 2002; Turney_comma_ 2002; Dave et al_comma_ 2003; Ng et al. 2006; McDonald et al_comma_ 2007).
432,0,It is clearly different from our work as we are interested in conditional sentences.
433,0,They may rely only on this information (e.g._comma_ (Turney_comma_ 2002; Whitelaw et al._comma_ 2005; Riloff and Wiebe_comma_ 2003))_comma_ or they may combine it with additional information as well (e.g._comma_ (Yu and Hatzivassiloglou_comma_ 2003; Kim and Hovy_comma_ 2004; Bloom et al._comma_ 2007; Wilson et al._comma_ 2005a)).
434,1,Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification.
435,0,However_comma_ while his system learns the semantic orientation of the phrases in a review in an unsupervised manner_comma_ this information is used to predict the polarity of a review heuristically.
436,0,Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney_comma_ 2002; Wilson et al._comma_ 2005; Pang and Lee_comma_ 2008).
437,0,2 Literature Survey The task of sentiment analysis has evolved from document level analysis (e.g._comma_ (Turney._comma_ 2002); (Pang and Lee_comma_ 2004)) to sentence level analysis (e.g._comma_ (Hu and Liu._comma_ 2004); (Kim and Hovy._comma_ 2004); (Yu and Hatzivassiloglou_comma_ 2003)).
438,0,These researchers first set priors on words using a prior polarity lexicon.
439,0,2 Related Work There has been a large and diverse body of research in opinion mining_comma_ with most research at the text (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Popescu and Etzioni_comma_ 2005; Ounis et al._comma_ 2006)_comma_ sentence (Kim and Hovy_comma_ 2005; Kudo and Matsumoto_comma_ 2004; Riloff et al._comma_ 2003; Yu and Hatzivassiloglou_comma_ 2003) or word (Hatzivassiloglou and McKeown_comma_ 1997; Turney and Littman_comma_ 2003; Kim and Hovy_comma_ 2004; Takamura et al._comma_ 2005; Andreevskaia and Bergler_comma_ 2006; Kaji and Kitsuregawa_comma_ 2007) level.
440,0,3 Related Work Many methods have been developed for automatically identifying subjective (opinion_comma_ sentiment_comma_ attitude_comma_ affect-bearing_comma_ etc.) words_comma_ e.g._comma_ (Turney_comma_ 2002; Riloff and Wiebe_comma_ 2003; Kim and Hovy_comma_ 2004; Taboada et al._comma_ 2006; Takamura et al._comma_ 2006).
441,0,The research of opinion mining began in 1997_comma_ the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al._comma_ 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al._comma_ 2003; Pang et al._comma_ 2002; Zagibalov et al._comma_ 2008;).
442,0,There has been a great deal of previous work in sentiment analysis that worked with reviews_comma_ but they were typically restricted to using reviews extracted from one or two well-known sources_comma_ bypassing automatic review detection.
443,0,Examples of such early work include (Turney_comma_ 2002; Pang et al._comma_ 2002; Dave et al._comma_ 2003; Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005).
444,0,1   Introduction In the community of sentiment analysis (Turney 2002; Pang et al._comma_ 2002; Tang et al._comma_ 2009)_comma_ transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work_comma_ because sentiment expression often behaves with strong domain-specific nature.
445,0,In general_comma_ previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou_comma_ 2005) and unsupervised methods (Turney_comma_ 2002)_comma_ machine learning techniques and sentiment classification considering rating scales (Pang_comma_ Lee and Vaithyanathan_comma_ 2002)_comma_ and scoring of features (Dave_comma_ Lawrence and Pennock_comma_ 2003).
446,0,Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs_comma_ which is denoted as the semantic oriented method.
447,0,Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons (Das and Chen_comma_ 2001) to semi-automated approaches (Hu and Liu_comma_ 2004; Zhuang et al._comma_ 2006; Kim and Hovy_comma_ 2004)_comma_ and even an almost fully automated approach (Turney_comma_ 2002).
448,1,Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification.
449,0,However_comma_ while his system learns the semantic orientation of phrases in a review in an unsupervised manner_comma_ such information is used to heuristically predict the polarity of a review.
450,0,3 Method 3.1 Standard text classication approach We take our starting point from topic-based text classication (Dumais et al._comma_ 1998; Joachims_comma_ 1998) and sentiment classication (Turney_comma_ 2002; Pang and Lee_comma_ 2008).
451,0,1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews_comma_ product reviews_comma_ news and blog reviews (Pang et al._comma_ 2002; Turney_comma_ 2002).
452,0,Turney (2002) suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon.
453,0,Our work builds upon Turneys work on semantic orientation (Turney_comma_ 2002) and synonym learning (Turney_comma_ 2001)_comma_ in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries.
454,0,We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification.
455,0,Turney (Turney_comma_ 2001; Turney_comma_ 2002) reported that the NEAR operator outperformed simple page co-occurrence for his purposes; our early experiments informally showed the same for this work.
456,0,Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al._comma_ 2004)_comma_ and also on accurate sentiment polarity assignment (Turney_comma_ 2002; Yi et al._comma_ 2003; Pang and Lee_comma_ 2004; Sindhwani and Melville_comma_ 2008; Melville et al._comma_ 2009).
457,1,The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al._comma_ 2003)_comma_ or quality annotation data for statistical training.
458,0,Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms_comma_ as in METEOR (Banerjee and Lavie_comma_ 2005).
459,0,METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.
460,0,Banerjee and Lavie (2005) introduce the Meteor metric_comma_ which also incorporates recall on the unigram level and further provides facilities incorporating stemming_comma_ and WordNet synonyms as a more flexible match.
461,0,Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation.
462,0,255 Meteor (Banerjee and Lavie_comma_ 2005)_comma_ Precision and Recall (Melamed et al. _comma_ 2003)_comma_ and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity_comma_ and have inexact models of allowable variation in translation.
463,1,Recently_comma_ researchers have explored additional knowledge sources that could enhance automatic evaluation.
464,0,Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley_comma_ 2004; Banerjee and Lavie_comma_ 2005).
465,0,Our work complements these approaches: we focus on the impact of paraphrases_comma_ and study their contribution to the accuracy of automatic evaluation.
466,0,Other metrics assess the impact of alignments externally_comma_ e.g._comma_ different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g. _comma_ BLEU (Papineni et al. _comma_ 2002) or METEOR (Banerjee and Lavie_comma_ 2005)).
467,0,We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al. _comma_ 2001)_comma_ the NIST score (accumulated up to 5-grams) (Doddington_comma_ 2002)_comma_ the General Text Matching (GTM) F-measure (e = 1_comma_2) (Melamed et al. _comma_ 2003)_comma_ and the METEOR measure (Banerjee and Lavie_comma_ 2005).
468,0,These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references.
469,0,Additionally_comma_ METEOR considers stemming_comma_ and allows for WordNet synonymy lookup.
470,0,289 system BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR development EU-baseline 0.0737 2.8832 0.3131 0.2216 0.2881 WNG-baseline 0.1149 3.3492 0.3604 0.2605 0.3288 SYSTRAN 0.1625 3.9467 0.4257 0.2971 0.4394 test EU-baseline 0.0790 2.8896 0.3131 0.2262 0.2920 WNG-baseline 0.0951 3.1307 0.3471 0.2510 0.3219 SYSTRAN 0.1463 3.7873 0.4085 0.2921 0.4295 acl05-test EU-baseline 0.2381 6.5848 0.5699 0.2429 0.5153 Table 1: MT Results on development and test sets_comma_ for the two baseline systems compared to SYSTRAN and to the EU baseline system on the ACL-2005 SMT workshop test set extracted from the Europarl Corpus.
471,0,METEOR reflects the METEOR score.
472,0,290 Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR EU EU 0.0737 2.8832 0.3131 0.2216 0.2881 EU WNG 0.1062 3.4831 0.3714 0.2631 0.3377 EU D1 0.0959 3.2570 0.3461 0.2503 0.3158 EU D2 0.0896 3.2518 0.3497 0.2482 0.3163 EU D1 + D2 0.0993 3.3773 0.3585 0.2579 0.3244 EU EU + D1 + D2 0.0960 3.2851 0.3472 0.2499 0.3160 EU D1 + D2 + WNG 0.1094 3.4954 0.3690 0.2662 0.3372 EU EU + D1 + D2 + WNG 0.1080 3.4248 0.3638 0.2614 0.3321 WNG EU 0.0743 2.8864 0.3128 0.2202 0.2689 WNG WNG 0.1149 3.3492 0.3604 0.2605 0.3288 WNG D1 0.0926 3.1544 0.3404 0.2418 0.3050 WNG D2 0.0845 3.0295 0.3256 0.2326 0.2883 WNG D1 + D2 0.0917 3.1185 0.3331 0.2394 0.2995 WNG EU + D1 + D2 0.0856 3.0361 0.3221 0.2312 0.2847 WNG D1 + D2 + WNG 0.0980 3.2238 0.3462 0.2479 0.3117 WNG EU + D1 + D2 + WNG 0.0890 3.0974 0.3309 0.2373 0.2941 Table 2: MT Results on development set_comma_ for several translation/language model configurations.
473,0,Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR development EU EU + D1 + D2 + WNG 0.1272 3.6094 0.3856 0.2727 0.3695 WNG EU + D1 + D2 + WNG 0.1269 3.3740 0.3688 0.2676 0.3452 test EU EU + D1 + D2 + WNG 0.1133 3.4180 0.3720 0.2650 0.3644 WNG EU + D1 + D2 + WNG 0.1015 3.1084 0.3525 0.2552 0.3343 Table 3: MT Results on development and test sets after tuning for the EU + D1 + D2 + WNG language model configuration for the two translation models_comma_ EU and WNG.
474,0,In order 291 Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR development EUWNG WNG 0.1288 3.7677 0.3949 0.2832 0.3711 EUWNG EU + D1 + D2 + WNG 0.1182 3.6034 0.3835 0.2759 0.3552 EUWNG EU + D1 + D2 + WNG (TUNED) 0.1554 3.8925 0.4081 0.2944 0.3998 EU+WNG WNG 0.1384 3.9743 0.4096 0.2936 0.3804 EU+WNG EU + D1 + D2 + WNG 0.1235 3.7652 0.3911 0.2801 0.3606 EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1618 4.1415 0.4234 0.3029 0.4130 test EUWNG WNG 0.1123 3.6777 0.3829 0.2771 0.3595 EUWNG EU + D1 + D2 + WNG 0.1183 3.5819 0.3737 0.2772 0.3518 EUWNG EU + D1 + D2 + WNG (TUNED) 0.1290 3.6478 0.3920 0.2810 0.3885 EU+WNG WNG 0.1227 3.8970 0.3997 0.2872 0.3723 EU+WNG EU + D1 + D2 + WNG 0.1199 3.7353 0.3846 0.2812 0.3583 EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1400 3.8930 0.4084 0.2907 0.3963 Table 4: MT Results on development and test sets for the two strategies for combining translations models.
475,0,In order to improve sentence-level evaluation performance_comma_ several metrics have been proposed_comma_ including ROUGE-W_comma_ ROUGE-S (Lin and Och_comma_ 2004) and METEOR (Banerjee and Lavie_comma_ 2005).
476,0,METEOR is essentially a unigram based metric_comma_ which prefers the monotonic word alignment between MT output and the references by penalizing crossing word alignments.
477,0,There are two problems with METEOR.
478,0,First_comma_ it doesnt consider gaps in the aligned words_comma_ which is an important feature for evaluating the sentence uency; second_comma_ it cannot use multiple references simultaneously.1 ROUGE and METEOR both use WordNet and Porter Stemmer to increase the chance of the MT output words matching the reference words.
479,0,In order to take advantage of loose-sequence-based metrics and avoid the problems in ROUGE and METEOR_comma_ we propose a new metric SIA_comma_ which is based on loose sequence alignment but enhanced with the following features: 1METEOR and ROUGE both compute the score based on the best reference 539  Computing the string alignment score based on the gaps in the common sequence.
480,0,For the purpose of increasing hitting chance of MT outputs in references_comma_ we use a stochastic word matching in the string alignment instead of WORDSTEM and WORD-NET used in METEOR and ROUGE.
481,0,The remainder of the paper is organized as follows: section 2 gives a recap of BLEU_comma_ ROUGEW and METEOR; section 3 describes the three components of SIA; section 4 compares the performance of different metrics based on experimental results; section 5 presents our conclusion.
482,0,2 Recap of BLEU_comma_ ROUGE-W and METEOR The most commonly used automatic evaluation metrics_comma_ BLEU (Papineni et al. _comma_ 2002) and NIST (Doddington_comma_ 2002)_comma_ are based on the assumption that The closer a machine translation is to a promt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 1: Alignment Example for ROUGE-W fessional human translation_comma_ the better it is (Papineni et al. _comma_ 2002).
483,0,METEOR is a metric sitting in the middle of the n-gram based metrics and the loose se540 mt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 2: Alignment Example for METEOR quence based metrics.
484,0,METEOR doesnt require the alignment to be monotonic_comma_ which means crossing word mappings (e.g. a b is mapped to b a) are allowed_comma_ though doing so will get a penalty.
485,0,Figure 2 shows the alignments of METEOR based on the same example as ROUGE.
486,0,Though the two alignments have the same number of word mappings_comma_ mt2 gets more crossed word mappings than mt1_comma_ thus it will get less credits in METEOR.
487,0,Both ROUGE and METEOR normalize their evaluation result based on the MT output length (precision) and the reference length (recall)_comma_ and the nal score is computed as the F-mean of them.
488,0,3.2 Stochastic Word Mapping In ROUGE and METEOR_comma_ PORTER-STEM and WORD-NET are used to increase the chance of the MT output words matching the references.
489,0,function STO COMPUTE SCORE(mt_comma_ ref_comma_ i_comma_ j_comma_ n_comma_ p) if mt[i] == ref [j] then return 1/p(i  n)  (j  p); else return similarity(mt[i]_comma_ref [i])(in)(jp) ; end if end function Figure 5: Compute Stochastic Word Matching Score 3.3 Iterative Alignment Scheme ROUGE-W_comma_ METEOR_comma_ and WER all score MT output by rst computing a score based on each available reference_comma_ and then taking the highest score as the nal score for the MT output.
490,0,4 Experiments Evaluation experiments were conducted to compare the performance of different metrics including BLEU_comma_ ROUGE_comma_ METEOR and SIA.3 The test data for the experiments are from the MT evaluation workshop at ACL05.
491,0,For each MT output_comma_ there are two sets of human scores available_comma_ and 3METEOR and ROUGE can be downloaded at http://www.cs.cmu.edu/ alavie/METEOR and http://www.isi.edu/licensed-sw/see/rouge function GET ALIGN SCORE1(mt_comma_ ref_comma_ mttable_comma_ reftable) triangleright Compute the alignment score of the MT output mt with length M and the reference ref with length N_comma_ without considering the positions in mttable and reftable M = |mt|; N = |ref|; for i = 1; i  M; i = i +1 do for j = 1; j  N; j = j +1 do for k = 1; k  i; k = k +1 do for m = 1; m  j; m = m +1 do scorei_comma_j_comma_k_comma_m = maxscorei1_comma_j_comma_k_comma_m_comma_ scorei_comma_j1_comma_k_comma_m}; end for end for if i is not in mttable and j is not in reftable then scorei_comma_j_comma_i_comma_j = max n=1_comma_M;p=1_comma_N {scorei_comma_j_comma_i_comma_j_comma_ scorei1_comma_j1_comma_n_comma_p + COMPUTE SCORE(mt_comma_ ref_comma_ i_comma_ j_comma_ n_comma_ p)}; end if end for end for return scoreM_comma_N_comma_M_comma_NM and the corresponding alignment; end function Figure 7: Alignment Algorithm Based on Gaps Without Considering Aligned Positions m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London m: England with France discussed this crisis in London r2: England and France discussed the crisis in London r1: Britain and France consulted about this crisis in London with each other m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London Figure 8: Alignment Example for SIA 543 we randomly choose one as the score used in the experiments.
492,0,4.2 METEOR vs. SIA SIA is designed to take the advantage of loosesequence-based metrics without losing word-level information.
493,0,The decay facB-3 R 1 R 2 M S F 0.167 0.152 0.192 0.167 0.202 A 0.306 0.304 0.287 0.332 0.322 O 0.265 0.256 0.266 0.280 0.292 Table 2: Sentence level evaluation results of BLEU_comma_ ROUGE_comma_ METEOR and SIA tor in SIA is determined by optimizing the overall evaluation for E09_comma_ and then used with SIA to evaluate the other 5514 sentences based on the four sets of references.
494,0,To compare the performance of SIA with BLEU_comma_ ROUGE and METEOR_comma_ the evaluation results based on the same testing data is given in Table 2.
495,0,B3 denotes BLEU-3; R 1 denotes the skipped bigram based ROUGE metric which considers all skip distances and uses PORTER-STEM; R 2 denotes ROUGE-W with PORTER-STEM; M denotes the METEOR metric using PORTER-STEM and WORD-NET synonym; S denotes SIA.
496,1,We see that METEOR_comma_ as the other metric sitting in the middle of n-gram based metrics and loose sequence metrics_comma_ achieves improvement over BLEU in both adequacy and uency evaluation.
497,0,Though METEOR gets the best results in adequacy evaluation_comma_ in uency evaluation_comma_ it is worse than the loose-sequence-based metric ROUGE-W-STEM.
498,0,It achieves the best results in uency evaluation and comparable results to METEOR in adequacy evaluation_comma_ and the balanced performance leads to the best overall evaluation results in the experiment.
499,0,Its system-level score is computed as the arithmetic mean of the sentence level scores_comma_ and low mean high B-3 (-09.8%) 0.238 0.264 0.290 (+09.9%) R 1 (-10.2%) 0.229 0.255 0.281 (+10.0%) R 2 (-10.0%) 0.238 0.265 0.293 (+10.4%) M (-09.0%) 0.254 0.279 0.304 (+08.8%) S (-08.7%) 0.265 0.291 0.316 (+08.8%) Table 5: 95% signi cance intervals for sentencelevel overall evaluation WLS WLS WLS WLS PROB INCS PROB INCS F 0.189 0.202 0.188 0.202 A 0.295 0.310 0.311 0.322 O 0.270 0.285 0.278 0.292 Table 7: Results of different components in SIA WLS WLS WLS WLS INCS INCS INCS INCS STEM WN STEM WN F 0.188 0.188 0.187 0.191 A 0.311 0.313 0.310 0.317 O 0.278 0.280 0.277 0.284 Table 8: Results of SIA working with Porter-Stem and WordNet so are ROUGE_comma_ METEOR and the human judgments.
500,0,This scheme is similar to ROUGE-W and METEOR.
501,0,When PORTER-STEM and WordNet are 545 B-6 R 1 R 2 M S F 0.514 0.466 0.458 0.378 0.532 A 0.876 0.900 0.906 0.875 0.928 O 0.794 0.790 0.792 0.741 0.835 Table 6: Results of BLEU_comma_ ROUGE_comma_ METEOR and SIA in system level evaluation both used_comma_ PORTER-STEM is used rst.
502,0,Since INCS and PROB are independent of WLS_comma_ we believe they can also be used to improve other metrics such as ROUGE-W and METEOR.
503,0,An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in (Banerjee and Lavie_comma_ 2005).
504,0,However_comma_ error analysis is still a rather unexplored area.
505,0,Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference_comma_ like CDER (Leusch et al. _comma_ 2006)_comma_ which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie_comma_ 2005)_comma_ which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al. _comma_ 2005)_comma_ which makes use of stemming_comma_ WordNet synonymy_comma_ verb class synonymy_comma_ matching noun phrase heads_comma_ and proper name matching.
506,0,A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation_comma_ in that it requires considerable external knowledge resources like WordNet_comma_ verb class databases_comma_ and extensive text preparation: stemming_comma_ tagging_comma_ etc. The advantage of our method is that it produces relevant paraphrases with nothing more than the evaluation bitext and a widely available word and phrase alignment software_comma_ and therefore can be used with any existing evaluation metric.
507,0,For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al. _comma_ 2001)_comma_ NIST (n = 5) (Lin and Hovy_comma_ 2002)_comma_ GTM F1-measure (e = 1_comma_2) (Melamed et al. _comma_ 2003)_comma_ 1-WER (Nieen et al. _comma_ 2000)_comma_ 1-PER (Leusch et al. _comma_ 2003)_comma_ ROUGE (ROUGE-S*) (Lin and Och_comma_ 2004) and METEOR3 (Banerjee and Lavie_comma_ 2005).
508,0,167 Spanish-to-English System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S* Baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643 LDV-COMBO 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671 English-to-Spanish System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S* Baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028 LDV-COMBO 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240 Table 1: MT results comparing the LDV-COMBO system to a baseline system_comma_ for the test set both on the Spanish-to-English and English-to-Spanish tasks.
509,0,65 Table 1: Evaluation results on the IWSLT06 dataset: integrating the WSD translation predictions improves BLEU_comma_ NIST_comma_ METEOR_comma_ WER_comma_ PER_comma_ CDER and TER across all 3 different available test sets.
510,0,BLEU NIST METEOR METEOR (no syn) TER WER PER CDER Test 1 SMT 42.21 7.888 65.40 63.24 40.45 45.58 37.80 40.09 SMT+WSD 42.38 7.902 65.73 63.64 39.98 45.30 37.60 39.91 Test 2 SMT 41.49 8.167 66.25 63.85 40.95 46.42 37.52 40.35 SMT+WSD 41.97 8.244 66.35 63.86 40.63 46.14 37.25 40.10 Test 3 SMT 49.91 9.016 73.36 70.70 35.60 40.60 32.30 35.46 SMT+WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58 Table 2: Evaluation results on the NIST test set: integrating the WSD translation predictions improves BLEU_comma_ NIST_comma_ METEOR_comma_ WER_comma_ PER_comma_ CDER and TER Exper.
511,0,BLEU NIST METEOR METEOR (no syn) TER WER PER CDER SMT 20.41 7.155 60.21 56.15 76.76 88.26 61.71 70.32 SMT+WSD 20.92 7.468 60.30 56.79 71.34 83.87 57.29 67.38 5.1 Data set Preliminary experiments are conducted using training and evaluation data drawn from the multilingualBTECcorpus_comma_whichcontainssentencesusedin conversations in the travel domain_comma_ and their translations in several languages.
512,0,In addition to the widely used BLEU (Papineni et al. _comma_ 2002) and NIST (Doddington_comma_ 2002) scores_comma_ we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie_comma_ 2005) and four edit-distance style metrics_comma_ Word Error Rate (WER)_comma_ Positionindependent word Error Rate (PER) (Tillmann et al. _comma_ 1997)_comma_ CDER_comma_ which allows block reordering (Leusch et al. _comma_ 2006)_comma_ and Translation Edit Rate (TER) (Snover et al. _comma_ 2006).
513,0,Note that we report Meteor scores computed both with and without using WordNet synonyms to match translation candidates and references_comma_ showing that the improvement is not due to context-independent synonym matches at evaluation time.
514,0,There exists a variety of different metrics_comma_ e.g._comma_ word error rate_comma_ position-independent word error rate_comma_ BLEU score (Papineni et al. _comma_ 2002)_comma_ NIST score (Doddington_comma_ 2002)_comma_ METEOR (Banerjee and Lavie_comma_ 2005)_comma_ GTM (Turian et al. _comma_ 2003).
515,0,Each of them has advantages and shortcomings.
516,0,Experimental results were only reported for the METEOR metric (Banerjee and Lavie_comma_ 2005).
517,0,METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
518,0,In recent years_comma_ many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al. _comma_ 2000; Akiba et al. _comma_ 2001; Papineni et al. _comma_ 2002; NIST_comma_ 2002; Leusch et al. _comma_ 2003; Turian et al. _comma_ 2003; Babych and Hartley_comma_ 2004; Lin and Och_comma_ 2004; Banerjee and Lavie_comma_ 2005; Gimenez et al. _comma_ 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently.
519,0,Many methods for calculating the similarity have been proposed (Niessen et al. _comma_ 2000; Akiba et al. _comma_ 2001; Papineni et al. _comma_ 2002; NIST_comma_ 2002; Leusch et al. _comma_ 2003; Turian et al. _comma_ 2003; Babych and Hartley_comma_ 2004; Lin and Och_comma_ 2004; Banerjee and Lavie_comma_ 2005; Gimenez et al. _comma_ 2005).
520,0,In our research_comma_ 23 scores_comma_ namely BLEU (Papineni et al. _comma_ 2002) with maximum n-gram lengths of 1_comma_ 2_comma_ 3_comma_ and 4_comma_ NIST (NIST_comma_ 2002) with maximum n-gram lengths of 1_comma_ 2_comma_ 3_comma_ 4_comma_ and 5_comma_ GTM (Turian et al. _comma_ 2003) with exponents of 1.0_comma_ 2.0_comma_ and 3.0_comma_ METEOR (exact) (Banerjee and Lavie_comma_ 2005)_comma_ WER (Niessen et al. _comma_ 2000)_comma_ PER (Leusch et al. _comma_ 2003)_comma_ and ROUGE (Lin_comma_ 2004) with n-gram lengths of 1_comma_ 2_comma_ 3_comma_ and 4 and 4 variants (LCS_comma_ S_comma_SU_comma_ W-1.2)_comma_ were used to calculate each similarity S i . Therefore_comma_ the value of m in Eq.
521,0,Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie_comma_ 2005).
522,0,Therefore_comma_ if we can somehow find the alignments between the source sentence and the reference/MT output_comma_ we could be smarter in selecting the overlapping words to be counted in the 1In metrics such as METEOR_comma_ ROUGE_comma_ SIA (Liu and Gildea_comma_ 2006)_comma_ the positions of words do make difference_comma_ but it has nothing to do with the word itself.
523,0,As a comparison_comma_ we also show the results of BLEU_comma_ NIST_comma_ METEOR_comma_ ROUGE_comma_ WER_comma_ and HWCM.
524,0,For METEOR and ROUGE_comma_ WORDNET and PORTER-STEMMER are enabled_comma_ and for SIA_comma_ the decay factor is set to 0.6.
525,1,SSCN2 and SSCN u are also competitive to the state-of-art MT metrics such as METEOR and SIA.
526,0,The two reordering based metrics_comma_ PRS and MPR_comma_ are not as good as the other testing metrics_comma_ in terms Fluency Adequacy Overall ROUGE W 24.8 27.8 29.0 ROUGE S 19.7 30.9 28.5 METEOR 24.4 34.8 33.1 SIA 26.8 32.1 32.6 NIST 1 09.6 22.6 18.5 WER 22.5 27.5 27.7 PRS 14.2 19.4 18.7 MPR 11.0 18.2 16.5 BLEU(1) 18.4 29.6 27.0 BLEU(2) 20.4 31.1 28.9 BLEU(3) 20.7 30.4 28.6 HWCM(2) 22.1 30.3 29.2 SSCN1(1) 24.2 29.6 29.8 SSCN2(1) 22.9 33.0 31.3 SSCN u(1) 23.8 34.2 32.5 SSCN i(1) 23.4 28.0 28.5 pSSCN1(1) 24.9 30.2 30.6 pSSCN2(1) 23.8 34.0 32.4 pSSCN u(1) 24.5 34.6 33.1 pSSCN i(1) 24.1 28.8 29.3 SSCN1(2) 24.0 29.6 29.7 SSCN2(2) 23.3 31.5 31.8 SSCN u(2) 24.1 34.5 32.8 SSCN i(2) 23.1 27.8 28.2 pSSCN1(2) 24.9 30.2 30.6 pSSCN2(2) 24.3 34.4 32.8 pSSCN u(2) 25.2 35.4 33.9 pSSCN i(2) 23.9 28.7 29.1 Table 1: Performance of Component Metrics of the individual performance.
527,0,The major improvement happens in the 3rd_comma_ 4th_comma_ 9th_comma_ 14th_comma_ and 30th metrics_comma_ which are METEOR_comma_ SIA_comma_ DUPP a_comma_ pSSCN1(1)_comma_ and PRS.
528,0,Among all the testing metrics including BLEU_comma_ NIST_comma_ METEOR_comma_ ROUGE_comma_ and SIA_comma_ our new metric_comma_ pSSCN u(2)_comma_ based on source-sentence constrained bigrams_comma_ achieves the best adequacy and overall evaluation results_comma_ and the second best result in fluency evaluation.
529,0,Metrics such as ROUGE_comma_ Head Word Chain (HWC)_comma_ METEOR_comma_ and other recently proposed methods all offer different ways of comparing machine and human translations.
530,0,METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately (Banerjee and Lavie_comma_ 2005).
531,0,Our full feature vector consists of r  18 adequacy features_comma_ where r is the number of reference systems used_comma_ and 26 fluency features: Adequacy features: These include features derived from BLEU (e.g. _comma_ n-gram precision_comma_ where 1  n  5_comma_ length ratios)_comma_ PER_comma_ WER_comma_ features derived from METEOR (precision_comma_ recall_comma_ fragmentation)_comma_ and ROUGE-related features (nonconsecutive bigrams with a gap size of g_comma_ where 1  g  5 and longest common subsequence).
532,0,We include two standard reference-based metrics_comma_ BLEU and METEOR_comma_ as baseline comparisons.
533,0,For each pseudo reference configuration_comma_ we consider three metrics: BLEU_comma_ METEOR_comma_ and the regressiontrained metric (using the full feature set).
534,0,When the best MT reference system (MT2) is included as pseudo references_comma_ regressionbased metrics are typically better than or not statistically different from standard applications of BLEU and METEOR with 4 human references.
535,0,BLEU-S(2) METEOR Regr (adq.
536,0,0.538* 0.473* 0.459* 0.247 BLEU-S(2) 0.466 0.419 0.397 0.321* METEOR 0.464 0.418 0.410 0.312 4 MTRefs Regr.
537,0,0.498 0.429 0.421 0.243 BLEU-S(2) 0.386 0.349 0.404 0.240 METEOR 0.445 0.354 0.333 0.243 Best 2 MTRefs Regr.
538,0,0.492 0.418 0.403 0.201 BLEU-S(2) 0.391 0.330 0.394 0.268 METEOR 0.430 0.333 0.327 0.267 Mid 2 MTRefs Regr.
539,0,0.450 0.413 0.388 0.219 BLEU-S(2) 0.362 0.314 0.310 0.282 METEOR 0.391 0.315 0.284 0.274 Worst 2 MTRefs Regr.
540,0,0.430 0.386 0.365 0.158 BLEU-S(2) 0.320 0.298 0.316 0.223 METEOR 0.351 0.306 0.302 0.228 Best MTRef Regr.
541,0,0.461 0.401 0.414 0.122 BLEU-S(2) 0.371 0.330 0.380 0.242 METEOR 0.375 0.318 0.392 0.283 Table 3: Correlation comparisons of metrics by test systems.
542,1,It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision (Banerjee and Lavie_comma_ 2005).
543,0,METEOR is based on the weighted harmonic mean of the precision and recall measured on unigram matches as follows a56a58a57a45a59a60a13 a3a16a15a17a3a18a4a61a19a12a21 a0 a53a34a62 a63a65a64a18a66a68a67a34a63 a4 a28 a0a70a69 a53a27a54a72a71 a13a74a73a61a75 a62 a19a47a76a20a46 (2) where a62 is the total number of unigram matches_comma_ a63a77a64 is the hypothesis length_comma_ a63 a4 is the reference length and a73 is the minimum number of a1 -gram matches that covers the alignment.
544,0,The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to a53a27a54a72a71 when a73 a21 a62 ; i.e._comma_ 313 there are no matching a1 -grams higher than a1 a21 a0 . By default_comma_ METEOR script counts the words that match exactly_comma_ and words that match after a simple Porter stemmer.
545,0,The METEOR scores are also between a53 and a0_comma_ higher being better.
546,0,For BLEU and METEOR_comma_ the loss function would be a0 a69 a6a8a7a10a9 a11a60a13 a3 a38 a15a17a3a41a39 a19 and a0 a69 a56 a57a45a59a65a13 a3 a38 a15a17a3a41a39 a19 . It has been found that multiple hypotheses from each system may be used to improve the quality of 314 the combination output (Sim et al. _comma_ 2007).
547,0,Similarly_comma_ weights which maximize BLEU or METEOR may be optimized.
548,0,The Arabic tuning TER BLEU MTR system A 44.93 45.71 66.09 system B 46.41 43.07 64.79 system C 46.10 46.41 65.33 system D 44.36 46.83 66.91 system E 45.35 45.44 65.69 system F 47.10 44.52 65.28 no weights 42.35 48.91 67.76 baseline 42.19 49.86 68.34 TER tuned 41.88 51.45 68.62 BLEU tuned 42.12 51.72 68.59 MTR tuned 54.08 38.93 71.42 Table 1: Mixed-case TER and BLEU_comma_ and lower-case METEOR scores on Arabic NIST MT03+MT04.
549,0,Arabic test TER BLEU MTR system A 42.98 49.58 69.86 system B 43.79 47.06 68.62 system C 43.92 47.87 66.97 system D 40.75 52.09 71.23 system E 42.19 50.86 70.02 system F 44.30 50.15 69.75 no weights 39.33 53.66 71.61 baseline 39.29 54.51 72.20 TER tuned 39.10 55.30 72.53 BLEU tuned 39.13 55.48 72.81 MTR tuned 51.56 41.73 74.79 Table 2: Mixed-case TER and BLEU_comma_ and lowercase METEOR scores on Arabic NIST MT05.
550,1,However_comma_ the METEOR tuning yields extremely high TER and low BLEU scores.
551,0,This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR 317 Chinese tuning TER BLEU MTR system A 56.56 29.39 54.54 system B 55.88 30.45 54.36 system C 58.35 32.88 56.72 system D 57.09 36.18 57.11 system E 57.69 33.85 58.28 system F 56.11 36.64 58.90 no weights 53.11 37.77 59.19 baseline 53.40 38.52 59.56 TER tuned 52.13 36.87 57.30 BLEU tuned 53.03 39.99 58.97 MTR tuned 70.27 28.60 63.10 Table 3: Mixed-case TER and BLEU_comma_ and lower-case METEOR scores on Chinese NIST MT03+MT04.
552,0,Even though METEOR has been shown to be a good metric on a given MT output_comma_ tuning to optimize METEOR results in a high insertion rate and low precision.
553,0,Also_comma_ the METEOR score using the METEOR optimized weights is very high.
554,0,Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F other combinations are better than any single system on all metrics apart from the METEOR tuned combinations.
555,0,The test set results follow clearly the tuning results again the TER tuned combination is the best in terms of TER_comma_ the BLEU tuned in terms of BLEU_comma_ and the METEOR tuned in Chinese test TER BLEU MTR system A 56.57 29.63 56.63 system B 56.30 29.62 55.61 system C 59.48 31.32 57.71 system D 58.32 33.77 57.92 system E 58.46 32.40 59.75 system F 56.79 35.30 60.82 no weights 53.80 36.17 60.75 baseline 54.34 36.44 61.05 TER tuned 52.90 35.76 58.60 BLEU tuned 54.05 37.91 60.31 MTR tuned 72.59 26.96 64.35 Table 4: Mixed-case TER and BLEU_comma_ and lowercase METEOR scores on Chinese NIST MT05.
556,0,terms of METEOR.
557,0,Again_comma_ the METEOR tuned weights hurt the other metrics significantly.
558,0,The combination weights were tuned to optimize three automatic evaluation metrics: TER_comma_ BLEU and METEOR.
559,0,It also seems like 318 METEOR should not be used in tuning due to high insertion rate and low precision.
560,0,Metrics in the Rouge family allow for skip n-grams (Lin and Och_comma_ 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie_comma_ 2005) and GTM (Melamed et al. _comma_ 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea_comma_ 2006) in that word class information is used.
561,0,They fall into the following major categories1: String-based metrics over references These include the nine Kulesza and Shieber features as well as precision_comma_ recall_comma_ and fragmentation_comma_ as calculated in METEOR; ROUGE-inspired features that are non-consecutive bigrams with a gap size of m_comma_ where 1  m  5 (skip-m-bigram)_comma_ and ROUGE-L (longest common subsequence).
562,0,As a baseline comparison_comma_ we report the correlation rates of three standard automatic metrics: BLEU_comma_ METEOR_comma_ which incorporates recall and stemming_comma_ and HWCM_comma_ which uses syntax.
563,0,As the graph indicates_comma_ even with a limited amount of human assessment data_comma_ regression models can be trained to be comparable to standard metrics (represented by METEOR in the graph).
564,0,The model with a more complex feature set does require more training data_comma_ but its correlation began to overtake METEOR after 2000 training examples.
565,1,The pair of graphs show_comma_ especially in the case of the larger feature set_comma_ that a large improvement in classification accuracy does not bring proportional improvement in its corresponding metricss correlation; with an accuracy of near 90%_comma_ its correlation coefficient is 0.362_comma_ well below METEOR.
566,0,This is expected_comma_ since there are many ways to create bad translations_comma_ so seeing a partic885 R03-all R03-Bottom5 R03-Top5 BLEU METEOR HWCM 2004-Chn1 0.495 0.460 0.518 0.456 0.457 0.444 2004-Chn2 0.398 0.330 0.440 0.352 0.347 0.344 2004-Chn3 0.425 0.389 0.459 0.369 0.402 0.369 2004-Chn4 0.432 0.392 0.434 0.400 0.400 0.362 2004-Chn5 0.452 0.441 0.443 0.370 0.426 0.326 2004-Chn6 0.405 0.392 0.406 0.390 0.357 0.380 2004-Chn7 0.443 0.432 0.448 0.390 0.408 0.392 2004-Chn8 0.237 0.256 0.256 0.265 0.259 0.179 2004-Chn9 0.581 0.569 0.591 0.527 0.537 0.535 2004-Chn10 0.314 0.313 0.354 0.321 0.303 0.358 2004-all 0.602 0.567 0.617 0.588 0.563 0.546 Table 2: Metric correlations within each system.
567,0,Comparing the LFG-based evaluation method with other popular metrics: BLEU_comma_ NIST_comma_ General Text Matcher (GTM) (Turian et al. _comma_ 2003)_comma_ Translation Error Rate (TER) (Snover et al. _comma_ 2006)1_comma_ and METEOR (Banerjee and Lavie_comma_ 2005)_comma_ we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment.
568,0,Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference_comma_ like CDER (Leusch et al. _comma_ 2006)_comma_ which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie_comma_ 2005)_comma_ which uses stemming and WordNet synonymy.
569,0,We evaluated the dependency triples obtained from the translation against the dependency triples for the reference_comma_ calculating the f-score_comma_ and applied other metrics (TER_comma_ METEOR_comma_ BLEU_comma_ NIST_comma_ and GTM) to the set in order to compare scores.
570,0,baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1.
571,0,baseline modified TER 0.0 7.841 METEOR 1.0 0.9956 BLEU 1.0000 0.8485 NIST 11.1690 10.7422 (96.18%) GTM 100 99.35 dep f-score 100 100 dep_preds f-score 100 100 Table 2.
572,0,The translations were scored with a range of metrics: BLEU_comma_ NIST_comma_ GTM_comma_ TER_comma_ METEOR_comma_ and the dependency-based method.
573,0,In addition_comma_ METEOR also has an option of including WordNet in the evaluation process.
574,0,As can be seen_comma_ all the metrics scored Pharaoh higher_comma_ inlcuding METEOR and the dependency-based method that were boosted with WordNet.
575,1,Interestingly_comma_ next to METEOR boosted with WordNet_comma_ it is the dependency-based method_comma_ and especially the predicates-only version_comma_ that shows the least bias towards the phrase-based translation.
576,0,metric PH score  LM score TER 1.997 BLEU 7.16% NIST 6.58% dep 4.93% dep+paraphr 4.80% GTM 3.89% METEOR 3.80% dep_preds 3.79% dep+paraphr_preds 3.70% dep+WordNet 3.55% dep+WordNet_preds 2.60% METEOR+WordNet 1.56% Table 3.
577,1,Similarly to METEOR_comma_ the dependency-based method shows on the whole lower bias than other metrics.
578,0,As in the previous experiment_comma_ the translation was scored using BLEU_comma_ NIST_comma_ GTM_comma_ TER_comma_ METEOR_comma_ and the dependency-based method.
579,1,As to the correlation with human evaluation of translation accuracy_comma_ our method currently falls short of METEOR and even NIST.
580,0,This is caused by the fact that both METEOR and NIST assign relatively little importance to the position of a specific word in a sentence_comma_ therefore rewarding the translation for content rather than linguistic form.
581,1,The WordNet-boosted dependencybased method scores only slightly lower than METEOR with WordNet.
582,0,Note that using stems and their synonyms as used in METEOR (Banerjee and Lavie_comma_ 2005) could also be considered for word similarity.
583,0,A new automatic metric METEOR (Banerjee and Lavie_comma_ 2005) uses stems and synonyms of the words.
584,0,This measure counts the number of exact word matches between the output and the reference.
585,1,Nevertheless_comma_ none of these measures or extensions takes into account linguistic knowledge about actual translation errors_comma_ for example what is the contribution of verbs in the overall error rate_comma_ how many full forms are wrong whereas their base forms are correct_comma_ etc. A framework for human error analysis has been proposed in (Vilar et al. _comma_ 2006) and a detailed analysis of the obtained results has been carried out.
586,0,METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgements.
587,1,In an experiment on 16_comma_800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project_comma_ we compare the LFG-based evaluation method with other popular metrics like BLEU_comma_ NIST_comma_ General Text Matcher (GTM) (Turian et al. _comma_ 2003)_comma_ Translation Error Rate (TER) (Snover et al. _comma_ 2006)1_comma_ and METEOR (Banerjee and Lavie_comma_ 2005)_comma_ and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment.
588,0,Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference_comma_ like CDER (Leusch et al. _comma_ 2006)_comma_ which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie_comma_ 2005)_comma_ which uses stemming and WordNet synonymy.
589,0,Additionally_comma_ the same ?translationreference??set was scored with other metrics (TER_comma_ METEOR_comma_ BLEU_comma_ NIST_comma_ and GTM).
590,0,baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1.
591,0,As in the previous experiment_comma_ the translation was scored using BLEU_comma_ NIST_comma_ GTM_comma_ TER_comma_ METEOR_comma_ and our labelled dependencybased method.
592,0,Lastly_comma_ we added WordNet synonyms into the matching process to accommodate lexical variation_comma_ and to compare our WordNet-enhanced method with the WordNet-enhanced version of METEOR.
593,1,As to the correlation with human evaluation of translation accuracy_comma_ our method currently falls 108 short of METEOR.
594,0,This is caused by the fact that METEOR assign relatively little importance to the position of a specific word in a sentence_comma_ therefore rewarding the translation for content rather than linguistic form.
595,0,Interestingly_comma_ while METEOR_comma_ with or without WordNet_comma_ considerably outperforms all other metrics when it comes to the correlation with human judgements of translation accuracy_comma_ it falls well behind most versions of our dependency-based method in correlation with human scores of translation fluency.
596,0,This pattern is very noticeable in Table 3: if a metric is (relatively) good at correlating with fluency_comma_ its accuracy correlation suffers (GTM might serve as an example here)_comma_ and the opposite holds as well (see METEOR?s scores).
597,0,Legend: d = dependency f-score_comma_ _pr = predicate-only f-score_comma_ 2_comma_ 10_comma_ 50 = n-best parses; var = partial-match version; M = METEOR_comma_ WN = WordNet6 improves_comma_ the community will need metrics that are more sensitive in this respect.
598,0,Och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation; BLEU (Papineni et al. _comma_ 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used_comma_ e.g._comma_ (Banerjee and Lavie_comma_ 2005; Snover et al. _comma_ 2006).
599,0,5We plan to include METEOR scores in future experiments.
600,0,Other work has incorporated paraphrases into MT evaluation: Russo-Lassner et al.(2005) use a combination of paraphrase-based features to evaluate translation output; Zhou et al.(2006) propose a new metric that extends n-gram matching to include synonyms and paraphrases; and Lavie?s METEOR metric (Banerjee and Lavie_comma_ 2005) can be used with additionalknowledgesuchasWordNetinordertosupport inexact lexical matches.
601,0,They are: ??Meteor (Banerjee and Lavie_comma_ 2005)?Meteor measures precision and recall of unigrams when comparing a hypothesis translation 142 Language Pair Test Set Adequacy Fluency Rank Constituent English-German Europarl 1_comma_416 1_comma_418 1_comma_419 2_comma_626 News Commentary 1_comma_412 1_comma_413 1_comma_412 2_comma_755 German-English Europarl 1_comma_525 1_comma_521 1_comma_514 2_comma_999 News Commentary 1_comma_626 1_comma_620 1_comma_601 3_comma_084 English-Spanish Europarl 1_comma_000 1_comma_003 1_comma_064 1_comma_001 News Commentary 1_comma_272 1_comma_272 1_comma_238 1_comma_595 Spanish-English Europarl 1_comma_174 1_comma_175 1_comma_224 1_comma_898 News Commentary 947 949 922 1_comma_339 English-French Europarl 773 772 769 1_comma_456 News Commentary 729 735 728 1_comma_313 French-English Europarl 834 833 830 1_comma_641 News Commentary 1_comma_041 1_comma_045 1_comma_035 2_comma_036 English-Czech News Commentary 2_comma_303 2_comma_304 2_comma_331 3_comma_968 Czech-English News Commentary 1_comma_711 1_comma_711 1_comma_733 0 Totals 17_comma_763 17_comma_771 17_comma_820 27_comma_711 Table 2: The number of items that were judged for each task during the manual evaluation against a reference.
602,1,It flexibly matches words using stemming and WordNet synonyms.
603,0,Its flexible matching was extended to French_comma_ Spanish_comma_ German and Czech for this workshop (Lavie and Agarwal_comma_ 2007).
604,0,??Maximumcorrelationtrainingonadequacyand on fluency (Liu and Gildea_comma_ 2007)?a linear combination of different evaluation metrics (Bleu_comma_ Meteor_comma_ Rouge_comma_ WER_comma_ and stochastic iterative alignment) with weights set to maximize Pearson?s correlation with adequacy and fluency judgments.
605,0,While these are based on a relatively few number of items_comma_ and while we have not performed any tests to determine whether the differences in ? are statistically significant_comma_ the results 7The Czech-English conditions were excluded since there were so few systems 146 are nevertheless interesting_comma_ since three metrics have higher correlation than Bleu: ??Semantic role overlap (Gimenez and M`arquez_comma_ 2007)_comma_ which makes its debut in the proceedings of this workshop ??ParaEval measuring recall (Zhou et al. _comma_ 2006)_comma_ which has a model of allowable variation in translation that uses automatically generated paraphrases (Callison-Burch_comma_ 2007) ??Meteor (Banerjee and Lavie_comma_ 2005) which also allows variation by introducing synonyms and by flexibly matches words using stemming.
606,0,They were: semantic role overlap (newly introduced in this workshop) ParaEval-recall and Meteor.
607,0,metric AD EQ UA CY FL UE NC Y RA NK CO NS TI TU EN TO VE RA LL Semantic role overlap .774 .839 .803 .741 .789 ParaEvalRecall .712 .742 .768 .798 .755 Meteor .701 .719 .745 .669 .709 Bleu .690 .722 .672 .602 .671 1-TER .607 .538 .520 .514 .644 Max adequcorrelation .651 .657 .659 .534 .626 Max fluency correlation .644 .653 .656 .512 .616 GTM .655 .674 .616 .495 .610 Dependency overlap .639 .644 .601 .512 .599 ParaEvalPrecision .639 .654 .610 .491 .598 1-WER of verbs .378 .422 .431 .297 .382 Table 7: Average corrections for the different automatic metrics when they are used to evaluate translations into English metric AD EQ UA CY FL UE NC Y RA NK CO NS TI TU EN TO VE RA LL Bleu .657 .445 .352 .409 .466 1-TER .589 .419 .361 .380 .437 Max fluency correlation .534 .419 .368 .400 .430 Max adequcorrelation .498 .414 .385 .409 .426 Meteor .490 .356 .279 .304 .357 1-WER of verbs .371 .304 .359 .359 .348 Table 8: Average corrections for the different automatic metrics when they are used to evaluate translations into the other languages 147 This year?s evaluation also measured the agreement between human assessors by computing the Kappa coefficient.
608,1,We might find better suited metrics_comma_ such as METEOR (Banerjee and Lavie_comma_ 2005)_comma_ which is oriented towards word selection8.
609,1,c2007 Association for Computational Linguistics Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Alon Lavie and Abhaya Agarwal Language Technologies Institute Carnegie Mellon University Pittsburgh_comma_ PA_comma_ 15213_comma_ USA {alavie_comma_abhayaa}@cs.cmu.edu Abstract Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality_comma_ significantly outperforming the more commonly used Bleu metric.
610,0,It is one of several automatic metrics used in this year?s shared task within the ACL WMT-07 workshop.
611,0,This paper recaps the technical details underlying the metric and describes recent improvements in the metric.
612,1,The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish_comma_ French and German_comma_ in addition to English.
613,0,Meteor_comma_ as well as several other proposed metrics such as GTM (Melamed et al. _comma_ 2003)_comma_ TER (Snover et al. _comma_ 2006) and CDER (Leusch et al. _comma_ 2006) aim to address some of these weaknesses.
614,0,Meteor_comma_ initially proposed and released in 2004 (Lavie et al. _comma_ 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level.
615,0,Previous publications on Meteor (Lavie et al. _comma_ 2004; Banerjee and Lavie_comma_ 2005) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics.
616,0,This paper recaps the technical details underlying Meteor and describes recent improvements in the metric.
617,0,The latest release extends Meteor to support evaluation of MT output in Spanish_comma_ French and German_comma_ in addition to English.
618,0,2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation.
619,0,Given a pair of strings to be compared_comma_ Meteor creates a word alignment between the two strings.
620,0,Once a final alignment has been produced between the system translation and the reference translation_comma_ the Meteor score for this pairing is computed as follows.
621,0,To take into account the extent to which the matched unigrams in the two strings are in the same word order_comma_ Meteorcomputes a penalty for a given alignment as follows.
622,0,Finally_comma_ the Meteor score for the alignment between the two strings is calculated as: score = (1??Pen) Fmean In all previous versions of Meteor_comma_ the values of the three parameters mentioned above were set to be:  = 0.9_comma_  = 3.0 and  = 0.5_comma_ based on experimentation performed in early 2004.
623,0,3 Meteor Implementations for Spanish_comma_ French and German We have recently expanded the implementation of Meteor to support evaluation of translations in Spanish_comma_ French and German_comma_ in addition to English.
624,0,The word-matching component within the English version of Meteor uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference.
625,0,In order to construct instances of Meteor for Spanish_comma_ French and German_comma_ we created new languagespecific ?stemming??modules.
626,0,Meteor versions for Spanish_comma_ French and German therefore currently include only ?exact??
627,0,The second main language-specific issue which required adaptation is the tuning of the three parameters within Meteor_comma_ described in section 4.
628,0,4 Optimizing Metric Parameters The original version of Meteor (Banerjee and Lavie_comma_ 2005) has instantiated values for three parameters in the metric: one for controlling the relative weight of precision and recall in computing the Fmean score (); one governing the shape of the penalty as a function of fragmentation () and one for the relative weight assigned to the fragmentation penalty ().
629,0,In all versions of Meteor to date_comma_ these parameters were instantiated with the values  = 0.9_comma_  = 3.0 and  = 0.5_comma_ based on early data experimentation.
630,0,Parameter adapta229 Corpus Judgments Systems NIST 2003 Ara-to-Eng 3978 6 NIST 2004 Ara-to-Eng 347 5 WMT-06 Eng-to-Fre 729 4 WMT-06 Eng-to-Ger 756 5 WMT-06 Eng-to-Spa 1201 7 Table 1: Corpus Statistics for Various Languages tion is also an issue in the newly created Meteor instances for other languages.
631,0,5 Conclusions In this paper we described newly developed language-specific instances of the Meteor metric and the process of optimizing metric parameters for different human measures of translation quality and for different languages.
632,0,We are currently exploring broadening the set of features used in Meteor to include syntax-based features and alternative notions of synonymy.
633,0,The latest release of Meteor is freely available on our website at: http://www.cs.cmu.edu/~alavie/METEOR/ Acknowledgements The work reported in this paper was supported by NSF Grant IIS-0534932.
634,0,al. 2006)_comma_ we are interested in applying alternative metrics such a Meteor (Banerjee and Lavie 2005).
635,0,METEOR We run all modules: ?exact??
636,0,in that order (Banerjee and Lavie_comma_ 2005).
637,0,Let us note that ROUGE and METEOR may consider stemming (i.e. _comma_ morphological variations).
638,0,Additionally_comma_ METEOR may perform a lookup for synonyms in WordNet (Fellbaum_comma_ 1998).
639,0,fr2en de2en Level Metric in out in out 1-PER 0.73 0.64 0.57 0.46 1-WER 0.73 0.73 0.32 0.38 BLEU 0.71 0.87 0.60 0.67 Lexical NIST 0.74 0.82 0.56 0.63 GTM 0.84 0.86 0.12 0.70 METEOR 0.92 0.95 0.76 0.81 ROUGE 0.85 0.89 0.65 0.79 SP-Op-* 0.81 0.88 0.64 0.71 SP-Oc-* 0.81 0.89 0.65 0.75 Shallow SP-NISTl-5 0.75 0.81 0.56 0.64 Syntactic SP-NISTp-5 0.75 0.91 0.77 0.77 SP-NISTc-5 0.73 0.88 0.71 0.54 DP-HWCw-4 0.76 0.88 0.64 0.74 DP-HWCc-4 0.93 0.97 0.88 0.72 DP-HWCr-4 0.92 0.96 0.91 0.76 Syntactic DP-Ol-* 0.87 0.94 0.84 0.84 DP-Oc-* 0.91 0.95 0.88 0.87 DP-Or-* 0.87 0.97 0.91 0.88 CP-STM-9 0.93 0.95 0.93 0.87 NE-Me-* 0.80 0.79 0.93 0.63 NE-Oe-* 0.79 0.76 0.91 0.59 NE-Oe-** 0.81 0.87 0.63 0.70 SR-Mr-* 0.83 0.95 0.92 0.84 Shallow SR-Or-* 0.89 0.95 0.88 0.90 Semantic SR-Or 0.95 0.85 0.80 0.75 SR-Mrv-* 0.77 0.92 0.72 0.85 SR-Orv-* 0.81 0.93 0.76 0.94 SR-Orv 0.84 0.93 0.81 0.92 Table 4: WMT 2006.
640,0,For instance_comma_ BLEU and_comma_ in general_comma_ all metrics based on lexical matching alone_comma_ except METEOR_comma_ obtain significantly lower levels of correlation than metrics based on deeper linguistic similarities.
641,0,The problem with lexical metrics is that they are unable to capture the actual quality of the ?Systran??system.
642,1,Interestingly_comma_ METEOR obtains a higher correlation_comma_ which_comma_ in the case of French-to-English_comma_ rivals the top-scoring metrics based on deeper linguistic features.
643,0,The reason_comma_ however_comma_ does not seem to be related to its additional linguistic operations (i.e. _comma_ stemming or synonymy lookup)_comma_ but rather to the METEOR matching strategy itself (unigram precision/recall).
644,0,es2en Level Metric in out 1-PER 0.82 0.78 1-WER 0.88 0.83 BLEU 0.89 0.87 Lexical NIST 0.88 0.84 GTM 0.86 0.80 METEOR 0.84 0.81 ROUGE 0.89 0.83 SP-Op-* 0.88 0.80 SP-Oc-* 0.89 0.84 Shallow SP-NISTl-5 0.88 0.85 Syntactic SP-NISTp-5 0.85 0.86 SP-NISTc-5 0.84 0.83 DP-HWCw-4 0.94 0.83 DP-HWCc-4 0.91 0.87 DP-HWCr-4 0.91 0.88 Syntactic DP-Ol-* 0.91 0.84 DP-Oc-* 0.88 0.83 DP-Or-* 0.88 0.84 CP-STM-9 0.89 0.86 NE-Me-* 0.75 0.76 NE-Oe-* 0.71 0.71 NE-Oe-** 0.88 0.80 SR-Mr-* 0.86 0.82 Shallow SR-Or-* 0.92 0.92 Semantic SR-Or 0.91 0.92 SR-Mrv-* 0.89 0.88 SR-Orv-* 0.91 0.92 SR-Orv 0.91 0.91 Table 5: WMT 2006.
645,0,On the other hand_comma_ if we remove ?LinearB??(see 262 ar2en Level Metric ALL SMT 1-PER -0.35 0.75 1-WER -0.50 0.69 BLEU 0.06 0.83 Lexical NIST 0.04 0.81 GTM 0.03 0.92 ROUGE -0.17 0.81 METEOR 0.05 0.86 SP-Op-* 0.05 0.84 SP-Oc-* 0.12 0.89 Shallow SP-NISTl-5 0.04 0.82 Syntactic SP-NISTp-5 0.42 0.89 SP-NISTc-5 0.44 0.68 DP-HWCw-4 0.52 0.86 DP-HWCc-4 0.80 0.75 DP-HWCr-4 0.88 0.86 Syntactic DP-Ol-* 0.51 0.94 DP-Oc-* 0.53 0.91 DP-Or-* 0.72 0.93 CP-STM-9 0.74 0.95 NE-Me-* 0.33 0.78 NE-Oe-* 0.24 0.82 NE-Oe-** 0.04 0.81 SR-Mr-* 0.72 0.96 Shallow SR-Or-* 0.61 0.87 Semantic SR-Or 0.66 0.75 SR-Mrv-* 0.68 0.97 SR-Orv-* 0.47 0.84 SR-Orv 0.46 0.81 Table 6: NIST 2005.
646,0,LC scores are obtained by matching the exact form of the words as METEOR(exact) does.
647,0,NIST+LC combination score is better than METEOR(exact) at sentence and document level_comma_ and also better than METEOR(exact&syn) (syn means wn_synonymy module in METEOR) at document level.
648,0,Mean Correlation BLEU 4 0.245 NIST 5 0.307 GTM (e=2) 0.251 METEOR(exact) 0.306 METEOR(exact&syn) 0.327 DP 0.246 LC 0.263 BLEU+DP 0.270 BLEU+ LC 0.288 BLEU+ DP +LC 0.307 NIST+ LC 0.322 NIST+ DP +LC 0.333  Table11: Sentence level ranking (DP means dependency and LC means linguistic categories)   Mean Correlation BLEU 4 0.305 NIST 5 0.373 GTM (e=2) 0.327 METEOR(exact) 0.363 METEOR(exact&syn) 0.394 DP 0.323 LC 0.369 BLEU+DP 0.325 BLEU+ LC 0.387 BLEU+ DP +LC 0.332 NIST+ LC 0.409 NIST+ DP +LC 0.359 Table 12: Document level ranking 8 Comparison with Related Work This work is inspired by (Yu_comma_ 1993) with many extensions.
649,0,There are many recent work motivated by ngram based approach.
650,0,(Banerjee and Lavie_comma_ 2005) calculated the scores by matching the unigrams on the surface forms_comma_ stemmed forms and senses.
651,0,There are many differences between these ngram based methods and our approach.
652,0,In ngram approach_comma_ a sentence is viewed as a collection of n-grams with different length without differentiating the specific linguistic phenomena.
653,0,Furthermore_comma_ in n-gram approach_comma_ only one general score at the system level is provided which make it not suitable for system diagnoses_comma_ while in our approach we can give scores of linguistic categories and provide much richer information to help developers to find the concrete strength and flaws of the system_comma_ in addition to the general score.
654,0,The n-gram based metric is not very effective when comparing the systems with different architectures or systems with similar general score_comma_ while our approach is more effective in both cases by digging into the multiple linguistic levels and disclosing the latent differences of the systems.
655,0,Table 2 shows that the total entropy of European Table 3: Automatic Evaluation Results BLEU (%) METEOR (%) en-* *-en ja-* *-ja en-* *-en ja-* *-ja ar 18.21 51.01 13.03 46.09 40.90 69.01 37.52 58.02 da 59.70 70.90 45.94 55.34 75.08 82.56 64.41 65.83 de 56.48 69.25 41.99 59.20 74.01 81.48 63.69 69.61 en   61.56 68.53   78.19 75.39 es 65.22 73.82 51.77 63.24 78.15 85.28 68.30 72.17 fr 64.69 71.04 52.36 63.16 79.28 83.05 71.14 72.82 id 48.35 59.69 40.59 57.24 66.82 75.83 62.33 69.00 it 56.80 70.43 43.45 60.77 72.41 82.96 62.35 70.70 ja 68.53 61.56   75.39 78.19 ko 37.00 58.82 69.96 85.10 57.89 75.92 83.25 89.73 ms 40.99 57.63 36.13 55.84 61.08 74.75 58.73 67.33 nl 57.46 72.85 41.43 59.70 75.88 84.52 63.42 72.19 pt-b 59.99 69.41 46.50 58.07 72.77 80.70 64.68 69.14 pt 62.81 70.25 48.24 59.20 75.65 83.32 67.38 68.32 ru 44.46 61.23 36.08 55.13 66.41 73.75 60.59 64.55 th 46.49 51.35 43.75 50.85 62.47 73.12 60.25 62.91 vi 55.18 57.42 50.86 55.07 71.04 73.98 68.67 70.81 zh 53.08 59.33 51.68 69.43 69.85 74.68 65.88 77.62 languages like Danish_comma_ German_comma_ English_comma_ Spanish_comma_ etc. does not differ much.
656,0,This is con rmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al._comma_ 2002) and METEOR (Banerjee and Lavie_comma_ 2005) where scores range between 0 (worst) and 1 (best).
657,0,1 Introduction B (Papineni et al._comma_ 2002) was one of the first automatic evaluation metrics for machine translation (MT)_comma_ and despite being challenged by a number of alternative metrics (Melamed et al._comma_ 2003; Banerjee and Lavie_comma_ 2005; Snover et al._comma_ 2006; Chan and Ng_comma_ 2008)_comma_ it remains the standard in the statistical MTliterature.Callison-Burchetal.(2006)havesubjected B to a searching criticism_comma_ with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments.Bothcasesinvolvecomparisonsbetween statistical MT systems and other translation methods (human post-editing and a rule-based MT system)_comma_ and they recommend that the use of B be restrictedtocomparisonsbetweenrelatedsystemsor different versions of the same systems.
658,0,In none of these cases did we repeat minimum-error-rate training; all these systems were trained using max-B. The metrics we tested were:  METEOR (Banerjee and Lavie_comma_ 2005)_comma_ version 0.6_comma_usingtheexact_comma_Porter-stemmer_comma_andWordNet synonmy stages_comma_ and the optimized parameters  = 0.81_comma_  = 0.83_comma_  = 0.28 as reported in (Lavie and Agarwal_comma_ 2007).
659,0,The 4-GRR can also be formulated as a finitestate automaton_comma_ with states {(i_comma_m) | 0  i  |r|_comma_0  m  3}_comma_ initial state (0_comma_0)_comma_ final states (|r|_comma_m)_comma_ and the following transitions: For 0  i < |r|_comma_ 0  m  3: (i_comma_m) ri+1:m+1 (i+1_comma_min{m+1_comma_3}) match (i_comma_m) epsilon1: (i+1_comma_0) deletion (i_comma_m) star:0 (i+1_comma_0) substitution 615 Metric Adq Flu Rank Con Avg Sem.roleoverlap 77.4 83.9 80.3 74.1 78.9 ParaEvalrecall 71.2 74.2 76.8 79.8 75.5 METEOR 70.1 71.9 74.5 66.9 70.9 B 68.9 72.1 67.2 60.2 67.1 WER 51.0 54.2 34.5 52.4 48.0 B- 73.9 76.7 73.5 63.4 71.9 4-GRR 72.3 75.5 74.3 64.2 71.6 Table 1: Our new metrics correlate with human judgmentsbetterthanB (case-sensitive).Adq=Adequacy_comma_ Flu = Fluency_comma_ Con = Constituent_comma_ Avg = Average.
660,0,Table 1 shows theresultsalongwithB andthethreemetricsthat achieved higher correlations than B: semantic role overlap (Gimenez and Marquez_comma_ 2007)_comma_ ParaEval recall (Zhou et al._comma_ 2006)_comma_ and METEOR (Banerjee and Lavie_comma_ 2005).
661,1,Other well-known metrics are WER (Nieen et al._comma_ 2000)_comma_ NIST (Doddington_comma_ 2002)_comma_ GTM (Melamed et al._comma_ 2003)_comma_ ROUGE (Lin and Och_comma_ 2004a)_comma_ METEOR (Banerjee and Lavie_comma_ 2005)_comma_ and TER (Snover et al._comma_ 2006)_comma_ just to name a few.
662,0,All these metrics take into account information at the lexical level1_comma_ and_comma_ therefore_comma_ their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann_comma_ 2003).
663,0,However_comma_ none of current metrics provides_comma_ in isolation_comma_ a global measure of quality.
664,0,Indeed_comma_ all metrics focus on partial aspects of quality.
665,0,The main problem of relying on partial metrics is that we may obtain biased evaluations_comma_ which may lead us to derive inaccurate conclusions.
666,0,Corroborating the findings by Culy and Riehemann (2003)_comma_ they showed that BLEU overrates SMT systems with respect to other types of systems_comma_ such 1ROUGE and METEOR may consider morphological variations.
667,0,METEOR may also look up for synonyms in WordNet.
668,0,322 KING Rsnt Level Metric AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05 1-WER 0.70 0.51 0.48 0.61 0.53 0.47 0.38 0.47 1-PER 0.64 0.43 0.45 0.58 0.50 0.51 0.29 0.40 1-TER 0.73 0.54 0.53 0.66 0.54 0.50 0.38 0.49 BLEU 0.70 0.49 0.52 0.59 0.50 0.46 0.36 0.39 NIST 0.74 0.53 0.55 0.68 0.53 0.55 0.37 0.46 Lexical GTM.e1 0.67 0.49 0.48 0.61 0.41 0.50 0.26 0.29 GTM.e2 0.69 0.52 0.51 0.64 0.49 0.54 0.43 0.48 ROUGEL 0.73 0.59 0.49 0.65 0.58 0.60 0.41 0.52 ROUGEW 0.75 0.62 0.54 0.68 0.59 0.57 0.48 0.54 METEORwnsyn 0.75 0.56 0.57 0.69 0.56 0.56 0.35 0.41 SP-Op-* 0.66 0.48 0.49 0.59 0.51 0.57 0.38 0.41 SP-Oc-* 0.65 0.44 0.46 0.59 0.55 0.58 0.42 0.41 Shallow SP-NISTl 0.73 0.51 0.55 0.66 0.53 0.54 0.38 0.44 Syntactic SP-NISTp 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39 SP-NISTiob 0.69 0.48 0.49 0.59 0.32 0.36 0.27 0.26 SP-NISTc 0.60 0.42 0.39 0.52 0.26 0.27 0.16 0.16 DP-HWCw 0.58 0.40 0.42 0.53 0.41 0.08 0.35 0.40 DP-HWCc 0.50 0.32 0.33 0.41 0.41 0.17 0.38 0.32 DP-HWCr 0.56 0.40 0.37 0.46 0.42 0.16 0.39 0.43 DP-Ol-* 0.58 0.48 0.41 0.52 0.52 0.48 0.36 0.37 Syntactic DP-Oc-* 0.65 0.45 0.44 0.55 0.49 0.51 0.43 0.41 DP-Or-* 0.71 0.57 0.54 0.64 0.55 0.55 0.50 0.50 CP-Op-* 0.67 0.47 0.47 0.60 0.53 0.57 0.38 0.46 CP-Oc-* 0.66 0.51 0.49 0.62 0.57 0.59 0.45 0.50 CP-STM 0.64 0.42 0.43 0.58 0.39 0.13 0.34 0.30 NE-Oe-** 0.65 0.45 0.46 0.57 0.47 0.56 0.32 0.39 Shallow SR-Or-* 0.48 0.22 0.34 0.41 0.28 0.10 0.32 0.21 Semantic SR-Orv 0.36 0.13 0.24 0.27 0.27 0.12 0.25 0.24 DR-Or-* 0.62 0.47 0.50 0.55 0.47 0.46 0.43 0.37 Semantic DR-Orp-* 0.58 0.42 0.43 0.50 0.37 0.35 0.36 0.26 Optimal Combination 0.79 0.64 0.61 0.70 0.64 0.63 0.54 0.61 Table 2: Metric Meta-evaluation and human acceptability (Rsnt)_comma_ computed over the subsets of sentences for which human assessments are available.
669,0,323 Opt.K(AE.04) = {SP-NISTp} Opt.K(CE.04) = {ROUGEW_comma_SP-NISTp_comma_ ROUGEL} Opt.K(AE.05) = {METEORwnsyn_comma_SP-NISTp_comma_ DP-Or-*} Opt.K(CE.05) = {SP-NISTp} Opt.R(AE.04) = {ROUGEW_comma_ROUGEL_comma_CP-Oc-*_comma_METEORwnsyn_comma_DP-Or-*_comma_DP-Ol-*_comma_GTM.e2_comma_DR-Or-*_comma_CP-STM} Opt.R(CE.04) = {ROUGEL_comma_CP-Oc-*_comma_ROUGEW_comma_SP-Op-*_comma_METEORwnsyn_comma_DP-Or-*_comma_GTM.e2_comma_1-WER_comma_DR-Or-*} Opt.R(AE.05) = {DP-Or-*_comma_ROUGEW} Opt.R(CE.05) = {ROUGEW_comma_ROUGEL_comma_DP-Or-*_comma_CP-Oc-*_comma_1-TER_comma_GTM.e2_comma_DP-HWCr_comma_CP-STM} Table 3: Optimal metric sets 4.3 Finding Optimal Metric Combinations In that respect_comma_ we study the applicability of the two combination strategies presented.
670,1,The results show that_comma_ as compared to BLEU_comma_ several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez_comma_ 2007)_comma_ ParaEval-recall (Zhou et al._comma_ 2006)_comma_ and METEOR (Banerjee and Lavie_comma_ 2005) achieve higher correlation.
671,0,2.4 METEOR Given a pair of strings to compare (a system translation and a reference translation)_comma_ METEOR (Banerjee and Lavie_comma_ 2005) first creates a word alignment between the two strings.
672,0,Based on the number of word or unigram matches and the amount of string fragmentation represented by the alignment_comma_ METEOR calculates a score for the pair of strings.
673,0,Once the final alignment has been produced_comma_ unigram precision P (number of unigram matches m divided by the total number of system unigrams) and unigram recall R (m divided by the total number of reference unigrams) are calculated and combined into a single parameterized harmonic mean (Rijsbergen_comma_ 1979): Fmean = P  RP + (1  )R (1) To account for longer matches and the amount of fragmentation represented by the alignment_comma_ METEOR groups the matched unigrams into as few chunks as possible and imposes a penalty based on the number of chunks.
674,0,The METEOR score for a pair of sentences is: score = bracketleftBigg 1   parenleftbiggno.
675,0,Note that METEOR consists of three parameters that need to be optimized based on experimentation: _comma_ _comma_ and .
676,0,To address this_comma_ standard measures like precision and recall could be used_comma_ as in some previous research (Banerjee and Lavie_comma_ 2005; Melamed et al._comma_ 2003).
677,0,59 Metric Adequacy Fluency Rank Constituent Average MAXSIMn+d 0.780 0.827 0.875 0.760 0.811 MAXSIMn 0.804 0.845 0.893 0.766 0.827 Semantic-role 0.774 0.839 0.804 0.742 0.790 ParaEval-recall 0.712 0.742 0.769 0.798 0.755 METEOR 0.701 0.719 0.746 0.670 0.709 BLEU 0.690 0.722 0.672 0.603 0.672 Table 1: Overall correlations on the Europarl and News Commentary datasets.
678,0,5 Results To evaluate our metric_comma_ we conduct experiments on datasets from the ACL-07 MT workshop and NIST 4Available at: http://sourceforge.net/projects/mstparser Europarl Metric Adq Flu Rank Con Avg MAXSIMn+d 0.749 0.786 0.857 0.651 0.761 MAXSIMn 0.749 0.786 0.857 0.651 0.761 Semantic-role 0.815 0.854 0.759 0.612 0.760 ParaEval-recall 0.701 0.708 0.737 0.772 0.730 METEOR 0.726 0.741 0.770 0.558 0.699 BLEU 0.803 0.822 0.699 0.512 0.709 Table 2: Correlations on the Europarl dataset.
679,0,News Commentary Metric Adq Flu Rank Con Avg MAXSIMn+d 0.812 0.869 0.893 0.869 0.861 MAXSIMn 0.860 0.905 0.929 0.881 0.894 Semantic-role 0.734 0.824 0.848 0.871 0.819 ParaEval-recall 0.722 0.777 0.800 0.824 0.781 METEOR 0.677 0.698 0.721 0.782 0.720 BLEU 0.577 0.622 0.646 0.693 0.635 Table 3: Correlations on the News Commentary dataset.
680,1,During the workshop_comma_ only three automatic metrics (Semantic-role overlap_comma_ ParaEval-recall_comma_ and METEOR) achieve higher correlation than BLEU.
681,0,In all our results for MAXSIM in this paper_comma_ we follow METEOR and use =0.9 (weighing recall more than precision) in our calculation of Fmean via Equation 1_comma_ unless otherwise stated.
682,0,Note that these results are substantially Metric Adq Flu Avg MAXSIMn+d 0.943 0.886 0.915 MAXSIMn 0.829 0.771 0.800 METEOR (optimized) 1.000 0.943 0.972 METEOR 0.943 0.886 0.915 BLEU 0.657 0.543 0.600 Table 4: Correlations on the NIST MT 2003 dataset.
683,0,Since implementations of the BLEU and METEOR metrics are publicly available_comma_ we score the system submissions using BLEU (version 11b with its default settings)_comma_ METEOR_comma_ and MAXSIM_comma_ showing the resulting correlations in Table 4.
684,0,For METEOR_comma_ when used with its originally proposed parameter values of (=0.9_comma_ =3.0_comma_ =0.5)_comma_ which the METEOR researchers mentioned were based on some early experimental work (Banerjee and Lavie_comma_ 2005)_comma_ we obtain an average correlation value of 0.915_comma_ as shown in the row METEOR.
685,0,When METEOR was run with these new parameter values_comma_ it returned an average correlation value of 61 0.972_comma_ as shown in the row METEOR (optimized).
686,0,Under this setting_comma_ the correlation achieved by MAXSIM is comparable to that achieved by METEOR.
687,0,We measure translation performance by the BLEU (Papineni et al._comma_ 2002) and METEOR (Banerjee and Lavie_comma_ 2005) scores with multiple translation references.
688,0,85 BLEU Scores Table 04dev 04test 05test 06dev 06test HMM 0.367 0.407 0.473 0.200 0.190 Model-4 0.380 0.403 0.485 0.210 0.204 New 0.411 0.427 0.500 0.216 0.208 METEOR Scores Table 04dev 04test 05test 06dev 06test HMM 0.532 0.586 0.675 0.482 0.471 Model-4 0.540 0.593 0.682 0.492 0.480 New 0.568 0.614 0.691 0.505 0.487 Table 3: Translation Results 4.2 Translation Results Our baseline phrase table training method is the ViterbiExtract algorithm.
689,0,The translation results as measured by BLEU and METEORscoresarepresentedinTable3.
690,0,Wenotice that Model-4 based phrase table performs roughly 1% better in terms of both BLEU and METEOR scores than that based on HMM.
691,0,Similar superior results are observed when measured with METEOR score.
692,0,In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och_comma_ 2004)_comma_ METEOR (Banerjee and Lavie_comma_ 2005)_comma_ HWCM (Liu and Gildea 2005)_comma_ and the metric proposed in Albrecht and Hwa (2007) using the full feature set.
693,0,The results are summarized in Table 2: Metric Coefficient Our Metric 0.515 Albrecht_comma_ 2007 0.520 Smoothed BLEU 0.272 METEOR 0.318 HWCM 0.288 Table 2: Comparison among various metrics.
694,0,The results are summarized in Table 3:  Metric Coefficient Our Metric 0.329 Albrecht_comma_ 2007 0.309 Smoothed BLEU 0.269 METEOR 0.290 HWCM 0.260 Table 3: Cross year experiment result.
695,0,The total 53 features are computationally heavy (for the features from METEOR_comma_ ROUGE_comma_ HWCM and STM).
696,0,METEOR_comma_ is examined in section 5.
697,0,It is dubious whether SWD is useful regarding recall-oriented metrics like METEOR (Banerjee and Lavie_comma_ 2005)_comma_ since SWD removes information in source sentences.
698,0,This suspicion is to certain extent confirmed by our application of METEOR to the translation outputs of Experiment 1 (c.f. Table 7)_comma_ which shows that all SWD models achieve lower METEOR scores than the baseline.
699,0,However_comma_ SWD is not entirely harmful to METEOR: if SWD is applied to parameter tuning only but not for the test set_comma_ (i.e. Experiment 2)_comma_ even higher METEOR scores can be obtained.
700,0,In future experiments_comma_ maximum METEOR training should be used instead of maximum BLEU training so as to examine if SWD is really useful for parameter tuning.
701,0,7 Experiment 1 Experiment 2 SWD for both dev/test SWD for dev only Data baseline model 1 model 2 model 3 model 1 model 2 model 3 FBIS 50.07 47.90 49.83 49.34 51.58 51.08 51.17 BFT 52.47 50.55 51.89 52.10 54.72 54.43 54.30 NIST 52.12 49.86 50.97 51.59 54.14 53.82 54.01 Table 7: METEOR scores in Experiments 1 and 2 6 Conclusion and Future Work In this paper_comma_ we have explained why the handling of spurious source words is not a trivial problem and how important it is. Three solutions_comma_ with increasing sophistication_comma_ to the problem of SWD are presented.
702,0,The first one is already mentioned in the last section: maximum METEOR training should be implemented in order to fully test the effect of SWD regarding METEOR.
703,0,Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al._comma_ 2002)_comma_ NIST (Doddington_comma_ 2002)_comma_ and METEOR (Banerjee and Lavie_comma_ 2005_comma_ version 0.6).5 All measures used were the case-sensitive_comma_ corpuslevel versions.
704,0,Significance was tested using a paired bootstrap (Koehn_comma_ 2004) with 1000 samples (p < 0.05).6 4http://www.statmt.org/wmt08 5METEOR details: For English_comma_ we use exact matching_comma_ Porter stemming_comma_ and WordNet synonym matching.
705,0,6Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu.edu/MT. 13 Chinese  English Testing on UN Testing on News (NIST 2005) Context features BLEU NIST METEOR BLEU NIST METEOR Training on in-domain data only: None 0.3715 7.918 0.6486 0.2700 7.986 0.5314 Training on all data: None 0.3615 7.797 0.6414 0.2593 7.697 0.5200 Lexical 0.3898 8.231 0.6697 0.2522 7.852 0.5273 Shallow:  1 POS tag 0.3611 7.713 0.6430 0.2669 8.243 0.5526 Shallow:  2 POS tags 0.3657 7.808 0.6455 0.2591 7.843 0.5288 Lexical + Shallow 0.3886 8.245 0.6675 0.2628 7.881 0.5290 Syntactic 0.3717 7.899 0.6531 0.2653 8.123 0.5403 Lexical + Syntactic 0.3926 8.224 0.6636 0.2572 7.774 0.5234 Positional 0.3647 7.766 0.6469 0.2648 7.891 0.5275 All 0.3772 8.176 0.6582 0.2566 7.775 0.5225 Feature selection (see Sec.
706,0,On News data_comma_ context features did not give a significant advantage on the BLEU score_comma_ though syntactic and  1 POS contexts did give significant NIST and METEOR improvements over the in-domain baseline.
707,0,14 English  German Context features BLEU NIST METEOR None 0.2069 6.020 0.2811 Lexical 0.2018 6.031 0.2772 Shallow 0.2017 5.911 0.2748 Syntactic 0.2077 6.049 0.2829 Positional 0.2045 5.930 0.2772 Lex.
708,0,15 devtest06 test07 test08 System BLEU NIST METEOR BLEU NIST METEOR BLEU NIST METEOR Baseline 0.2009 5.866 0.2719 0.2051 5.957 0.2782 0.2003 5.889 0.2720 Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772 Table 5: English  German shared task system results using WMT-08 Europarl parallel data for training_comma_ dev06 for tuning_comma_ and three test sets_comma_ including the final 2008 test set.
709,0,Moses provides BLEU (K.Papineni et al._comma_ 2001) and NIST (Doddington_comma_ 2002)_comma_ but Meteor (Banerjee and Lavie_comma_ 2005; Lavie and Agarwal_comma_ 2007) and TER (Snover et al._comma_ 2006) can easily be used instead.
710,0,5http://opennlp.sourceforge.net/ We use the standard four-reference NIST MTEval data sets for the years 2003_comma_ 2004 and 2005 (henceforth MT03_comma_ MT04 and MT05_comma_ respectively) for testing and the 2002 data set for tuning.6 BLEU4 (Papineni et al._comma_ 2002)_comma_ METEOR (Banerjee and Lavie_comma_ 2005) and multiple-reference Word Error Rate scores are reported.
711,0,The number of links of each alignment set over all 6http://www.nist.gov/speech/tests/mt/ 7http://gps-tsc.upc.es/veu/soft/soft/marie/ 58 Align TM ttLM BLEU mWER METEOR MT03 U 3 .4453 51.94 .6356 rS 3 .4586 50.67 .6401 rST 3 .4600 50.64 .6416 rST 4 .4610 50.20 .6401 rST 4 5 .4689 49.36 .6411 MT04 U 3 .4244 50.12 .6055 rS 3 .4317 49.89 .6085 rST 3 .4375 49.69 .6109 rST 4 .4370 49.07 .6093 rST 4 5 .4366 48.70 .6092 MT05 U 3 .4366 50.40 .6306 rS 3 .4447 49.77 .6353 rST 3 .4484 49.09 .6386 rST 4 .4521 48.69 .6377 rST 4 5 .4561 48.07 .6401 Table 2: Evaluation results for experiments on translation units_comma_ alignment and modeling.
712,0,Previous publications on Meteor (Lavie et al._comma_ 2004; Banerjee and Lavie_comma_ 2005; Lavie and Agarwal_comma_ 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics.
713,1,Many researchers (Banerjee and Lavie_comma_ 2005; Liu and Gildea_comma_ 2006)_comma_ have observed consistent gains by using more flexible matching criteria.
714,0,We 157 System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%) Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20 Moses 31.15 7.48 56.80 55.14 59.85 42.79 Table 3: Evaluations based on different metrics with comparison to Moses.
715,0,Furthermore_comma_ the BLEU score performance suggests that our model is not very powerful_comma_ but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang_comma_ 2007) based on various evaluation metrics_comma_ including BLEU score_comma_ NIST score (Doddington_comma_ 2002)_comma_ METEOR (Banerjee and Lavie_comma_ 2005)_comma_ TER (Snover et al._comma_ 2006)_comma_ WER and PER.
716,0,It is shown that our systems TER_comma_ WER and PER scores are very close to Moses_comma_ though the gaps in BLEU_comma_ NIST and METEOR are significant_comma_ which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences.
717,0,For instance_comma_ BLEU and ROUGE (Lin and Och_comma_ 2004) are based on n-gram precisions_comma_ METEOR (Banerjee and Lavie_comma_ 2005) and STM (Liu and Gildea_comma_ 2005) use word-class or structural information_comma_ Kauchak (2006) leverages on paraphrases_comma_ and TER (Snover et al._comma_ 2006) uses edit-distances.
718,0,However_comma_ one important characteristic of the decoder used in our experiments is its ability to leave its models on disk_comma_ loading only the parts of the models necesSource BLEU NIST WER PER GTM METEOR ar 0.4457 (0.00) 8.9386 (0.00) 0.4458 (0.00) 0.3742 (0.00) 0.7469 (0.00) 0.6766 (0.00) da 0.6640 (0.64) 11.4500 (1.64) 0.2560 (0.08) 0.2174 (2.42) 0.8338 (0.68) 0.8154 (1.23) de 0.6642 (0.79) 11.4107 (0.44) 0.2606 (2.18) 0.2105 (0.14) 0.8348 (-0.13) 0.8132 (-0.07) es 0.7345 (0.00) 12.1384 (0.00) 0.2117 (0.00) 0.1668 (0.00) 0.8519 (0.00) 0.8541 (0.00) fr 0.6666 (0.95) 11.7443 (0.63) 0.2548 (4.82) 0.2172 (6.50) 0.8408 (0.48) 0.8293 (1.29) id 0.5295 (9.56) 10.3459 (4.11) 0.3899 (21.17) 0.3239 (4.65) 0.7960 (1.35) 0.7521 (2.35) it 0.6702 (1.01) 11.5604 (0.41) 0.2590 (3.25) 0.2090 (0.62) 0.8351 (0.36) 0.8171 (0.05) ja 0.5971 (3.47) 10.6346 (2.56) 0.3779 (5.53) 0.2842 (2.80) 0.8125 (0.74) 0.7669 (0.67) ko 0.5898 (1.78) 10.2151 (1.31) 0.3891 (0.74) 0.3138 (-0.10) 0.7880 (0.36) 0.7397 (0.35) ms 0.5102 (10.19) 9.9775 (2.75) 0.4058 (18.53) 0.3355 (3.59) 0.7815 (0.18) 0.7247 (2.49) nl 0.6906 (2.55) 11.9092 (1.47) 0.2415 (3.21) 0.1872 (1.73) 0.8548 (0.39) 0.8399 (0.36) pt 0.6623 (0.35) 11.6913 (0.26) 0.2549 (2.52) 0.2110 (2.68) 0.8396 (0.02) 0.8265 (-0.07) ru 0.5877 (0.34) 10.1233 (-1.10) 0.3447 (1.99) 0.2928 (1.71) 0.7900 (0.15) 0.7537 (-0.40) th 0.4857 (1.50) 9.5901 (1.17) 0.4883 (-0.23) 0.3579 (2.03) 0.7608 (0.45) 0.7104 (1.23) vi 0.5118 (0.67) 9.8588 (1.85) 0.4274 (-0.05) 0.3301 (0.12) 0.7806 (1.05) 0.7254 (0.43) zh 0.5742 (0.00) 10.1263 (0.00) 0.3937 (0.00) 0.3172 (0.00) 0.7936 (0.00) 0.7343 (0.00) Table 3.
719,0,Evaluation schemes To obtain a balanced view of the merits of our proposed approach_comma_ in our experiments we used 6 evaluation techniques to evaluate our systems.
720,0,These were: BLEU (Papineni_comma_ 2001)_comma_ NIST (Doddington_comma_ 2002)_comma_ WER (Word Error Rate)_comma_ PER (Position-independent WER)_comma_ GTM (General Text Matcher)_comma_ and METEOR (Banerjee and Lavie_comma_ 2005).
721,0,109 machine translation evaluation (e.g._comma_ Banerjee and Lavie_comma_ 2005; Lin and Och_comma_ 2004)_comma_paraphraserecognition (e.g._comma_ Brockett and Dolan_comma_ 2005; Hatzivassiloglou et al._comma_ 1999)_comma_ and automatic grading (e.g._comma_ Leacock_comma_2004;Marn_comma_ 2004).
722,0,We evaluate translation output using case-insensitive BLEU (Papineni et al._comma_ 2001)_comma_ as provided by NIST_comma_ and METEOR (Banerjee and Lavie_comma_ 2005)_comma_ version 0.6_comma_ with Porter stemming and WordNet synonym matching.
723,0,226 QDG Configurations BLEU METEOR synchronous 0.4008 0.6949 + nulls_comma_ root-any 0.4108 0.6931 + child-parent_comma_ same node 0.4337 0.6815 + sibling 0.4881 0.7216 + grandparent/child 0.5015 0.7365 + c-command 0.5156 0.7441 + other 0.5142 0.7472 Table 5: QG configuration comparison.
724,0,The BLEU score does not increase_comma_ however_comma_ when we permit all configurations in the final row of the table_comma_ and the METEOR score increases only slightly.
725,0,6.7 Discussion We note that these results are not state-of-theart on this dataset (on this task_comma_ Moses/MERT achieves0.6838BLEUand0.8523METEORwith maximum phrase length 3).14 Our aim has been to 13In fact_comma_ the strictest synchronous model used the almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized.
726,0,Therefore_comma_ we also carried out evaluations using the NIST (Doddington_comma_ 2002)_comma_ METEOR (Banerjee and Lavie_comma_ 2005)_comma_ WER (Hunt_comma_ 1989)_comma_ PER (Tillmann et al._comma_ 1997) and TER (Snover et al._comma_ 2005) machine translation evaluation techniques.
727,0,It is clear from Table 3 that for most of the language pairs (67% of them for BLEU_comma_ and a similar percentage for all the other metrics except METEOR)_comma_ better evaluation scores were achieved by using a reverse decoding strategy than a forward strategy.
728,0,Metric Bi>For Bi>Rev Rev>For BLEU 98.90 84.93 67.65 NIST 98.53 78.31 75.00 METEOR 99.63 95.96 50.74 WER 99.26 92.85 66.18 PER 98.53 84.97 70.59 TER 99.63 91.18 68.75 Table 4: Summary of the results using several automatic metrics for evaluation.
729,0,The quality of the translation output is mainly evaluated using BLEU_comma_ with NIST (Doddington_comma_ 2002) and METEOR (Banerjee and Lavie_comma_ 2005) as complementary metrics.
730,0,BLEU NIST METEOR CS 8.43 4.6272 0.3778 Stanford 10.45 5.0675 0.3699 BS-SingleBest 7.98 4.4374 0.3510 BS-WordLattice 9.04 4.6667 0.3834 Table 4: BS on NIST task BLEU NIST METEOR CS 0.1931 6.1816 0.4998 LDC 0.2037 6.2089 0.4984 BS-SingleBest 0.1865 5.7816 0.4602 BS-WordLattice 0.2041 6.2874 0.5124 Table 5: BS on IWSLT 2006 task BLEU NIST METEOR CS 0.2959 6.1216 0.5216 LDC 0.3174 6.2464 0.5403 BS-SingleBest 0.3023 6.0476 0.5125 BS-WordLattice 0.3171 6.3518 0.5603 Table 6: BS on IWSLT 2007 task 6.2 Parameter Search Graph The reliability estimation process is computationally intensive.
731,0,Results are according to BLEU_comma_ NIST and METEOR (MET) metrics.
732,0,The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al._comma_ 2002)_comma_ NIST (Doddington_comma_ 2002) and METEOR (Banerjee and Lavie_comma_ 2005) scores.
733,0,(The 95% statistical significance boundary is about  0.53 Bleu on the test data_comma_ 0.077 Nist and 0.16 Meteor according to bootstrap resampling) We see similar gains in Nist and Meteor metrics as shown in Table 9.
734,0,In Owczarzak (2008)_comma_ the method achieves equal or higher correlations with human judgments than METEOR (Banerjee and Lavie_comma_ 2005)_comma_ one of the best-performingautomaticMTevaluationmetrics.
735,1,We compare this metric against a combination metric of four state-of-theart scores (BLEU_comma_ NIST_comma_ TER_comma_ and METEOR) in two different settings.
736,0,A number of metrics have been designed to account forparaphrase_comma_eitherbymakingthematchingmore intelligent (TER_comma_ Snover et al.(2006))_comma_ or by using linguistic evidence_comma_ mostly lexical similarity (METEOR_comma_ Banerjee and Lavie (2005); MaxSim_comma_ Chan and Ng (2008))_comma_ or syntactic overlap (Owczarzak et al.(2008); Liu and Gildea (2005)).
737,0,Unfortunately_comma_ each metrics tend to concentrate on one particular type of linguistic information_comma_ none of which always correlates well with human judgments.
738,0,METEORR consists of METEOR v0.7.
739,0,4Available from http://www.nist.gov. 300 Evaluation Data Metrics train test BLEUR METEORR NISTR TERR MTR RTER MT+RTER Sentence-level Ar+Ch Ur 49.9 49.1 49.5 50.1 50.1 54.5 55.6 Ar+Ur Ch 53.9 61.1 53.1 50.3 57.3 58.0 62.7 Ch+Ur Ar 52.5 60.1 50.4 54.5 55.2 59.9 61.1 System-level Ar+Ch Ur 73.9 68.4 50.0 90.0 92.7 77.4 81.0 Ar+Ur Ch 38.5 44.3 40.0 59.0 51.8 47.7 57.3 Ch+Ur Ar 59.7 86.3 61.9 42.1 48.1 59.7 61.7 Table 1: Expt.
740,0,METEORR achieves the best correlation for Chinese and Arabic_comma_ but fails for Urdu_comma_ apparently the most difficult language.
741,0,TERR shows the best result for Urdu_comma_ but does worse than METEORR for Arabic and even worse than BLEUR for Chinese.
742,0,Even though METEORR still does somewhat better than MTR and RTER_comma_ we consider this an important confirmation for the usefulness of entailment features in MT evaluation_comma_ and for their robustness.5 In addition_comma_ the combined model MT+RTER is best for all three languages_comma_ outperforming METEORR for each language pair.
743,0,BLEUR_comma_ METEORR_comma_ and NISTR significantly predict one language each (all Arabic); TERR_comma_ MTR_comma_ and RTER predict two languages.
744,0,METEORR does not improve_comma_ which is to be expected given the model definition.
745,0,[doc WL-34-174270-7483871_comma_ sent 4_comma_ system1] Gold: 6 METEORR: 2.8 RTER: 6.1  Only function words unaligned (will_comma_ this)  Alignment fact/reality: hypernymy is ok in upward monotone context REF: What does BBCs Haroon Rasheed say after a visit to Lal Masjid Jamia Hafsa complex?
746,0,[doc WL-12-174261-7457007_comma_ sent 2_comma_ system2] Gold: 1 METEORR: 4.5 RTER: 1.2  Hypothesis root node unaligned  Missing alignments for subjects  Important entities in hypothesis cannot be aligned  Reference_comma_ hypothesis differ in polarity Table 2: Expt.
747,0,Finally_comma_ we provide a qualitative comparison of RTERs performance against the best baseline metric_comma_ METEORR.
748,1,Since the computation of RTER takes considerably more resources than METEORR_comma_ it is interesting to compare the predictions of RTER against METEORR.
749,0,The first example (top) shows a good translation that is erroneously assigned a low score by METEORR because (a) it cannot align fact and reality (METEORR aligns only synonyms) and (b) it punishes the change of word order through its penalty term.
750,0,The second example (bottom) shows a very bad translation that is scored highly by METEORR_comma_ sincealmostallofthereferencewordsappeareither literally or as synonyms in the hypothesis (marked in italics).
751,0,In combination with METEORRs concentration on recall_comma_ this is sufficient to yield a moderately high score.
752,1,1: Among individual metrics_comma_ METEORR and TERR do better than BLEUR and NISTR.
753,0,Feature set Consistency(%) System-level correlation () BLEUR 49.6 69.3 METEORR 51.1 72.6 NISTR 50.2 70.4 TERR 51.2 72.5 MTR 51.5 73.1 RTER 51.8 78.3 MT+RTER 52.5 75.8 WMT 08 (worst) 44 37 WMT 08 (best) 56 83 Table 4: Expt.
754,0,Again_comma_ we see better results for METEORR and TERR than for BLEUR and NISTR_comma_ and the individual metrics do worse than the combination models.
755,0,Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet_comma_ and Zhou et al.(2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases.
756,0,These approaches reduce the risk that a good translation is rated poorly due to lexical deviation_comma_ but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf.the bottom example in Table 2).
757,0,In a different work_comma_ Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations.
758,0,In order to address this issue_comma_ they computed the translation-by-translation correlation with human judgements (i.e._comma_ correlation at the segment level).
759,0,At the lexical level_comma_ we have included several standard metrics_comma_ based on different similarity assumptions: edit distance (WER_comma_ PER and TER)_comma_ lexical precision (BLEU and NIST)_comma_ lexical recall (ROUGE)_comma_ and F-measure (GTM and METEOR).
760,1,This result supports the intuition in (Banerjee and Lavie_comma_ 2005) that correlation at segment level is necessary to ensure the reliability of metrics in different situations.
761,0,We have studied 100 sentence evaluation cases from representatives of each metric family including: 1PER_comma_ BLEU_comma_ DP-Or-star_comma_ GTM (e = 2)_comma_ METEOR and ROUGEL.
762,0,The authors claim that the existing automatic metrics_comma_ including some of the new and seemingly more reliable ones as e.g. Meteor (cf.(Banerjee and Lavie_comma_ 2005)) .
763,0,they are all quite rough measures of translation similarity_comma_ and have inexact models of allowable variation in translation. This claim is supported by a construction of translation variations which have identical BLEU score_comma_ but which are very different for a human judge.
764,0,However_comma_ there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information_comma_ e.g._comma_ by the integration of word similarity information as in Meteor (Banerjee and Lavie_comma_ 2005) or MaxSim (Chan and Ng_comma_ 2008).
765,1,We plan to assess the additional benefit of the full entailment feature set against the TRADMT feature set extended by a proper lexical similarity metric_comma_ such as METEOR.
766,0,3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set_comma_ we first compare each output to the reference translation using BLEU (Papineni et al._comma_ 2001) and METEOR (Banerjee and Lavie_comma_ 2005) and a combined scoring scheme provided by the ULC toolkit (Gimenez and Marquez_comma_ 2008).
767,0,For German-English_comma_ we selected the three with the highest METEOR scores and another two with high METEOR scores but low similarity scores to the first three.
768,0,2.1 Alignment Sentences from different systems are aligned in pairs using a modified version of the METEOR (Banerjee and Lavie_comma_ 2005) matcher.
769,0,In this paper_comma_ translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations (Papineni et al._comma_ 2002)_comma_ and (2) the METEOR metrics that calculates unigram overlaps between translations (Banerjee and Lavie_comma_ 2005).
770,0,Scores of both metrics range between 0 (worst) and 1 (best) and are displayed in percent  gures.
771,0,107 Table 3: Baseline Performance NC Eval EP Eval BLEU METEOR BLEU METEOR baseline 17.56 40.52 33.00 56.50 4.2 Effects of Model Adaptation In order to investigate the effect of model adaptation_comma_ each model component was optimized separately using the method described in Section 2.
772,0,The combination of NC and EP models using equal weights achieves only a slight improvement for the NC task (BLEU: +0.4%_comma_ METEOR: +0.4%)_comma_ but a large improvement for the EP task (BLEU: +1.0%_comma_ METEOR: +1.7%).
773,0,In total_comma_ model adaptation gains 1.1% and 1.3% in BLEU and 0.8% and 1.8% in METEOR for the NC and EP translation tasks_comma_ respectively.
774,0,Table 4: Effects of Model Adaptation weight NC Eval EP Eval optimization BLEU METEOR BLEU METEOR  17.92 40.72 34.00 58.20 tm 18.13 40.95 34.05 58.23 tm+lm 18.25 41.23 34.12 58.22 tm+dm 18.36 41.06 34.24 58.34 tm+lm+dm 18.65 41.35 34.35 58.36 4.3 Effects of Transliteration In order to investigate the effects of transliteration_comma_ we trained three different transliteration using the phrase-table of the baseline systems trained on (a) only the NC corpus_comma_ (b) only the EP corpus_comma_ and (c) on the merged corpus (NC+EP).
775,0,Table 5 summarizes the haracter-based automatic evaluation scores for the word error rate (WER) metrics_comma_ i.e._comma_ the edit distance between the system output and the closest reference translation (Niessen et al._comma_ 2000)_comma_ as well as the BLEU and METEOR metrics.
776,0,Transliteration consisTable 5: Transliteration Performance Training character-based Data WER BLEU METEOR NC 13.10 83.62 86.74 EP 11.76 85.93 87.89 NC+EP 11.72 86.08 87.89 tently improves the translation quality of all mixture models_comma_ although the gains obtained for the NC task (BLEU: +1.3%_comma_ METEOR: +1.3%) are much larger than those for the EP task (BLEU: +0.1%_comma_ METEOR: +0.2%) which is due to the larger amount of untranslatable words in the NC evaluation data set.
777,0,Overall our experiments show that the incorporation of mixture models and phrase-based transliteration techniques largely out-performed standard phrase-based SMT engines gaining a total of 2.4% in BLEU and 2.1% in METEOR for the news domain.
778,0,We did not modify the SGML files used for calculating BLEU and METEOR scores in any way.
779,0,System BLEU METEOR LR no processing 18.91 49.50 1.0097 c+w 19.37 49.69 1.0067 c+w_comma_ s/s 19.18 51.13 1.0035 c+w_comma_ old reordering 19.61 50.44 1.0092 c+w_comma_ new reordering 19.91 50.84 1.0059 c+w_comma_ new reordering_comma_ s/s (submitted_comma_ bug) 19.65 51.57 1.0093 * c+w_comma_ new reordering_comma_ s/s 19.73 51.59 1.0062 as * IRSTLM quantized 19.52 51.33 1.0003 as * IRSTLM 19.75 51.61 1.0013 as * IRSTLM 21.2 quantized 19.52 51.51 1.0095 as * RANDLM 19.67 51.73 1.0067 as * RANDLM 21.2 21.03 51.96 1.0111 Table 1: German to English_comma_ dev-2009b (case sensitive)_comma_ c+w = char+word normalization_comma_ s/s = splitting/stemming_comma_ 21.2 = larger LM System BLEU METEOR LR no processing 13.55 38.31 0.9910 c+w (no second step) 14.11 38.27 0.9991 c+w_comma_ s/s_comma_ second step (submitted_comma_ bug) 12.34 37.89 1.0338 c+w_comma_ s/s_comma_ second step 13.05 37.94 1.0157 Table 2: English to German_comma_ dev-2009b (case sensitive)_comma_ c+w = char+word normalization_comma_ s/s = splitting/stemming The second error that we handled was that S-RC constituents which do not have a complementizer are reordered incorrectly.
780,0,Experiments are presented in table 1_comma_ using BLEU (Papineni et al._comma_ 2001) and METEOR5 (Banerjee and Lavie_comma_ 2005)_comma_ and we also show the length ratio (ratio of hypothesized tokens to reference tokens).
781,1,For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al._comma_ 2008).
782,0,Each system increases in both BLEU and METEOR as improvements are added.
783,1,However_comma_ we trust the METEOR results more due to their better correlation with human judgements.
784,0,We used all of the German data to train the language 5METEOR used default weights_comma_ stemming and Wordnet synsets.
785,0,METEOR7 did not correlate as well as BLEU for translation out of English in WMT 2008.
786,0,Our best system improves by 2.46 METEOR and 1.12 BLEU over a standard Moses system.
787,0,Apart from BLEU_comma_ a standard automatic measure METEOR (Banerjee and Lavie_comma_ 2005) was used for evaluation.
788,0,5.4 Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev)_comma_ and BLEU and METEOR scores obtained on test dataset (Test).
789,0,Dev Test BLEU BLEU METEOR Plain 48.31 45.02 65.98 BL 48.46 47.10 68.10 SBR 48.75 47.52 67.33 SBR+lattice 48.90 48.78 68.85 Table 2: Summary of BTEC experimental results.
790,0,Dev Test BLEU BLEU METEOR Plain 41.83 43.80 62.03 BL 42.68 43.52 62.17 SBR 42.71 44.01 63.29 SBR+lattice 43.05 44.89 63.30 Table 3: Summary of NIST50K experimental results.
791,0,The METEOR score is only slightly better for the SBR configurations in case of BTEC task; in the case of NIST50K the METEOR improvement is more evident.
792,0,This is not incorrect but will affect translation quality as measured by automatic metrics which compare 3http://www.statmt.org/wmt09/baseline.html 23 Test set Decoding Input METEOR BLEU NIST no. 1 Moses 49.05 20.45 6.135 +postprocess (transprob) 48.73 19.93 6.064 +postprocess (bestmatch) 50.01 20.64 6.220 +decode (transprob) 49.04 20.44 6.128 +decode (bestmatch) 49.36 20.70 6.179 no. 2 Moses 49.60 21.10 6.211 +postprocess (transprob) 49.20 20.43 6.128 +postprocess (bestmatch) 50.56 21.19 6.291 +decode (transprob) 49.58 21.02 6.201 +decode (bestmatch) 50.60 21.21 6.243 Table 3: Enforcing one translation per discourse can help METEOR_comma_ BLEU and NIST scores when using the supervised sense disambiguation technique (bestmatch).
793,0,5.2 Impact on translation quality As reported in Table 3_comma_ small increases in METEOR (Banerjee and Lavie_comma_ 2005)_comma_ BLEU (Papineni et al._comma_ 2002) and NIST scores (Doddington_comma_ 2002) suggest that SMT output matches the references better after postprocessing or decoding with the suggested lemma translations.
794,1,Semantic collocations are harder to extract than cooccurrence patterns--the state of the art does not enable us to find semantic collocations automatically t. This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon_comma_ we can at least achieve semi-automatic extraction of semantic collocations (see also Calzolari and Bindi (1990) I But note the important work by Hindle \[HindlegO\] on extracting semantically similar nouns based on their substitutability in certain verb contexts.
795,0,We see his work as very similar in spirit.
796,0,The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks_comma_ 1990; Zernik and Jacobs_comma_ 1990; Hindle_comma_ 1990).
797,0,More specifically_comma_ two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment (Hindle and Rooth_comma_ 1990) and pronoun references (Dagan and Itai_comma_ 1990a; Dagan and Itai_comma_ 1990b).
798,1,His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words (such as in (Hindle_comma_ 1990)).
799,0,(1) a. I expected \[nv the man who smoked NP\] to eat ice-cream h. I doubted \[NP the man who liked to eat ice-cream NP\] Current high-coverage parsers tend to use either custom_comma_ hand-generated lists of subcategorization frames (e.g. _comma_ Hindle_comma_ 1983)_comma_ or published_comma_ handgenerated lists like the Ozford Advanced Learner's Dictionary of Contemporary English_comma_ Hornby and Covey (1973) (e.g. _comma_ DeMarcken_comma_ 1990).
800,0,Three recent papers in this area are Church and Hanks (1990)_comma_ Hindle (1990)_comma_ and Smadja and McKeown (1990).
801,0,The latter two are concerned exclusively with collocation relations between open-class words and not with grammatical properties.
802,0,In (Hindle_comma_1990; Zernik_comma_ 1989; Webster el Marcus_comma_ 1989) cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification.
803,0,All these studies are based on th~ (strong) assumption that syntactic similarity in wor(~ patterns implies semantic similarity.
804,0,(Hindle_comma_ 1990; Hindle and Rooths_comma_1991) and (Smadja_comma_ 1991) use syntactic markers to increase the significance of the data.
805,0,Despite the use of these methods to add evidence to the data_comma_ the major problem with word-pairs collections is that reliable results are obtained only for a small subset of high-frequency words on very large corpora_comma_ otherwise the association ratio becomes unstable.
806,0,For example_comma_ Church run his experiment on a corpus with over 20-30 millions words_comma_ and Hindle reports 6 millions words as not being an adequate corpus.
807,0,The model of prepositional attachment preference proposed by Hindle is also too weak if applied only to verb-preposition and nounpreposition pairs.
808,0,Combining statistical and parsing methods has been done by (Hindle_comma_ 1990; Hindle and Rooths_comma_1991) and (Smadja and McKewon_comma_ 1990; Smadja_comma_1991).
809,0,Hindle performs syntactic parsing before collocational analysis (syntax-first).
810,0,For example_comma_ parsing the sentence: fabbrica di scarpe per uomo e per bambino (*manufacture of shoes for man and child) produces the following relations: N_prep N(fabbrica_comma_di_comma_scarpe) N prep_N(fabbrica_comma_per_comma_uomo) N_prep_N(fabbrica_comma_per_comma_bambino) N_prep_N(scarpe_comma_per_comma_uomo) N_prep_N(scarpe_comma_per_comma_bambino) N_cong_N(uomo_comma_e_comma_bambino) Unlike Church and Hindle_comma_ we are not interested in collecting binary or ternary relations between words within a sentence_comma_ but rather in detecting recurring binary syntactic associations in the corpus.
811,0,D. Hindle_comma_ Noun classification from predicate argument structures_comma_ in (ACL_comma_1990).
812,0,There bas recently been work in the detection of semantically related nouns via_comma_ for example_comma_ shared argument structures (Hindle 1990)_comma_ and shared dictionary definition context (Wilks e al. 1990).
813,0,These approaches attempt to infer relationships among \[exical terms by looking at very large text samples and determining which ones are related in a statistically significant way.
814,0,In comparison_comma_ most corpus-based algorithms employ substantially larger corpora (e.g. _comma_ 1 million words (de Marcken_comma_ 1990)_comma_ 2.5 million words (Brent_comma_ 1991)_comma_ 6 million words (Hindle_comma_ 1990)_comma_ 13 million words (Hindle_comma_ & Rooth_comma_ 1991)).
815,0,8Interestingly_comma_ in work on the automated classification of nouns_comma_ (Hindle_comma_ 1990) also noted problems with 'empty' words that depend on their complements for meaning.
816,0,\[Hindle 1990\] confirmed that word association ratios can be used for measuring similarity between nouns.
817,0,Our algorithm considers joint frequencies of pairs of word groups (as \[Brown et.
818,0,al. 1992\] does) in contrast to joint frequencies of word pairs as in \[Church and Hanks_comma_ 1990\] and \[Hindle 1990\].
819,0,Hindle_comma_ D. _comma_ (1990) 'Noun Classification from Predicate-Argument Structures_comma_' Proceedings of the 28th Annual Meeting of the ACL_comma_ pp.
820,0,Most work on corpora of naturally occurring language 244 Michael R. Brent From Grammar to Lexicon either uses no a priori grammatical knowledge (Brill and Marcus 1992; Ellison 1991; Finch and Chater 1992; Pereira and Schabes 1992)_comma_ or else it relies on a large and complex grammar (Hindle 1990_comma_ 1991).
821,0,Many other projects have used statistics in a way that summarizes facts about the text but does not draw any explicit conclusions from them (Finch and Chater 1992; Hindle 1990).
822,0,We have found_comma_ however_comma_ that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships_comma_ using a strategy similar to that employed by Hindle (1990) for detecting synonyms.
823,0,We then generate the set of verbs that take T as direct object and calculate the mutual information value for each verb/T collocation (cf.Hindle 1990).
824,0,Using techniques described in Church and Hindle (1990)_comma_ Church and Hanks (1990)_comma_ and Hindle and Rooth (1991)_comma_ Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus.
825,1,While full automatic extraction of semantic collocations is not yet feasible_comma_ some recent research in related areas is promising.
826,1,Hindle (1990) reports interesting results of this kind based on literal collocations_comma_ where he parses the corpus (Hindle 1983) into predicate-argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments.
827,0,For example_comma_ as a list of the most frequent objects for the verb drink in his corpus_comma_ Hindle found beer_comma_ tea_comma_ Pepsi_comma_ and champagne.
828,0,Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words_comma_ he develops a similarity metric for nouns based on their substitutability in certain verb contexts.
829,0,Hindle thus finds sets of semantically similar nouns based on syntactic co-occurrence data.
830,1,The sets he extracts are promising; for example_comma_ the ten most similar nouns to treaty in his corpus are agreement_comma_ plan_comma_ constitution_comma_ contract_comma_ proposal_comma_ accord_comma_ amendment_comma_ rule_comma_ law_comma_ and legislation.
831,0,This work is very close in spirit to our own investigation here; the emphasis on syntactic co-occurrence enables Hindle to extract his similarity lists automatically; they 17 For illustration_comma_ we use an abbreviated version of the lexical entries under discussion_comma_ highlighting only certain qualia for the verbs.
832,0,Statistical data about these various cooccurrence relations is employed for a variety of applications_comma_ such as speech recognition (Jelinek_comma_ 1990)_comma_ language generation (Smadja and McKeown_comma_ 1990)_comma_ lexicography (Church and Hanks_comma_ 1990)_comma_ machine translation (Brown et al. _comma_ ; Sadler_comma_ 1989)_comma_ information retrieval (Maarek and Smadja_comma_ 1989) and various disambiguation tasks (Dagan et al. _comma_ 1991; Hindle and Rooth_comma_ 1991; Grishman et al. _comma_ 1986; Dagan and Itai_comma_ 1990).
833,0,The search is based on the property that when computing sim(wl_comma_ w2)_comma_ words that have high mutual information values 5The nominator in our metric resembles the similarity metric in (Hindle_comma_ 1990).
834,0,Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of 'similar' events that have been seen.
835,0,In Hindle's proposal_comma_ words are similar if we have strong statistical evidence that they tend to participate in the same events.
836,0,His notion of similarity seems to agree with our intuitions in many cases_comma_ but it is not clear how it can be used directly to construct word classes and corresponding models of association.
837,0,More recent papers Hindle (1990)_comma_ Pereira and Tishby (1992) proposed to cluster nouns on the basis of a metric derived from the distribution of subject_comma_ verb and object in the texts.
838,0,Both papers use as a source of information large corpora_comma_ but differ in the type of statistical approach used to determine word similarity.
839,0,These studies_comma_ though valuable_comma_ leave several open problems: 70 1) A metric of conceptual closeness based on mere syntactic similarity is questionable_comma_ particularly if applied to verbs.
840,0,A number of knowledge-rich \[Jacobs and Rau_comma_ 1990_comma_ Calzolari and Bindi_comma_ 1990_comma_ Mauldin_comma_ 1991\] and knowledge-poor \[Brown et al. _comma_ 1992_comma_ Hindle_comma_ 1990_comma_ Ruge_comma_ 1991_comma_ Grefenstette_comma_ 1992\] methods have been proposed for recognizing when words are similar.
841,0,The knowledge-rich approaches require either a conceptual dependency representation_comma_ or semantic tagging of the words_comma_ while the knowledge-poor approaches require no previously encoded semantic information_comma_ and depend on frequency of co-occurrence of word contexts to determine similarity.
842,0,\[Hindle_comma_ 1990\] D. Hindle.
843,0,Noun classification from predicate-argument structures.
844,0,Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this has not been found to be effective for 1R)_comma_ lemma of the word (e.g. 'corpus' for 'corpora')_comma_ phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c_comma_ Church 1988))_comma_ and subject-predicate identification (e.g. Hindle 1990).
845,0,Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991_comma_ 1993a; Hindle and Rooths 1991_comma_1993; Sekine 1992) (Bogges et al. 1992)_comma_ sense preference (Yarowski 1992)_comma_ acquisition of selectional restrictions (Basili et al. 1992b_comma_ 1993b; Utsuro et al. 1993)_comma_ lexical preference in generation (Smadjia 1991)_comma_ word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c)_comma_ etc. In the majority of these papers_comma_ even though the (precedent or subsequent) statistical processing reduces the number of accidental associations_comma_ very large corpora (10_comma_000_comma_000 words) are necessary to obtain reliable data on a 'large enough' number of words.
846,0,In addition_comma_ most papers produce a performance evaluation of their methods but do not provide a measure of the coverage_comma_ i.e. the percentage of cases for which their method actually provides a (right or wrong) solution.
847,0,The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993).
848,0,More specifically_comma_ two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990_comma_ 1991).
849,1,His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words (such as in Hindle 1990).
850,0,Some researchers apply shallow or partial parsers (Smadja_comma_ 1991; Hindle_comma_ 1990) to acquiring specific patterns from texts.
851,0,These tell us that it is not necessary to completely parse the texts for some applications.
852,0,1 Introduction Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data.
853,0,have been proposed (Hindle_comma_ 1990; Brown et al. _comma_ 1992; Pereira et al. _comma_ 1993; Tokunaga et al. _comma_ 1995).
854,1,The realization of such an automatic construction method would make it possible to a) save the cost of constructing a thesaurus by hand_comma_ b) do away with subjectivity inherent in a hand made thesaurus_comma_ and c) make it easier to adapt a natural language processing system to a new domain.
855,1,In the past five years_comma_ important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks_comma_ 1990; Hindle_comma_ 1990; Smadja_comma_ 1993; Grei~nstette_comma_ 1994; Grishman and Sterling_comma_ 1994).
856,0,Most of these approaches_comma_ however_comma_ need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation.
857,0,In other words_comma_ it is not always possible to resort to statistical methods.
858,0,Section 4 compares our results to Itindle's ones (Hindle_comma_ 1990).
859,0,2 Simplifying parse trees to classify words 2.1 The need for normalized syntactic contexts As Hindle's work proves it_comma_ among others (Grishman and Sterling_comma_ 1994; Grefenstette_comma_ 1994:)_comma_ the mere existence of robust syntactic parsers makes it possible to parse large corpora in order to automate the discovery of syntactic patterns in the spirit of Harris's distributional hypothesis.
860,0,Itowever_comma_ Harris' methodology implies also to simplify and transform each parse tree 2_comma_ so as to obtain so-called 'elementary sentences' exhibiting the main conceptual classes for the domain (Sager lIa'or instance_comma_ Hindle (Hindle_comma_ 1990) needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures.
861,0,4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of (Hindle_comma_ 1990) shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning.
862,0,Hindle uses the observed frequencies within a specific syntactic pattern (subject/verb_comma_ and verb/object) to derive a cooccu_comma_> rence score which is an estimate of mutual information (Church and Hanks_comma_ 1990).
863,0,We adapted this score to noun phrase patterns) However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis.
864,0,This difference stems from the fact that this cooccurrence score overestimates rare events and underlines the collocations specific to each form.
865,0,As opposed to Hindle's lists of similar words which are centered on pivot words whose neighbors are all on the same level_comma_ in SYCLADE graphs_comma_ a word is represented by its role in a whole syntactic and conceptual network.
866,0,Since a handmade thesaurus is not slfitahle for machine use_comma_ and expensive to compile_comma_ automatical construction of~a thesaurus has been attempted using corpora (Hindle_comma_ 1990).
867,0,llowever_comma_ the thesaurus constructed by such ways does not contain so many nouns_comma_ and these nouns are specified by the used corpus.
868,0,In other words_comma_ we cannot construct the general thesaurus from only a corpus.
869,0,2.3 Measuring the similarity between classes (step 3) In step 3_comma_ we measure the similarity between two primitive classes by using the method given by Hindle (Hindle_comma_ 1990).
870,0,In fact_comma_ we are considering 'word usage rather than word meanin\]' (Zernik_comma_ 1990) following in this the distributional point of view_comma_ see (Harris_comma_ 1968)_comma_ (Hindle_comma_ 1990).
871,0,Statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for ANLP tools (Hindle_comma_ 1990)_comma_ (Zernik_comma_ 1990)_comma_ (Resnik_comma_ 1993)_comma_ or for automatic thesaurus generation (Grefenstette_comma_ 1994).
872,0,MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) We can use this definition to derive an estimate of the connectedness between words_comma_ in terms of collocations (Smadja_comma_ 1993)_comma_ but also in terms of phrases and grammatical relations (Hindle_comma_ 1990).
873,0,Limitations of handcrafted thesauri can be summarized as follows \[Hatzivassiloglou and McKeown_comma_ 1993; Uramoto_comma_ 1996; Hindle_comma_ 1990\].
874,0,This criticism leads us to automatic approaches for building thesauri from large corpora \[Hirschman et al. _comma_ 1975; Hindle_comma_ 1990; Hatzivassiloglou and McKeown_comma_ 1993; Pereira et al. _comma_ 1993; Tokunaga et aL_comma_ 1995; Ushioda_comma_ 1996\].
875,0,\[Hindle_comma_ 1990\] Hindle_comma_ D. Noun classification from predicate-argument structures.
876,0,Its value can be com769 simHindle(Wl_comma_ W2) ---~(r_comma_w)CT(wx)fqT(w2)Are{su~j-of.obj-of} min(I(wl_comma_ r_comma_ w)_comma_ I(w2_comma_ r_comma_ w) ) simHindle~ (Wl_comma_ W2) = ~(r_comma_w)eT(wl)nT(w~) min(I(wl_comma_ r_comma_ w)_comma_ I(w2_comma_ r_comma_ w) ) simcosine(Wl_comma_ W2) = ' \[T(wl)nT(w2)\[ %/\[T( wl )\[  IT(w2 ) l simDi~e(Wl w2) : 2xlT(~)nT(w2)l ' IT(wx)l+lT(w2)l T(wl)NT(w2) simJacard(Wl_comma_ W2) : iT(wl)l+ T(w2)l_lT(wl)nT(w2)l Figure 1: Other Similarity Measures puted as follows: I(w_comma_r_comma_w') = _ Iog(PMLE(B)PMLE(A\[B)PMLE(CIB)) --(-log PMLE (A_comma_/3_comma_ C)) = log IIw_comma_r_comma_w ll*_comma_r_comma_*\[ IIw_comma_r_comma_*llx *_comma_r_comma_w'll It is worth noting that I(w_comma_r_comma_w') is equal to the mutual information between w and w' (Hindle_comma_ 1990).
877,0,The measure simHindle is the same as the similarity measure proposed in (Hindie_comma_ 1990)_comma_ except that it does not use dependency triples with negative mutual information.
878,0,The measure simHindle r is the same as simHindle except that all types of dependency relationships are used_comma_ instead of just subject and object relationships.
879,0,It can be seen that sim_comma_ Hindler and cosine are significantly more similar to WordNet than Roget is_comma_ but are significantly less similar to Roget than WordNet is. The differences between Hindle and Hindler clearly demonstrate that the use of other types of dependencies in addition to subject and object relationships is very beneficial.
880,0,The performance of sim_comma_ Hindler and cosine are quite close.
881,0,Since the 95% confidence inter771 Table  Evaluation with WordNet and Roget Roget sim Hindle~ cosine Hindle WordNet average aava 0.178397 0.001636 0.212199 0.001484 0.204179 0.001424 0.199402 0.001352 0.164716 0.001200 Roget average WordNet 0.178397 sim 0.149045 Hindle~ 0.14663 cosine 0.135697 Hindle 0.115489 ~av~ 0.001636 0.001429 0.001383 0.001275 0.001140 vals of all the differences in Table 2 are on the positive side_comma_ one can draw the statistical conclusion that simis better than simHindler_comma_ which is better than simeosine.
882,0,Table 2: Distribution of Differences sim-Hindler sim-cosine Hindler-cosine WordNet average aavg 0.008021 0.000428 0.012798 0.000386 0.004777 0.000561 Roget average aavg sim-Hindler 0.002415 0.000401 sim-cosine 0.013349 0.000375 Hindle~-cosine 0.010933 0.000509 4 Future Work Reliable extraction of similar words from text corpus opens up many possibilities for future work.
883,0,Ours is 772 similar to (Grefenstette_comma_ 1994; Hindle_comma_ 1990; Ruge_comma_ 1992) in the use of dependency relationship as the word features_comma_ based on which word similarities are computed.
884,0,In (Hindle_comma_ 1990)_comma_ a small set of sample results are presented.
885,0,Predicate argument structures_comma_ which consist of complements (case filler nouns and case markers) and verbs_comma_ have also been used in the task of noun classification (Hindle 1990).
886,0,Our predicate-argument structure-based thesatmis is based on the method proposed by Hindie (Hindle_comma_ 1990)_comma_ although Hindle did not apply it to information retrieval.
887,0,Instead_comma_ he used mutual information statistics as a Similarity coefficient_comma_ wheras we used the Dice coefficient for normalization purposes.
888,0,Hindle only extracted the subject-verb and the object-verb predicatearguments_comma_ while we also extract adjective-noun predicate-arguments.
889,0,Words appearing in similax grammatical contexts are assumed to be similar_comma_ and therefore classified into the same class (Lin_comma_ 1998; Grefenstette_comma_ 1994; Grefenstette_comma_ 1992; Ruge_comma_ 1992; Hindle_comma_ 1990).
890,0,Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990)_comma_ although Hindle did not apply it to information retrieval.
891,0,Hindle only extracted subject-verb and object-verb relations_comma_ while we also extract adjective-noun and noun-noun relations_comma_ in the manner of Grefenstette (1994)_comma_ who applied his 99 Proceedings of EACL '99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri.
892,0,Arguably the most widely used is the mutual information (Hindle_comma_ 1990; Church and Hanks_comma_ 1990; Dagan et al. _comma_ 1995; Luk_comma_ 1995; D. Lin_comma_ 1998a).
893,0,Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs.
894,0,To extract semantic information of words such as synonyms and antonyms from corpora_comma_ previous research used syntactic structures (Hindle 1990_comma_ Hatzivassiloglou 1993 and Tokunaga 1995)_comma_ response time to associate synonyms and antonyms in psychological experiments (Gross 1989)_comma_ or extracting related words automatically from corpora (Grefensette 1994).
895,0,32-39 Proceedings of HLT-NAACL 2003 similar distribution patterns (Hindle_comma_ 1990; Peraira_comma_ et al. _comma_ 1993; Grefenstette_comma_ 1994).
896,0,There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998).
897,0,The output of these programs is a ranked list of similar words to each word.
898,0,The most frequently used resource for synonym extraction is large monolingual corpora (Hindle_comma_ 1990; Crouch and Yang_comma_ 1992; Grefenstatte_comma_ 1994; Park and Choi_comma_ 1997; Gasperin et al. _comma_ 2001 and Lin_comma_ 1998).
899,0,The methods used the contexts around the investigated words to discover synonyms.
900,0,The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as cat and dog_comma_ which are similar but not synonymous.
901,0,1 Introduction Distributional Similarity has been an active research area for more than a decade (Hindle_comma_ 1990)_comma_ (Ruge_comma_ 1992)_comma_ (Grefenstette_comma_ 1994)_comma_ (Lee_comma_ 1997)_comma_ (Lin_comma_ 1998)_comma_ (Dagan et al. _comma_ 1999)_comma_ (Weeds and Weir_comma_ 2003).
902,0,Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al. _comma_ 1990)_comma_ (Hindle_comma_ 1990)_comma_ (Lin_comma_ 1998)_comma_ (Dagan_comma_ 2000)_comma_ defined by: )()( )_comma_(log)_comma_( 2 fPwP fwPfwMI = A known weakness of MI is its tendency to assign high weights for rare features.
903,1,Yet_comma_ similarity measures that utilize MI showed good performance.
904,0,2.2 Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990_comma_ Lin 1998).
905,0,These systems mostly employ clustering algorithms to group words according to their meanings in text.
906,0,However_comma_ such clustering algorithms fail to name their classes.
907,0,Hindle_comma_ D. 1990.
908,0,Noun classification from predicate-argument structures.
909,0,The words we want to aggregate for text analysis are not rigorous synonyms_comma_ but the role is the same_comma_ so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words (Hindle_comma_ 1990; Strzalkowski_comma_ 1992).
910,0,Our method is similar to (Hindle_comma_ 1990)_comma_ (Lin_comma_ 1998)_comma_ and (Gasperin_comma_ 2001) in the use of dependency relationships as the word features.
911,0,Hindle (1990) used noun-verb syntactic relations_comma_ and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs.
912,0,These methods are useful for the organization of words deep within a hierarchy_comma_ but do not seem to provide a solution for the top levels of the hierarchy.
913,0,Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan_comma_ Pereira_comma_ and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999).
914,0,One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).
915,0,The output of these programs is a ranked list of similar words to each word.
916,0,For example_comma_ the words corruption and abuse are similar because both of them can be subjects of verbs like arouse_comma_ become_comma_ betray_comma_ cause_comma_ continue_comma_ cost_comma_ exist_comma_ force_comma_ go on_comma_ grow_comma_ have_comma_ increase_comma_ lead to_comma_ and persist_comma_ etc_comma_ and both of them can modify nouns like accusation_comma_ act_comma_ allegation_comma_ appearance_comma_ and case_comma_ etc. Many methods have been proposed to compute distributional similarity between words_comma_ e.g._comma_ (Hindle_comma_ 1990)_comma_ (Pereira et al. 1993)_comma_ (Grefenstette 1994) and (Lin 1998).
917,0,Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared.
918,1,Similarity-based smoothing (Hindle 1990; Brown et al. 1992; Dagan_comma_ Marcus_comma_ and Markovitch 1993; Pereira_comma_ Tishby_comma_ and Lee 1993; Dagan_comma_ Lee_comma_ and Pereira 1999) provides an intuitively appealing approach to language modeling.
919,0,This hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation (Hindle 1990; Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff 2003).
920,0,4.5 Hindles Measure Hindle (1990) proposed an MI-based measure_comma_ which he used to show that nouns could be reliably clustered based on their verb co-occurrences.
921,0,We consider the variant of 458 Weeds and Weir Co-occurrence Retrieval Figure 1 Variation (with parameters  and ) in development set mean similarity between neighbor sets of the additive t-test based CRM and of dist  . Hindles Measure proposed by (Lin 1998a)_comma_ which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: sim hind (w 1_comma_ w 2 ) = summationdisplay T(w 1 )T(w 2 ) min(I(c_comma_ w 1 )_comma_ I(c_comma_ w 2 )) (38) where T(w 1 ) ={c : I(c_comma_ n) > 0}.
922,0,Researchers have mostly looked at representing words by their surrounding words (Lund and Burgess 1996) and by their syntactical contexts (Hindle 1990; Lin 1998).
923,0,However_comma_ these representations do not distinguish between the different senses of words.
924,0,4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text (Hindle_comma_ 1990; Lin_comma_ 1998).
925,0,If the bound is too tight to allow the correct parse of some sentence_comma_ we would still like to allow an accurate partial parse: a sequence of accurate parse fragments (Hindle_comma_ 1990; Abney_comma_ 1991; Appelt et al. _comma_ 1993; Chen_comma_ 1995; Grefenstette_comma_ 1996).
926,0,For example_comma_ the words test and exam are similar because both of them follow verbs such as administer_comma_ cancel_comma_ cheat on_comma_ conduct_comma_  and both of them can be preceded by adjectives such as academic_comma_ comprehensive_comma_ diagnostic_comma_ difficult_comma_  Many methods have been proposed to compute distributional similarity between words (Hindle_comma_ 1990; Pereira et al. _comma_ 1993; Grefenstette_comma_ 1994; Lin_comma_ 1998).
927,0,Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared.
928,0,They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
929,0,To date_comma_ researchers have harvested_comma_ with varying success_comma_ several resources_comma_ including concept lists (Lin and Pantel 2002)_comma_ topic signatures (Lin and Hovy 2000)_comma_ facts (Etzioni et al. 2005)_comma_ and word similarity lists (Hindle 1990).
930,0,To our knowledge_comma_ no previous harvesting algorithm addresses all these properties concurrently.
931,0,Various methods (Hindle_comma_ 1990; Lin_comma_ 1998; Hagiwara et al. _comma_ 2005) have been proposed for synonym acquisition.
932,0,For example_comma_ Hindle (1990) used cooccurrences between verbs and their subjects and objects_comma_ and proposed a similarity metric based on mutual information_comma_ but no exploration concerning the effectiveness of other kinds of word relationship is provided_comma_ although it is extendable to any kinds of contextual information.
933,0,The only difference is that we 5See also work on partial parsing as a task in its own right: Hindle (1990) inter alia.
934,0,1 Introduction NLP researchers have developed many algorithms for mining knowledge from text and the Web_comma_ including facts (Etzioni et al. 2005)_comma_ semantic lexicons (Riloff and Shepherd 1997)_comma_ concept lists (Lin and Pantel 2002)_comma_ and word similarity lists (Hindle 1990).
935,0,3.2 (m_comma_n)-cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity_comma_ i.e._comma_ the idea that two words with similar meanings will be used in similar contexts (Hindle_comma_ 1990).
936,0,We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy_comma_ and in particular will likely share a hypernym as a near parent.
937,0,Many methods have been proposed to compute distributional similarity between words_comma_ e.g._comma_ (Hindle_comma_ 1990)_comma_ (Pereira et al. _comma_ 1993)_comma_ (Grefenstette_comma_ 1994) and (Lin_comma_ 1998).
938,0,Almost all of the methods represent a word by a feature vector_comma_ where each feature corresponds to a type of context in which the word appeared.
939,0,They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
940,0,We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindles (1990) measure_comma_ the weighted Lin measure (Wu and Zhou_comma_ 2003)_comma_ the -Skew divergence measure (Lee_comma_ 1999)_comma_ the Jensen-Shannon (JS) divergence measure (Lin_comma_ 1991)_comma_ Jaccards coef cient (van Rijsbergen_comma_ 1979) and the Confusion probability (Essen and Steinbiss_comma_ 1992).
941,0,924 Ranking scheme BNC Google Scholar Frequency-based 0.123 0.446 Sentence-window 0.200 0.344 Fixedsize-window 0.184 0.342 Hindle 0.293 0.416 Weighted Lin 0.358 0.509 -Skew 0.361 0.486 Jensen-Shannon 0.404 0.550 Jaccards coef.
942,0,For example_comma_ the words test and exam are similar because both of them can follow verbs such as administer_comma_ cancel_comma_ cheat on_comma_ conduct_comma_ etc. Many methods have been proposed to compute distributional similarity between words_comma_ e.g._comma_ (Hindle_comma_ 1990; Pereira et al. _comma_ 1993; Grefenstette_comma_ 1994; Lin_comma_ 1998).
943,0,Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared.
944,0,They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed.
945,0,This second source of evidence is sometimes referred to as distributional similarity (Hindle_comma_ 1990).
946,0,We will be using the similarity metrics shown in Table 1: Cosine_comma_ the Dice and Jaccard coefficients_comma_ and Hindles (1990) and Lins (1998) mutual information-based metrics.
947,0,As Table 2 shows_comma_ Lin and Jaccard worked best (though Lin has very low coverage)_comma_ Dice and Hindle not as good_comma_ and Cosine showed the worst performance.
948,0,Hindle (1990) uses a mutual-information based metric derived from the distribution of subject_comma_ verb and object in a large corpus to classify nouns.
949,0,These works focus only on relatedness of individual words and do not describe how the automatic estimation of semantic similarity can be useful in real-world tasks.
950,0,Others proposed distributional similarity measures between words (Hindle_comma_ 1990; Lin_comma_ 1998; Lee_comma_ 1999; Weeds et al._comma_ 2004).
951,0,Some researchers (Hindle_comma_ 1990; Grefenstette_comma_ 1994; Lin_comma_ 1998) classify terms by similarities based on their distributional syntactic patterns.
952,0,These methods often extract not only synonyms_comma_ but also semantically related terms_comma_ such as antonyms_comma_ hyponyms and coordinate terms such as cat and dog. Some studies make use of bilingual corpora or dictionaries to nd synonyms in a target language (Barzilay and McKeown_comma_ 2001; Shimohata and Sumita_comma_ 2002; Wu and Zhou_comma_ 2003; Lin et al._comma_ 2003).
953,1,A wide range of contextual information_comma_ such as surrounding words (Lowe and McDonald_comma_ 2000; Curran and Moens_comma_ 2002a)_comma_ dependency or case structure (Hindle_comma_ 1990; Ruge_comma_ 1997; Lin_comma_ 1998)_comma_ and dependency path (Lin and Pantel_comma_ 2001; Pado and Lapata_comma_ 2007)_comma_ has been utilized for similarity calculation_comma_ and achieved considerable success.
954,0,However_comma_ amajorproblemwhichariseswhenadopting distributional similarity is that it easily yields a huge amount of unique contexts.
955,0,This can lead to high dimensionality of context space_comma_ often up to the orderoftensorhundredsofthousands_comma_ whichmakes the calculation computationally impractical.
956,0,Our method is thus related to previous work based on Harris (1985)s distributional hypothesis.2 It has been used to determine both word and syntactic path similarity (Hindle_comma_ 1990; Lin_comma_ 1998a; Lin and Pantel_comma_ 2001).
957,0,Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts.
958,0,A number of researches which utilized distributional similarity have been conducted_comma_ including (Hindle_comma_ 1990; Lin_comma_ 1998; Geffet and Dagan_comma_ 2004) and many others.
959,0,Although they have been successful in acquiring related words_comma_ various parameters such as similarity measures and weighting are involved.
960,0,Distributional approaches_comma_ on the other hand_comma_ rely on text corpora_comma_ and model relatedness by comparing the contexts in which two words occur_comma_ assuming that related words occur in similar context (e.g._comma_ Hindle (1990)_comma_ Lin (1998)_comma_ Mohammad and Hirst (2006)).
961,0,glish nouns first appeared in Hindle (1990).
962,0,Syntactic context information is used (Hindle_comma_ 1990; Ruge_comma_ 1992; Lin_comma_ 1998) to compute term similarities_comma_ based on which similar words to a particular word can directly be returned.
963,0,This has been now an active research area for a couple of decades (Hindle_comma_ 1990; Lin_comma_ 1998; Weeds and Weir_comma_ 2003).
964,0,Furthermore_comma_ use of the self-training techniques described in (McClosky et al. _comma_ 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data.
965,1,This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.
966,1,Recent work_comma_ (McClosky et al. _comma_ 2006)_comma_ has shown that adding many millions of words of machine parsed and reranked LA Times articles does_comma_ in fact_comma_ improve performance of the parser on the closely related WSJ data.
967,0,To use the data from NANC_comma_ we use self-training (McClosky et al. _comma_ 2006).
968,1,Finally_comma_ the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al.(2006) find that the self-trained models help considerably when parsing WSJ.
969,0,The trends are the same as in (McClosky et al. _comma_ 2006): Adding NANC data improves parsing performance on BROWN development considerably_comma_ improving the f-score from 83.9% to 86.4%.
970,1,While (McClosky et al. _comma_ 2006) showed that this technique was effective when testing on WSJ_comma_ the true distribution was closer to WSJ so it made sense to emphasize it.
971,0,Thus_comma_ the WSJ+NANC model has better oracle rates than the WSJ model (McClosky et al. _comma_ 2006) for both the WSJ and BROWN domains.
972,1,We should keep in mind that (1) a treebank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al.(2006)_comma_ and (2) that our treebank PCFG is binarized as in Klein and Manning (2005) to make results comparable.
973,0,All supervised parsers are reaching an asymptote and further improvement does not seem to come from more hand-annotated data but by adding unsupervised or semi-unsupervised techniques (cf.McClosky et al. 2006).
974,1,Recently there have been some improvements to the Charniak parser_comma_ use n-best re-ranking as reported in (Charniak and Johnson_comma_ 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al. _comma_ 2006a; McClosky et al. _comma_ 2006b).
975,0,The syntactic parser is the version that is selftrained using 2_comma_500_comma_000 sentences from NANC_comma_ and where the starting version is trained only on WSJ data (McClosky et al. _comma_ 2006b).
976,0,As far as we know_comma_ language modeling always improves with additional training data_comma_ so we add data from the North American News Text Corpus (NANC) (Graff_comma_ 1995) automatically parsed with the Charniak parser (McClosky et al. _comma_ 2006) to train our language model on up to 20 million additional words.
977,0,Another way to look the algorithm is from the self-training perspective (McClosky et al. _comma_ 2006).
978,0,This can either be semi-supervised parsing_comma_ using both annotated and unannotated data (McClosky et al. _comma_ 2006) or unsupervised parsing_comma_ training entirely on unannotated text.
979,0,While most parsing methods are currently supervised or semi-supervised (McClosky et al. 2006; Henderson 2004; Steedman et al. 2003)_comma_ they depend on hand-annotated data which are difficult to come by and which exist only for a few languages.
980,0,The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penns WSJ and has been previously used by McClosky et al.(2006) in improving a supervised parser by selftraining.
981,1,Recently_comma_ (McClosky et al. _comma_ 2006a; McClosky et al. _comma_ 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson_comma_ 2005).
982,0,McClosky et al (2006a) use sections 2-21 of the WSJ PennTreebank as seed data and between 50K to 2_comma_500K unlabeled NANC corpus sentences as self-training data.
983,0,They train the PCFG parser and the reranker with the manually annotated WSJ data_comma_ and parse the NANC data with the 50-best PCFG parser.
984,0,Then they proceed in two directions.
985,0,In the first_comma_ they reorder the 50-best parse list with the reranker to create a new 1-best list.
986,0,In the second_comma_ they leave the 1-best list produced by the generative PCFG parser untouched.
987,0,Then they combine the 1-best list (each direction has its own list) with the WSJ training set_comma_ to retrain the PCFG parser.
988,0,There are two major differences between these papers and the current one_comma_ stemming from their usage of a reranker and of large seed data.
989,0,First_comma_ when their 1-best list of the base PCFG parser was used as self training data for the PCFG parser (the second direction)_comma_ the performance of the base parser did not improve.
990,0,Second_comma_ these papers did not explore self-training when the seed is small_comma_ a scenario whose importance has been discussed above.
991,0,As a result_comma_ the good results of (McClosky et al_comma_ 2006a; 2006b) with large seed sets do not immediately imply success with small seed sets.
992,0,For the Brown corpus_comma_ we based our division on (Bacchiani et al. _comma_ 2006; McClosky et al. _comma_ 2006b).
993,0,Unknown words were not identified in (McClosky et al. _comma_ 2006a) as a useful predictor for the benefit of self-training.
994,0,622 We also identified a length effect similar to that studied by (McClosky et al. _comma_ 2006a) for self-training (using a reranker and large seed_comma_ as detailed in Section 2).
995,0,Due to space limitations we do not discuss it here.
996,0,Indeed_comma_ in the II scenario_comma_ (Steedman et al. _comma_ 2003a; McClosky et al. _comma_ 2006a; Charniak_comma_ 1997) reported no improvement of the base parser for small (500 sentences_comma_ in the first paper) and large (40K sentences_comma_ in the last two papers) seed datasets respectively.
997,0,In the II_comma_ OO_comma_ and OI scenarios_comma_ (McClosky et al_comma_ 2006a; 2006b) succeeded in improving the parser performance only when a reranker was used to reorder the 50-best list of the generative parser_comma_ with a seed size of 40K sentences.
998,1,We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al. _comma_ 2006).
999,1,David McClosky_comma_ Eugene Charniak_comma_ and Mark Johnson Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence_comma_ RI 02912 {dmcc|ec|mj}@cs.brown.edu Abstract Self-training has been shown capable of improving on state-of-the-art parser performance (McClosky et al._comma_ 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak_comma_ 1997; Steedman et al._comma_ 2003).
1000,1,Studies prior to McClosky et al.(2006) failed to show a benefit to parsing from self-training (Charniak_comma_ 1997; Steedman et al._comma_ 2003).
1001,0,While the recent success of self-training has demonstrated its merit_comma_ it remains unclear why self-training helps in some cases but not others.
1002,0,McClosky et al.(2006) showed that self-training improves parsing accuracy when the two-stage Charniak and Johnson (2005) reranking parser is used.
1003,0,4 Testing the Four Hypotheses The question of why self-training helps in some cases (McClosky et al._comma_ 2006; Reichart and Rappoport_comma_ 2007) but not others (Charniak_comma_ 1997; Steedman et al._comma_ 2003) has inspired various theories.
1004,0,To test the phase transition hypothesis_comma_ we use the same parser as McClosky et al.(2006) but train on only a fraction of WSJ to see if self-training is still helpful.
1005,0,The self-training protocol is the same as in (Charniak_comma_ 1997; McClosky et al._comma_ 2006; Reichart and Rappoport_comma_ 2007): we parse the entire unlabeled corpus in one iteration.
1006,0,From McClosky et al.(2006)_comma_ we know that when 100% of the training data is used_comma_ self-training does not improve performance without a reranker.
1007,0,To test this_comma_ first we present some statistics on the nbest lists (n = 50) from the baseline WSJ trained parser and self-trained model3 from McClosky et al.(2006).
1008,0,However_comma_ we know from McClosky et al.(2006) that on average_comma_ parses do not change between the WSJ and self-trained models and when they do_comma_ they only improve slightly more than half the time.
1009,0,For comparison_comma_ we also include the results of experiments using the full feature set_comma_ as in McClosky et al.(2006)_comma_ labeled ALL.
1010,0,First_comma_ we perform an extension of the factor analysis similar to that in McClosky et al.(2006).
1011,0,This is done via a generalized linear regression model intended to determine which features of parse trees can predict when the self-training model will perform better.
1012,1,The significant predictors from McClosky et al.(2006) such as the number of conjunctions or sentence length continue to be helpful whereas unknown words are a weak predictor at best.
1013,1,Its success stories range from parsing (McClosky et al._comma_ 2006) to machine translation (Ueffing_comma_ 2006).
1014,0,glish (previously used for self-training of parsers (McClosky et al._comma_ 2006)).
1015,0,Theyalsoappliedself-training to domain adaptation of a constituency parser (McClosky et al._comma_ 2006b).
1016,0,Their method simply adds parsed unlabeled data without selecting it to the training set.
1017,1,There are only a few successful studies_comma_ such as (Ando and Zhang_comma_ 2005) for chunking and (McClosky et al._comma_ 2006a; McClosky et al._comma_ 2006b) on constituency parsing.
1018,1,Tighter integration of semantics into the parsing models_comma_ possibly in the form of discriminative reranking models (Collins and Koo_comma_ 2005; Charniak and Johnson_comma_ 2005; McClosky et al._comma_ 2006)_comma_ is a promising way forward in this regard.
1019,0,type system F1% D Collins (2000) 89.7 Henderson (2004) 90.1 Charniak and Johnson (2005) 91.0 updated (Johnson_comma_ 2006) 91.4 this work 91.7 G Bod (2003) 90.7Petrov and Klein (2007) 90.1 S McClosky et al.(2006) 92.1 Table 4: Comparison of our final results with other best-performing systems on the whole Section 23.
1020,1,McClosky et al.(2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data.
1021,1,However more recent results have shown that it can indeed improve parser performance (Bacchiani et al._comma_ 2006; McClosky et al._comma_ 2006a; McClosky et al._comma_ 2006b).
1022,0,Two previous papers would seem to address this issue: the work by Bacchiani et al.(2006) and McClosky et al.(2006b).
1023,0,However_comma_ in both cases the evidence is equivocal.
1024,0,This second point is emphasized by the second paper on self-training for adaptation (McClosky et al._comma_ 2006b).
1025,1,This paper is based on the C/J parser and thus its results are much more in line with modern expectations.
1026,0,So_comma_ for example_comma_ McClosky et al.(2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level.
1027,0,These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al._comma_ 2003)_comma_ or by the same parser with a reranking component in a type of selftraining scenario(McCloskyetal._comma_ 2006).
1028,0,McClosky et al.(2006) effectively utilized unlabeled data to improve parsing accuracy on the standard WSJ training set_comma_ but they used a two-stage parser comprised of Charniaks lexicalized probabilistic parser with n-best parsing and a discriminative reranking parser (Charniak and Johnson_comma_ 2005)_comma_ and thus it would be better categorized as co-training (McClosky et al._comma_ 2008).
1029,0,It is worth noting that their attempts at selftraining Charniaks lexicalized parser directly resulted in no improvement.
1030,1,For English_comma_ self-training contributes 0.83% absolute improvement to the PCFG-LA parser_comma_ which is comparable to the improvement obtained from using semi-supervised training with the twostage parser in (McClosky et al._comma_ 2006).
1031,0,Note that their improvement is achieved with the addition of 2_comma_000k unlabeled sentences using the combination of a generative parser and a discriminative reranker_comma_ compared to using only 210k unlabeled sentences with a single generative parser in our approach.
1032,0,1 Introduction Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications_comma_ and it is inadequate to process Web-scale corpora for knowledge acquisition (Pantel_comma_ 2007; Saeger et al._comma_ 2009) or semi-supervised learning (McClosky et al._comma_ 2006; Spoustov et al._comma_ 2009).
1033,1,The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al._comma_ 2008)_comma_ parse re-ranking (McClosky et al._comma_ 2006)_comma_ pronoun resolution (Nguyen and Kim_comma_ 2008)_comma_ and semantic role labeling (Liu and Sarkar_comma_ 2007).
1034,0,The other is the self-training (McClosky et al. 2006) which first parses and reranks the NANC corpus_comma_ and then use them as additional training data to retrain the model.
1035,1,Performance with Charniak parser enhanced by re-ranking plus self-training 5.7 Comparison with Other State-of-the-art Results Table 11 and table 12 compare our method with the other state-of-the-art methods; we use I_comma_ B_comma_ R_comma_ S and C to denote individual model (Charniak 2000; Collins 2000; Bod 2003; Petrov and Klein 2007)_comma_ bilingual-constrained model (Burkett and Klein 2008) 1 _comma_ re-ranking model (Charniak and Johnson 2005_comma_ Huang 2008)_comma_ self-training model (David McClosky 2006) and combination model (Sagae and Lavie 2006) respectively.
1036,0,The two tables clearly show that our method advance the state-of-the-art results on both Chinese and English syntax parsing.
1037,0,System  F1-Measure I Petrov and Klein (2007) 89.5 Charniak (2000) 89.7 Bod (2003) 90.7 R Collins (2000) 89.7 Charniak and Johnson (2005) 91.4 Huang (2008) 91.7 S David McClosky (2006) 92.1 C Sagae and Lavie (2006) 92.1 Our method 92.6    Table 12.
1038,0,Third_comma_ we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al._comma_ 2006).
1039,0,A totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in (McClosky et al._comma_ 2006).
1040,0,The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text.
1041,0,The problem itself has started to get attention only recently (Roark and Bacchiani_comma_ 2003; Hara et al._comma_ 2005; Daume III and Marcu_comma_ 2006; Daume III_comma_ 2007; Blitzer et al._comma_ 2006; McClosky et al._comma_ 2006; Dredze et al._comma_ 2007).
1042,0,In contrast_comma_ semi-supervised domain adaptation (Blitzer et al._comma_ 2006; McClosky et al._comma_ 2006; Dredze et al._comma_ 2007) is the scenario in which_comma_ in addition to the labeled source data_comma_ we only have unlabeled and no labeled target domain data.
1043,0,Semi-supervised adaptation is a much more realistic situation_comma_ while at the same time also considerably more difficult.
1044,0,2 Motivation and Prior Work While several authors have looked at the supervised adaptation case_comma_ there are less (and especially less successful) studies on semi-supervised domain adaptation (McClosky et al._comma_ 2006; Blitzer et al._comma_ 2006; Dredze et al._comma_ 2007).
1045,0,Of these_comma_ McClosky et al.(2006) deal specifically with selftraining for data-driven statistical parsing.
1046,1,They show that together with a re-ranker_comma_ improvements 37 are obtained.
1047,0,So far_comma_ most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea_comma_ 2001; Roark and Bacchiani_comma_ 2003; McClosky et al._comma_ 2006; Shimizu and Nakagawa_comma_ 2007)_comma_ i.e. systems employing (constituent or dependency based) treebank grammars (Charniak_comma_ 1996).
1048,0,Parse selection constitutes an important part of many parsing systems (Johnson et al._comma_ 1999; Hara et al._comma_ 2005; van Noord and Malouf_comma_ 2005; McClosky et al._comma_ 2006).
1049,0,Recently there have been some works on using multiple treebanks for domain adaptation of parsers_comma_ where these treebanks have the same grammar formalism (McClosky et al._comma_ 2006b; Roark and Bacchiani_comma_ 2003).
1050,1,4.3 Using Unlabeled Data for Parsing Recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the WSJ data_comma_ even when labeled data is relatively large (McClosky et al._comma_ 2006a; Reichart and Rappoport_comma_ 2007).
1051,0,Our results on Chinese data confirm previous findings on English data shown in (McClosky et al._comma_ 2006a; Reichart and Rappoport_comma_ 2007).
1052,0,McClosky et al.(2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Brown corpus.
1053,0,Their results indicated that both unlabeled in-domain data and labeled out-of-domain data can help domain adaptation.
1054,0,In comparison with these works_comma_ we conduct our study in a different setting where we work with multiple heterogeneous treebanks.
1055,0,Uses for k-best lists include minimum Bayes risk decoding (Goodman_comma_ 1998; Kumar and Byrne_comma_ 2004)_comma_ discriminative reranking (Collins_comma_ 2000; Charniak and Johnson_comma_ 2005)_comma_ and discriminative training (Och_comma_ 2003; McClosky et al._comma_ 2006).
1056,0,Three class of solutions are possible to reduce data sparseness: (1) enlarging the data manually or automatically (e.g.(McClosky et al._comma_ 2006) uses selftraining to perform this step) (2) smoothing_comma_ usually this is performed using a markovization procedure (Collins_comma_ 1999; Klein and Manning_comma_ 2003a) and (3) make the data more coarse (i.e. clustering).
1057,0,Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes_comma_ 1992; Klein and Manning_comma_ 2004; Smith and Eisner_comma_ 2004; Smith and Eisner_comma_ 2005) over bootstrapping approaches like selftraining (McClosky et al._comma_ 2006) to feature-based enhancements of discriminative reranking models (Koo et al._comma_ 2008) and the application of semisupervised SVMs (Wang et al._comma_ 2008).
1058,1,Such approaches have shown promise in applications such as web page classification (Blum and Mitchell_comma_ 1998)_comma_ named entity classification (Collins and Singer_comma_ 1999)_comma_ parsing (McClosky et al._comma_ 2006)_comma_ and machine translation (Ueffing_comma_ 2006).
1059,0,The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) and Self-training (Abney_comma_ 2007; McClosky et al._comma_ 2006).
1060,0,A preliminary evaluation favors the use of SCL over the simpler self-training techniques.
1061,0,1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems (Hara et al._comma_ 2005; van Noord and Malouf_comma_ 2005; McClosky et al._comma_ 2006).
1062,0,We examine Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) for this task_comma_ and compare it to several variants of Self-training (Abney_comma_ 2007; McClosky et al._comma_ 2006).
1063,0,In this paper_comma_ we extend that line of work and compare SCL to bootstrapping approaches such as self-training.
1064,0,Studies on self-training have focused mainly on generative_comma_ constituent based parsing (Steedman et al._comma_ 2003; McClosky et al._comma_ 2006; Reichart and Rappoport_comma_ 2007).
1065,0,In contrast_comma_ McClosky et al.(2006) focus on large seeds and exploit a reranking-parser.
1066,1,Improvements are obtained (McClosky et al._comma_ 2006; McClosky and Charniak_comma_ 2008)_comma_ showing that a reranker is necessary for successful self-training in such a high-resource scenario.
1067,0,While they self-trained a generative model_comma_ we examine self-training and SCL for semi-supervised adaptation of a discriminative parse selection system.
1068,0,5 Conclusions and Future Work The paper compares Structural Correspondence Learning (Blitzer et al._comma_ 2006) with (various instances of) self-training (Abney_comma_ 2007; McClosky et al._comma_ 2006) for the adaptation of a parse selection model to Wikipedia domains.
1069,0,The empirical findings show that none of the evaluated self-training variants (delible/indelible_comma_ single versus multiple iterations_comma_ various selection techniques) achieves a significant improvement over the baseline.
1070,0,The more indirect exploitation of unlabeled data through SCL is more fruitful than pure self-training.
1071,1,Dirichlet priors can be used to bias HMMs toward more skewed distributions (Goldwater and Griffiths_comma_ 2007; Johnson_comma_ 2007)_comma_ which is especially useful in the weakly supervised setting consideredhere.
1072,0,FollowingJohnson(2007)_comma_Iusevariational Bayes EM (Beal_comma_ 2003) during the M-step for the transition distribution: l+1j|i = f(E[ni_comma_j] +i)f(E[n i] + |C|i) (3) f(v) = exp((v)) (4) 60 (v) = braceleftBigg g(v 1 2) ifv> 7 (v+ 1)  1v o.w.
1073,0,Other work aims to do truly unsupervised learning of taggers_comma_ such as Goldwater and Griffiths (2007) and Johnson (2007).
1074,0,No tag dictionaries areassumed_comma_ andthemodelsareparametrizedwith Dirichlet priors.
1075,0,The states of these models implicitly represent tags; however_comma_ it actually is not clear whatthestatesinsuchmodelstrulyrepresent: they are (probably interesting) clusters that may or may not correspond to what we normally think of as parts-of-speech.
1076,0,It would thus be interesting to considertheinductionofcategorieswithgrammarbased priors with such models.
1077,0,1 Introduction There has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text_comma_ both parts-of-speech (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007; Biemann_comma_ 2006; Dasgupta and Ng_comma_ 2007) and deeper grammatical structure like constituency and dependency trees (Klein and Manning_comma_ 2004; Smith_comma_ 2006; Bod_comma_ 2006; Seginer_comma_ 2007; Van Zaanen_comma_ 2001).
1078,1,For an HMM with a set of states T and a set of output symbols V : t  T t  Dir(1_comma_|T|) (1) t  T t  Dir(1_comma_|V |) (2) ti|ti1_comma_ ti1  Multi(ti1) (3) wi|ti_comma_ ti  Multi(ti) (4) One advantage of the Bayesian approach is that the prior allows us to bias learning toward sparser structures_comma_ by setting the Dirichlet hyperparameters _comma_ to a value less than one (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007).
1079,1,There is evidence that this leads to better performance on some part-of-speech induction metrics (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007).
1080,0,Johnson (2007) evaluates both estimation techniques on the Bayesian bitag model; Goldwater and Griffiths (2007) emphasize the advantage in the MCMC approach of integrating out the HMM parameters in a tritag model_comma_ yielding a tagging supported by many different parameter settings.
1081,0,Following the setup in Johnson (2007)_comma_ we initialize the transition and emission distributions to be uniform with a small amount of noise_comma_ and run EM and VB for 1000 iterations.
1082,0,In our VB experiments we set i = j = 0.1_comma_i  {1_comma__comma_|T|}_comma_j  {1_comma__comma_|V |}_comma_ which yielded the best performance on most reported metrics in Johnson (2007).
1083,1,We use maximum marginal decoding_comma_ which Johnson (2007) reports performs better than Viterbi decoding.
1084,0,One option is what Johnson (2007) calls many-to-one (M-to-1) accuracy_comma_ in which each induced tag is labeled with its most frequent gold tag.
1085,1,Although this results in a situation where multiple induced tags may share a single gold tag_comma_ it does not punish a system for providing tags of a finer granularity than the gold standard.
1086,0,In cases where the number of gold tags is different than the number of induced tags_comma_ some must necessarily remain unassigned (Johnson_comma_ 2007).
1087,0,Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths_comma_ 2007; Johnson_comma_ 2007).
1088,0,These methods can also be used to approximate other distributions that are important to us_comma_ such as the conditional distribution P(t | w) of POS tags (i.e._comma_ HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods.
1089,0,Johnson (2007) compared two Bayesian inference algorithms_comma_ Variational Bayes and what we call here a point-wise collapsed Gibbs sampler_comma_ and found that Variational Bayes produced the best solution_comma_ and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM.
1090,0,On the other hand_comma_ Goldwater and Griffiths (2007) reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task.
1091,0,We replicate the results of both papers and show that the difference in their results stems from differences in the sizes of the training data and numbers of states in their models.
1092,0,It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.
1093,0,The difference between explicit and collapsed samplers corresponds exactly to the difference between the two PCFG sampling algorithms presented in Johnson et al.(2007).
1094,0,t | nt_comma_  Dir(nt +) t | nprimet_comma_prime  Dir(nprimet +prime) (5) P(ti | wi_comma_ti_comma__comma_)  ti|ti1wi|titi+1|ti (6) The Dirichlet distributions in (5) are non-uniform; nt is the vector of state-to-state transition counts in t leaving state t in the current state vector t_comma_ while 347 nprimet is the vector of state-to-word emission counts for state t. See Johnson et al.(2007) for a more detailed explanation_comma_ as well as an algorithm for sampling from the Dirichlet distributions in (5).
1095,0,The samplers that Goldwater and Griffiths (2007) and Johnson (2007) describe are pointwise collapsed Gibbs samplers.
1096,0,As Johnson et al.(2007) explains_comma_ samples of the HMM parameters  and  can be obtained using (5) if required.
1097,0,The collapsed blocked Gibbs sampler is a straight-forward application of the Metropoliswithin-Gibbs approach proposed by Johnson et al.(2007) for PCFGs_comma_ so we only sketch it here.
1098,0,Finally_comma_ following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.
1099,0,Following these authors_comma_ we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 348 All50 All17 120K50 120K17 24K50 24K17 EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165 VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599 GSe_comma_p 0.47826 0.43424 0.36984 0.44125 0.29953 0.36811 GSe_comma_b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032 GSc_comma_p 0.49910star 0.45028 0.42785 0.43652 0.39182 0.39164 GSc_comma_b 0.49486star 0.46193 0.41162 0.42278 0.38497 0.36793 Figure 2: Average greedy 1-to-1 accuracy of state sequences produced by HMMs estimated by the various estimators.
1100,0,The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used.
1101,0,Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005)_comma_ while Johnson (2007) evaluated against the full Penn Treebank tag set.
1102,0,Also_comma_ the studies differed in the size of the corpora used.
1103,0,The largest corpus that Goldwater and Griffiths (2007) studied contained 96_comma_000 words_comma_ while Johnson (2007) used all of the 1_comma_173_comma_766 words in the full Penn WSJ treebank.
1104,0,We ran each estimator with the eight different combinations of values for the hyperparameters  and prime listed below_comma_ which include the optimal values for the hyperparameters found by Johnson (2007)_comma_ and report results for the best combination for each estimator below 1.
1105,0,prime 1 1 1 0.5 0.5 1 0.5 0.5 0.1 0.1 0.1 0.0001 0.0001 0.1 0.0001 0.0001 Further_comma_ we ran each setting of each estimator at least 10 times (from randomly jittered initial starting points) for at least 1_comma_000 iterations_comma_ as Johnson (2007) showed that some estimators require many iterations to converge.
1106,1,Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy_comma_ confirming the results reported by Johnson (2007).
1107,0,Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007).
1108,0,3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging.
1109,0,However_comma_ in experiments in unsupervised POS tag learning using HMM structured models_comma_ Johnson (2007) shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipfs law_comma_ which is prominent in natural languages.
1110,0,As pointed out by Johnson (2007)_comma_ in effect this expression adds to c a small value that asymptotically approaches  0.5 as c approaches _comma_ and 0 as c approaches 0.
1111,0,Unlike Johnson (2007)_comma_ who found optimal performance when  was approximately 104_comma_ we observed monotonic increases in performance as  dropped.
1112,0,Bayesian approaches can also improve performance (Goldwater and Griffiths_comma_ 2007; Johnson_comma_ 2007; Kurihara and Sato_comma_ 2006).
1113,0,Though these methods have improved induction accuracy_comma_ at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model_comma_ and thus are not completely immune to the difficulties associated with early approaches.
1114,1,Recent work (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007; Gao and Johnson_comma_ 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results.
1115,0,Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags.
1116,0,Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori_comma_ while Goldwater & Griffiths (2007) leave this question as future work.
1117,1,Given the parameters{pi0_comma_pi_comma__comma_K}of the HMM_comma_ the joint distribution over hidden states s and observationsy can be written (with s0 = 0): p(s_comma_y|pi0_comma_pi_comma__comma_K) = Tproductdisplay t=1 p(st|st1)p(yt|st) As Johnson (2007) clearly explained_comma_ training the HMM with EM leads to poor results in PoS tagging.
1118,0,Recent work (Goldwater and Griffiths_comma_ 2007; Johnson_comma_ 2007; Gao and Johnson_comma_ 2008) on this task explored a variety of methodologies to address this issue.
1119,0,The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwater & Griffiths (2007) versus Johnson (2007)) supports this claim.
1120,0,4.1 Variational Bayes Beal (2003) and Johnson (2007) describe variational Bayes for hidden Markov model in detail_comma_ which can be directly applied to our bilingual model.
1121,0,WiththisBayesianextension_comma_theemission probability of our first model can be summarized as follows: e | Dir()_comma_ fi |ei = e Multi(e).
1122,0,Johnson (2007) and Zhang et al.(2008a) show having small  helps to control overfitting.
1123,0,Importantly_comma_ this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson_comma_ 2007).
1124,0,Similar to Goldwater and Griffiths (2007) and Johnson (2007)_comma_ Toutanova and Johnson (2007) also use Bayesian inference for POS tagging.
1125,0,Nevertheless_comma_ EM sometimes fails to find good parameter values.2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states (Johnson_comma_ 2007).
1126,0,There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings_comma_ ranging from putting priors over grammar probabilities (Johnson et al._comma_ 2007) to putting non-parametric priors over derivations (Johnson et al._comma_ 2006) to learning the set of states in a grammar (Finkel et al._comma_ 2007; Liang et al._comma_ 2007).
1127,0,Mostcommonlyvariational (Johnson_comma_ 2007; Kurihara and Sato_comma_ 2006) or sampling techniques are applied (Johnson et al._comma_ 2006).
1128,0,For example_comma_ if we make a mean-field assumption_comma_ with respect to hidden structure and weights_comma_ the variationalalgorithmforapproximatelyinferringthe distribution over  and trees y resembles the traditional EM algorithm very closely (Johnson_comma_ 2007).
1129,0,For instance_comma_ on unsupervised part-ofspeech tagging_comma_ EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson_comma_ 2007).
1130,0,HMMs have been used many times for POS tagging and chunking_comma_ in supervised_comma_ semisupervised_comma_ and in unsupervised settings (Banko and Moore_comma_ 2004; Goldwater and Griffiths_comma_ 2007; Johnson_comma_ 2007; Zhou_comma_ 2004).
1131,0,The overall POS tag distribution learnt by EM is relatively uniform_comma_ as noted by Johnson (2007)_comma_ and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed.
1132,0,In order to build models that perform well in new (target) domains we usually find two settings (Daume III_comma_ 2007): In the semi-supervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain_comma_ and the baseline is that of the system c2008.
1133,1,In the supervised setting_comma_ a recent paper by Daume III (2007) shows that_comma_ using a very simple feature augmentation method coupled with Support Vector Machines_comma_ he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks.
1134,0,There are two tasks(Daume III_comma_ 2007) for the domain adaptation problem.
1135,0,This is comparable to the accuracy of 96.29% reported by (Daume III_comma_ 2007) on the newswire domain.
1136,0,Daumes work is the only study known to us that uses the ACE dataset_comma_ but not the proprietary ACE value score.
1137,0,For research involving training and eval1005  55  60  65  70  75  80  85  90  95  100 02 02-03 02-04 02-05 02-06 02-07 02-08 02-09 02-10 02-12 02-14 02-21 WSD accuracy (%) Section number WSD Accuracies on Section 23 59.2 76.8 77.5 60.5 77.1 77.5 64.4 77.1 77.6 73.3 78.9 80.3 76.8 79.3 80.9 80.2 79.9 82.1 80.5 80.5 82.6 81.6 80.8 83.1 85.8 83.3 85.6 87.5 86.1 87.6 88.3 87.2 88.7 89.1 87.9 88.9 ON SC+ON SC+ON Augment Figure 1: WSD accuracies evaluated on section 23_comma_ using SEMCOR and different OntoNotes sections as training data.
1138,0,SC+ON: SEMCOR and OntoNotes as training data_comma_ SC+ON Augment: Combining SEMCOR and OntoNotes via the Augment domain adaptation technique.
1139,0,In particular_comma_ we use a feature augmentation technique recently introduced by Daume III (2007)_comma_ and active learning (Lewis and Gale_comma_ 1994) to perform domain adaptation of WSD systems.
1140,0,5 Combining In-Domain and Out-of-Domain Data for Training In this section_comma_ we will first introduce the AUGMENT technique of Daume III (2007)_comma_ before showing the performance of our WSD system with and without using this technique.
1141,1,5.1 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by Daume III (2007) is a simple yet very effective approach to performing domain adaptation.
1142,0,This technique is applicable when one has access to training data from the source domain and a small amount of training data from the target domain.
1143,0,The technique essentially augments the feature space of an instance.
1144,0,We see that the technique essentially treats the first part of the augmented feature space as holding general features that are not meant to be differentiated between different 1006 domains.
1145,1,Despite its relative simplicity_comma_ this AUGMENT technique has been shown to outperform other domain adaptation techniques on various tasks such as named entity recognition_comma_ part-of-speech tagging_comma_ etc. 5.2 Experimental Results As mentioned in Section 4_comma_ training our WSD system on SEMCOR examples gave a relatively low accuracy of 76.2%_comma_ as compared to the 89.1% accuracy obtained from training on the OntoNotes section 0221 examples.
1146,0,We also performed another set of experiments_comma_ where instead of simply combining the SEMCOR and OntoNotes examples_comma_ we applied the AUGMENT technique when combining these examples_comma_ treating SEMCOR examples as out-of-domain (source domain) data and OntoNotes examples as in-domain (target domain) data.
1147,0,We similarly show the resulting accuracies as SC+ON Augment in Figure 1.
1148,0,Comparing the SC+ON and SC+ON Augment accuracies in Figure 1_comma_ we see that the AUGMENT technique always helps to improve the accuracy of our WSD system.
1149,0,Further_comma_ notice from the first few sets of results in the figure that when we have access to limited in-domain training examples from OntoNotes_comma_ incorporating additional outof-domain training data from SEMCOR (either using the strategies SC+ON or SC+ON Augment) achieves better accuracies than ON.
1150,0,The right half of Figure 1 shows the accuracy trend of the various strategies_comma_ in the unlikely event DSthe set of SEMCOR training examples DAthe set of OntoNotes sections 02-21 examples DT empty while DAnegationslash=  pmin WSD system trained on DS and DT using AUGMENT technique for each dDA do bsword sense prediction for d using  pconfidence of prediction bs if p < pmin then pminp_comma_ dmind end end DADA{dmin} provide correct sense s for dmin and add dmin to DT end Figure 2: The active learning algorithm.
1151,1,Although we observe that in this scenario_comma_ ON performs better than SC+ON_comma_ SC+ON Augment continues to perform better than ON (where the improvement is statistically significant) till the result for sections 02-09.
1152,0,Beyond that_comma_ as we add more OntoNotes examples_comma_ significance testing reveals that the SC+ON Augment and ON strategies give comparable performance.
1153,1,This means that the SC+ON Augment strategy_comma_ besides giving good performance when one has few in-domain examples_comma_ does continue to perform well even when one has a large number of in-domain examples.
1154,1,6 Active Learning with AUGMENT Technique So far in this paper_comma_ we have seen that when we have access to some in-domain examples_comma_ a good strategy is to combine the out-of-domain and in-domain examples via the AUGMENT technique.
1155,0,1007 WSD Accuracies on Section 23 76 78 80 82 84 86 88 90 SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Iteration Number W SD  A cc ur ac y ( %) 50 100 150 200 300 400 500 all Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types.
1156,1,Also_comma_ since we have found that the AUGMENT technique is useful in increasing WSD accuracy_comma_ we will apply the AUGMENT technique during each iteration of active learning to combine the SEMCOR examples and the selected adaptation examples.
1157,0,We then use the AUGMENT technique to combine the set of examples in DS and DT to train a new WSD system_comma_ which is then applied again on the set DA of remaining adaptation examples_comma_ and this active learning process continues until we have used up all the adaptation examples.
1158,1,Based on these results_comma_ we propose that when there is a need to apply a previously trained WSD system to a different domain_comma_ one can apply the AUGMENT technique with active learning on the most frequent word types_comma_ to greatly reduce the annotation effort required while obtaining a substantial improvement in accuracy.
1159,1,We show that by combining the AUGMENT domain adaptation technique with active learning_comma_ we are able to effectively reduce the amount of annotation effort required for domain adaptation.
1160,0,There are many possible methods for combining unlabeled and labeled data (Daume III_comma_ 2007)_comma_ but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses.
1161,0,Other techniques have tried to quantify the generalizability of certain features across domains (Daume III and Marcu_comma_ 2006; Jiang and Zhai_comma_ 2006)_comma_ or tried to exploit the common structure of related problems (Ben-David et al._comma_ 2007; Scholkopf et al._comma_ 2005).
1162,0,Daume allows an extra degree of freedom among the features of his domains_comma_ implicitly creating a two-level feature hierarchy with one branch for general features_comma_ and another for domain specific ones_comma_ but does not extend his hierarchy further (Daume III_comma_ 2007)).
1163,0,Many adaptation methods operate by simple augmentations of the target feature space_comma_ as we have donehere(DaumeIII_comma_2007).
1164,0,Previous work in domain adaptation can be classified into two categories: [S+T+]_comma_ where a small_comma_ labeled target domain data is available_comma_ e.g.(Blitzer et al._comma_ 2006; Jiang and Zhai_comma_ 2007; Daume III_comma_ 2007; Finkel and Manning_comma_ 2009)_comma_ or [S+T-]_comma_ where no labeled target domain data is available_comma_ e.g.(Blitzer et al._comma_ 2006; Jiang and Zhai_comma_ 2007).
1165,0,In both cases_comma_ and especially for [S+T-]_comma_ domain adaptation can leverage on large amounts of unlabeled data in the target domain.
1166,0,Daume III (Daume III_comma_ 2007) divided features into three classes: domainindependent features_comma_ source-domain features and target-domain features.
1167,0,He assumed the existence of training data in the target-domain (under the setting [S+T+])_comma_ so that the three classes of features can be jointly trained using source and target domain labeled data.
1168,0,In order to build models that perform well in new (target) domains we usually find two settings (Daume III_comma_ 2007).
1169,1,In the supervised setting_comma_ a recent paper by Daume III (2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks.
1170,1,His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu_comma_ 2006; Chelba and Acero_comma_ 2004).
1171,0,The last row shows the results for the feature augmentation algorithm (Daume III_comma_ 2007).
1172,0,Therefore_comma_ whenever we have access to a large amount of labeled data from some source (out-of-domain)_comma_ but we would like a model that performs well on some new target domain (Gildea_comma_ 2001; Daume III_comma_ 2007)_comma_ we face the problem of domain adaptation.
1173,0,The problem itself has started to get attention only recently (Roark and Bacchiani_comma_ 2003; Hara et al._comma_ 2005; Daume III and Marcu_comma_ 2006; Daume III_comma_ 2007; Blitzer et al._comma_ 2006; McClosky et al._comma_ 2006; Dredze et al._comma_ 2007).
1174,0,We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daume III_comma_ 2007): supervised and semi-supervised.
1175,0,In supervised domain adaptation (Gildea_comma_ 2001; Roark and Bacchiani_comma_ 2003; Hara et al._comma_ 2005; Daume III_comma_ 2007)_comma_ besides the labeled source data_comma_ we have access to a comparably small_comma_ but labeled amount of target data.
1176,1,Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only_comma_ target only_comma_ or the union of the data) achieve a relatively high performance level and are surprisingly difficult to beat (Daume III_comma_ 2007).
1177,1,Thus_comma_ one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data_comma_ it is often more fruitful to either just use that_comma_ or to apply simple adaptation techniques (Daume III_comma_ 2007; Plank and van Noord_comma_ 2008).
1178,0,Daume III (2007) further augments the feature space on the instances of both domains.
1179,0,We show that the method of (Daume III_comma_ 2007)_comma_ which was presented as a simple preprocessing step_comma_ is actually equivalent_comma_ except our representation explicitly separates hyperparameters which were tied in his work.
1180,0,We demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and (Daume III_comma_ 2007) within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser.
1181,0,We also show that the domain adaptation work of (Daume III_comma_ 2007)_comma_ which is presented as an ad-hoc preprocessing step_comma_ is actually equivalent to our formal model.
1182,0,However_comma_ our representation of the model conceptually separates some of the hyperparameters which are not separated in (Daume III_comma_ 2007)_comma_ and we found that setting these hyperparameters with different values from one another was critical for improving performance.
1183,0,2.4 Formalization of (Daume III_comma_ 2007) As mentioned earlier_comma_ our model is equivalent to that presented in (Daume III_comma_ 2007)_comma_ and can be viewed as a formal version of his model.2 In his presentation_comma_ the adapation is done through feature augmentation.
1184,0,Recall that the log likelihood of our model is:  d parenleftBigg Lorig(Dd;d) i (d_comma_i _comma_i)2 2 2d parenrightBigg  i (_comma_i)2 2 2 We now introduce a new variable d = d _comma_ and plug it into the equation for log likelihood:  d parenleftBigg Lorig(Dd;d +) i (d_comma_i)2 2 2d parenrightBigg  i (_comma_i)2 2 2 The result is the model of (Daume III_comma_ 2007)_comma_ where the d are the domain-specific feature weights_comma_ and d are the domain-independent feature weights.
1185,0,In his formulation_comma_ the variances  2d =  2 for all domains d. This separation of the domain-specific and independent variances was critical to our improved performance.
1186,0,Because Daume III (2007) views the adaptation as merely augmenting the feature space_comma_ each of his features has the same prior mean and variance_comma_ regardless of whether it is domain specific or independent.
1187,0,He could have set these parameters differently_comma_ but he did not.3 In our presentation of the model_comma_ we explicitly represent different variances for each domain_comma_ as well as the top level parameters.
1188,0,Trained and tested using the same technique as (Daume III_comma_ 2007).
1189,0,5 Related Work We already discussed the relation of our work to (Daume III_comma_ 2007) in Section 2.4.
1190,0,Unlike our technique_comma_ in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g._comma_ (Daume III_comma_ 2007; Chelba and Acero_comma_ 2004; Daume III and Marcu_comma_ 2006)).
1191,0,The second problem_comma_ domain adaptation_comma_ is very well-studied_comma_ e.g. by Blitzer et al.(2006) and Daume III (2007) (and see below for discussions)_comma_ so in this paper we focus on the less studied_comma_ but equally important problem of annotationstyle adaptation.
1192,0,This method is very similar to some ideas in domain adaptation (Daume III and Marcu_comma_ 2006; Daume III_comma_ 2007)_comma_ but we argue that the underlying problems are quite different.
1193,0,Domain adaptation assumes the labeling guidelines are preserved between the two domains_comma_ e.g._comma_ an adjective is always labeled as JJ regardless of from Wall Street Journal (WSJ) or Biomedical texts_comma_ and only the distributions are different_comma_ e.g._comma_ the word control is most likely a verb in WSJ but often a noun in Biomedical texts (as in control experiment).
1194,0,ald_comma_ 2008)_comma_ and is also similar to the Pred baseline for domain adaptation in (Daume III and Marcu_comma_ 2006; Daume III_comma_ 2007).
1195,0,For example_comma_ (Daume III_comma_ 2007) shows that training a learning algorithm on the weighted union of different data sets (which is basically what we did) performs almost as well as more involved domain adaptation approaches.
1196,0,While transfer learning was proposed more than a decade ago (Thrun_comma_ 1996; Caruana_comma_ 1997)_comma_ its application in natural language processing is still a relatively new territory (Blitzer et al._comma_ 2006; Daume III_comma_ 2007; Jiang and Zhai_comma_ 2007a; Arnold et al._comma_ 2008; Dredze and Crammer_comma_ 2008)_comma_ and its application in relation extraction is still unexplored.
1197,0,Daume III (2007) proposed a simple feature augmentation method to achieve domain adaptation.
1198,0,The model presented above is based on our previous work (Jiang and Zhai_comma_ 2007c)_comma_ which bears the same spirit of some other recent work on multitask learning (Ando and Zhang_comma_ 2005; Evgeniou and Pontil_comma_ 2004; Daume III_comma_ 2007).
1199,0,Also_comma_ the aspect of generalizing features across different products is closely related to fully supervised domain adaptation (Daume III_comma_ 2007)_comma_ and we plan to combine our approach with the idea from Daume III (2007) to gain insights into whether the composite back-off features exhibit different behavior in domain-general versus domain-specific feature sub-spaces.
1200,0,1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts_comma_ as attested in the last Senseval and Semeval competitions (Kilgarriff_comma_ 2001; Mihalcea et al._comma_ 2004; Pradhan et al._comma_ 2007).
1201,1,For instance_comma_ (Daume III_comma_ 2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks.
1202,1,His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu_comma_ 2006; Chelba and Acero_comma_ 2004).
1203,1,Nonetheless_comma_ such global properties can improve the accuracy of a model_comma_ so recent NLP papers have considered practical techniques for decoding with them.
1204,0,Such techniques include Gibbs sampling (Finkel et al. _comma_ 2005)_comma_ a general-purpose Monte Carlo method_comma_ and integer linear programming (ILP)_comma_ (Roth and Yih_comma_ 2005)_comma_ a general-purpose exact framework for NP-complete problems.
1205,0,That is a significant shortcoming_comma_ because in many domains_comma_ hard or soft global constraints on the label sequence are motivated by common sense:  For named entity recognition_comma_ a phrase that appears multiple times should tend to get the same label each time (Finkel et al. _comma_ 2005).
1206,0,should appear with at most one value in each announcement_comma_ although the field and value may be repeated (Finkel et al. _comma_ 2005).
1207,0,Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al. _comma_ 1997)_comma_ the dictionary HMM model (Kou et al. _comma_ 2005) and Maximum Entropy Markov Models (MEMMs) (Finkel et al. _comma_ 2004).
1208,0,information about the previous state (Finkel et al. _comma_ 2005).
1209,0,In a recent study by Finkel et al. _comma_ (2005)_comma_ nonlocal information is encoded using an independence model_comma_ and the inference is performed by Gibbs sampling_comma_ which enables us to use a stateof-the-art factored model and carry out training efficiently_comma_ but inference still incurs a considerable computational cost.
1210,1,However_comma_ other types of nonlocal information have also been shown to be effective (Finkel et al. _comma_ 2005) and we will examine the effectiveness of other non-local information which can be embedded into label information.
1211,1,Global information is known to be useful in other NLP tasks_comma_ especially in the named entity recognition task_comma_ and several studies successfully used global features (Chieu and Ng_comma_ 2002; Finkel et al. _comma_ 2005).
1212,0,Finkel et al.(2005) used simulated annealing with Gibbs sampling to find a solution in a similar situation.
1213,0,Unlike simulated annealing_comma_ this approach does not need to define a cooling 707 schedule.
1214,0,We also conduct experiments using simulated annealing in decoding_comma_ as conducted by Finkel et al.(2005) for information extraction.
1215,0,Finkel et al.(2005) proposed a method incorporating non-local structure for information extraction.
1216,0,They attempted to use label consistency of named entities_comma_ which is the property that named entities with the same lexical form tend to have the same label.
1217,0,They defined two probabilistic models; a local model based on conditional random fields and a global model based on loglinear models.
1218,0,In their method_comma_ interactions between labels in the whole document were considered_comma_ and they used Gibbs sampling and simulated annealing for decoding.
1219,0,Our model is largely similar to their model.
1220,0,However_comma_ in their method_comma_ parameters of the global model were estimated using relative frequencies of labels or were selected by hand_comma_ while in our method_comma_ global model parameters are estimated from training data so as to fit to the data according to the objective function.
1221,0,Finkel et al.(2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text_comma_ based on some statistics from training data.
1222,0,They then employ Gibbs sampling (Geman and Geman_comma_ 1984) for dealing with their local feature weights and their non-local penalties to do approximate inference.
1223,0,This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRFs; in contrast Finkel et al.(2005) reported an increase in running time by a factor of 30 over the sequential CRF_comma_ with their Gibbs sampling approximate inference.
1224,0,A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily_comma_ a newspaper_comma_ and China_comma_ the country (Finkel et al. _comma_ 2005).
1225,0,At the same time_comma_ the simplicity of our two-stage approach keeps inference time down to just the inference time of two sequential CRFs_comma_ when compared to approaches such as those of Finkel et al.(2005) who report that their inference time with Gibbs sampling goes up by a factor of about 30_comma_ compared to the Viterbi algorithm for the sequential CRF.
1226,0,Most existing work to capture labelconsistency_comma_ has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity_comma_ (Finkel et al. _comma_ 2005; Sutton and McCallum_comma_ 2004)_comma_ where n is the number of occurrences of the given entity.
1227,0,Most work has looked to model non-local dependencies only within a document (Finkel 1125 et al. _comma_ 2005; Chieu and Ng_comma_ 2002; Sutton and McCallum_comma_ 2004; Bunescu and Mooney_comma_ 2004).
1228,0,Our model can capture the weaker but still important consistency constraints across the whole document collection_comma_ whereas previous work has not_comma_ for reasons of tractability.
1229,0,The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus_comma_ which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney_comma_ 2004) and (Finkel et al. _comma_ 2005).
1230,0,Additionally_comma_ our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast_comma_ approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 (Finkel et al. _comma_ 2005).
1231,0,Our approach helps correct many such errors based on occurrences of the token in other 1126 F1 scores on the CoNLL Dataset Approach LOC ORG MISC PER ALL Relative Error reduction Bunescu and Mooney (2004) (Relational Markov Networks) Only Local Templates 80.09 Global and Local Templates 82.30 11.1% Finkel et al.(2005)(Gibbs Sampling) Local+Viterbi 88.16 80.83 78.51 90.36 85.51 Non Local+Gibbs 88.51 81.72 80.43 92.29 86.86 9.3% Our Approach with the 2-stage CRF Baseline CRF 88.09 80.88 78.26 89.76 85.29 + Document token-majority features 89.17 80.15 78.73 91.60 86.50 + Document entity-majority features 89.50 81.98 79.38 91.74 86.75 + Document superentity-majority features 89.52 82.27 79.76 92.71 87.15 12.6% + Corpus token-majority features 89.48 82.36 79.59 92.65 87.13 + Corpus entity-majority features 89.72 82.40 79.71 92.65 87.23 + Corpus superentity-majority features (All features) 89.80 82.39 79.76 92.57 87.24 13.3% Table 3: Table showing improvements obtained with our additional features_comma_ over the baseline CRF.
1232,0,We also compare our performance against (Bunescu and Mooney_comma_ 2004) and (Finkel et al. _comma_ 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF.
1233,0,7 Related Work Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004)_comma_ who use a Relational Markov Network (RMN) (Taskar et al. _comma_ 2002) to explicitly model long-distance dependencies_comma_ Sutton and McCallum (2004)_comma_ who introduce skip-chain CRFs_comma_ which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al.(2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty.
1234,0,The approach of Finkel et al.(2005) makes it possible a to model a broader class of longdistance dependencies than Sutton and McCallum (2004)_comma_ because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities.
1235,0,The disadvantage of their approach is the relatively ad-hoc selection of penalties and the high computational cost of running Gibbs sampling.
1236,0,An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (Sutton and McCallum_comma_ 2004; Finkel et al. _comma_ 2005).
1237,0,However_comma_ this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency.
1238,0,Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques_comma_ including TRP (Sutton and McCallum_comma_ 2004) and Gibbs sampling (Finkel et al. _comma_ 2005).
1239,0,4 Relation to Previous Work There is a significant volume of work exploring the use of CRFs for a variety of chunking tasks_comma_ including named-entity recognition_comma_ gene prediction_comma_ shallow parsing and others (Finkel et al. _comma_ 2005; Culotta et al. _comma_ 2005; Sha and Pereira_comma_ 2003).
1240,0,The current work indicates that these systems might be improved by moving to a semi-CRF model.
1241,0,For example_comma_ non-local features such as same phrases in a document do not have different entity classes were shown to be useful in named entity recognition (Sutton and McCallum_comma_ 2004; Bunescu and Mooney_comma_ 2004; Finkel et al. _comma_ 2005; Krishnan and Manning_comma_ 2006).
1242,0,Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum_comma_ 2004; Bunescu and Mooney_comma_ 2004; Finkel et al. _comma_ 2005; Roth and Yih_comma_ 2005; Krishnan and Manning_comma_ 2006; Nakagawa and Matsumoto_comma_ 2006)_comma_ these present a problem that the types of non-local features are somewhat constrained.
1243,0,For example_comma_ Finkel et al.(2005) enabled the use of non-local features by using Gibbs sampling.
1244,0,However_comma_ it is unclear how to apply their methodofdeterminingtheparametersofanon-local model to other types of non-local features_comma_ which they did not used.
1245,0,We used non-local features based on Finkel et al.(2005).
1246,0,Thistype of non-local feature was not used by Finkel et al.(2005) or Krishnan and Manning (2006).
1247,0,The performance of the related work (Finkel et al. _comma_ 2005; Krishnan and Manning_comma_ 2006) is listed in Table4.
1248,1,Wecanseethatthenalperformanceofour algorithm was worse than that of the related work.
1249,0,Method dev test Finkel et al. _comma_ 2005 (Finkel et al. _comma_ 2005) baseline CRF 85.51 + non-local features 86.86 Krishnan and Manning_comma_ 2006 (Krishnan and Manning_comma_ 2006) baseline CRF 85.29 + non-local features 87.24 Table 5: Summary of performance with POS/chunk tags by TagChunk.
1250,1,The resulting performance of the proposed algorithm with non-local features is higher than that of Finkel et al.(2005) and comparable with that of Krishnan and Manning (2006).
1251,1,However_comma_ the achieved accuracy was not better than that of related work (Finkel et al. _comma_ 2005; Krishnan and Manning_comma_ 2006) based on CRFs.
1252,0,We extract the named entities from the web pages using the Stanford Named Entity Recognizer (Finkel et al. _comma_ 2005).
1253,0,This tagger identifies and labels names of places_comma_ organizations and people in the input.
1254,0,In this paper_comma_ Stanford Named Entity Recognizer (Finkel et al. 2005) is used to classify noun phrases into four semantic categories: PERSON_comma_ LOCATION_comma_ ORGANIZARION and MISC.
1255,0,The reason is that the named entity recognizer contains only four semantic categories.
1256,0,It is too coarse to distinguish 50 fined-categories.
1257,0,There are two methods to use non-local information.
1258,0,One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features.
1259,0,In the first approach_comma_ heuristic rules are used to find the dependencies (Bunescu and Mooney_comma_ 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al._comma_ 2005).
1260,0,Furthermore_comma_ high computational cost is spent for approximate inference.
1261,0,Corpus Time Period Size Articles Words New Indian Express (English) 2007.01.01 to 2007.08.31 2_comma_359 347_comma_050 Dinamani (Tamil) 2007.01.01 to 2007.08.31 2_comma_359 256_comma_456 Table 1: Statistics on Comparable Corpora  From the above corpora_comma_ we first extracted all the NEs from the English side_comma_ using the Stanford NER tool [Finkel et al_comma_ 2005].
1262,0,Starting out with a chunking pipeline_comma_ which uses a classical combination of tagger and chunker_comma_ with the Stanford POS tagger (Toutanova et al._comma_ 2003)_comma_ the YamCha chunker (Kudoh and Matsumoto_comma_ 2000) and the Stanford Named Entity Recognizer (Finkel et al._comma_ 2005)_comma_ the desire to use richer syntactic representations led to the development of a parsing pipeline_comma_ which uses Charniak and Johnsons reranking parser (Charniak and Johnson_comma_ 2005) to assign POS tags and uses base NPs as chunk equivalents_comma_ while also providing syntactic trees that can be used by feature extractors.
1263,0,Finkel et al.(2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents.
1264,1,The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al._comma_ 2005).
1265,0,It is expected that a fine grained named entity recognizer (NER) should make good use of the accurate question type information.
1266,0,However_comma_ due to the lack of a fine grained NER tool at hand_comma_ we employ the Stanford NER package (Finkel et al._comma_ 2005) which identifies only four types of named entities.
1267,0,Even with such a coarse named entity recognizer_comma_ the experiments show that the question classifier plays an important role in determining the performance of a question answering system.
1268,0,5.1 CoNLL named entities presence feature We use Stanford named entity recognizer (NER) (Finkel et al._comma_ 2005) to identify CoNLL style NEs7 as possible answer strings in a candidate sentence for a given type of question.
1269,0,One can imagine that if a fine grained NER is available (rather than the current four type coarse NER)_comma_ the potential gain is much significant.
1270,0,Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al._comma_ 2005).
1271,0,As discussed above_comma_ all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al._comma_ 2001; Sha and Pereira_comma_ 2003; Finkel et al._comma_ 2005; Ratinov and Roth_comma_ 2009).
1272,0,Instead_comma_ we opt to utilize the Stanford NER tagger (Finkel et al._comma_ 2005) over the sentences in a document and annotate each NP with the NER label assigned to that mention head.
1273,0,4.1 NER features We used the features generated by the CRF package (Finkel et al._comma_ 2005).
1274,0,The different NER systems that we tested are the following ones:  CBC-NER system M (in short CBC M) based on the CBC systems NE resource using the manual cluster annotation (line 1 in Table 1)_comma_  CBC-NER system A (in short CBC A) based on the CBC systems NE resource using the automatic cluster annotation (line 1 in Table 1)_comma_  XIP NER or in short XIP (Brun and Hag`ege_comma_ 2004) (line 2 in Table 1)_comma_  Stanford NER (or in short Stanford) associated to the following model provided by the tool and which was trained on different news Systems Prec.
1275,0,F-me. 1 CBC-NER system M 71.67 23.47 35.36CBC-NER system A 70.66 32.86 44.86 2 XIP NER 77.77 56.55 65.48 XIP + CBC M 78.41 60.26 68.15 XIP + CBC A 76.31 60.48 67.48 3 Stanford NER 67.94 68.01 67.97 Stanford + CBC M 69.40 71.07 70.23 Stanford + CBC A 70.09 72.93 71.48 4 GATE NER 63.30 56.88 59.92 GATE + CBC M 66.43 61.79 64.03 GATE + CBC A 66.51 63.10 64.76 5 Stanford + XIP 72.85 75.87 74.33 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 6 GATE + XIP 69.38 66.04 67.67 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 7 GATE + Stanford 63.12 69.32 66.07 GATE + Stanford + CBC M 65.09 72.05 68.39 GATE + Stanford + CBC A 65.66 73.25 69.25 Table 1: Results given by different hybrid NER systems and coupled with the CBC-NER system corpora (CoNLL_comma_ MUC6_comma_ MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al._comma_ 2005) (line 3 in Table 1)_comma_  GATE NER or in short GATE (Cunningham et al._comma_ 2002) (line 4 in Table 1)_comma_  and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1).
1276,0,First_comma_ in the sentence From the start_comma_ his parents_comma_ Lourdes and Hemery_comma_ were with him._comma_ the baseline hybrid system Stanford + XIP annotated the ambiguous NE Lourdes as <location> whereas Stanford + XIP + CBC A gave the correct annotation <person>.
1277,0,Second_comma_ in the sentence Got 3 percent chance of survival_comma_ what ya gonna do? The back read_comma_ A) Fight Through_comma_ b) Stay Strong_comma_ c) Overcome Because I Am a Warrior._comma_ the baseline hybrid system Stanford + XIP annotated Warrior as <organization> whereas Stanford + XIP + CBC A corrected this annotation with <none>.
1278,0,Finally_comma_ in the sentence Matthew_comma_ also a favorite to win in his fifth and final appearance_comma_ was stunningly eliminated during the semifinal round Friday when he misspelled secernent._comma_ the baseline hybrid system Stanford + XIP didnt give any annotation to Matthew whereas Stanford + XIP + CBC A allowed to give the annotation <person>.
1279,0,We parse the data using the Collins Parser (Collins_comma_ 1997)_comma_ and then tag person_comma_ location and organization names using the Stanford Named Entity Recognizer (Finkel et al._comma_ 2005).
1280,0,Some stem from work on graphical models_comma_includingloopybeliefpropagation(Suttonand McCallum_comma_ 2004; Smith and Eisner_comma_ 2008)_comma_ Gibbs sampling (Finkel et al._comma_ 2005)_comma_ sequential Monte Carlo methods such as particle filtering (Levy et al._comma_ 2008)_comma_ and variational inference (Jordan et al._comma_ 1999; MacKay_comma_ 1997; Kurihara and Sato_comma_ 2006).
1281,0,In all the experiments_comma_ our source side language is English_comma_ and the Stanford Named Entity Recognizer (Finkel et al_comma_ 2005) was used to extract NEs from the source side article.
1282,0,It should be noted here that while the precision of the NER 803 used was consistently high_comma_ its recall was low_comma_ (~40%) especially in the New Indian Express corpus_comma_ perhaps due to the differences in the data used for training the NER and the data on which we used it.
1283,0,For the named entity features_comma_ we used a fairly standard feature set_comma_ similar to those described in (Finkel et al._comma_ 2005).
1284,0,Our features were based on those in (Finkel et al._comma_ 2005).
1285,0,We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al._comma_ 2005).
1286,0,The tagger provides each word with a label from {person_comma_ location_comma_ organization_comma_ miscellaneous_comma_ none}.
1287,0,To implement this method_comma_ we rst use the Stanford Named Entity Recognizer4 (Finkel et al._comma_ 2005)toidentifythesetofpersonandorganisation entities_comma_ E_comma_ from each article in the corpus.
1288,0,One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al._comma_ 2005).
1289,0,The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al._comma_ 2005).
1290,0,NER proves to be a knowledgeintensive task_comma_ and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia_comma_ Nonlocal Features_comma_ Word-class Model 90.80 (Suzuki and Isozaki_comma_ 2008) Semi-supervised on 1Gword unlabeled data 89.92 (Ando and Zhang_comma_ 2005) Semi-supervised on 27Mword unlabeled data 89.31 (Kazama and Torisawa_comma_ 2007a) Wikipedia 88.02 (Krishnan and Manning_comma_ 2006) Non-local Features 87.24 (Kazama and Torisawa_comma_ 2007b) Non-local Features 87.17 + (Finkel et al._comma_ 2005) Non-local Features 86.86 Table 7: Results for CoNLL03 data reported in the literature.
1291,0,Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al._comma_ 2005; Krishnan and Manning_comma_ 2006; Kazama and Torisawa_comma_ 2007) and dependency parsing (Nakagawa_comma_ 2007) with a great deal of success.
1292,1,Lins (1998) information-theoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD (McCarthy et al._comma_ 2004).
1293,0,It operates over dependency relations.
1294,0,A potential caveat with Lins (1998) distributional similarity measure is its reliance on syntactic information for obtaining dependency relations.
1295,0,On the British National Corpus (BNC)_comma_ using Lins (1998) similarity method_comma_ we retrieve the following neighbors for the first and second sense_comma_ respectively: 1.
1296,0,As described in Section 3 we retrieved neighbors using Lins (1998) similarity measure on a RASP parsed (Briscoe and Carroll_comma_ 2002) version of the BNC.
1297,1,The best accuracies are observed when the labelsarecreatedfromdistributionallysimilarwords using Lins (1998) dependency-based similarity measure (Depend).
1298,0,Point-wise mutual information (Lin_comma_ 1998) and Relative Feature Focus (Geffet and Dagan_comma_ 2004) are well-known examples.
1299,0,Feature comparison measures: to convert two feature sets into a scalar value_comma_ several measures have been proposed_comma_ such as cosine_comma_ Lins measure (Lin_comma_ 1998)_comma_ Kullback-Leibler (KL) divergence and its variants.
1300,0,Lins measure Lin (1998) proposed a symmetrical measure: Par Lin (s  t)= summationtext fF s F t (w(s_comma_f)+w(t_comma_f)) summationtext fF s w(s_comma_f)+ summationtext fF t w(t_comma_f) _comma_ where F s and F t denote sets of features with positive weights for words s and t_comma_ respectively.
1301,0,Although this measure has been widely cited and has so far exhibited good performance_comma_ its symmetry seems unnatural.
1302,0,Moreover_comma_ it may not work well for dealing with general predicate phrases because it is hard to enumerate all phrases to determine the weights of features w(_comma_f).We thus simply adopted the co-occurrence frequency of the phrase and the feature as in (Fujita and Sato_comma_ 2008).
1303,0,Applications of word clustering include language modeling (Brown et al._comma_ 1992)_comma_ text classification (Baker and McCallum_comma_ 1998)_comma_ thesaurus construction (Lin_comma_ 1998) and so on.
1304,0,Others proposed distributional similarity measures between words (Hindle_comma_ 1990; Lin_comma_ 1998; Lee_comma_ 1999; Weeds et al._comma_ 2004).
1305,0,Three K-means algorithms using different distributional similarity or dissimilarity measures: cosine_comma_ -skew divergence (Lee_comma_ 1999) 4 _comma_ and Lins similarity (Lin_comma_ 1998).
1306,0,405 PRF 1 proposed .383 .437 .408 multinomial mixture .360 .374 .367 Newman (2004) .318 .353 .334 cosine .603 .114 .192 -skew divergence (Lee_comma_ 1999) .730 .155 .255 Lins similarity (Lin_comma_ 1998) .691 .096 .169 CBC (Lin and Pantel_comma_ 2002) .981 .060 .114 Table 3: Precision_comma_ recall_comma_ and F-measure.
1307,0,Chantree et al.(2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation.
1308,0,Lin extracted from a corpus dependency triples of two words and the grammatical relationship between them_comma_ and considered that similar words are likely to have similar dependency relations.
1309,0,One is automatic thesaurus acquisition_comma_ that is_comma_ to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd_comma_ 1997; Lin_comma_ 1998; Caraballo_comma_ 1999; Thelen and Riloff_comma_ 2002; You and Chen_comma_ 2006).
1310,0,Following Lin (1998)_comma_ we use syntactic dependencies between words to model their semantic properties.
1311,0,Various methods (Hindle_comma_ 1990; Lin_comma_ 1998) of automatically acquiring synonyms have been proposed.
1312,0,They are usually based on the distributional hypothesis (Harris_comma_ 1985)_comma_ which states that semantically similar words share similar contexts_comma_ and they can be roughly viewed as the combinations of two steps: context extraction and similarity calculation.
1313,0,However_comma_ all of the existing research conducted only a posteriori comparison_comma_ and as Weeds et al. pointed out_comma_ there is no one best measure for all applications.
1314,0,4.1 Features We used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies (Ruge_comma_ 1997; Lin_comma_ 1998).
1315,0,Given a wordq_comma_ its set of featuresFq and feature weightswq(f) for f Fq_comma_ a common symmetric similarity measure is Lin similarity (Lin_comma_ 1998a): Lin(u_comma_v) = summationtext fFuFv[wu(f)+wv(f)]summationtext fFu wu(f)+ summationtext fFv wv(f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: wq(f) =log[Pr(f|q)Pr(f) ].
1316,0,The similarity between two templatestand tprime is the geometric average: DIRT(t_comma_tprime) = radicalBig Linx(t_comma_tprime)Liny(t_comma_tprime) where Linx is the Lin similarity between Xs instantiations of t and Xs instantiations of tprime in a corpus (equivalently for Liny).
1317,0,Texts are represented by dependency parse trees (using the Minipar parser (Lin_comma_ 1998b)) and templates by parse sub-trees.
1318,0,We first adapted DIRT for unary templates (unary-DIRT_comma_ applying Lin-similarity to the single feature vector)_comma_ as well as its output filtering by LEDIR.
1319,0,After initial analysis_comma_ we found that given a right hand side template r_comma_ symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relationsl_comma_rin which l and r are related but do not necessarily participate in an entailment or equivalence relation_comma_ e.g. the wrong rule kill X injure X.
1320,0,The second approach  using syntactic relations  has been adopted by many researchers_comma_ in order to acquire semantically similar words.
1321,0,One of the most important is Lins (1998).
1322,0,A number of similarity measures have been developed to calculate semantic similarity in a hierarchical wordnet.
1323,1,Among these measures_comma_ the most important are Wu & Palmers (Wu and Palmer_comma_ 1994)_comma_ Resniks (Resnik_comma_ 1995) and Lins (Lin_comma_ 1998).
1324,0,Where Pantel and Lin use Lins (1998) measure_comma_ we use Wu and Palmers (1994) measure.
1325,1,Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)s information-theoretic metric work best.
1326,0,4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees_comma_ 2002) using Minipar (Lin_comma_ 1998b)_comma_ and collected verb-object and verb-subject frequencies_comma_ building an empirical MI model from this data.
1327,0,Lin (1998a)s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14)_comma_ because these have similar subjects to eat.
1328,0,Discriminative_comma_ context-specific training seems to yield a better set of similar predicates_comma_ e.g. the highest-ranked contexts for DSPcooc on the verb join_comma_3 lead 1.42_comma_ rejoin 1.39_comma_ form 1.34_comma_ belong to 1.31_comma_ found 1.31_comma_ quit 1.29_comma_ guide 1.19_comma_ induct 1.19_comma_ launch (subj) 1.18_comma_ work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin_comma_ 1998a): participate 0.164_comma_ lead 0.150_comma_ return to 0.148_comma_ say 0.143_comma_ rejoin 0.142_comma_ sign 0.142_comma_ meet 0.142_comma_ include 0.141_comma_ leave 0.140_comma_ work 0.137 Other features are also weighted intuitively.
1329,0,We also test an MI model inspired by Erk (2007): MISIM(n_comma_v) = log summationdisplay nSIMS(n) Sim(n_comma_n) Pr(v_comma_n ) Pr(v)Pr(n) We gather similar words using Lin (1998a)_comma_ mining similar verbs from a comparable-sized parsed corpus_comma_ and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)s approach to obtaining web-counts.
1330,1,They have been successfully applied in several tasks_comma_ such as information retrieval (Salton et al._comma_ 1975) and harvesting thesauri (Lin_comma_ 1998).
1331,0,Two LUs close in the space are likely to be in a paradigmatic relation_comma_ i.e. to be close in a is-a hierarchy (Budanitsky and Hirst_comma_ 2006; Lin_comma_ 1998; Pado_comma_ 2007).
1332,1,This similarity score is computed as a max over a number of component scoring functions_comma_ some based on external lexical resources_comma_ including:  various string similarity functions_comma_ of which most are applied to word lemmas  measures of synonymy_comma_ hypernymy_comma_ antonymy_comma_ and semantic relatedness_comma_ including a widelyused measure due to Jiang and Conrath (1997)_comma_ based on manually constructed lexical resources such as WordNet and NomBank  a function based on the well-known distributional similarity metric of Lin (1998)_comma_ which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources both manually and automatically constructedis critical to the success of MANLI.
1333,0,Distributional measures of distance_comma_ such as those proposed by Lin (1998)_comma_ quantify how similar the two sets of contexts of a target word pair are.
1334,0,Equation 1 is a modified form of Lins measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lina0 w1 a1 w2 a2a4a3 w a5 T a6 w1 a7a9a8 T a6 w2 a7 a0 Ia0 w1 a1 w a2a11a10 Ia0 w2 a1 w a2a12a2 w a13a14a5 T a6 w1 a7 Ia0 w1 a1 wa15 a2a16a10 w a13a13a17a5 T a6 w2 a7 Ia0 w2 a1 wa15 a15 a2 (1) Here w1 and w2 are the target words; Ia0 x a1 y a2 is the pointwise mutual information between x and y; and T a0 x a2 is the set of all words y that have positive pointwise mutual information with the word x (Ia0 x a1 y a2a19a18 0).
1335,0,They proposed a way to obtain distributional distance between word senses_comma_ using any of the distributional measures such as cosine or that proposed by Lin_comma_ and showed that this approach performed markedly better than the traditional worddistance approach.
1336,0,Equation 2 shows how Lins formula is used to determine distributional distance between two thesaurus categories c1 and c2: Lina0 c1 a1 c2 a2a4a3 w a5 T a6 c1 a7a9a8 T a6 c2 a7 a0 Ia0 c1 a1 w a2a16a10 Ia0 c2 a1 w a2a12a2 w a13a17a5 T a6 c1 a7 Ia0 c1 a1 wa15 a2a11a10 wa13a13a17a5 T a6 c2 a7 Ia0 c2 a1 wa15a15 a2 (2) Here T a0 c a2 is the set of all words w that have positive pointwise mutual information with the thesaurus category c (Ia0 c a1 w a2a20a18 0).
1337,0,For each word pair from the antonym set_comma_ we calculated the distributional distance between each of their senses using Mohammad and Hirsts (2006) method of concept distance along with the modified form of Lins (1998) distributional measure (equation 2).
1338,0,Again we used Mohammad and Hirsts (2006) method along with Lins (1998) distributional measure to determine the distributional closeness of two thesaurus concepts.
1339,0,Our approach to STC uses a thesaurus based on corpus statistics (Lin_comma_ 1998) for real-valued similarity calculation.
1340,0,Some researchers (Hindle_comma_ 1990; Grefenstette_comma_ 1994; Lin_comma_ 1998) classify terms by similarities based on their distributional syntactic patterns.
1341,0,These methods often extract not only synonyms_comma_ but also semantically related terms_comma_ such as antonyms_comma_ hyponyms and coordinate terms such as cat and dog. Some studies make use of bilingual corpora or dictionaries to nd synonyms in a target language (Barzilay and McKeown_comma_ 2001; Shimohata and Sumita_comma_ 2002; Wu and Zhou_comma_ 2003; Lin et al._comma_ 2003).
1342,1,A wide range of contextual information_comma_ such as surrounding words (Lowe and McDonald_comma_ 2000; Curran and Moens_comma_ 2002a)_comma_ dependency or case structure (Hindle_comma_ 1990; Ruge_comma_ 1997; Lin_comma_ 1998)_comma_ and dependency path (Lin and Pantel_comma_ 2001; Pado and Lapata_comma_ 2007)_comma_ has been utilized for similarity calculation_comma_ and achieved considerable success.
1343,0,However_comma_ amajorproblemwhichariseswhenadopting distributional similarity is that it easily yields a huge amount of unique contexts.
1344,1,3.1 Context Extraction We adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies (Ruge_comma_ 1997; Lin_comma_ 1998).
1345,0,2 Related Work ThisworkbuildsuponthatofMcCarthyetal.(2004) which acquires predominant senses for target words from a large sample of text using distributional similarity (Lin_comma_ 1998) to provide evidence for predominance.
1346,0,The evidence from the distributional similarity is allocated to the senses using semantic similarityfromWordNet(PatwardhanandPedersen_comma_ 2003).
1347,0,In this approach we extend the denition overlap by considering the distributional similarity (Lin_comma_ 1998) rather than identify of the words in the two denitions.
1348,0,McCarthy et al. use a distributional similarity thesaurus acquired from corpus data using the method of Lin (1998) for nding the predominant sense of a word where the senses are dened by WordNet.
1349,0,The thesaurus provides the k nearest neighbours to each target word_comma_ along with the distributional similarity score between the target word and its neighbour.
1350,0,Let w be a target word and Nw = fn1_comma_n2nkg be the ordered set of the top scoring k neighbours of w from the thesaurus with associated distributional similarity scores fdss(w_comma_n1)_comma_dss(w_comma_n2)_comma_dss(w_comma_nk)g using (Lin_comma_ 1998).
1351,0,We propose using distributional similarity (using (Lin_comma_ 1998)) as an approximation of semantic distancebetweenthewordsinthetwoglosses_comma_rather than requiring an exact match.
1352,0,We adopt the similarity score proposed by Lin (1998) as the distributional similarity score and use 50 nearest neighbours in line with McCarthy et al. For the random baseline we select one word sense at random for each word token and average the precision over 100 trials.
1353,0,We use the similarity proposed by Lin (1998).
1354,0,The method uses mutual information and dependency relationships as the phrase features.
1355,0,We extend the method to Japanese by using a particle as the dependency relationships.
1356,0,This approach is similar to conventional techniques for automatic thesaurus construction (Lin_comma_ 1998).
1357,0,Our next steps will be to take a closer look at the following work: clustering of similar words (Lin_comma_ 1998)_comma_ topic signatures (Lin and Hovy_comma_ 2000) and Kilgariffs sketch engine (Kilgarriff et al._comma_ 2004).
1358,0,The earliest work in this direction are those of (Hindle_comma_ 1990)_comma_ (Lin_comma_ 1998)_comma_ (Dagan et al._comma_ 1999)_comma_ (Chen and Chen_comma_ 2000)_comma_ (Geffet and Dagan_comma_ 2004) and (Weeds and Weir_comma_ 2005).
1359,0,They used distributional similarity.
1360,0,Lin (1998) proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus.
1361,0,He compared the result of automat ically created thesaurus with WordNet and Roget_comma_ and reported that the result was signicantly closer to WordNet than Roget Thesaurus was.
1362,0,Like McCarthy et al.(2004) we use k = 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998).
1363,0,Thus we rank each sense wsi WSw using Prevalence Score wsi = (11)  njNw dssnj  wnss(wsi_comma_nj) wsiWSw wnss(wsi_comma_nj) where the WordNet similarity score (wnss) is defined as: wnss(wsi_comma_nj)= max nsxNSnj (wnss(wsi_comma_nsx)) 2.2 Building the Thesaurus The thesaurus was acquired using the method described by Lin (1998).
1364,0,For every pair of nouns_comma_ where each noun had a total frequency in the triple data of 10 or more_comma_ we computed their distributional similarity using the measure given by Lin (1998).
1365,0,Curran (2002) and Lin (1998) use syntactic features in the vector definition.
1366,0,Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik_comma_ 1995)_comma_ synonym extraction (Lin_comma_ 1998a)_comma_ and automatic thesauri generation (Curran_comma_ 2002).
1367,0,Lin (1998b) defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept.
1368,0,Method Correlation Edge-counting 0.664 Jiang & Conrath (1998) 0.848 Lin (1998a) 0.822 Resnik (1995) 0.745 Li et al.(2003) 0.891 ball) and (Jerusalem_comma_ Israel)).
1369,0,(Strube and Ponzetto_comma_ 2006) 0.19-0.48 Leacock & Chodrow (1998) 0.36 Lin (1998b) 0.36 Resnik (1995) 0.37 Proposed 0.504 7 Conclusion We proposed a relational model to measure the semantic similarity between two words.
1370,0,Pereira et al.(1993)_comma_ Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition.
1371,0,While great effort has been made for improving the computational complexity of these methods (Gorman and Curran_comma_ 2006)_comma_ they still remain data and computation intensive.
1372,0,Wiebe (2000) uses Lin (1998a) style distributionally similar adjectives in a cluster-and-label process to generate sentiment lexicon of adjectives.
1373,0,3http://www.openoffice.org Another corpora based method due to Turney and Littman (2003) tries to measure the semantic orientation O(t) for a term t by O(t) = summationdisplay tiS+ PMI(t_comma_ti) summationdisplay tjS PMI(t_comma_tj) where S+ and S are minimal sets of polar terms that contain prototypical positive and negative terms respectively_comma_ and PMI(t_comma_ti) is the pointwise mutual information (Lin_comma_ 1998b) between the terms t and ti.
1374,0,The thesaurus was produced using the metric described by Lin (1998) with input from the grammatical relation data extracted using the 90 million words of written English from the British National Corpus (BNC) (Leech_comma_ 1992) using the RASP parser (Briscoe and Carroll_comma_ 2002).
1375,0,The common types of features include contextual (Lin_comma_ 1998)_comma_ co-occurrence (Yang and Callan_comma_ 2008)_comma_ and syntactic dependency (Pantel and Lin_comma_ 2002; Pantel and Ravichandran_comma_ 2004).
1376,0,Clustering-based approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors (Brown et al._comma_ 1992; Lin_comma_ 1998).
1377,0,While Kazama and Torisawa used a chunker_comma_ we parsed the definition sentence using Minipar (Lin_comma_ 1998b).
1378,0,The second uses Lin dependency similarity_comma_ a syntacticdependency based distributional word similarity resource described in (Lin_comma_ 1998a)9.
1379,0,Syntactic context information is used (Hindle_comma_ 1990; Ruge_comma_ 1992; Lin_comma_ 1998) to compute term similarities_comma_ based on which similar words to a particular word can directly be returned.
1380,0,Semantic DSN: The construction of this network is inspired by (Lin_comma_ 1998).
1381,0,Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin_comma_ 1998; Curran and Moens_comma_ 2002).
1382,1,Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems (Pad and Lapata_comma_ 2007; Lin_comma_ 1998)_comma_ for the task of categorisation simple pattern based spaces have been shown to perform equally good if not better (Poesio and Almuhareb_comma_ 2005b; Almuhareb and Poesio_comma_ 2005b).
1383,0,In particular we work with dependency paths that can reach beyond direct dependencies as opposed to Lin (1998) but in the line of Pado and Lapata (2007).
1384,0,As a basis mapping function  we used a generalisation of the one used by Grefenstette (1994) and Lin (1998).
1385,0,They map a dependency between two words to a pair consisting of the relation label l and the end word of the dependency end(pi).
1386,0,As we use paths that span more than a single relation_comma_ this approach is not directly applicable to our setup.
1387,0,Example of such algorithms are (Pereira et al._comma_ 1993) and (Lin_comma_ 1998) that use syntactic features in the vector definition.
1388,0,Pereira (1993)_comma_ Curran (2002) and Lin (1998) use syntactic features in the vector definition.
1389,0,In particular_comma_ this method has been used for word sense disambiguation (Lin_comma_ 1997) and thesaurus construction (Lin_comma_ 1998).
1390,0,2.1.2 Research on Syntax-Based SMT A number of researchers (Alshawi_comma_ 1996; Wu_comma_ 1997; Yamada and Knight_comma_ 2001; Gildea_comma_ 2003; Melamed_comma_ 2004; Graehl and Knight_comma_ 2004; Galley et al. _comma_ 2004) have proposed models where the translation process involves syntactic representations of the source and/or target languages.
1391,0,One class of approaches make use of bitext grammars which simultaneously parse both the source and target languages.
1392,0,Another class of approaches make use of syntactic information in the target language alone_comma_ effectively transforming the translation problem into a parsing problem.
1393,0,Note that these models have radically different structures and parameterizations from phrasebased models for SMT.
1394,0,As yet_comma_ these systems have not shown significant gains in accuracy in comparison to phrase-based systems.
1395,0,Galley et al.(2004) describe how to learn hundreds of millions of treetransformation rules from a parsed_comma_ aligned Chinese/English corpus_comma_ and Galley et al.(submitted) describe probability estimators for those rules.
1396,0,3 DerivTool In order to test whether good translations can be generated with rules learned by Galley et al.(2004)_comma_ we created DerivTool as an environment for interactively using these rules as a decoder would.
1397,0,(Gildea_comma_ 2003) and (Galley et al. _comma_ 2004) discuss different ways of generalizing the tree-level crosslinguistic correspondence relation_comma_ so it is not confined to single tree nodes_comma_ thereby avoiding a continuity assumption.
1398,0,To this end_comma_ the translational correspondence is described within a translation rule_comma_ i.e._comma_ (Galley et al. _comma_ 2004) (or a synchronous production)_comma_ rather than a translational phrase pair; and the training data will be derivation forests_comma_ instead of the phrase-aligned bilingual corpus.
1399,0,In this work_comma_ we employ a syntax-based model that applies a series of tree/string (xRS) rules (Galley et al. _comma_ 2004; Graehl and Knight_comma_ 2004) to a source language string to produce a target language phrase structure tree.
1400,0,Step 2 involves extracting minimal xRS rules (Galley et al. _comma_ 2004) from the set of string/tree/alignments triplets.
1401,0,1 Introduction Several recent syntax-based models for machine translation (Chiang_comma_ 2005; Galley et al. _comma_ 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers.
1402,0,In this framework_comma_ both alignment (synchronous parsing) and decoding can be thought of as parsing problems_comma_ whose complexity is in general exponential in the number of nonterminals on the right hand side of a grammar rule.
1403,0,In contrast_comma_ the rule extraction method of Galley et al.(2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses.
1404,0,This approach results in rules with many nonterminals_comma_ making good binarization techniques critical.
1405,0,Suppose we have the following tree-to-string rules_comma_ following Galley et al.(2004): (3) S(x0:NP_comma_ VP(x2:VP_comma_ x1:PP))x0 x1 x2 NP(NNP(Powell))Baoweier VP(VBD(held)_comma_ NP(DT(a) NPS(meeting))) juxing le huitan PP(TO(with)_comma_ NP(NNP(Sharon)))yu Shalong where the reorderings of nonterminals are denoted by variables xi.
1406,0,Our rule set is obtained by rst doing word alignment using GIZA++ on a Chinese-English parallel corpus containing 50 million words in English_comma_ then parsing the English sentences using a variant of Collins parser_comma_ and nally extracting rules using the graph-theoretic algorithm of Galley et al.(2004).
1407,1,However_comma_ to be more expressive and flexible_comma_ it is often easier to start with a general SCFG or tree-transducer (Galley et al. _comma_ 2004).
1408,0,Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al. _comma_ 2004).
1409,0,Following Galley et al.(2004)_comma_ we use an extended tree-to-string transducer (xRs) with multi-level left-hand-side (LHS) trees.1 Since the right-hand-side (RHS) string can be viewed as a flat one-level tree with the same nonterminal root from LHS (Fig.
1410,0,These rules can be learned from a parallel corpus using English parsetrees_comma_ Chinese strings_comma_ and word alignment (Galley et al. _comma_ 2004).
1411,0,Similarly to (Galley et al. _comma_ 2004)_comma_ the tree-to-string alignment templates discussed in this paper are actually transformation rules.
1412,0,As a result_comma_ the task of our decoder is to find the best target string while Galleys is to seek the most likely target tree.
1413,0,In this paper_comma_ we take the framework for acquiring multi-level syntactic translation rules of (Galley et al. _comma_ 2004) from aligned tree-string pairs_comma_ and present two main extensions of their approach: first_comma_ instead of merely computing a single derivation that minimally explains a sentence pair_comma_ we construct a large number of derivations that include contextually richer rules_comma_ and account for multiple interpretations of unaligned words.
1414,0,Galley et al.(2004) alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora_comma_ which we review below.
1415,0,We contrast our work with (Galley et al. _comma_ 2004)_comma_ highlight some severe limitations of probability estimates computed from single derivations_comma_ and demonstrate that it is critical to account for many derivations for each sentence pair.
1416,0,Finally_comma_ we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al. _comma_ 2004).
1417,0,For this_comma_ we need a formalism that is expressive enough to deal with cases of syntactic divergence between source and target languages (Fox_comma_ 2002): for any given (pi_comma_f_comma_a) triple_comma_ it is useful to produce a derivation that minimally explains the transformation between pi and f_comma_ while remaining consistent with a. Galley et al.(2004) present one such formalism (henceforth GHKM).
1418,0,Formally_comma_ transformational rules ri presented in (Galley et al. _comma_ 2004) are equivalent to 1-state xRs transducers mapping a given pattern (subtree to match in pi) to a right hand side string.
1419,0,We will refer to them as lhs(ri) and rhs(ri)_comma_ respectively.
1420,0,Aparticularinstancemay look like this: VP(AUX(does)_comma_ RB(not)_comma_ x0:VB)  ne_comma_ x0_comma_ pas lhs(ri) can be any arbitrary syntax tree fragment.
1421,0,Now we give a brief overview of how such transformational rules are acquired automatically in GHKM.1 In Figure 1_comma_ the (pi_comma_f_comma_a) triple is represented as a directed graph G (edges going downward)_comma_ with no distinction between edges of pi and alignments.
1422,0,Rules that satisfy this property are said to be induced by G.2 For example_comma_ rule (d) in Table 1 is valid according to GHKM_comma_ since the spans corresponding to 1Note that we use a slightly different terminology.
1423,0,Conversely_comma_NP(x0:DT_comma_x1:CD:_comma_x2:NNS) is not the lhs of any rule extractible from G_comma_ since its frontier constituents CD[2] and NNS[2] have overlapping spans.3 Finally_comma_ the GHKM procedure produces a single derivation from G_comma_ which is shown in Table 1.
1424,0,The concern in GHKM was to extract minimal rules_comma_ whereas ours is to extract rules of any arbitrary size.
1425,0,2.2 Unaligned words While the general theory presented in GHKM accounts for any kind of derivation consistent with G_comma_ it does not particularly discuss the case where some words of the source-language string f are not aligned to any word of e_comma_ thus disconnected from the rest of the graph.
1426,0,The approach of building one minimal derivation for G as in the algorithm described in GHKM assumes that we commit ourselves to a particular heuristic to attach the unaligned item to a certain constituent of pi_comma_ e.g._comma_ highest attachment (in the example_comma_ a132 is attached to NP[4-8] and the heuristic generates rule (f)).
1427,0,963 2.3 Algorithm The linear-time algorithm presented in GHKM is only a particular case of the more general one we describe here_comma_ which is used to extract all rules_comma_ minimal and composed_comma_ induced by G. Similarly to the GHKM algorithm_comma_ ours performs a topdown traversal of G_comma_ but differs in the operations it performs at each node n  F: we must explore all subtrees rooted at n_comma_ find all consistent ways of attaching unaligned words of f_comma_ and build valid derivations in accordance to these attachments.
1428,0,In the minimal-rule extraction of GHKM_comma_ only three rules are extracted from the example corpus_comma_ i.e. rules r2_comma_ r3_comma_ and r4.
1429,0,8 Conclusions In this paper_comma_ we developed probability models for the multi-level transfer rules presented in (Galley et al. _comma_ 2004)_comma_ showed how to acquire larger rules that crucially condition on more syntactic context_comma_ and how to pack multiple derivations_comma_ including interpretations of unaligned words_comma_ into derivation forests.
1430,0,We presented some theoretical arguments for not limiting extraction to minimal rules_comma_ validated them on concrete examples_comma_ and presented experiments showing that contextually richer rules provide a 3.63 BLEU point increase over the minimal rules of (Galley et al. _comma_ 2004).
1431,0,Unlike Fox (2002) and Galley et al.(2004)_comma_ we measured failure rates per corpus rather than per sentence pair or per node in a constraining tree.
1432,0,We would expect the opposite effect with hand-aligned data (Galley et al. _comma_ 2004).
1433,0,Analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals (Groves et al. _comma_ 2004; Chiang_comma_ 2005)_comma_ or_comma_ given a constraining parse tree_comma_ to flatten it (Fox_comma_ 2002; Zens and Ney_comma_ 2003; Galley et al. _comma_ 2004).
1434,0,Both of these approaches can increase coverage of the training data_comma_ but_comma_ as explained in Section 2_comma_ they risk losing generalization ability.
1435,0,Besides being linguistically motivated_comma_ the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox_comma_ 2002; Galley et al. _comma_ 2004).
1436,0,Following Galley et al.(2004)_comma_ we use a special class of extended tree-to-string transducer (xRs for short) with multilevel left-hand-side (LHS) trees.1 Since the righthand-side (RHS) string can be viewed as a flat onelevel tree with the same nonterminal root from LHS (Fig.
1437,0,2)_comma_ this framework is closely related to STSGs: they both have extended domain of locality on the source-side_comma_ while our framework remains as a CFG on the target-side.
1438,1,We denote (t) to be the root symbol of tree t. When writing these rules_comma_ we avoid notational overhead by introducing a short-hand form from Galley et al.(2004) that integrates the mapping into the tree_comma_ which is used throughout Section 1.
1439,0,Galley et al.(2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side_comma_ which will be used in our experiments in Section 6.
1440,0,We first word-align them by GIZA++_comma_ then parse the English side by a variant of Collins (1999) parser_comma_ and finally apply the rule-extraction algorithm of Galley et al.(2004).
1441,0,However_comma_ many of these models are not applicable to parallel treebanks because they assume translation units where either the source text_comma_ the target text or both are represented as word sequences without any syntactic structure (Galley et al. _comma_ 2004; Marcu et al. _comma_ 2006; Koehn et al. _comma_ 2003).
1442,0,From wordlevel alignments_comma_ such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al._comma_ 2004)_comma_ or with the the word-level alignments alone without reference to external syntactic analysis (Chiang_comma_ 2005)_comma_ which is the scenario we address here.
1443,0,Motivated by the same problem raised by Fox (2002)_comma_ Galley et al.(2004) study what rule can better explain human translation data.
1444,0,Fox (2002)_comma_ Galley et al (2004) and Wellington et al.(2006) examine TEM only.
1445,0,It reconfirms that only allowing sibling nodes reordering as done in SCFG may be inadequate for translational equivalence modeling (Galley et al._comma_ 2004) 4 . 3) All the three models on the FBIS corpus show much lower performance than that on the other two corpora.
1446,0,This implies that the complexity of structure divergence between two languages is higher than suggested in literature (Fox_comma_ 2002; Galley et al._comma_ 2004).
1447,0,However_comma_ as discussed in prior arts (Galley et al._comma_ 2004) and this paper_comma_ linguistically-informed SCFG is an inadequate model for parallel corpora due to its nature that only allowing child-node reorderings.
1448,0,Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al._comma_ 2004; Quirk et al._comma_ 2005).
1449,0,We trained three Arabic-English syntax-based statistical MT systems (Galley et al._comma_ 2004; Galley et al._comma_ 2006) using max-B training (Och_comma_ 2003): one on a newswire development set_comma_ one on a weblog development set_comma_ and one on a combined development set containing documents from both genres.
1450,0,As a result_comma_ they are being used in a variety of applications_comma_ such as question answering (Hermjakob_comma_ 2001)_comma_ speech recognition (Chelba and Jelinek_comma_ 1998)_comma_ language modeling (Roark_comma_ 2001)_comma_ language generation (Soricut_comma_ 2006) and_comma_ most notably_comma_ machine translation (Charniak et al._comma_ 2003; Galley et al._comma_ 2004; Collins et al._comma_ 2005; Marcu et al._comma_ 2006; Huang et al._comma_ 2006; Avramidis and Koehn_comma_ 2008).
1451,0,Depending on the type of input_comma_ these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu_comma_ 1997; Chiang_comma_ 2005; Galley et al._comma_ 2006)_comma_ and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin_comma_ 2004; Ding and Palmer_comma_ 2005; Quirk et al._comma_ 2005; Liu et al._comma_ 2006; Huang et al._comma_ 2006).
1452,0,These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al.(2004).
1453,0,Accurate identification of sub-sentential translation equivalents_comma_ however_comma_ is a critical process in all data-driven MT approaches_comma_ including a variety of data-driven syntax-based approaches that have been developed in recent years.
1454,0,(Chiang_comma_ 2005) (Imamura et al._comma_ 2004) (Galley et al._comma_ 2004).
1455,0,The underlying formalisms used has been quite broad and include simple formalisms such as ITGs (Wu_comma_ 1997)_comma_ hierarchicalsynchronousrules(Chiang_comma_ 2005)_comma_ string to tree models by (Galley et al._comma_ 2004) and (Galley et al._comma_ 2006)_comma_ synchronous CFG models such (Xia and McCord_comma_ 2004) (Yamada and Knight_comma_ 2001)_comma_ synchronous Lexical Functional Grammar inspired approaches (Probst et al._comma_ 2002) and others.
1456,0,Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus_comma_ typically the target side.
1457,0,Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of (Galley et al._comma_ 2004).
1458,0,Current tree-based models that integrate linguistics and statistics_comma_ such as GHKM (Galley et al._comma_ 2004)_comma_ are not able to generalize well from a single phrase pair.
1459,0,For example_comma_ from the data in Figure 1_comma_ GHKM can learn rule (a) to translate nouns with two pre-modifiers_comma_ but does not generalize to learn translation rules (b) (d) without the optional adjective or noun modifiers.
1460,0,3 Learning the Model Instead of using bilingual parsing to directly train our model from strings as done by Nesson et al.(2006)_comma_ we follow the method of Galley et al.(2004) by dividing the training process into steps.
1461,0,If all children are considered required_comma_ the result is the same as the GHKM rules of Galley et al.(2004) and has the same problemlots of low count_comma_ syntactically over-constrained rules.
1462,0,This can be done using a method inspired by the rule-extraction approach of Galley et al.(2004)_comma_ but instead of directly operating on the parse tree we process the English TIG derivation tree.
1463,0,Table 1 shows the comparison between our baseline model (minimal GHKM on head-out binarized parse trees) and different models of adjoining_comma_ measured with case-insensitive_comma_ NISTtokenized BLEU (IBM definition).
1464,1,1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al._comma_ 2004; Liu et al._comma_ 2006_comma_ 2007; Zhang et al._comma_ 2007_comma_ 2008a; Mi et al._comma_ 2008; Mi and Huang 2008; Zhang et al._comma_ 2009).
1465,0,The tree-to-string model (Galley et al. 2004; Liu et al. 2006) views the translation as a structure mapping process_comma_ which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules_comma_ finally combines these target translations into a complete sentence.
1466,0,For example_comma_ Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints_comma_ which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al._comma_ 2004; Mi and Huang_comma_ 2008).
1467,0,To make things worse_comma_ languages are non-isomorphic_comma_ i.e._comma_ there is no 1to-1 mapping between tree nodes_comma_ thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner_comma_ 2003; Galley et al._comma_ 2004).
1468,0,We envision the use of a clever datastructure would reduce the complexity_comma_ but leave this to future work_comma_ as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al._comma_ 2004).
1469,1,This algorithm is referred to as GHKM (Galley et al._comma_ 2004) and is widely used in SSMT systems (Galley et al._comma_ 2006; Liu et al._comma_ 2006; Huang et al._comma_ 2006).
1470,0,The word alignment used in GHKM is usually computed independent ofthesyntacticstructure_comma_andasDeNeroandKlein (2007) and May and Knight (2007) have noted_comma_ Ch-En En-Ch Union Heuristic 28.6% 33.0% 45.9% 20.1% Table 1: Percentage of corpus used to generate big templates_comma_ based on different word alignments 9-12 13-20 21 Ch-En 18.2% 17.4% 64.4% En-Ch 15.9% 20.7% 63.4% Union 9.8% 15.1% 75.1% Heuristic 24.6% 27.9% 47.5% Table 2: In the selected big templates_comma_ the distribution of words in the templates of different sizes_comma_ which are measured based on the number of symbols in their RHSs is not the best for SSMT systems.
1471,0,1313 E2C C2E Union Heuristic w/ Big 13.37 12.66 14.55 14.28 w/o Big 13.20 12.62 14.53 14.21 Table 3: BLEU-4 scores (test set) of systems based on GIZA++ word alignments 5 6 7 8  BLEU-4 14.27 14.42 14.43 14.45 14.55 Table 4: BLEU-4 scores (test set) of the union alignment_comma_ using TTS templates up to a certain size_comma_ in terms of the number of leaves in their LHSs 4.1 Baseline Systems GHKM (Galley et al._comma_ 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods_comma_ including union and the diagonal growing heuristic (Koehn et al._comma_ 2003).
1472,0,4.2 Learning Phrasal TTS Templates To test our learning methods_comma_ we start with the TTS templates generated based on e2c_comma_ c2e_comma_ and union alignments using GHKM.
1473,0,From this data_comma_ we use the the GHKM minimal-rule extraction algorithm of (Galley et al._comma_ 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB))$x1 de x0 Though this rule can be used in either direction_comma_ here we use it right-to-left (Chinese to English).
1474,0,Meanwhile_comma_ translation grammars have grown in complexity from simple inversion transduction grammars (Wu_comma_ 1997) to general tree-to-string transducers (Galley et al._comma_ 2004) and have increased in size by including more synchronous tree fragments (Galley et al._comma_ 2006; Marcuetal._comma_2006; DeNeefeetal._comma_2007).
1475,0,stituent alignments (Galley et al._comma_ 2004).
1476,0,Language modeling (Chen and Goodman_comma_ 1996)_comma_ noun-clustering (Ravichandran et al._comma_ 2005)_comma_ constructing syntactic rules for SMT (Galley et al._comma_ 2004)_comma_ and finding analogies (Turney_comma_ 2008) are examples of some of the problems where we need to compute relative frequencies.
1477,0,From the above discussion_comma_ we can see that traditional tree sequence-based method uses single tree as translation input while the forestbased model uses single sub-tree as the basic translation unit that can only learn tree-to-string (Galley et al. 2004; Liu et al._comma_ 2006) rules.
1478,0,4 Training This section discusses how to extract our translation rules given a triple nullnull_comma_null null _comma_nullnull . As we know_comma_ the traditional tree-to-string rules can be easily extracted from nullnull_comma_null null _comma_nullnull  using the algorithm of Mi and Huang (2008) 2 . We would like  2  Mi and Huang (2008) extend the tree-based rule extraction algorithm (Galley et al._comma_ 2004) to forest-based by introducing non-deterministic mechanism.
1479,0,By constructing a theory that gives formal semantics to word alignments_comma_ Galley et al.(2004) give principled answers to these questions for extracting tree-to-string rules.
1480,0,Their GHKM procedure draws connections among word alignments_comma_ derivations_comma_ and rules.
1481,0,They first identify the tree nodes that subsume tree-string pairs consistent with word alignments and then extract rules from these nodes.
1482,1,By this means_comma_ GHKM proves to be able to extract all valid tree-to-string rules from training instances.
1483,0,Although originally developed for the tree-to-string case_comma_ it is possible to extend GHKM to extract all valid tree-to-tree rules from aligned packed forests.
1484,0,In this section_comma_ we introduce our tree-to-tree rule extraction method adapted from GHKM_comma_ which involves four steps: (1) identifying the correspondence between the nodes in forest pairs_comma_ (2) identifying minimum rules_comma_ (3) inferring composed rules_comma_ and (4) estimating rule probabilities.
1485,0,We propose a number of attributes for nodes_comma_ most of which derive from GHKM_comma_ to facilitate the identification.
1486,0,Note that while a consistent node is equal to a frontier node in GHKM_comma_ this is not the case in our method because we have a tree on the target side.
1487,0,While Galley (2004) describes extracting treeto-string rules from 1-best trees_comma_ Mi and Huang et al.(2008) go further by proposing a method for extracting tree-to-string rules from aligned foreststring pairs.
1488,0,The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al._comma_ 2004).
1489,0,3.3 Tree Transducer Grammars Syntactic machine translation (Galley et al._comma_ 2004) uses tree transducer grammars to translate sentences.
1490,0,To overcome these limitations_comma_ many syntaxbased SMT models have been proposed (Wu_comma_ 1997; Chiang_comma_ 2007; Ding et al._comma_ 2005; Eisner_comma_ 2003; Quirk et al._comma_ 2005; Liu et al._comma_ 2007; Zhang et al._comma_ 2007; Zhang et al._comma_ 2008a; Zhang et al._comma_ 2008b; Gildea_comma_ 2003; Galley et al._comma_ 2004; Marcu et al._comma_ 2006; Bod_comma_ 2007).
1491,0,The basic motivation behind syntax-based model is that the syntax information has the potential to model the structure reordering and discontiguous corresponding by the intrinsic structural generalization ability.
1492,0,Although remarkable progresses have been reported_comma_ the strict syntactic constraint (the both sides of the rules should strictly be a subtree of the whole syntax parse) greatly hinders the utilization of the non-syntactic translation equivalents.
1493,0,Firstly_comma_ they classify all the GHKM2 rules (Galley et al._comma_ 2004; Galley et al._comma_ 2006) into two categories: lexical rules and non-lexical rules.
1494,0,The former are the rules whose source side has no source words.
1495,0,In other words_comma_ a non-lexical rule is a purely ab1A forest means a sub-tree sequence derived from a given parse tree 2One reviewer asked about the acronym GHKM.
1496,0,We guess it is an acronym for the authors of (Galley et al._comma_ 2004): Michel Galley_comma_ Mark Hopkins_comma_ Kevin Knight and Daniel Marcu.
1497,1,This approach has been shown to be accurate_comma_ relatively efficient_comma_ and robust using both generative and discriminative models (Roark_comma_ 2001; Roark_comma_ 2004; Collins and Roark_comma_ 2004).
1498,0,Beam-search parsing using an unnormalized discriminative model_comma_ as in Collins and Roark (2004)_comma_ requires a slightly different search strategy than the original generative model described in Roark (2001; 2004).
1499,0,A generative parsing model can be used on its own_comma_ and it was shown in Collins and Roark (2004) that a discriminative parsing model can be used on its own.
1500,0,Both left-corner strategy (Ratnaparkhi_comma_ 1997; Roark_comma_ 2001; Prolo_comma_ 2003; Henderson_comma_ 2003; Collins and Roark_comma_ 2004) and head-corner strategy (Henderson_comma_ 2000; Yamada and Matsumoto_comma_ 2003) were employed in incremental parsing.
1501,0,We also employ the voted perceptron algorithm (Freund and Schapire_comma_ 1999) and the early update technique as in (Collins and Roark_comma_ 2004).
1502,0,Discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated (Riezler et al. _comma_ 2002; Clark and Curran_comma_ 2004; Collins and Roark_comma_ 2004; Taskar et al. _comma_ 2004).
1503,0,Our approach is related to those of Collins and Roark (2004) and Taskar et al.(2004) for phrase structure parsing.
1504,0,Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm.
1505,0,However_comma_ to use parse features with sufficient history_comma_ their parsing algorithm must prune heuristically most of the possible parses.
1506,0,Though these approaches represent good first steps towards discriminatively-trained parsers_comma_ they have not yet been able to display the benefits of discriminative training that have been seen in namedentity extraction and shallow parsing.
1507,0,In particular_comma_ most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy_comma_ 2002; Shen and Joshi_comma_ 2003; Shen et al. _comma_ 2003; Collins and Roark_comma_ 2004).
1508,0,These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones_comma_ while at the same time maintaining the tractability of learning.
1509,0,For comparison to previous results_comma_ table 2 lists the results on the testing set for our best model (TOP-Efficient-Freq20) and several other statistical parsers (Collins_comma_ 1999; Collins and Duffy_comma_ 2002; Collins and Roark_comma_ 2004; Henderson_comma_ 2003; Charniak_comma_ 2000; Collins_comma_ 2000; Shen and Joshi_comma_ 2004; Shen et al. _comma_ 2003; Henderson_comma_ 2004; Bod_comma_ 2003).
1510,0,When compared to other kernel methods_comma_ our approach performs better than those based on the Tree kernel (Collins and Duffy_comma_ 2002; Collins and Roark_comma_ 2004)_comma_ and is only 0.2% worse than the best results achieved by a kernel method for parsing (Shen et al. _comma_ 2003; Shen and Joshi_comma_ 2004).
1511,1,1 Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies (Collins_comma_ 2003; Charniak and Johnson_comma_ 2005; Roark and Collins_comma_ 2004) and have also been shown to be adequate in recovering dependency relationships (Collins et al. _comma_ 1999; Levy and Manning_comma_ 2004; Dubey and Keller_comma_ 2003).
1512,0,Its also worth noting that Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features_comma_ one of which encoded the sentence-final punctuation.
1513,0,Training discriminative parsers is notoriously slow_comma_ especially if it requires generating examples by repeatedly parsing the treebank (Collins & Roark_comma_ 2004; Taskar et al. _comma_ 2004).
1514,1,Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore_comma_ 2005)_comma_ sequence analysis (Daume and Marcu_comma_ 2005; McDonald et al. _comma_ 2005a) and phrase-structure parsing (Collins and Roark_comma_ 2004).
1515,0,Thelistsmaybeused withannotation and a tuning process_comma_ such as in (Collins and Roark_comma_ 2004)_comma_ to iteratively alter feature weights and improve results.
1516,1,2.2 Perceptron-based training To tune the parameters w of the model_comma_ we use the averaged perceptron algorithm (Collins_comma_ 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark_comma_ 2004; Roark et al. _comma_ 2004).
1517,1,Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark_comma_ 2004; Henderson_comma_ 2004; Taskar et al. _comma_ 2004).
1518,0,However_comma_ relying on information from a generative model might prevent these approaches from realizing the accuracy gains achieved by discriminative methods on other NLP tasks.
1519,0,Another problem is training speed: Discriminative parsers are notoriously slow to train.
1520,0,Collins and Roark (2004) saw a LFMS improvement of 0.8% over their baseline discriminative parser after adding punctuation features_comma_ one of which encoded the sentence-final punctuation.
1521,0,The left-to-right parser would likely improve if we were to use a left-corner transform (Collins & Roark_comma_ 2004).
1522,0,Collins and Roark (2004) and Taskar et al.(2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature.
1523,0,Although generating training examples in advance without a working parser (Turian & Melamed_comma_ 2005) is much faster than using inference (Collins & Roark_comma_ 2004; Henderson_comma_ 2004; Taskar et al. _comma_ 2004)_comma_ our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor.
1524,1,This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark (2004).5 The perceptron algorithm is a convenient choice because it converges quickly  usually taking only a few iterations over the training set (Collins_comma_ 2002; Collins and Roark_comma_ 2004).
1525,0,Using a variant of the voted perceptron (Collins_comma_ 2002; Collins and Roark_comma_ 2004; Crammer and Singer_comma_ 2003)_comma_ we discriminatively trained our parser in an on-line fashion.
1526,0,3 Online Learning Again following (McDonald et al. _comma_ 2005)_comma_ we have used the single best MIRA (Crammer and Singer_comma_ 2003)_comma_ which is a variant of the voted perceptron (Collins_comma_ 2002; Collins and Roark_comma_ 2004) for structured prediction.
1527,1,Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark_comma_ 2004; Henderson_comma_ 2004; Taskar et al. _comma_ 2004).
1528,0,However_comma_ relying upon information from a generative model might limit the potential of these approaches to realize the accuracy gains achieved by discriminative methods on other NLP tasks.
1529,0,Another difficulty is that discriminative parsing approaches can be very task-specific and require quite a bit of trial and error with different hyper-parameter values and types of features.
1530,0,Although generating training examples in advance without a working parser (Sagae & Lavie_comma_ 2005) is much faster than using inference (Collins & Roark_comma_ 2004; Henderson_comma_ 2004; Taskar et al. _comma_ 2004)_comma_ our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor.
1531,0,In fact_comma_ when the perceptron update rule of (Dekel et al. _comma_ 2004)  which modifies the weights of every divergent node along the predicted and true paths  is used in the ranking framework_comma_ it becomes virtually identical with the standard_comma_ flat_comma_ ranking perceptron of Collins (2002).5 In contrast_comma_ our approach shares the idea of (Cesa-Bianchi et al. _comma_ 2006a) that if a parent class has been predicted wrongly_comma_ then errors in the children should not be taken into account. We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark_comma_ 2004)_comma_ which searches through a complex decision space step-by-step and is immediately updated at the first wrong move.
1532,1,Our work fuses this idea of selective hierarchical updates with the simplicity of the perceptron algorithm and the flexibility of arbitrary feature sharing inherent in the ranking framework.
1533,0,Recently_comma_ severalmethods(Collins and Roark_comma_ 2004; Daume III and Marcu_comma_ 2005; McDonald and Pereira_comma_ 2006) have been proposed with similar motivation to ours.
1534,1,These methods alleviate this problem by using some approximation in perceptron-type learning.
1535,0,7 Discussion As we mentioned_comma_ there are some algorithms similar to ours (Collins and Roark_comma_ 2004; Daume III and Marcu_comma_ 2005; McDonald and Pereira_comma_ 2006; Liang et al. _comma_ 2006).
1536,0,The differences of our algorithm from these algorithms are as follows.
1537,0,CollinsandRoark(2004)proposedanapproximate incremental method for parsing.
1538,0,Their method can be used for sequence labeling as well.
1539,0,These studies_comma_ however_comma_ did not explain the validity of their updating methods in terms of convergence.
1540,0,Collins and Roark (2004) used the averaged perceptron (Collins_comma_ 2002a).
1541,0,With regard to the local update_comma_ (B)_comma_ in Algorithm 4.2_comma_ early updates (Collins and Roark_comma_ 2004) and y-good requirement in (Daume III and Marcu_comma_ 2005) resemble our local update in that they tried to avoid the situation where the correct answer cannot be output.
1542,0,We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins_comma_ 2002; Collins and Roark_comma_ 2004; Crammer and Singer_comma_ 2003).
1543,0,2.3 Online Learning Again following (McDonald et al. _comma_ 2005)_comma_ we have used the single best MIRA (Crammer and Singer_comma_ 2003)_comma_ which is a margin aware variant of perceptron (Collins_comma_ 2002; Collins and Roark_comma_ 2004) for structured prediction.
1544,0,Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses early update to update the parameters when an error is encountered.
1545,1,Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark_comma_ 2004)_comma_ chunking (Daume and Marcu_comma_ 2005)_comma_ and machine translation (Cowan et al. _comma_ 2006).
1546,0,This linear model is learned using a variant of the incremental perceptron algorithm (Collins and Roark_comma_ 2004; Daume and Marcu_comma_ 2005).
1547,0,3For decoding_comma_ loc is averaged over the training iterations as in Collins and Roark (2004).
1548,0,We proposed a Perceptron like learning algorithm (Collins and Roark_comma_ 2004; Daume III and Marcu_comma_ 2005) for guided learning.
1549,0,In (Daume III and Marcu_comma_ 2005)_comma_ as well as other similar works (Collins_comma_ 2002; Collins and Roark_comma_ 2004; Shen and Joshi_comma_ 2005)_comma_ only left-toright search was employed.
1550,0,Furthermore_comma_ compared to the above works_comma_ our guided learning algorithm is more aggressive on learning.
1551,0,In (Collins and Roark_comma_ 2004; Shen and Joshi_comma_ 2005)_comma_ a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates.
1552,0,Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model.
1553,0,1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al. _comma_ 2002; Taskar et al. _comma_ 2004; Collins and Roark_comma_ 2004; Turian and Melamed_comma_ 2006).
1554,0,Parsing research has also begun to adopt discriminative methods from the Machine Learning literature_comma_ such as the perceptron (Freund and Schapire_comma_ 1999; Collins and Roark_comma_ 2004) and the largemargin methods underlying Support Vector Machines (Taskar et al. _comma_ 2004; McDonald_comma_ 2006).
1555,0,The existing work most similar to ours is Collins and Roark (2004).
1556,0,They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation.
1557,0,One possible direction for future work is to compare the search-based approach of Collins and Roark with our DP-based approach.
1558,0,5.1 Relationship to 'supervised' training To illustrate the relationship between the above symbolic training method for preference scoring and corpus-based methods_comma_ perhaps the easiest way is to compare it to an adaptation (Collins and Roark_comma_ 2004) of the perceptron training method to the problem of obtaining a best parse (either directly_comma_ or for parse reranking)_comma_ because the two methods are analogous in a number of ways.
1559,0,As to analysis of NPs_comma_ there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (Collins and Roark_comma_ 2004; McDonald et al._comma_ 2005)_comma_ and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available.
1560,0,Some recent work on incremental parsing (Collins and Roark_comma_ 2004; Shen and Joshi_comma_ 2005) showed another way to handle this problem.
1561,0,In these incremental parsers_comma_ tree structures are used to represent the left context.
1562,0,We still use complex structures to represent the partial analyses_comma_ so as to employ both top-down and bottom-up information as in (Collins and Roark_comma_ 2004; Shen and Joshi_comma_ 2005).
1563,1,Variants of this method have been successfully used in many NLP tasks_comma_ like shallow processing (Daume III and Marcu_comma_ 2005)_comma_ parsing (Collins and Roark_comma_ 2004; Shen and Joshi_comma_ 2005) and word alignment (Moore_comma_ 2005).
1564,1,Beam-search has been successful in many NLP tasks (Koehn et al._comma_ 2003; 562 Inputs: training examples (xi_comma_yi) Initialization: set vectorw = 0 Algorithm: // R training iterations; N examples for t = 1R_comma_ i = 1N: zi = argmaxyGEN(xi) (y) vectorw if zi negationslash= yi: vectorw = vectorw + (yi)(zi) Outputs: vectorw Figure 1: The perceptron learning algorithm Collins and Roark_comma_ 2004)_comma_ and can achieve accuracy that is close to exact inference.
1565,0,During training_comma_ the early update strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage_comma_ parsing is stopped immediately_comma_ and the model is updated using the current best partial item.
1566,0,Here_comma_ it might be useful to relax the strict linear control regime by exploring beam search strategies_comma_ e.g. along the lines of Collins and Roark (2004).
1567,1,Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark_comma_ 2001; Henderson_comma_ 2003; Collins and Roark_comma_ 2004)_comma_ and we will use such rich models to derive our scores.
1568,0,These findings are in line with Collins & Roarks (2004) results with incremental parsing with perceptrons_comma_ where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning.
1569,0,Following Collins and Roark (2004) we also use the early-update strategy_comma_ where an update happens whenever the goldstandard action-sequence falls off the beam_comma_ with the rest of the sequence neglected.
1570,0,In shift-reduce parsing_comma_ further mistakes are often caused by previous ones_comma_ so only the first mistake in each sentence (if there is one) is easily identifiable;7 this is also the argument for early update in applying perceptron learning to these incremental parsing algorithms (Collins and Roark_comma_ 2004) (see also Section 2).
1571,0,This algorithm and its many variants are widely used in the computational linguistics community (Collins_comma_ 2002a; Collins and Duffy_comma_ 2002; Collins_comma_ 2002b; Collins and Roark_comma_ 2004; Henderson and Titov_comma_ 2005; Viola and Narasimhan_comma_ 2005; Cohen et al._comma_ 2004; Carreras et al._comma_ 2005; Shen and Joshi_comma_ 2005; Ciaramita and Johnson_comma_ 2003).
1572,1,It is an online training algorithm and has been successfully used in many NLP tasks_comma_ such as POS tagging (Collins_comma_ 2002)_comma_ parsing (Collins and Roark_comma_ 2004)_comma_ Chinese word segmentation (Zhang and Clark_comma_ 2007; Jiang et al._comma_ 2008)_comma_ and so on.
1573,0,Several incremental parsing methods have been proposed so far (Collins and Roark_comma_ 2004; Roark_comma_ 2001; Roark_comma_ 2004).
1574,0,In these methods_comma_ the parsers can produce the candidates of partial parse trees on a word-by-word basis.
1575,0,However_comma_ they suffer from the problem of infinite local ambiguity_comma_ i.e._comma_ they may produce an infinite number of candidates of partial parse trees.
1576,0,2 Incremental Parsing This section gives a description of Collins and Roarks incremental parser (Collins and Roark_comma_ 2004) and discusses its problem.
1577,0,Collins and Roarks parser uses a grammar defined by a 6-tuple G = (V_comma_T_comma_S_comma_#_comma_C_comma_B).
1578,0,An allowable triple is a tuple X_comma_Y_comma_Z where X_comma_Y_comma_Z  V . The triple specifies which nonterminal symbol Z is allowed to follow a nonterminal symbol Y under a parent X. For each initial fragment of a sentence_comma_ Collins and Roarks incremental parser produces partial parse trees which span all words in the fragment.
1579,0,3 Incremental Parsing Method Based on Adjoining Operation In order to avoid the problem of infinite local ambiguity_comma_ the previous works have adopted the following approaches: (1) a beam search strategy (Collins and Roark_comma_ 2004; Roark_comma_ 2001; Roark_comma_ 2004)_comma_ (2) limiting the allowable chains to those actually observed in the treebank (Collins and Roark_comma_ 2004)_comma_ and (3) transforming the parse trees with a selective left-corner transformation (Johnson and Roark_comma_ 2000) before inducing the allowable chains and allowable triples (Collins and Roark_comma_ 2004).
1580,0,As we have seen in Figure 1_comma_ Collins and Roarks parser produces partial parse trees such as (c)_comma_ (d) and (e).
1581,0,The limited contexts used in this model are similar to the previous methods (Collins and Roark_comma_ 2004; Roark_comma_ 2001; Roark_comma_ 2004).
1582,1,To achieve efficient parsing_comma_ we use a beam search strategy like the previous methods (Collins and Roark_comma_ 2004; Roark_comma_ 2001; Roark_comma_ 2004).
1583,0,Each queue Hi stores the only N-best 43 Table 1: Parsing results LR(%) LP(%) F(%) Roark (2004) 86.4 86.8 86.6 Collins and Roark (2004) 86.5 86.8 86.7 No adjoining 86.3 86.8 86.6 Non-monotonic adjoining 86.1 87.1 86.6 Monotonic adjoining 87.2 87.7 87.4 partial parse trees.
1584,0,To tackle this problem_comma_ we defined 2The best results of Collins and Roark (2004) (LR=88.4%_comma_ LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead.
1585,1,Albeit simple_comma_ the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark_comma_ 2004; Collins_comma_ 2004; Zettlemoyer and Collins_comma_ 2005; Zettlemoyer and Collins_comma_ 2007).
1586,0,It is possible to prove that_comma_ provided the training set (xi_comma_zi) is separable with margin > 0_comma_ the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark_comma_ 2004).
1587,0,See also (Collins_comma_ 2004) for convergence theorems and proofs.
1588,0,Our approach is based on earlier work on LFG semantic form extraction (van Genabith et al. _comma_ 1999) and recent progress in automatically annotating the Penn-II treebank with LFG f-structures (Cahill et al. _comma_ 2004b).
1589,0,In this paper we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II treebank_comma_ with about 1 million words in 50_comma_000 sentences_comma_ based on the automatic LFG f-structure annotation algorithm described in (Cahill et al. _comma_ 2004b).
1590,0,We utilise the automatic annotation algorithm of (Cahill et al. _comma_ 2004b) to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation (i.e. an attribute value structure equation).
1591,0,(Cahill et al. _comma_ 2004b) provide four sets of annotation principles_comma_ one for non-coordinate configurations_comma_ one for coordinate configurations_comma_ one for traces (long distance dependencies) and a final catch all and clean up phase.
1592,0,The algorithm of (Cahill et al. _comma_ 2004b) translates the traces into corresponding re-entrancies in the f-structure representation (Figure 1).
1593,0,(Cahill et al. _comma_ 2004b) measure annotation quality in terms of precision and recall against manually constructed_comma_ gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II.
1594,0,The algorithm currently achieves an F-score of 96.3% for complete f-structures and 93.6% for preds-only f-structures.1 Our semantic form extraction methodology is based on the procedure of (van Genabith et al. _comma_ 1999): For each f-structure generated_comma_ for each level of embedding we determine the local PRED value and collect the subcategorisable grammatical functions present at that level of embedding.
1595,1,We are already using the extracted semantic forms in parsing new text with robust_comma_ wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II treebank (Cahill et al. _comma_ 2004a).
1596,0,Our approach is to use finite-state approximations of long-distance dependencies_comma_ as they are described in (Schneider_comma_ 2003a) for Dependency Grammar (DG) and (Cahill et al. _comma_ 2004) for Lexical Functional Grammar (LFG).
1597,0,Our approach is based on earlier work on LFG semantic form extraction (van Genabith_comma_ Sadler_comma_ and Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III Treebanks with LFG f-structures (Cahill et al. 2002; Cahill_comma_ McCarthy_comma_ et al. 2004).
1598,0,However_comma_ more recent work (Cahill et al. 2002; Cahill_comma_ McCarthy_comma_ et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994)_comma_ containing more than 1_comma_000_comma_000 words and 49_comma_000 sentences.
1599,0,We utilize the automatic annotation algorithm of Cahill et al.(2002) and Cahill_comma_ McCarthy_comma_ et al.(2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations.
1600,0,More recently_comma_ Burke_comma_ Cahill_comma_ et al.(2004a) carried out an evaluation of the automatic annotation algorithm against the publicly available PARC 700 Dependency Bank (King et al. 2003)_comma_ a set of 700 randomly selected sentences from Section 23 which have been parsed_comma_ converted to dependency format_comma_ and manually corrected and extended by human validators.
1601,0,A detailed discussion of the issues inherent in this process and a full analysis of results is presented in Burke_comma_ Cahill_comma_ et al.(2004a).
1602,0,It is important to realize that the induction of lexical resources is part of a larger project on the acquisition of wide-coverage_comma_ robust_comma_ probabilistic_comma_ deep unification grammar resources from treebanks Burke_comma_ Cahill_comma_ et al.(2004b).
1603,0,We are already using the extracted semantic forms in parsing new text with robust_comma_ wide-coverage probabilistic LFG grammar approximations automatically acquired from the f-structure-annotated Penn-II treebank_comma_ specifically in the resolution of LDDs_comma_ as described in Cahill_comma_ Burke_comma_ et al.(2004).
1604,0,We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank (Brants et al. 2002) and Penn Chinese Treebank (Xue_comma_ Chiou_comma_ and Palmer 2002)_comma_ extracting wide-coverage_comma_ probabilistic LFG grammar 361 Computational Linguistics Volume 31_comma_ Number 3 approximations and lexical resources for German (Cahill et al. 2003) and Chinese (Burke_comma_ Lam_comma_ et al. 2004).
1605,0,Finally_comma_ since non-projective constructions often involve long-distance dependencies_comma_ the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson_comma_ 2002; Dienes and Dubey_comma_ 2003; Jijkoun and de Rijke_comma_ 2004; Cahill et al. _comma_ 2004; Levy and Manning_comma_ 2004; Campbell_comma_ 2004).
1606,0,Most of this work has so far focused either on post-processing to recover non-local dependencies from context-free parse trees (Johnson_comma_ 2002; Jijkoun and De Rijke_comma_ 2004; Levy and Manning_comma_ 2004; Campbell_comma_ 2004)_comma_ or on incorporating nonlocal dependency information in nonterminal categories in constituency representations (Dienes and Dubey_comma_ 2003; Hockenmaier_comma_ 2003; Cahill et al. _comma_ 2004) or in the categories used to label arcs in dependency representations (Nivre and Nilsson_comma_ 2005).
1607,1,Even robust parsers using linguistically sophisticated formalisms_comma_ such as TAG (Chiang_comma_ 2000)_comma_ CCG (Clark and Curran_comma_ 2004b; Hockenmaier_comma_ 2003)_comma_ HPSG (Miyao et al. _comma_ 2004) and LFG (Riezler et al. _comma_ 2002; Cahill et al. _comma_ 2004)_comma_ often use training data derived from the Penn Treebank.
1608,0,c2006 Association for Computational Linguistics Robust PCFG-Based Generation using Automatically Acquired LFG Approximations Aoife Cahill1 and Josef van Genabith1_comma_2 1 National Centre for Language Technology (NCLT) School of Computing_comma_ Dublin City University_comma_ Dublin 9_comma_ Ireland 2 Center for Advanced Studies_comma_ IBM Dublin_comma_ Ireland {acahill_comma_josef}@computing.dcu.ie Abstract We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al. _comma_ 2004) automatically extracted from treebanks_comma_ maximising the probability of a tree given an f-structure.
1609,0,In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage_comma_ robust Lexical Functional Grammar (LFG) approximations automatically extracted from treebanks (Cahill et al. _comma_ 2004).
1610,0,3 PCFG-Based Generation for Treebank-Based LFG Resources Cahill et al.(2004) present a method to automatically acquire wide-coverage robust probabilistic LFG approximations1 from treebanks.
1611,0,The method is based on an automatic f-structure annotation algorithm that associates nodes in treebank trees with f-structure equations.
1612,0,Cahill et al.(2004) present two parsing architectures: the pipeline and the integrated parsing architecture.
1613,0,The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al.(2004).
1614,0,The generation process takes an f-structure (such as the f-structure on the right in Figure 1) as input and outputs the most likely f-structure annotated tree (such as the tree on the left in Figure 1) given the input fstructure argmaxTreeP(Tree|F-Str) where the probability of a tree given an fstructure is decomposed as the product of the probabilities of all f-structure annotated productions contributing to the tree but where in addition to conditioning on the LHS of the production (as in the integrated parsing architecture of Cahill et al.(2004)) each production X  Y is now also conditioned on the set of f-structure features Feats -linked2 to the LHS of the rule.
1615,0,1034 Conditioning F-Structure Features Grammar Rules Probability {PRED_comma_ SUBJ_comma_ COMP_comma_ TENSE} VP(=)  VBD(=) SBAR(COMP=) 0.4998 {PRED_comma_ SUBJ_comma_ COMP_comma_ TENSE} VP(=)  VBP(=) SBAR(COMP=) 0.0366 {PRED_comma_ SUBJ_comma_ COMP_comma_ TENSE} VP(=)  VBD(=)_comma_ S(COMP=) 6.48e-6 {PRED_comma_ SUBJ_comma_ COMP_comma_ TENSE} VP(=)  VBD(=) S(COMP=) 3.88e-6 {PRED_comma_ SUBJ_comma_ COMP_comma_ TENSE} VP(=)  VBP(=)_comma_ SBARQ(COMP=) 7.86e-7 {PRED_comma_ SUBJ_comma_ COMP_comma_ TENSE} VP(=)  VBD(=) SBARQ(COMP=) 1.59e-7 Table 1: Example VP Generation rules automatically extracted from Sections 0221 of the Penn-II Treebank P(Tree|F-Str) := productdisplay X  Y in Tree (X) = Feats P(X  Y |X_comma_Feats) (1) P(X  Y |X_comma_Feats) = P(X  Y_comma_X_comma_Feats)P(X_comma_Feats) = (2) P(X  Y_comma_Feats) P(X_comma_Feats)  #(X  Y_comma_Feats) #(X  _comma_Feats) (3) and where probabilities are estimated using a simple MLE and rule counts (#) from the automatically f-structure annotated treebank resource of Cahill et al.(2004).
1616,0,This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al.(2004) into probabilistic generation grammars.
1617,0,Our backoff uses the built-in lexical macros4 of the automatic fstructure annotation algorithm of Cahill et al.(2004) to identify potential part-of-speech categories corresponding to a particular set of features.
1618,0,In each case_comma_ we use the automatically generated f-structures from Cahill et al.(2004) from the original Section 23 treebank trees as f-structure input to our generation experiments.
1619,0,Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese_comma_ German and Spanish (Burke et al. _comma_ 2004; Cahill et al. _comma_ 2005; ODonovan et al. _comma_ 2005) has shown that given a suitable treebank_comma_ it is possible to automatically acquire high quality LFG resources in a very short space of time.
1620,0,6 Conclusion and Further Work We present a new architecture for stochastic LFG surface realisation using the automatically annotated treebanks and extracted PCFG-based LFG approximations of Cahill et al.(2004).
1621,0,1 Introduction The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual LexicalFunctional Grammar (LFG) (Bresnan_comma_ 2001) resources from treebanks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al. _comma_ 2004).
1622,0,The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al. _comma_ 2004) uses configurational_comma_ categorial_comma_ function tag and trace information.
1623,0,3 Previous Work 3.1 LFG Annotation A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II treebank has been described by Cahill et al.(2004).
1624,0,It has been shown that the methods can be ported to other languages and treebanks (Burke et al. _comma_ 2004; Cahill et al. _comma_ 2003)_comma_ including Cast3LB (ODonovan et al. _comma_ 2005).
1625,0,Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al.(2004)_comma_ which assigns grammatical Tag Meaning ATR Attribute of copular verb CAG Agent of passive verb CC Compl.
1626,0,In addition to CFG-oriented approaches_comma_ a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al. _comma_ 2003)_comma_ CCG (Clark and Hockenmaier_comma_ 2002)_comma_ LFG (Riezler et al. _comma_ 2002; Cahill et al. _comma_ 2004) and Dependency Grammar (Nivre and Nilsson_comma_ 2005) incorporate non-local dependencies into their deep syntactic or semantic representations.
1627,1,Inspired by (Cahill et al. _comma_ 2004)s methodology which was originally designed for English and Penn-II treebank_comma_ our approach to Chinese non-local dependency recovery is based on Lexical-Functional Grammar (LFG)_comma_ a formalism that involves both phrase structure trees and predicate-argument structures.
1628,0,In Section 3 we review (Cahill et al. _comma_ 2004)s method for recovering English NLDs in treebank-based LFG approximations.
1629,0,3.2 F-Structure Based NLD Recovery (Cahill et al. _comma_ 2004) presented a NLD recovery algorithm operating at LFG f-structure for treebankbased LFG approximations.
1630,0,The method automatically converts Penn-II treebank trees with traces and coindexation into proper f-structures where traces and coindexation in treebank trees (Figure 2(a)) are represented as corresponding reentrances in fstructures (Figure 2(c))_comma_ and from the f-structures automatically extracts subcat frames by collecting all arguments of the local predicate at each level of the f-structures_comma_ and further acquires finite approximations of FU equations by extracting paths linking the reentracies occurring in the f-structures.
1631,0,(Cahill et al. _comma_ 2004)s approach for English resolves three LDD types in parser output trees without traces and coindexation (Figure 2(b))_comma_ i.e. topicalisation (TOPIC)_comma_ wh-movement in relative clauses (TOPIC REL) and interrogatives (FOCUS).
1632,1,Inspired by (Cahill et al. _comma_ 2004; Burke et al. _comma_ 2004)_comma_ we have implemented an f-structure annotation algorithm to automatically obtain f-structures from CFG-trees in the CTB5.1.
1633,0,4.2 Adaptation to Chinese (Cahill et al. _comma_ 2004)s algorithm (Section 3.2) only resolves certain NLDs with known types of antecedents (TOPIC_comma_ TOPIC REL and FOCUS) at fstructures.
1634,0,In order to resolve all Chinese NLDs represented in the CTB_comma_ we modify and substantially extend the (Cahill et al. _comma_ 2004) (henceforth C04 for short) algorithm as follows: Given the set of subcat frames s for the word w_comma_ and a set of paths p for the trace t_comma_ the algorithm traverses the f-structure f to: predict a dislocated argument t at a sub-fstructure h by comparing the local PRED:w to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending GF a exists in a sub-f-structure within f; or leave t without an antecedent if an empty path for t exists In the modified algorithm_comma_ we condition the probability of NLD path p (including the empty path without an antecedent) on the GF associated of the trace t rather than the antecedent a as in C04.
1635,0,For this reason we use more syntactic features w feats in addition to word form to discriminate between appropriate subcat frames s. For a given word w_comma_ w feats include: 261 w pos: the part-of-speech of w w gf: the grammatical function of w P(s|w_comma_w feats) replaces C04s P(s|w) as lexical subcat frame probability and is estimated as: P(s|w_comma_w feats) = count(s_comma_w_comma_w feats)summationtextn i=1 count(si_comma_w_comma_w feats) (3) As more conditioning features may cause sever sparse-data problems_comma_ in order to increase the coverage of the automatically acquired subcat frames_comma_ the subcat frame frequencies count(s_comma_w_comma_w feats) are smoothed by backing off to ws part-of-speech w pos according to Eq.
1636,0,Table 3 shows the types of NLD that can be recovered by C04 and by the algorithm presented in Section 4.2.
1637,0,Exploiting the C04 algorithm to resolve the whtrace in relativisation_comma_ including ungovernable GFs TOPIC and ADJUNCT.
1638,0,Antecedent Trace Topic Rel Other Null Argument Adjunct C04    Ours     Table 3: Comparison of the ability of NLD recovery for Chinese between C04 and our algorithm 5 Experiments and Evaluation For all our experiments_comma_ we used the first 760 articles (chtb 001.fid to chtb 931.fid_comma_ 10_comma_384 sentences) of CTB5.1_comma_ from which 75 double-annotated files (chtb 001.fid to chtb 043.fid and chtb 900.fid to chtb 931.fid_comma_ 1_comma_046 sentences) were used as test data_comma_6 75 files (chtb 306.fid to chtb 325.fid and chtb 400.fid to chtb 454.fid_comma_ 1_comma_082 sentences) were held out as development data_comma_ while the other 610 files (8_comma_256 sentences) were used as training data.
1639,0,For comparison_comma_ we implemented the C04 algorithm on our data and evaluated the result.
1640,0,Since the basic algorithm focus on argument traces_comma_ results for arguments only are given separately.
1641,0,Table 7 shows that the C04 algorithm achieves a high precision but as expected a low recall due to its limitation to certain types of NLDs.
1642,0,By contrast_comma_ our basic algorithm scored higher recall but lower precision_comma_ which is understandable as the C04 algorithm identifies the trace given a known antecedent_comma_ whereas our algorithm tries to identify both the trace and antecedent.
1643,0,F (Cahill et al. _comma_ 2004) overall 95.98 57.86 72.20 73.00 40.28 51.91 90.16 54.35 67.82 65.54 36.16 46.61 args only 98.64 42.03 58.94 82.69 30.54 44.60 86.36 36.80 51.61 66.08 24.40 35.64 Basic Model overall 92.44 91.28 91.85 63.87 62.15 63.00 63.12 62.33 62.72 42.69 41.54 42.10 args only 89.42 92.95 91.15 60.89 63.45 62.15 47.92 49.81 48.84 31.41 32.73 32.06 Basic Model with Subject Path Constraint overall 92.16 91.36 91.76 63.72 62.20 62.95 75.96 75.30 75.63 50.82 49.61 50.21 args only 89.04 93.08 91.02 60.69 63.52 62.07 66.15 69.15 67.62 42.77 44.76 44.76 Table 7: Evaluation of trace insertion and antecedent recovery for C04 algorithm_comma_ our basic algorithm and basic algorithm with the subject path constraint.
1644,0,We also combine our basic algorithm (Section 4.2) with (Cahill et al. _comma_ 2004)s algorithm in order to resolve the modifier-function traces.
1645,0,The two algorithms may conflict due to (i) inserting the same trace at the same site but related to different antecedents or (ii) resolving the same antecedent to different traces.
1646,1,We keep the traces inserted by the C04 algorithm and abandon those inserted by our algorithm in case of conflict_comma_ as the results in Section 5.2 suggest that C04 has a higher precision than ours.
1647,0,Our method revises and considerably extends the approach of (Cahill et al. _comma_ 2004) originally designed for English_comma_ and_comma_ to the best of our knowledge_comma_ is the first NLD recovery algorithm for Chinese.
1648,0,The evaluation shows that our algorithm considerably outperforms (Cahill et al. _comma_ 2004)s with respect to Chinese data.
1649,0,The LFG annotation algorithm of (Cahill et al. _comma_ 2004) was used to produce the f-structures for development_comma_ test and training sets.
1650,0,The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004).
1651,0,1 Introduction Parsers have been developed for a variety of grammar formalisms_comma_ for example HPSG (Toutanova et al. _comma_ 2002; Malouf and van Noord_comma_ 2004)_comma_ LFG (Kaplan et al. _comma_ 2004; Cahill et al. _comma_ 2004)_comma_ TAG (Sarkar and Joshi_comma_ 2003)_comma_ CCG (Hockenmaier and Steedman_comma_ 2002; Clark and Curran_comma_ 2004b)_comma_ and variants of phrase-structure grammar (Briscoe et al. _comma_ 2006)_comma_ including the phrase-structure grammar implicit in the Penn Treebank (Collins_comma_ 2003; Charniak_comma_ 2000).
1652,0,The translation and reference files are analyzed by a treebank-based_comma_ probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al. _comma_ 2004)_comma_ which produces a set of dependency triples for each input.
1653,0,Cahill et al.(2004) presents Penn-II Treebankbased LFG parsing resources.
1654,0,Her approach distinguishes 32 types of dependencies_comma_ including grammatical functions and morphological information.
1655,0,In this paper_comma_ we use the parser developed by Cahill et al.(2004)_comma_ which automatically annotates input text with c-structure trees and f-structure dependencies_comma_ reaching high precision and recall rates.
1656,0,The translation and reference files are analyzed by a treebank-based_comma_ probabilistic LFG parser (Cahill et al. _comma_ 2004)_comma_ which produces a set of dependency triples for each input.
1657,0,105 Cahill et al.(2004) presents a set of Penn-II Treebank-based LFG parsing resources.
1658,0,Their approach distinguishes 32 types of dependencies_comma_ including grammatical functions and morphological information.
1659,0,This set can be divided into two major groups: a group of predicate-only dependencies and non-predicate dependencies.
1660,0,In this paper_comma_ we use the parser developed by Cahill et al.(2004)_comma_ which automatically annotates input text with c-structure trees and f-structure dependencies_comma_ obtaining high precision and recall rates.
1661,0,1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars_comma_ for example LFG (Kaplan et al. _comma_ 2004; Cahill et al. _comma_ 2004)_comma_ HPSG (Toutanova et al. _comma_ 2002; Malouf and van Noord_comma_ 2004)_comma_ TAG (Sarkar and Joshi_comma_ 2003) and CCG (Hockenmaier andSteedman_comma_2002; ClarkandCurran_comma_2004b).
1662,0,Methods for doing so_comma_ for stochastic parser output_comma_ are described by Johnson (2002) and Cahill et al (2004).
1663,0,1999)_comma_ OpenCCG (White_comma_ 2004) and XLE (Crouch et al._comma_ 2007)_comma_ or created semi-automatically (Belz_comma_ 2007)_comma_ or fully automatically extracted from annotated corpora_comma_ like the HPSG (Nakanishi et al._comma_ 2005)_comma_ LFG (Cahill and van Genabith_comma_ 2006; Hogan et al._comma_ 2007) and CCG (White et al._comma_ 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al._comma_ 1993).
1664,0,Cahill and van Genabith (2006) and Hogan et al.(2007) present a chart generator using wide-coverage PCFG-based LFG approximations automatically acquired from treebanks (Cahill et al._comma_ 2004).
1665,0,Our approach is data-driven: following the methodology in (Cahill et al._comma_ 2004; Guo et al._comma_ 2007)_comma_ we automatically convert the English PennII treebank and the Chinese Penn Treebank (Xue et al._comma_ 2005) into f-structure banks.
1666,0,The f-structures are created automatically by annotating nodes in the gold standard WSJ trees with LFG functional equations and then passing these equations through a constraint solver (Cahill et al._comma_ 2004).
1667,0,The context-free gold standard parse trees were transformed into fstructures using the automatic procedure of Cahill et al.(2004).
1668,0,(Cahill et al._comma_ 2004) managed to extract LFG subcategorisation frames and paths linking long distance dependencies reentrancies from f-structures generated automatically for the PennII treebank trees and used them in an long distance dependency resolution algorithm to parse new text.
1669,1,They achieved around 80% f-score for fstructures parsing on the WSJ part of the Penn-II treebank_comma_ a score comparable to the ones of the state-ofthe-art hand-crafted grammars.
1670,0,3.2 The parsers The parsers that we chose to evaluate are the C&C CCG parser (Clark and Curran_comma_ 2007)_comma_ the Enju HPSG parser (Miyao and Tsujii_comma_ 2005)_comma_ the RASP parser (Briscoe et al._comma_ 2006)_comma_ the Stanford parser (Klein and Manning_comma_ 2003)_comma_ and the DCU postprocessor of PTB parsers (Cahill et al._comma_ 2004)_comma_ based on LFG and applied to the output of the Charniak and Johnson reranking parser.
1671,0,It has also obtained competitive scores on general GR evaluation corpora (Cahill et al._comma_ 2004).
1672,1,Hockenmaier and Steedman (2002)_comma_ Miyao et al.(2004) and Cahill et al.(2004) show fairly good results on the Penn Treebank (for CCG_comma_ HPSG and LFG_comma_ respectively): these parsers achieve accuracies on predicate-argument relations between 80% and 87%_comma_ which show the feasibility and scalability of this approach.
1673,0,However_comma_ while this is a simple method for a highly configurational language like English_comma_ it is more difficult to extend to languages with more complex morphology or with word orders that display more freedom.
1674,0,The main application of these techniques to written input has been in the robust_comma_ lexical tagging of corpora with part-of-speech labels (e.g. Garside_comma_ Leech_comma_ and Sampson 1987; de Rose 1988; Meteer_comma_ Schwartz_comma_ and Weischedel 1991; Cutting et al. 1992).
1675,0,This situation is very similar to that involved in training HMM text taggers_comma_ where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech_comma_ and the rest of the words in the sentence are also generated (e.g. \[Cutting et al. _comma_ 1992\]).
1676,0,The two systems we use are ENGCG (Karlsson et al. _comma_ 1994) and the Xerox Tagger (Cutting et al. _comma_ 1992).
1677,0,We discuss problems caused by the fact that these taggers use different tag sets_comma_ and present the results obtained by applying the combined taggers to a previously unseen sample of text.
1678,0,2.2 Xerox Tagger The Xerox Tagger 1_comma_ XT_comma_ (Cutting et al. _comma_ 1992) is a statistical tagger made by Doug Cutting_comma_ Julian Kupiec_comma_ Jan Pedersen and Penelope Sibun in Xerox PARC.
1679,0,It was trained on the untagged Brown Corpus (Francis and Kubera_comma_ 1982).
1680,0,The tagger itself is based on the Hidden Markov Model (Baum_comma_ 1972) and word equivalence classes (Kupiec_comma_ 1989).
1681,0,Although the tagger is trained with the untagged Brown corpus_comma_ there are several ways to 'force' it to learn.
1682,0,The tagger is reported (Cutting el al. _comma_ 1992) to have a better than 96 % accuracy in the analysis of parts of the Brown Corpus.
1683,0,More recently_comma_ Cutting et al.(1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities_comma_ by using an Baum-Welch re-estimation to automatically refine the model.
1684,0,For an introduction to the algorithms_comma_ see Cutting et al.(1992)_comma_ or the lucid description by Sharman (1990).
1685,0,The algorithm is again described by Cutting et al. and by Sharman_comma_ and a mathematical justification for it can be tbund in Huang et al.(1990).
1686,1,One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al. _comma_ 1992).
1687,1,An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.
1688,1,The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible.
1689,1,The results of the Xerox experiment appear very encouraging.
1690,0,In the first place_comma_ Cutting et al. do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation.
1691,0,Secondly_comma_ it is unclear how much the initial biasing contributes the success rate.
1692,1,The kind of biasing Cutting et al. describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not.
1693,0,The Xerox experiments (Cutting et al. _comma_ 1992) correspond to something between D1 and D2_comma_ and between TO and T1_comma_ in that there is some initial biasing of the probabilities.
1694,0,All 8_comma_907 articles were tagged by the Xerox Part-ofSpeech Tagger (Cutting et al. _comma_ 1992) 4.
1695,0,In this paper_comma_ a new part-of-speech tagging method hased on neural networks (Net-Tagger) is presented and its performance is compared to that of a llMM-tagger (Cutting et al. _comma_ 1992) and a trigrambased tagger (Kempe_comma_ 1993).
1696,0,It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the iIMM-tagger.
1697,0,Jelinek (1985) and Cutting et al.(1992) circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm (also know as the forward-backward algorithm).
1698,0,They report rates of correctly tagged words which are comparable to that presented by Church (1988) and Kempe (1993).
1699,0,The performance of tl_comma_e presented tagger is measured and compared to that of two other taggers (Cutting et al. _comma_ 1992; Kempe_comma_ 1993).
1700,0,5 TIIE LEXICON The lexicon which contains the a priori tag probabilities for each word is similar to the lexicon which was used by Cutting et al.(1992).
1701,0,No documentation of tile construction algorithm of the su\[lix lexicon in (Cutting et al. _comma_ 1992) was available.
1702,0,In tabh; 2_comma_ the accuracy rate of the Net-Tagger is cOrolLated to that of a trigram l)msed tagger (Kempe_comma_ 1993) and a lIidden Markov Model tagger (Cutting et al. _comma_ 1992) which were.
1703,0,trained and tested on the same data.
1704,0,In order to determine the influence of tim size of the training sample_comma_ the taggers were also trained on corpora of different sizes and tested again r. The resulting percentages of correctly tagged words are shown in figure 4.
1705,0,These experiments demonstrate that the performance of the Net-Tagger is comparable to that of the trigram tagger and better than that of the IIMM tagger.
1706,0,A comparison of the tagging results with those of a trigram tagger and a IIMM tagger showed that the accuracy is as high as that of the trigram tagger and the robustness on small training corpora is as good as that of the HMM tagger.
1707,0,Two main approaches have generally been considered: rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz_comma_ Tannenbaum_comma_ and Carstensen 1965; Marshall 1983; Leech_comma_ Garside_comma_ and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992).
1708,0,Brill's results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging (Jelinek_comma_ 1985; Church_comma_ 1988; DeRose_comma_ 1988; Cutting et al. _comma_ 1992; Weischedel et al. _comma_ 1993)_comma_ as well as showing promise for other applications.
1709,0,The corpus lines retained are part-of-speech tagged (Cutting et al. _comma_ 1992).
1710,0,This text was part-of-speech tagged using the Xerox HMM tagger (Cutting et al. _comma_ 1992).
1711,0,No pretagged text is necessary for Hidden Markov Models (Jelinek_comma_ 1985; Cutting et al. _comma_ 1991; Kupiec_comma_ 1992).
1712,0,We obtained 47_comma_025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al. _comma_ 1992) (group average agglomeration applied to a sample).
1713,0,3 The statistical model We use the Xerox part-of-speech tagger (Cutting et al. _comma_ 1992)_comma_ a statistical tagger made at the Xerox Palo Alto Research Center.
1714,1,3.1 Training The Xerox tagger is claimed (Cutting el al. _comma_ 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required.
1715,0,We ran the tagger on another text and counted the errors.
1716,0,The result was not good; 13 % of the words were tagged incorrectly.
1717,0,The tagger does not require a tagged corpus for training_comma_ but two types of biases can be set to tell the tagger what is correct and what is not: symbol biases and transition biases.
1718,0,We spent approximately one man-month writing biases and tuning the tagger.
1719,0,When it seemed that the results could not be further improved_comma_ we tested the tagger on a new corpus.
1720,0,We selected the Xerox tagger that learns from an untagged corpuS.
1721,0,This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions_comma_ e.g. Cutting et al. 1992).
1722,0,157 ena or the linguist's abstraction capabilities (e.g. knowledge about what is relevant in the context)_comma_ they tend to reach a 95-97% accuracy in the analysis of several languages_comma_ in particular English (Marshall 1983; Black et aL 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamb~ick 1994_comma_ etc.).
1723,0,Stochastic taggers use both contextual and morphological information_comma_ and the model parameters are usually defined or updated automatically from tagged texts (Cerf-Danon and E1-Beze 1991; Church 1988; Cutting et al. 1992; Dermatas and Kokkinakis 1988_comma_ 1990_comma_ 1993_comma_ 1994; Garside_comma_ Leech_comma_ and Sampson 1987; Kupiec 1992; Maltese * Department of Electrical Engineering_comma_ Wire Communications Laboratory (WCL)_comma_ University of Patras_comma_ 265 00 Patras_comma_ Greece.
1724,0,Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993)_comma_ up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.
1725,0,7 Independently_comma_ Cutting et aL (1992) quote a performance of 800 words per second for their part-of-speech tagger based on hidden Markov models.
1726,0,These methods have reported performance in the range of 95-99% 'correct' by word (DeRose 1988; Cutting et al. 1992; Jelinek_comma_ Mercer_comma_ and Roukos 1992; Kupiec 1992).
1727,1,A number of part-of-speech taggers are readily available and widely used_comma_ all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993).
1728,0,Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g. _comma_ Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994).
1729,0,Almost all recent work in developing automatically trained part-of-speech taggers has been on further exploring Markovmodel based tagging (Jelinek 1985; Church 1988; Derose 1988; DeMarcken 1990; Merialdo 1994; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994).
1730,0,1 Motivation Statistical part-of-speech disambiguation can be efficiently done with n-gram models (Church_comma_ 1988; Cutting et al. _comma_ 1992).
1731,0,These models are equivalent to Hidden Markov Models (HMMs) (Rabiner_comma_ 1989) of order n 1.
1732,0,Almost all of the work in the area of automatically trained taggers has explored Markov-model based part of speech tagging \[Jelinek_comma_ 1985; Church_comma_ 1988; Derose_comma_ 1988; DeMarcken_comma_ 1990; Cutting et al. _comma_ 1992; Kupiec_comma_ 1992; Charniak et al. _comma_ 1993; Weischedel et al. _comma_ 1993; Schutze and Singer_comma_ 1994; Lin et al. _comma_ 1994; Elworthy_comma_ 1994; Merialdo_comma_ 1995\].
1733,0,It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm \[Baum_comma_ 1972; Jelinek_comma_ 1985; Cutting et al. _comma_ 1992; Kupiec_comma_ 1992; Elworthy_comma_ 1994; Merialdo_comma_ 1995\].
1734,0,This method is employed in \[Kupiec_comma_ 1992; Cutting et al. _comma_ 1992\].
1735,0,Language models_comma_ such as N-gram class models (Brown et al. _comma_ 1992) and Ergodic Hidden Markov Models (Kuhn el_comma_ al. _comma_ 1994) were proposed and used in applications such as syntactic class (POS) tagging for English (Cutting et al. _comma_ 1992)_comma_ clustering and scoring of recognizer sentence hypotheses.
1736,0,The tagger used is thus one that does not need tagged and disambiguated material to be trained on_comma_ namely the XPOST originally constructed at Xerox Parc (Cutting et al. 1992_comma_ Cutting and Pedersen 1993).
1737,0,It is used_comma_as tagging mode\[ in English (Church_comma_ 1988; Cutting et al. _comma_ 1992) and morphological analysis nlodel (word segmentation and tagging) in Japanese (Nagata_comma_ 1994).
1738,0,It is a natural extension of the Viteri>i algorithm (Church_comma_ 1<_comma_)88; Cutting et al. _comma_ 1992) for those languages that do not have delimiters between words_comma_ and it can generate N-best morphological analysis hypotheses_comma_ like tree trellis search (Soong and l\[uang_comma_ 1991).
1739,0,A variety of diffexent methods have been investigated_comma_ most of which fall into two broad classes:  Probabilistic methods_comma_ e. g.
1740,0,(DeRose 1988; Cutting et al 1992; Merialdo 1994).
1741,0,Probabilistic taggers have typically been implemented as hidden Markov models_comma_ using prohabilistic models with two kinds of' basic probabilities:  The lexical probability of seeing the word w given the part-of-speech t: P(w I t).
1742,0,Models of this kind are usually referred to as nclass models_comma_ the most common instances of which are the biclass (n = 2) and triclass (n = 3) models.
1743,1,Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill_comma_ 1992; Cutting et al. _comma_ 1992).
1744,0,The first is the popular idea of statistical tagging e.g.(DeRose_comma_ 1988; Cutting et al. _comma_ 1992; Church_comma_ 1988).
1745,0,1 Introduction In the part-of-speech hterature_comma_ whether taggers are based on a rule-based approach (Klein and Simmons_comma_ 1963)_comma_ (Brill_comma_ 1992)_comma_ (Voutilainen_comma_ 1993)_comma_ or on a statistical one (Bahl and Mercer_comma_ 1976)_comma_ (Leech et al. _comma_ 1983)_comma_ (Merialdo_comma_ 1994)_comma_ (DeRose_comma_ 1988)_comma_ (Church_comma_ 1989)_comma_ (Cutting et al. _comma_ 1992)_comma_ there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones.
1746,0,Section 5 shows how our approach differs from the approach taken by Cutting and Kupiec.
1747,0,5 Comparison with other approaches In some sense_comma_ this approach is similar to the notion of 'ambiguity classes' explained in (Kupiec_comma_ 1992) and (Cutting et al. _comma_ 1992) where words that belong to the same part-of-speech figure together.
1748,0,In this approach_comma_ they use the notion of word equivalence or ambiguity classes to describe words belonging to the same part-of-speech categories.
1749,0,First_comma_ in their work_comma_ the most common words are estimated individually and the less common ones are 8 put together in their respective ambiguity classes; in our work_comma_ every word is equally treated by its respective genotype.
1750,0,Second_comma_ in their work_comma_ ambiguity classes can be marked with a preferred tag in order to help disambiguation whereas in our work_comma_ there is no special annotation since words get disambiguated through the sequential application of the modules.
1751,0,Third_comma_ and perhaps the most important_comma_ in our system_comma_ the linguistic and statistical estimations are entirely done on the genotypes only_comma_ regardless of the words.
1752,0,(Chanod and Tapanainen_comma_ 1995) compare two tagging frameworks for tagging French_comma_ one that is statistical_comma_ built upon the Xerox tagger (Cutting et al. _comma_ 1992)_comma_ and another based on linguistic constraints only.
1753,0,Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church_comma_ 1988; DeRose_comma_ 1988; Cutting et al. 1992; Merialdo_comma_ 1994_comma_ etc.).
1754,0,In 14 these approaches_comma_ a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus.
1755,0,Kupiec (1992) has proposed an estimation method for the N-gram language model using the Baum-Welch reestimation algorithm (Rabiner et al. _comma_ 1994) from an untagged corpus and Cutting et al.(1992) have applied this method to an English tagging system.
1756,0,Generalized Forward Backward Reestimation Generalization of the Forward and Viterbi Algorithm In English part of speech taggers_comma_ the maximization of Equation (1) to get the most likely tag sequence_comma_ is accomplished by the Viterbi algorithm (Church_comma_ 1988)_comma_ and the maximum likelihood estimates of the parameters of Equation (2) are obtained from untagged corpus by the ForwardBackward algorithm (Cutting et al. _comma_ 1992).
1757,0,However_comma_ it is impossible to apply the Viterbi algorithm and the Forward-Backward algorithm for word segmentation of those languages that have no delimiter between words_comma_ such as Japanese and Chinese_comma_ because word segmentation hypotheses overlap one another.
1758,0,The accuracy of the derived model depends heavily on the initial bias_comma_ but with a good choice results are comparable to those of method three (Cutting et al. _comma_ 1992).
1759,0,Others perform the division implicitly without discussing performance (e.g.(Cutting et al. _comma_ 1992)).
1760,0,5.4 Assigning GramInatical Function Labels Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf.e.g.(Cutting et al. _comma_ 1992) and (Feldweg_comma_ 1995)).
1761,0,For Czech_comma_ we created a prototype of the first step of this process -the part-of-speech (POS) tagger -using Rank Xerox tools (Tapanainen_comma_ 1995)_comma_ (Cutting et al. _comma_ 1992).
1762,0,We have also included the results of English tagging using the same Xerox tools.
1763,0,Then_comma_ the tagset will be reduced even further_comma_ but nevertheless_comma_ not as much as we did for the Xeroxtools-based experiment_comma_ because that tagset is too 'rough' for many applications_comma_ even though the results are good.
1764,0,The Xerox tagger (Cutting et al. 1992) comes with a set of rules that assign an unknown word a set of possible pos-tags (i.e. _comma_ POS-class) on the basis of its ending segment.
1765,0,As the baseline standard_comma_ we took the ending-guessing rule set supplied with the Xerox tagger (Cutting et al. 1992).
1766,0,When we compared the Xerox ending guesser with the induced ending-guessing rule set (Ending*)_comma_ we saw that its precision was about 6% poorer and_comma_ most importantly_comma_ it 416 Andrei Mikheev Unknown-Word Guessing could handle 6% fewer unknown words.
1767,0,This made the improvement over the baseline Xerox guesser 13% in precision and 7% in coverage on the test sample.
1768,0,Because of the similarities in the algorithms with the LISP implemented Xerox tagger_comma_ we could directly use the Xerox guessing rule set with the HMM tagger.
1769,0,This gave us a search-space of four basic combinations: the HMM tagger equipped with the Xerox guesser_comma_ the Brill tagger with its original guesser_comma_ the HMM tagger with our cascading (Prefix+Suffix+Suffixl+Ending-C*) guesser and the Brill tagger with the cascading guesser.
1770,0,We do not know whether the same holds for the Brill tagger and the Brill and Xerox guessers since we took them pretrained.
1771,0,Unknown Words Unknown Common Words Unknown Proper Nouns Tagger Guesser Metrics Error Error Coverage Error Coverage HMM Xerox mean 17.851643 30.022169 37.567270 10.785563 63.797113 s-error 0.484710 0.469922 1.687396 0.613745 1.714969 HMM Cascade mean 12.378716 21.266264 36.507909 7.776456 64.795969 s-error 0.917656 0.403957 2.336381 0.853958 2.206457 Brill Brill mean 14.688501 27.411736 38.998687 6.439525 62.160917 s-error 0.908172 0.539634 2.627234 0.501082 4.010992 Brill Cascade mean 11.327863 20.986240 37.933048 5.548990 63.816586 s-error 0.761576 0.480798 2.353510 0.561009 3.775991 the Brown Corpus_comma_ we obtained the error rate (mean) 0* (.)=4.003093 with the standard error deB=0.155599.
1772,0,The cascading guesser outperformed the other two guessers in general and most importantly in the non-proper noun category_comma_ where it had an advantage of 6.5% over Brill's guesser and about 8.7% over Xerox's guesser.
1773,0,The cascading guesser also helped to improve the accuracy on unknown proper nouns by about 1% in comparison to Brill's guesser and about 3% in comparison to Xerox's guesser.
1774,0,When we compared this distribution to that of the Xerox guesser we saw that the accuracy of the Xerox guesser itself was only about 6.5% lower than that of the cascading guesser 9 and the fact that it could handle 6% fewer unknown words than the cascading guesser resulted in the increase of incorrect assignments by the default strategy.
1775,0,The cascading guesser outperformed the guesser supplied with the Xerox tagger and the guesser supplied with Brill's tagger both on unknown proper nouns 9 We attribute this to the 13% lower precision of the Xerox guesser.
1776,0,For instance_comma_ our ending-guessing rules are akin to those of Xerox and the morphological rules resemble some rules of Brill's_comma_ but ours use more constraints and provide a set of all possible tags for a word rather than a single best tag.
1777,0,We believe that the technique for the induction of the ending-guessing rules is quite similar to that of Xerox 1 or Schmid (1994) but differs in the scoring and pruning methods.
1778,0,For instance_comma_ the ending guesser of Xerox includes 536 rules whereas our Ending * guesser includes 2_comma_196 guessing rules;  the information listed in a general-purpose lexicon can be considered to be of better quality than that derived from an annotated corpus_comma_ since it lists all possible readings for a word rather than only those that happen to occur in the corpus.
1779,0,We also believe that general-purpose lexicons contain less erroneous information than those derived from annotated corpora; 10 Xerox's technique is not documented and can be determined only by inspection of the source code.
1780,0,There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques_comma_ e.g._comma_ (Church_comma_ 1988; Cutting et al. _comma_ 1992; DeRose_comma_ 1988)_comma_ constraint-based techniques (Karlsson et al. _comma_ 1995; Voutilainen_comma_ 1995b; Voutilainen_comma_ Heikkil/i_comma_ and Anttila_comma_ 1992; Voutilainen and Tapanainen_comma_ 1993; Oflazer and KuruSz_comma_ 1994; Oflazer and Till 1996) and transformation-based techniques (Brilt_comma_ 1992; Brill_comma_ 1994; Brill_comma_ 1995).
1781,0,Second_comma_ the automatic approach_comma_ in which the model is automatically obtained from corpora (either raw or annotated) 1_comma_ and consists of n-grams (Garside et al. _comma_ 1987; Cutting et ah_comma_ 1992)_comma_ rules (Hindle_comma_ 1989) or neural nets (Schmid_comma_ 1994).
1782,0,These corpus-based models can be represented e.g. as collocational matrices (Garside et al.(eds).
1783,0,1987: Church 1988)_comma_ Hidden Markov models (cf.Cutting et al. 1992)_comma_ local rules (e.g. Hindle 1989) and neural networks (e.g. Schmid 1994).
1784,0,Taggers using these statistical language models are generally reported to assign the correct and unique tag to 95-97% of words in running text.
1785,0,4 Tagging Grammatical Functions 4.1 The Tagger In contrast to a standard part-of-speech tagger which estimates lexical and contextual probabilities of tags from sequences of word-tag pairs in a corpus_comma_ (e.g.(Cutting et al. _comma_ 1992; Feldweg_comma_ 1995))_comma_ the tagger for grammatical functions works with lexical and contextual probability measures Pq().
1786,0,In our experiments_comma_ we used the Hidden Markov Model (HMM) tagging method described in \[Cutting et aL_comma_ 1992\].
1787,0,In this method_comma_ the probability of seeing a given tag depends on the ambiguity class of the word and on the ambiguity class of the words preceding it.
1788,0,This class is used during the Xerox HMM tagging in place of more specific lexical (= word-based) probabilities.
1789,0,Lexical probabilities would more accurately inform the tagger with the frequency with which a certain word receives a certain tag_comma_ but acquiring this frequency requires much greater amounts of tagged text than is necessary with the ambiguity class method.
1790,0,4 Morphological Disambiguation There are two kinds of methods for morphological disambiguation: on one hand_comma_ statistical methods need little effort and obtain very good results (Church_comma_ 1988; Cutting el al._comma_ 1992)_comma_ at least when applied to English_comma_ but when we try to apply them to Basque we encounter additional problems; on the other hand_comma_ some rule-based systems (Brill_comma_ 1992; Voutilainen et al._comma_ 1992) are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.
1791,0,The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model (HMM) (Cutting et al. _comma_ 1992; Kupiec.
1792,0,However.
1793,0,since statistical approaches take into account neighboring tags only within a limited window (usually two or three)_comma_ sometimes the decision cannot cover all linguistic contexts necessary for POS disambiguation.
1794,0,Also the approaches are inappropriate for idiomatic expressions for which lexical terms need to be directly referenced.
1795,0,The statistical approaches are not enough especially for agglutinative languages (such as Korean) which have usually complex morphological structures.
1796,0,85 re-studied to overcome the limitations of staffstical approaches by learning symbolic tagging rules automatically from a corpus (Brill_comma_ 1992; Bril!.
1797,0,To gain the portability and robustness and also to overcome the limited coverage of statistical approaches_comma_ we adopt a hybrid method that can combine both statistical and rule-based approaches for POS disambiguation.
1798,0,Our statistical tagging model is adjusted from standard bi-grams using the Viterbi-search (Cutting et al. _comma_ 1992) plus on-the-fly extra computing of lexical probabilities for unknown morphemes.
1799,0,In contrast to a standard probabilistic POS tagger (e.g.(Cutting et al. _comma_ 1992; Feldweg_comma_ 1995))_comma_ the tagger for grammatical functions works with lexical (1) Selbst besucht ADV VVPP himself visited hat Peter Sabine VAFIN NE NE has Peter Sabine 'Peter never visited Sabine himself' l hie ADV never Figure 2: Example sentence and contextual probability measures PO.(') depending on the category of a mother node (Q).
1800,0,On the other hand_comma_ according to the data-driven approach_comma_ a frequency-based language model is acquired from corpora and has the forms of ngrams (Church_comma_ 1988; Cutting et al. _comma_ 1992)_comma_ rules (Hindle_comma_ 1989; Brill_comma_ 1995)_comma_ decision trees (Cardie_comma_ 1994; Daelemans et al. _comma_ 1996) or neural networks (Schmid_comma_ 1994).
1801,0,As a common strategy_comma_ POS guessers examine the endings of unknown words (Cutting et al. 1992) along with their capitalization_comma_ or consider the distribution of unknown words over specific parts-of-speech (Weischedel et aL_comma_ 1993).
1802,0,2.2 STT: A Statistical Tree-based Tagger The aim of statistical or probabilistic tagging (Church_comma_ 1988; Cutting et al. _comma_ 1992) is to assign the most likely sequence of tags given the observed sequence of words.
1803,0,For doing so_comma_ two kinds of information are used: the lexical probabilities_comma_ i.e_comma_ the probability of a particular tag conditional on the particular word_comma_ and the contextual probabilities_comma_ which describe the probability of a particular tag conditional on the surrounding tags.
1804,0,The SPECIALIST minimal commitment parser relies on the SPECIALIST Lexicon as well as the Xerox stochastic tagger (Cutting et al. 1992).
1805,0,After lexical look-up and resolution of category label ambiguity by the Xerox tagger_comma_ complementizers_comma_ conjunctions_comma_ modals_comma_ prepositions_comma_ and verbs are marked as boundaries.
1806,0,Text with locally-defined acronyms expanded is submitted to the Xerox tagger and the SPECIALIST parser.
1807,1,Recent comparisons of approaches that can be trained on corpora (van Halteren et al. _comma_ 1998; Volk and Schneider_comma_ 1998) have shown that in most cases statistical aproaches (Cutting et al. _comma_ 1992; Schmid_comma_ 1995; Ratnaparkhi_comma_ 1996) yield better results than finite-state_comma_ rule-based_comma_ or memory-based taggers (Brill_comma_ 1993; Daelemans et al. _comma_ 1996).
1808,0,They are only surpassed by combinations of different systems_comma_ forming a 'voting tagger'.
1809,0,5 Related work Cutting introduced grouping of words into equiva.lence classes based on the set of possible tags to reduce the number of the parameters (Cutting et al. _comma_ 1992) . Schmid used tile equivaleuce classes for smoothing.
1810,1,(Cutting et al. _comma_ 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.
1811,1,This impressive result triggered several follow-up studies in which the effect of hand tuning the tag dictionary was quantified as a combination of labeled and unlaPierre/NNP Vinken/NNP NP will/MD join/VB the/DT board/NN NP as/IN a/DT nonexecutive/JJ director/NN NP PP VP VP S Figure 1: An example of the kind of output expected from a statistical parser.
1812,0,beled data.
1813,0,POS disambiguation has usually been performed by statistical approaches_comma_ mainly using the hidden Markov model (HMM) in English research communities (Cutting et al. 1992; Kupiec 1992; Weischedel et al. 1993).
1814,0,These approaches are also dominant for Korean_comma_ with slight improvements to accommodate the agglutinative nature of Korean.
1815,0,Our statistical tagging model is modified from the standard bigrams (Cutting et al. 1992) using Viterbi search plus onthe-fly extra computing of lexical probabilities for unknown morphemes.
1816,0,Syntactic analysis The phrases in our bibliographic and clinical samples were then submitted to an underspecified syntactic analysis described by Rindflesch et al.(2000) that draws on a stochastic tagger (see (Cutting et al. _comma_ 1992) for details) as well as the SPECIALIST Lexicon5_comma_ a large syntactic lexicon of both general and medical English that is distributed with the UMLS.
1817,0,Although not perfect_comma_ this combination of resources effectively addresses the phenomenon of part-of-speech ambiguity in English.
1818,1,Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead.
1819,0,The prime public domain examples of such implementations include the TrigramsnTags tagger (Brandts 2000)_comma_ Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997).
1820,0,The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al. _comma_1992).
1821,1,It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model (Cutting et al. _comma_ 1992).
1822,0,Note that the notion of partial tagging described in Cutting_comma_ et al_comma_ is essentially different from what we consider here.
1823,0,Whereas they assume a lexicon which_comma_ for every term in the vocabulary_comma_ lists its possible parts of speech_comma_ we construct a lexicon which imposes a single sense (or a few senses; see Section 3.2) on each of the few thousand most frequent terms_comma_ but provides no information about other terms.
1824,0,As in Cutting_comma_ et al_comma_ however_comma_ we can use BaumWelch re-estimation to extract information from novel terms_comma_ and apply the Viterbi algorithm to dispose of a particular occurrence.
1825,0,This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al. _comma_ 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson_comma_ 2001).
1826,0,The initial phase relies on a parser that draws on the SPECIALIST Lexicon (McCray et al. 1994) and the Xerox Part-of-Speech Tagger (Cutting et al. 1992) to produce an underspecified categorial analysis.
1827,0,A further source of error in this sentence is that down-regulated was analyzed by the tagger as a past tense rather than past participle_comma_ thus causing the argument identification phase to look for an object to the right of this verb form.
1828,0,In the absence of an annotated corpus_comma_ dependencies can be derived by other means_comma_ e.g. part413 of-speech probabilities can be approximated from a raw corpus as in (Cutting et al. _comma_ 1992)_comma_ word-sense dependencies can be derived as definition-based similarities_comma_ etc. Label dependencies are set as weights on the arcs drawn between corresponding labels.
1829,0,Many approaches for POS tagging have been developed in the past_comma_ including rule-based tagging (Brill_comma_ 1995)_comma_ HMM taggers (Brants_comma_ 2000; Cutting and others_comma_ 1992)_comma_ maximum-entropy models (Rathnaparki_comma_ 1996)_comma_ cyclic dependency networks (Toutanova et al. _comma_ 2003)_comma_ memory-based learning (Daelemans et al. _comma_ 1996)_comma_ etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging).
1830,0,5.2 Assigning complex ambiguity tags In the tagging literature (e.g. _comma_ Cutting et al (1992)) an ambiguity class is often composed of the set of every possible tag for a word.
1831,0,It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al. _comma_ 1992).
1832,0,For English there are many POS taggers_comma_ employing machine learning techniques like transformation-based error-driven learning (Brill_comma_ 1995)_comma_ decision trees (Black et al. _comma_ 1992)_comma_ markov model (Cutting et al. 1992)_comma_ maximum entropy methods (Ratnaparkhi_comma_ 1996) etc. There are also taggers which are hybrid using both stochastic and rule-based approaches_comma_ such as CLAWS (Garside and Smith_comma_ 1997).
1833,0,The accuracy of these taggers ranges from 93-98% approximately.
1834,0,Stochastic models (Cutting et al. _comma_ 1992; Dermatas et al. _comma_ 1995; Brants_comma_ 2000) have been widely used in POS tagging for simplicity and language independence of the models.
1835,0,In such cases_comma_ additional information may be coded into the HMM model to achieve higher accuracy (Cutting et al. _comma_ 1992).
1836,0,The semi-supervised model described in Cutting et al.(1992)_comma_ makes use of both labeled training text and some amount of unlabeled text.
1837,0,Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers.
1838,1,4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.4 And ambiguity classes have been shown to be successfully employed_comma_ in a variety of ways_comma_ to improve POS tagging (e.g._comma_ Cutting et al._comma_ 1992; Daelemans et al._comma_ 1996; Dickinson_comma_ 2007; Goldberg et al._comma_ 2008; Tseng et al._comma_ 2005).
1839,0,There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning (Brill_comma_ 1995)_comma_ decision trees (Black et al._comma_ 1992)_comma_ Markov model (Cutting et al._comma_ 1992)_comma_ maximum entropy methods (Ratnaparkhi_comma_ 1996) etc for English.
1840,0,For a tagger to function as a practical component in a language processing system_comma_ a tagger must be robust_comma_ efficient_comma_ accurate_comma_ tunable and reusable (Cutting_comma_ 1992).
1841,0,The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks_comma_ 1990; Zernik and Jacobs_comma_ 1990; Hindle_comma_ 1990).
1842,0,In this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of 'corruption' with either 'investigator' or 'researcher'_comma_ without looking for any syntactic relation (as in Church and Hanks (1990)).
1843,0,INTRODUCTION Word associations have been studied for some time in the fields of psycholinguistics (by testing human subjects on words)_comma_ linguistics (where meaning is often based on how words co-occur with each other)_comma_ and more recently_comma_ by researchers in natural language processing (Church and Hanks_comma_ 1990; Hindle and Rooth_comma_ 1990; Dagan_comma_ 1990; McDonald et al. _comma_ 1990; Wilks et al. _comma_ 1990) using statistical measures to identify sets of associated words for use in various natural language processing tasks.
1844,0,Three recent papers in this area are Church and Hanks (1990)_comma_ Hindle (1990)_comma_ and Smadja and McKeown (1990).
1845,0,Church is also interested primarily in open-class collocations_comma_ but he does discuss verbs that tend to be followed by infinitives within his mutual information framework.
1846,0,Mutual information_comma_ as applied by Church_comma_ is a measure of the tendency of two items to appear near one-another -their observed frequency in nearby positions is divided by the expectation of that frequency if their positions were random and independent.
1847,0,To measure the tendency of a verb to be followed within a few words by an infinitive_comma_ Church uses his statistical disambiguator 2Error rates computed by hand verification of 200 examples for each SF using the tagged mode.
1848,0,A broad view of the possible scope of lexical semantics would thus be one which tries to chart out the systematic_comma_ generalizable aspects of word meanings_comma_ and of the relations between words_comma_ drawing on readily accessible sources of lexical knowledge_comma_ such as machine readable dictionaries_comma_ encyclopedias_comma_ and representative corpora_comma_ coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources_comma_ for instance custom-built parsers to cope with dictionary definitions (Vossen 1990)_comma_ statistical programs to deal with the distributional properties of lexical items in large corpora (Church & Hanks 1990) etc. At the same time this kind of massive data-acquisition should be made sensitive to the borders between perceptual experience_comma_ lexical knowledge and expert knowledge.
1849,0,Church K.W._comma_ and P. Hanks_comma_ 'Word association norms_comma_ mutual information_comma_ and lexicography'_comma_ Computational Linguistics_comma_16/l_comma_ 1990_comma_ pp.
1850,1,The results of these studies have important applications in lexicography_comma_ to detect lexicosyntactic regularities (Church and Hanks_comma_ 19901 (Calzolari and Bindi_comma_1990)_comma_ such as_comma_ for example~ support verbs (e.g. 'make-decision') prepositional verbs (e.g. 'rely-upon') idioms_comma_ semantic relations (e.g. 'part_of') and fixed expressions (e.g. 'kick the bucket').
1851,0,In (Calzolari and Bindi_comma_ 1990)_comma_ (Church and Hanks_comma_ 1990) the significance of an association (x_comma_y) is measured by the mutual information I(x_comma_y)_comma_ i.e. the probability of observing x and y together_comma_ compared with the probability of observing x and y independently.
1852,0,Despite the use of these methods to add evidence to the data_comma_ the major problem with word-pairs collections is that reliable results are obtained only for a small subset of high-frequency words on very large corpora_comma_ otherwise the association ratio becomes unstable.
1853,0,For example_comma_ Church run his experiment on a corpus with over 20-30 millions words_comma_ and Hindle reports 6 millions words as not being an adequate corpus.
1854,0,For example_comma_ parsing the sentence: fabbrica di scarpe per uomo e per bambino (*manufacture of shoes for man and child) produces the following relations: N_prep N(fabbrica_comma_di_comma_scarpe) N prep_N(fabbrica_comma_per_comma_uomo) N_prep_N(fabbrica_comma_per_comma_bambino) N_prep_N(scarpe_comma_per_comma_uomo) N_prep_N(scarpe_comma_per_comma_bambino) N_cong_N(uomo_comma_e_comma_bambino) Unlike Church and Hindle_comma_ we are not interested in collecting binary or ternary relations between words within a sentence_comma_ but rather in detecting recurring binary syntactic associations in the corpus.
1855,0,Congress of the Italian Association for Artificial Intelligence_comma_ Palermo_comma_ 1991 B. Boguraev_comma_ Building a Lexicon: the Contribution of Computers_comma_ IBM Report_comma_ T.J. Watson Research Center_comma_ 1991 M. Brent_comma_ Automatic Aquisition of Subcategorization frames from Untagged Texts_comma_ in (ACL_comma_ 1991) N. Calzolari_comma_ R. Bindi_comma_ Acquisition of Lexical Information from Corpus_comma_ in (COLING 1990) K. W. Church_comma_ P. Hanks_comma_ Word Association Norms_comma_ Mutual Information_comma_ and Lexicography_comma_ Computational Linguistics_comma_ vol.
1856,0,a Hindle and Rooth (1991) and Church and Hanks (1990) used partial parses generated by Fidditch to study word ~urrt.nc patterns m syntactic contexts.
1857,0,IC function is a derivative of Fano's mutual information formula recently used by Church and Hanks (1990) to compute word co-occurrence patterns in a 44 million word corpus of Associated Press news stories.
1858,0,They noted that while generally satisfactory_comma_ the mutual information formula often produces counterintuitive results for low-frequency data.
1859,0,This is particularly worrisome for relatively smaller IR collections since many important indexing terms would be eliminated from consideration.
1860,0,Using techniques described in Church and Hindle (1990)_comma_ Church and Hanks (1990)_comma_ and Hindle and Rooth (1991)_comma_ below are some examples of the most frequent V-O pairs from the AP corpus.
1861,0,Initially_comma_ we acquired patterns for verb-object collocations by analyzing lists of root word pairs that were weighted for relative co-occurrence in a corpus of articles 12 Susan W. McRoy Using Multiple Knowledge Sources from the Dow Jones News Service (cf.Church and Hanks 1990; Smadja and McKeown 1990).
1862,0,In addition_comma_ IC is stable even for relatively low frequency words_comma_ which can be contrasted with Fano's mutual information formula recently used by Church and Hanks (1990) to compute word cooccurrence patterns in a 44 million word corpus of Associated Press news stories.
1863,0,They noted that while generally satisfactory_comma_ the mutual information formula often produces counterintuitive results for lowfrequency data.
1864,0,Researchers such as (Evans et al. 1991) and (Church and Hanks 1990) have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb_comma_ verb-object pairs.
1865,1,INTRODUCTION Using mutual information for measuring word association has become popular since \[Church and Hanks_comma_ 1990\] defined word association ratio as mutual information between two words.
1866,0,Word association ratios are a promising tool for lexicography_comma_ but there seem to be at least two limitations to the method: 1) much data with low frequency words or word pairs cannot be used and 2) generalization of word usage still depends totally on lexicographers.
1867,0,In this paper_comma_ we propose an alternative (or extended) method for suggesting word associations using Chi-square statistics_comma_ which can be viewed as an approximation to mutual information.
1868,0,Rather than considering significance of joint frequencies of word pairs as \[Church and Hanks_comma_ 1990\] did_comma_ our algorithm uses joint frequencies of pairs of word groups instead.
1869,0,Our algorithm considers joint frequencies of pairs of word groups (as \[Brown et.
1870,0,al. 1992\] does) in contrast to joint frequencies of word pairs as in \[Church and Hanks_comma_ 1990\] and \[Hindle 1990\].
1871,0,Looking at Table 3 below_comma_ if we had to ignore cooccurrences with frequency less than five (as \[Church and Hanks 1990\] did)_comma_ there would be very little data.
1872,0,7 This discussion could also be cast in an information theoretic framework using the notion of 'mutual information' (Fano 1961)_comma_ estimating the variance of the degree of match in order to find a frequency-threshold (see Church and Hanks 1990).
1873,0,Using techniques described in Church and Hindle (1990)_comma_ Church and Hanks (1990)_comma_ and Hindle and Rooth (1991)_comma_ Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus.
1874,0,As discussed above_comma_ there is a growing body of research on deriving collocations from corpora (cf.Church and Hanks 1990; Klavans_comma_ Chodorow_comma_ and Wacholder 1990; Wilks et al. 1993; Smadja 1991a_comma_ 1991b; Calzolari and Bindi 1990).
1875,0,In addition_comma_ there are statistical tools that can help determine the relative strength of collocational relations.
1876,0,For example_comma_ Church and Hanks (1990) describe the use of the mutual information index for this purpose (cf.Calzolari and Bindi 1990).
1877,1,These tools are important in that the strongest collocational associations often represent different word senses_comma_ and thus 'they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags' (Church and Hanks 1990_comma_ p. 28).
1878,0,However_comma_ such tools do not directly characterize word senses or even provide any direct indication of the number of different senses that a word has.
1879,0,1 Church and Hanks (1990; Church et al. 1991) thus emphasize the importance of human judgment used in conjunction with these tools.
1880,0,Statistical data about these various cooccurrence relations is employed for a variety of applications_comma_ such as speech recognition (Jelinek_comma_ 1990)_comma_ language generation (Smadja and McKeown_comma_ 1990)_comma_ lexicography (Church and Hanks_comma_ 1990)_comma_ machine translation (Brown et al. _comma_ ; Sadler_comma_ 1989)_comma_ information retrieval (Maarek and Smadja_comma_ 1989) and various disambiguation tasks (Dagan et al. _comma_ 1991; Hindle and Rooth_comma_ 1991; Grishman et al. _comma_ 1986; Dagan and Itai_comma_ 1990).
1881,0,The mutual information of a cooccurrence pair_comma_ which measures the degree of association between the two words (Church and Hanks_comma_ 1990)_comma_ is defined as (Fano_comma_ 1961): P(xly) I(x_comma_y) -log 2 P(x_comma_y) _ log 2 (1) P(x)P(y) P(x) = log 2 P(y\[x) P(Y) where P(x) and P(y) are the probabilities of the events x and y (occurrences of words_comma_ in our case) and P(x_comma_ y) is the probability of the joint event (a cooccurrence pair).
1882,0,In the results we describe here_comma_ we use mutual information (Fano 1961_comma_ 27-28; Church and Hanks 1990) as the metric for neighbourhood pruning_comma_ pruning which occurs as the network is being generated.
1883,0,In our approach_comma_ we take into account both the relative positions of the nearby context words as well as the mutual information (Church & Hanks_comma_ 1990) associated with the occurrence of a particular context word.
1884,0,The similarities computed from these measures of the context contain information about both syntactic and semantic relations.
1885,0,The power of the approach appears to result from using a focused corpus_comma_ using detailed positional information_comma_ using mutual information measures and using a clustering method that updates the detailed context information when each new cluster is formed.
1886,0,Subsequently_comma_ a 600-dimensional vector of mutual information values_comma_ MI_comma_ is computed from the frequencies as follows_comma_ log 2 NZ~ + Ml(cw)= \[.\].j1\] This expresses the mutual information value for the context word c appearing with the target word w. The mutual information is large whenever a context word appears at a much higher frequency_comma_ fcw_comma_ in the neighborhood of a target word than would be predicted from the overall frequencies in the corpus_comma_ fc and fw.
1887,0,The formula adds 1 to the frequency ratio_comma_ so that a 0 (zero) occurrence corresponds to 0 mutual information.
1888,0,Church et al.(1989)_comma_ Wettler & Rapp (1989) and Church & Hanks (1990) describe algorithms which do this.
1889,0,However_comma_ the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects.
1890,0,In particular_comma_ mutual information (Church and Hanks_comma_ 1990; Wu and Su_comma_ 1993) and other statistical methods such as (Smadja_comma_ 1993) and frequency-based methods such as (Justeson and Katz_comma_ 1993) exclude infrequent phrases because they tend to introduce too much noise.
1891,0,In (Zernik 1990; Calzolari and Bindi 1990; Smadja 1989; Church and Hanks 1990) associations are detected in a 5 window.
1892,0,of AAAI 1991 (Church and llanks_comma_ 19901_comma_ K. Church and P. llauks_comma_ Word association norm_comma_ mutnal information and lexicography_comma_ Computational Linguistics_comma_ vol.
1893,0,Different statistical methods have Mso been tested to extract collocations from large corpora_comma_ as \[Church and lianks_comma_ 1990_comma_ Smadja aim McKeown_comma_ 1990\].
1894,0,In the field of eomputationa.1 linguistics_comma_ mutual information \[Brown et al. _comma_ 1988\]_comma_ 2 \[Church and Hanks_comma_ 1990\]_comma_ or a likelihood ratio test \[Dunning_comma_ 199a\] are suggested.
1895,0,The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993).
1896,0,Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation (Gale_comma_ Church_comma_ and Yarowsky 1992b_comma_ 1993; Sch6tze 1992_comma_ 1993) (see Section 7 for more details and Church and Hanks 1990; Smadja 1993_comma_ for other applications of these statistics).
1897,0,It is possible to apply these methods using statistics of the target language and thus incorporate them within the framework proposed here for target word selection.
1898,0,Lexical collocation functions_comma_ especially those determined statistically_comma_ have recently attracted considerable attention in computational linguistics (Calzolari and Bindi 1990; Church and Hanks 1990; Sekine et al. 1992; Hindle and Rooth 1993) mainly_comma_ though not exclusively_comma_ for use in disambiguation.
1899,1,Collocation has been applied successfully to many possible applications (Church et aal._comma_ 1989)_comma_ e.g_comma_ lexicography (Church and Hanks_comma_ 1990)_comma_ information retrieval (Salton_comma_ 1986a)_comma_ text input (Yamashina and Obashi_comma_ 1988)_comma_ etc. This paper will touch on its feasibility in topic identification.
1900,0,Other representative collocation research can be found in Church and Hanks (1990) and Smadja (1993).
1901,0,Unlike Choueka (1988)_comma_ Church and Hanks (1990) identify as collocations both interrupted and uninterrupted sequences of words.
1902,0,Unlike Church and Hanks (1990)_comma_ Smadja (1993) goes beyond the 'two-word' limitation and deals with 'collocations of arbitrary length'.
1903,0,Following Church and Hanks (1990)_comma_ they use mutual information to select significant two-word patterns_comma_ but_comma_ at the same time_comma_ a lexical inductive process is incorporated which_comma_ as they claim_comma_ can improve the collection of domain-specific terms.
1904,0,Previous research in automatic acquisition focuscs primarily on the use of statistical techniques_comma_ such as bilingual alignment (Church and Hanks_comma_ 1990; Klavans and Tzoukermann_comma_ 1996; Wu and Xia_comma_ 1995)_comma_ or extraction of syntactic constructions from online dictionaries and corpora (Brant_comma_ 1993; Dorr_comma_ Garman_comma_ and Weinberg_comma_ 1995).
1905,1,In the past five years_comma_ important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks_comma_ 1990; Hindle_comma_ 1990; Smadja_comma_ 1993; Grei~nstette_comma_ 1994; Grishman and Sterling_comma_ 1994).
1906,0,Most of these approaches_comma_ however_comma_ need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation.
1907,0,Hindle uses the observed frequencies within a specific syntactic pattern (subject/verb_comma_ and verb/object) to derive a cooccu_comma_> rence score which is an estimate of mutual information (Church and Hanks_comma_ 1990).
1908,0,There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words (Church and Hanks 1990); the distance between words (Smadja and Makeown 1990); and the number of combined words and frequency of appearance (Kita 1993_comma_ 1994).
1909,0,But it was not easy to identify and extract expressions of arbitrary lengths and high frequency of appearance from very large corpora.
1910,0,Researchers (Church & Hanks_comma_ 1990; Kita & al. _comma_ 1994_comma_ inter al).
1911,0,have noted that mutual information tends to be insensitive to high fi'equency patterns_comma_ and unstable for low frequency patterns.
1912,0,Given this_comma_ the mutual information ratio (Church & Hanks_comma_ 1990; Church & Mercer_comma_ 1993; Steier & Belew_comma_ 1991) is expressed by Formula 1.
1913,0,(Church & Hanks refer to this measure as the association ratio tbr technical reasons).
1914,0,592 \]./ --lOg2(RWT)))(~2 ) = (N *Occ(\[wl_comma_w2l)~ Formula 1: The mutual information ratio The instability of statistical measures seems to be a problem in statistical bigralns.
1915,0,To avoid this use the rule of thumb that a bigram must occur more than four times (cf.Church & Hanks_comma_ 1990:p.24) to be considered as a candidate/br an interesting bigram.
1916,0,\[\] Although we could fix this problem by redefining f(x_comma_ y) to be symmetric (by averaging the matrix with its transpose)_comma_ we have decided not to do so_comma_ since order information appears to be very interesting'.
1917,0,(Church & Hanks_comma_ 1990:p.24) Merkel_comma_ Nilsson_comma_ & Ahrenberg (1994) have constructed a system that uses frequency of recurrent segments to determine long phrases.
1918,0,In the field of statistical analysis of natural language data_comma_ it is common to use measures of lexical association_comma_ such as the informationtheoretic measure of mutual information_comma_ to extract useful relationships between words (e.g. Church and Hanks (1990)).
1919,0,Lexical association has its limits_comma_ however_comma_ since often either the data is insufficient to provide reliable word/word correspondences_comma_ or the task requires more abstraction than word/word correspondences permit.
1920,0,More rare words rather than common words are found even in standard dictionaries (Church and Hanks_comma_ 1990).
1921,0,We used *TH*=3 following 'a very rough rule of thumb' used for word-based mutual information in (Church and Hanks_comma_ 1990).
1922,0,Most of the previously proposed methods to extract compounds or to measure word association using mutual information (MI) either ignore or penalize items with low co-occurrence counts (Church and Hanks 1990_comma_ Su_comma_ Wu and Chang 1994)_comma_ because MI becomes unstable when the co-occurrence counts are very small.
1923,0,Previous research in automatic acquisition focuses primarily on the use of statistical techniques_comma_ such as bilingual alignment (Church and Hanks_comma_ 1990; Klavans and Tzoukermann_comma_ 1996; Wu and Xia_comma_ 1995)_comma_ or extraction of syntactic constructions from online dictionaries and corpora (Brent_comma_ 1993; Dorr_comma_ Garman_comma_ and Weinberg_comma_ 1995).
1924,0,Previous research in automatic acquisition focuses primarily on the use of statistical techniques_comma_ such as bilingual alignment (Church and Hanks_comma_ 1990; Klavans and Tzoukermann_comma_ 1995; Wu and Xia_comma_ 1995) or extraction of syntactic constructions from online dictionaries and corpora (Brent_comma_ 1993).
1925,0,Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words.
1926,0,Tools like Xtract (Smadja 1993) were based on the work of Church and others_comma_ but made a step forward by incorporating various statistical measurements like z-score and variance of distribution_comma_ as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output.
1927,0,Thus_comma_ given a hyponym definition (O) and a set of candidate hypernym definitions_comma_ this method selects the candidate hypernym definition (E) which returns the maximum score given by formula (1): SC(O_comma_ E) : E cw(wi_comma_ wj) (I) 'wIEOAwj6E The cooccurrence weight (cw) between two words can be given by Cooccurrence Frequency_comma_ Mutual Information (Church and Hanks_comma_ 1990) or Association Ratio (Resnik_comma_ 1992).
1928,0,In each experiment_comma_ performance IMutu'_comma_d Information provides an estimate of the magnitude of the ratio t)ctw(.(-n the joint prol)ability P(verb/noun_comma_1)reposition)_comma_ and the joint probability a.~suming indcpendcnce P(verb/noun)P(prcl)osition ) s(:(_comma_ (Church and Hanks_comma_ 1990).
1929,0,The classifier uses mutual information (MI) scores rather than the raw frequences of the occurring patterns (Church and Hanks_comma_ 1990).
1930,0,Computing MI scores is by now a standard procedure for measuring the co-occurrence between objects relative to their overall occurrence.
1931,0,MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) We can use this definition to derive an estimate of the connectedness between words_comma_ in terms of collocations (Smadja_comma_ 1993)_comma_ but also in terms of phrases and grammatical relations (Hindle_comma_ 1990).
1932,0,For instance the co-occurrence of verbs and the heads of their NP objects iN: size of the corpus_comma_ i.e. the number of stems): N Cobj (v n) = log2 /(v) /(n) N N All nouns are now classified by running a similaxity measure over their MI scores and the MI scores of each CoRELEx class.
1933,0,s e_comma_ the window to consider when extracting words related to word w_comma_ should span from postttuon w-5 to w+5 Maarek also defines the resolwng power of a parr m a document d as P = ~'Pd log Pc where Pd is the observed probabshty of appearance of the pan' m document d_comma_ Pc the observed probabdny of the pmr recorpus_comma_ and -log Pc the quantity of mformauon assocmted to the pmr It Is easdy seen that p wall be h|gher_comma_ the higher the frequency of the pmr m the document and the lower sts frequency m the corpus_comma_ which agrees wlth the sdea presented at the begmnmg of this sectton Church and Hanks (1990) propose the apphcatlon of the concept of mutual mformatton e(x_comma_y) ~_comma_(x.y) = hog2 ecx)e(y) 51 to the retrieval_comma_ ro a corpus_comma_ of pairs of lextcally related words They alsoconslder a word span of :e5 words and observe that 'roterestrog' pmr_comma_ s generally present a mutual mformatxon above 3 Salton and.Allan (1995) foc~as on paragraph level Each paragraph Is represented by a weighed vector_comma_ where each element is a term (typically.
1934,0,The proposed approach As stated before_comma_ the method of Church and Hanks identifies pmrs of lexlcally related words So_comma_ for instance_comma_ the pair \[conselho seguranfa\] (security conned)_comma_ with an assocmted mutual mforn~uon of 5.3_comma_ can be considered as a potential mdexang term_comma_ while the pan' \[para a\] (to the)_comma_ though 63 tunes roere frequent ro our corpus_comma_ having a mutual roformauon of 0.7_comma_ can be excluded We have then a erttenon for exclusion_comma_ that dispenses with the need for stop hsts_comma_ and that alms at assunng the exlstence of a leracal relation between the words of the rematrong pairs But not all pans of lexlcally related words are good rodexmg terms of a document The pair should also meet the reqmrement of being relevant m the considered document The method of Maarek proposes a measure of the resolvrog power of each pair ro the concerned document_comma_ thus enabling the selection_comma_ among all the poten.al indexing terms_comma_ of those that are relevant m each document For rostanco_comma_ \[estados umdos\] (united states) has a hxgh mutual roformatmn (8 1) but it can be of little relevance m an article about the hberatton of prisoners by the Serbs of Sarajevo (p=0007) The experiments earned out point to a threshold of the resolwng power around 0 01 We consider as relevant ro a document only the pairs vath a resolvrog power above this threshold When the same pmr occurs ro chfferent paragraphs of the same document_comma_ hnks can be estabhshed between those paragraphs At flus point_comma_ we only consider pairs that were not excluded ro prewous steps (mutual roformatton > 3 and resolwng power > 0 01) Though_comma_ each hnk Is not hnuted to pairs of words In fact_comma_ the 52 wider the hnk_comma_ the higher its relevance After processing a document_comma_ we often get overlapping pmrs For instance_comma_ m an amcle where the expression dos tr~s antJgos behgerantea (of the three former contenders) ts used repeatedly_comma_ the foll0vang pmrs were retrieved \[tr~s behgerantes\] \[an~gos behgerantes\] \[dos behgerantes\] 1 By ohserwng the overlap of these pmrs ro the very document_comma_ a single hnk can be retrieved_comma_ m the form of the tuple \[dos trOs antlgos behgerantes\] Adaptmg the roethod of Salton and Allan_comma_ we can formulate the hypothesis that the paragraph vath the larger number of hnks to other paragraphs would be of central impox~tance in the document In summary_comma_ the steps of the proposed method are *m a base corpus_comma_ compute the frequency of each word and the frequency of each co-occurrence_comma_ consadenng a window spanrong from posihon 14_comma_-5 to w+5_comma_ *to each document c~mpute_comma_ smuIarly_comma_ the frequency of each word and each co-occurrence_comma_ *exclude_comma_ from the co-r.e~m'ences \]dent:fled m the document_comma_ those presenting a mutual mformatlon or a resolving power under the defined thresholds (I(x_comma_y) < 3 or p < 0 01)_comma_  take the selected pans and group the overlapping ones_comma_ the resulting tuples (pairs and groups of pairs) occmTmg repeatedly ro different paragraphs estabhsh hnks between those paragraphs_comma_ *hypothetically_comma_ the paragraph presenting a larger number of hnk~ to other paragraphs wall be of central_comma_mportanco in the document It should be noted that this proposal_comma_ compared to Salton and Allan's_comma_ has the advantages (at least ro theory) of avotchng the use_comma_ always arbitrary_comma_ of stop hsts 2_comma_ and of basing the calculations exclusively on the tuples that are relevant ro the document_comma_ instead of using the heavy vectors containing all the terms of each paragraph We don't have_comma_ so far_comma_ enough data to make any clmm about the comparative quahty of the links 1 pairs \[tr~s antlgos\] \[dos an#gos\]_comma_ though considered relevant_comma_ didn't score enough mutuat reformat=on to be selected 2 the relevance of a word depends on the context_comma_ so_comma_ we prefer not to a pnon exclude any word_comma_ by sandtng it to a stop kst In fact_comma_ some of the tuplas we retheved as relevant include words that would otherw=se be pad of such a Ist An example m the pmr \[n~o ahnhados\] (nonaligned) where the word n_comma_~o (not) though quite significant tn context_comma_ would be excluded wa stop #st . i I |j :!
1935,0,robust mforrmatlon extractlon_comma_ and readlly-avmlable on-hne NLP resources These techtuques and resources allow us to create a richer indexed source of Imgmstlc and domain knowledge than other frequency approaches Our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon_comma_ ohtinned from automated methods m contrast to labor-lntenslve_comma_ discourse-based approaches Moreover_comma_ our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg hand-crafting domain-dependent rules of the knowledge-based approaches Our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 System Description Our summarization system DlmSum consmts of the Summarization Server and the Summarlzatzon Chent The Server extracts features (the Feature Extractor) from a document using various robust NLP techmques_comma_ described In Sectzon 2 1_comma_ and combines these features (the Feature Combiner) to basehne multiple combinations of features_comma_ as described m Section 2 2 Our work m progress to automattcally tram the Feature Combiner based upon user and apphcatlon needs m presented in Section 2 2 2 The Java-based Chent_comma_ which wdl be dmcnssed In Section 4_comma_ provides a graphical user interface (GUI) for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated sumInarles 2.1 Extracting Stlmmarization Features In this section_comma_ we describe how we apply robust NLP technology to extract summarization features Our goal IS to add more mtelhgence to frequencybased approaches_comma_ to acqmre domain knowledge In a more automated fashion_comma_ and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 2.1.1 Going Beyond a Word Frequency-based summarization systems typically use a single word stnng as a umt for counting frequencies Whde such a method IS very robust_comma_ it ignores the semantic content of words and their potential membership m multi-word phrases For example_comma_ zt does not dmtmgumh between 'bill' m 'Bdl Table 1 Collocations with 'chlps' {potato tortdla corn chocolate b~gle} chips {computer pentmm Intel macroprocessor memory} chips {wood oak plastlc} cchlps bsrgmmng clups blue clups mr chips Clmton' and 'bill' in 'reform bill' This may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multl-word phrases For DlrnSum_comma_ we use term frequency based on tf*Idf (Salton and McGdl_comma_ 1983_comma_ Brandow_comma_ Mitze_comma_ and Rau_comma_ 1995) to derive ssgnature words as one of the summarization features If single words were the sole basra of countmg for our summarization application_comma_ nome would be introduced both m term frequency and reverse document frequency However_comma_ recent advances in statmtlcal NLP and information extraction make it possible to utilize features which go beyond the single word level Our approach is to extract multi-word phrases automatlcally with high accuracy and use them as the basic unit in the summarization process_comma_ including frequency calculation Ftrst_comma_ just as word association methods have proven effective m lemcal analysis_comma_ e g (Church and Hanks_comma_ 1990)_comma_ we are exploring whether frequently occurring Collocatlonal reformation can improve on simple word-based approaches We have preprocessed about 800 MB of LA tlmes/Wastnngton Post newspaper articles nsmg a POS tagger (Bnll_comma_ 1993) and derived two-word noun collocations using mutual information The.
1936,0,of the works of (Kuplec_comma_ Pedersen_comma_ and Chen_comma_ 1995) and (Brandow_comma_ Mltze_comma_ .and Ran_comma_ 1995)_comma_ and advances summarmatlon technology by applynag corpus-based statistical NLP teehmques_comma_ robust information extraction_comma_ and readily avaalable on-hne resources Our prehxmnary experiments with combining different summarization features have been reported_comma_ and our current effort to learn to combine these features to produce the best summaries has been described The features derived by these robust NLP techmques were also utihzed m presentmg multiple summary.vtews to the user m a novel way References Advanced Research Projects Agency 1995 Proceed:rigs of S:zth Message Understanding Conference (MUC-6) Morgan Kanfmann Pubhshers Brandow_comma_ Ron_comma_ Karl Mltze_comma_ and Lisa Ran 1995 Automatic condensation of electromc pubhcatlous by sentence selection Information Processing and  Management_comma_ 31_comma_ forthcoming .Bull_comma_ Eric 1993 A Comps-based Approach to Language Learning Ph D thesm_comma_ Umverslty of Pennsylvania Church_comma_ Kenneth and Patrick Hanks 1990 Word  Aesoclatlon Norrns_comma_ Mutual Information_comma_ and Lexicography Computational Lmgmstscs_comma_ 16(1) Church_comma_ Kenneth W 1995 One term or two 9 In Proceedings of the 17th Annual International SIGIR Conference on Research and Development In Informatzon Retrzeral_comma_ pages 310-318 Edmundson_comma_ H P 1969 New methods m automatic abstracting Journal of the ACM_comma_ 16(2) 264-228 Fum_comma_ Dando_comma_ Glovanm Gmda_comma_ and Carlo Tasso 1985 Evalutatmg Importance A step towards text surnmarlzatlon In I3CAI85_comma_ pages 840-844IJCAi_comma_ AAAI Hahn_comma_ Udo 1990 Topic parsing Accounting for text macro structures m full-text analysm In format:on Processing and Management_comma_ 26(1)135170 Harman_comma_ Donna 1991 How effective is suttixang ~ Journal of the Amerlcan Sot:cry for Informatwn Sc:ence_comma_ 42(1) 7-15 Harman_comma_ Donna 1996 Overview of the fifth text retrieval conference (tree-5) In TREC-5 Conference Proceedings Jmg_comma_ Y and B Croft 1994 An Assoc:atwn Thesaurns for Informatzon Retrseval Umass Techmcal Report 94-I7 Center for Intelligent Information Retrieval_comma_ University of Massachusetts Johnson_comma_ F C_comma_ C D Prate_comma_ W J Black_comma_ and A P Neal 1993.
1937,0,The collocations have been calculated according to the method described in Church and Hanks (1990) by moving a window on the texts.
1938,0,The cohesion between two words is measured as in Church and Hanks (1990) by an estimation of the mutual information based on their collocation frequency.
1939,0,This value is normalized by the maximal mutual information with regard to the corpus_comma_ which is given by: /max ---log2 N2(Sw1) with N: corpus size and Sw: window size 5.
1940,0,Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks_comma_ 1990) and outputted to a lexicon.
1941,0,For instance_comma_ Church and Hanks (1990) calculated SA in terms of mutual infornlation between two words wl and w2: N * f(wl_comma_ w2) I(wl_comma_w2) = lo.q2 f(wt)f(w2) (1) here N is the size of the corpus used in the estilnation_comma_ f(wl_comma_W2) is the frequency of the cooccurrence_comma_ f(wl) and f(w2) that of each word.
1942,0,The cohesion between words has been evaluated with the mutual information measure_comma_ as in (Church and Hanks_comma_ 1990).
1943,0,l(x;y) = log (P(x_comma_y) / e(x)e(y) ) MI has been used to identify a variety of interesting linguistic phenomena_comma_ ranging from semantic relations of the doctor/nurse type to lexico-syntactic co-occurrence preferences of the save/from type (Church and Hanks_comma_ 1990).
1944,0,The combination of MI and RIDF is better than either by itself.
1945,0,RIDF is like MI_comma_ but different References Church_comma_ K. and P. Hanks (1990)Word association norms_comma_ mutual information_comma_ and lexicography Computational Linguistics_comma_ 16:1_comma_ pp.
1946,0,2Mutual information_comma_ though potentially of interest as a measure of collocational status_comma_ was not tested due to its well-known property of overemphasising the significance of rare events (Church and Hanks_comma_ 1990).
1947,0,We use mutual information as a tool for computing similarity between words.
1948,0,Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b (Church and Hanks_comma_ 1990).
1949,0,Arguably the most widely used is the mutual information (Hindle_comma_ 1990; Church and Hanks_comma_ 1990; Dagan et al. _comma_ 1995; Luk_comma_ 1995; D. Lin_comma_ 1998a).
1950,0,We then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information (MI) (Church and Hanks_comma_ 1990) statistics obtained only from the target document collection.
1951,0,More specifically_comma_ The mutual information statistics between pairs of words were used to determine whether English words from different sets generated by the translation process are 'compatible'.
1952,0,This is another area where we make use of mutual information obtained from a text corpus.
1953,1,Since the goal of disambiguation is to select the best pair among many alternatives as described above_comma_ the mutual information statistic is a natural choice in judging the degree to which two words co-occur within a certain text boundary.
1954,0,Mutual information values are calculated based on word co-occurrence statistics and used as a measure to calculate correlation between words.
1955,0,The mutual information Ml(x_comma_y) is defined as the following formula (Church and Hanks_comma_ 1990).
1956,0,p(x_comma_ y) N fw(X_comma_ y ) MI(x_comma_ y) = log 2 = log z (1) p(x)p(y) f(x)f(y) Here x and y are words occurring within a window of w words.
1957,0,In our query translation scheme_comma_ MI values are used to select most likely translations after each Korean query word is translated into one or more English words.
1958,0,Our use of MI values is based on the assumption that when two words co-occur in the same query_comma_ they are likely to cooccur in the same affinity in documents.
1959,0,In a sense_comma_ we are conjecturing mutual information can reveal some degree of semantic association between words.
1960,0,Table 2 gives some examples of MI values for the alternative word pairs for translated queries of TREC-6 Cross-Language IR Track.
1961,0,These MI values were extracted from the English text corpus consisting of 1988 1990 AP news_comma_ which contains 116_comma_759_comma_540 words.
1962,0,4 Disambiguation and Weight Calculation We can alleviate the translation ambiguity by discriminating against those word pairs with low MI values.
1963,0,The word pair with the highest MI value is considered to be the correct one among all the candidates in the two sets.
1964,0,Although we use the mutual information statistic to measure the association_comma_ others such as those used by Ballesteros & Croft (1998) can be considered.
1965,0,Figure 2 shows the MI values calculated for the word pairs comprising the translations of the original query.
1966,0,The lines indicate that mutual information values are available for the pairs_comma_ and the numbers show some of the significant MI values for the corresponding pairs among all the possible pairs.
1967,0,An Example of Word Pairs with MI Values Our bilingual word disambiguation and weighting schemes rely on both relative and absolute magnitudes of the MI vales.
1968,0,The algorithm first looks for the pair with the highest MI value and selects the best candidates before and after the pair by comparing the MI values for the pairs that are connected with the initially chosen pairs.
1969,0,It should be noted that the words not chosen in this process are not used in the translated query unless the MI values are greater than a threshold.
1970,0,Then the three MI values for the pairs containing air are compared to select the <automobile_comma_ air> pair_comma_ resulting in <automobile_comma_ air_comma_ pollution>.
1971,0,Wb = f(x) 0.5 + 0.5 (2) 0+1 Here x and 0 are a MI value and a threshold_comma_ respectively.
1972,0,The numerator_comma_ f(x)_comma_ gives the smallest integer greater than the MI value so that the resulting weight is the same for all the candidates whose MI values are within a certain interval.
1973,0,Based on our observation of the calculated MI values_comma_ we chose to use 3.0 as the cut-off value in choosing the best candidate and assign a fairly high weight.
1974,0,The cut-off value was determined purely based on the data we obtained; it can vary based on the new range of MI values when different corpora are used.
1975,0,Our query translation method uses mutual information extracted from the 1988 1990 AP corpus in order to solve the problems of the bilingual word disambiguation and query term weighting.
1976,0,Second_comma_ we intend to experiment with other co-occurrence metrics_comma_ instead of the mutual information statistic_comma_ for possible improvement.
1977,0,This investigation is motivated by our observation of some counterintuitive MI values.
1978,0,We preferred the log-likelihood ratio to other statistical scores_comma_ such as the association ratio (Church and Hanks_comma_ 1990) or ;(2_comma_ since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize (Dunning_comma_ 1993; Daille_comma_ 1996).
1979,0,Word association norms based on co-occurrence information have been proposed by (Church and Hanks 1990).
1980,0,2.1.3 Correlation analysis As a correlation measure between terms_comma_ we use mutual information (Church and Hanks 1990).
1981,0,The mutual inlbrmation between terms t~ and t i is defined by the following formula: g(ti_comma_ tj)/~' g(t_comma__comma_ t_comma_) Ml(ti_comma_ tj) = log__comma_ /i.i_comma_ {t'(t0/ i ~ f(t0}' { f(tj)//j~ f(ti_comma_ where f(t~) is the occurrence frequency of term t~_comma_ and g(ti_comma_ti ) is the co-occurrence frequency of terms t~ and tj.
1982,0,ed terms for each term is predetermined as well as a threshold for tile mutual information_comma_ and associated terms are selected based on the de-o scending order of mutual information.
1983,0,Mutual infornaation involves a problem in that it is overestimated for low-frequency terms (I)unning 1993).
1984,0,A large corpus is vahmble as a source of such nouns (Church and Hanks_comma_ 1990; Brown et al. _comma_ 1992).
1985,0,Church and Hanks (1990) use mutual information to identify collocations_comma_ a method they claim is reasonably effective for words with a frequency of not less than five.
1986,0,A frequency threshold of five seems quite low.
1987,0,Unfortunately_comma_ even this lower frequency threshold of five is too high for the extraction of side-effect-related terms from our medical abstracts.
1988,0,While we have observed reasonable results with both G 2 and Fisher's exact test_comma_ we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information (MI) measure (Church and Hanks 1990): I(x_comma_y) --log 2 P(x_comma_y) (4) P(x)P(y) In (4)_comma_ y is the seed term and x a potential target word.
1989,0,A high MI score for a given target word suggests an association between this target and the seed term.
1990,0,Or perhaps more precisely_comma_ a low MI score suggests a dissociation between target and seed word (Manning and Sch/itze 1999).
1991,0,The x-axis represents the MI 312 Weeber_comma_ Vos_comma_ and Baayen Extracting the Lowest-Frequency Words f-~k g o (a) / -< ~ 0.I 0.2~-'7~__7 _ .q_comma_-' '_comma_~'_comma_  ._comma_ 04.
1992,0,Panel (a) shows the F score as a function of both W/C-ratio and mutual information cut-off value.
1993,0,Note that F is rather indifferent to variation in window size and MI cut-off value.
1994,0,Interestingly_comma_ the highest possible MI cut-off point equals 4.27: the right-hand edge of the plateau.
1995,0,This has the unfortunate consequence that_comma_ with regard to their MI score_comma_ truly remarkably distributed target words become indistinguishable from the statistically unremarkable hapax legomena.
1996,0,Panel (b) of Figure 7 displays the corresponding results when we use Fisher's exact test rather than MI.
1997,0,Instead of varying the MI cut-off value_comma_ we vary the significance level a. Note that the resulting F scores tend to be roughly twice as high as those obtained with MI-based extraction.
1998,0,2 We conclude that_comma_ at least for the present word extraction task_comma_ Fisher's exact test compares favorably to mutual information (as does G2).
1999,0,The size of the complement was always determined with respect to these new conditional corpora_comma_ and not with respect to all MEDLINE 2 Note that we manipulate the a-levels in the same way as the MI cut-off values.
2000,0,org/pubs/citations/ j ournals/toms/1986-12-2/p154-meht a/ Mutual Information Given the definition of Mutual Information (Church and Hanks 1990)_comma_ I(x_comma_y) = log 2 P(x_comma_y) P(x)P(y)' we consider the distribution of a window word according to the contingency table (a) in Table 4.
2001,0,For mutual information (MI)_comma_ we use two different equations: one for two-element compound nouns (Church and Hanks_comma_ 1990) and the other for three-element compound nouns (Suet al. _comma_ 1994).
2002,0,The association relationship between two words can be indicated by their mutual information_comma_ which can be further used to discover phrases \[Church :& Hanks (1990)\].
2003,0,Strength of association between subject i and verb j is measured using mutual information (Church and Hanks 1990): )ln()_comma_( ji ij tftf tfNjiMI  = . Here tfij is the maximum frequency of subject-verb pair ij in the Reuters corpus_comma_ tfi is the frequency of subject head noun i in the corpus_comma_ tfj is the frequency of verb j in the corpus_comma_ and N is the number of terms in the corpus.
2004,0,We then rank-order the P X|Y MI XY M Z Pr Z|Y MI ZY G092log [P X P Y P X P Y ] f Y [P XY P XY ] f XY [P XY P XY ] f XY M iG13X_comma_X} jG13Y_comma_Y} (f ij G09 ij ) 2 ij f XY G09 XY XY (1G09( XY /N)) f XY G09 XY f XY (1G09(f XY /N)) Table 1: Probabilistic Approaches METHOD FORMULA Frequency (Guiliano_comma_ 1964) f XY Pointwise Mutual Information (MI) (Fano_comma_ 1961; Church and Hanks_comma_ 1990) log (P / PP) 2XY XY Selectional Association (Resnik_comma_ 1996) Symmetric Conditional Probability (Ferreira and Pereira_comma_ 1999) P / PP XY X Y 2 Dice Formula (Dice_comma_ 1945) 2 f / (f +f ) XY X Y Log-likelihood (Dunning_comma_ 1993; (Daille_comma_ 1996).
2005,0,Since we need knowledge-poor Daille_comma_ 1996) induction_comma_ we cannot use human-suggested filtering Chi-squared (G24 ) 2 (Church and Gale_comma_ 1991) Z-Score (Smadja_comma_ 1993; Fontenelle_comma_ et al. _comma_ 1994) Students t-Score (Church and Hanks_comma_ 1990) n-gram list in accordance to each probabilistic algorithm.
2006,0,Collocations were extracted according to the method described in (Church and Hanks_comma_ 1990) by moving a window on texts.
2007,0,One way of resolving query ambiguities is to use the statistics_comma_ such as mutual information (Church and Hanks_comma_ 1990)_comma_ to measure associations of query terms_comma_ on the basis of existing corpora (Jang et al_comma_ 1999).
2008,0,At this stage_comma_ we can apply statistical ambiguity resolution method based on mutual information.
2009,0,To observe the effect of clusters_comma_ we compared the results after disambiguation based on mutual information (tone_base and tone_rerank).
2010,0,We selected the best translation based on mutual information among all translation terms.
2011,0,Mutual information MI(x_comma_y) is defined as following (Church and Hanks_comma_ 1990): )()( )_comma_( log )()( )_comma_( log)_comma_( 22 yfxf yxfN ypxp yxp yxMI  == (4) where f(x) and f(y) are frequency of term x and term y_comma_ respectively.
2012,1,The performance of our implicit ambiguity resolution method for all translations (tall_rerank) shows 8.63% improvement compared with that of ambiguity resolution based on mutual information (tone_base).
2013,0,Our method after disambiguation (tone_rerank) using mutual information improved about 39.6% over vector space retrieval for all translations queries (tall_base).
2014,0,The cluster-based implicit disambiguation method_comma_ therefore_comma_ is more effective for performance improvement than the simple query disambiguation method based on mutual information; if used together_comma_ it shows yet further improvement.
2015,0,When our method was used with the query ambiguity resolution method based on mutual information_comma_ it showed 105.87% performance improvement of the monolingual retrieval.
2016,0,Equation (10) is of interest because the ratio p(C | v_comma_ r)/p(C | r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resniks association score_comma_ which will be introduced in Section 6.
2017,0,There are several distance measures suitable for this purpose_comma_ such as the mutual information(Church and Hanks_comma_ 1990)_comma_ the dice coefficient(Manning and Schueutze 8.5_comma_ 1999)_comma_ the phi coefficient(Manning and Schuetze 5.3.3_comma_ 1999)_comma_ the cosine measure(Manning and Schueutze 8.5_comma_ 1999) and the confidence(Arrawal and Srikant_comma_ 1995).
2018,0,a35 a1 a19 a35 a26a16a20 a35 a14 a27 a2 a34a37a36 a35 a1 a19a39a38 a26a16a20a40a38 a14 a2 a34a37a36a41a31 a1 a19a39a38a39a26a16a20a40a38 a14a43a42 a31 a1 a19 a35 a26a16a20 a35 a14 where a31 a1 a19a27a26a16a20 a14 a10 a22 a1 a21a23a22 a24 a1 a19a27a26a16a20 a14 a26 a21a23a22a25a28 a1 a19a27a26a16a20 a14 a26 a21a23a22 a30 a1 a19a27a26a16a20 a14 a26 a21a23a22a25a32 a1 a19 a26a16a20 a14a16a14 The following are examples of a22 a1a45a44 a26a47a46 a26a7a48 a26 a21 a14 . a1 cosine function a44 a49 a1a45a44a23a50 a46 a14 a5 a1a45a44a23a50 a48 a14 a1 dice coefficient a51 a5a52a44 a1a45a44a53a50 a46 a14a54a50 a1a45a44a53a50 a48 a14 a1 confidence a44 a44a26a50 a48 a1 pairwise mutual information a44 a3 a5 a7 a9a12a11 a44 a5 a3 a1a45a44a26a50 a46 a14 a5 a1a45a44a26a50 a48 a14 a1 phi coefficient a44 a5 a21a56a55 a46 a5 a48 a49 a1a45a44a26a50 a46 a14 a5 a1a45a44a26a50 a48 a14 a5 a1 a46 a50 a21 a14 a5 a1 a48 a50 a21 a14 a1 complementary similarity measure a44 a5 a21a56a55 a46 a5 a48 a49 a1a45a44a26a50 a48 a14 a5 a1 a46 a50 a21 a14a16a14 Implementation of a program that requires a0 a5 a34a23a35 memory space and a0 a5 a34 a35 a5 a3 computation time is easy.
2019,0,1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning_comma_ 1993; Church and Hanks_comma_ 1990; Dagan et al. _comma_ 1999).
2020,0,2.1.1 Pointwise Mutual Information This measure for word similarity was first used in this context by Church and Hanks (1990).
2021,0,The measure is given by equation 1 and is called Pointwise Mutual Information.
2022,0,It is a straightforward transformation of the independence assumption (on a specific point)_comma_ a56a59a58 a21 a9 a11 a21 a14 a62a76a7 a56a59a58 a21 a9 a62a47a55a72a56a59a58 a21 a14 a62_comma_ into a ratio.
2023,0,The window size may vary_comma_ Church and Hanks (1990) used windows of size 2 and 5.
2024,0,3 Related work Word collocation Various collocation metrics have been proposed_comma_ including mean and variance (Smadja_comma_ 1994)_comma_ the t-test (Church et al. _comma_ 1991)_comma_ the chi-square test_comma_ pointwise mutual information (MI) (Church and Hanks_comma_ 1990)_comma_ and binomial loglikelihood ratio test (BLRT) (Dunning_comma_ 1993).
2025,0,(Pantel and Lin_comma_ 2001) reports_comma_ however_comma_ that BLRT score can be also high for two frequent terms that are rarely adjacent_comma_ such as the word pair the the_comma_ and uses a hybrid of MI and BLRT.
2026,1,Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics_comma_ MI and Residual IDF (RIDF)_comma_ and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases.
2027,0,They took the intersection of each top 10% of phrases identified by MI and RIDF_comma_ but did not extend the approach to combining the two metrics into a unified score.
2028,0,a11a29a9 thea13 thea15 a1a4a3a6a5 a11a29a9 thea13 thea15 a11a29a9 thea15 a11a29a9 thea15a1a0 a2 since a11a2a9 thea13 thea15a4a3 a11a29a9 thea15 a11a29a9 thea15 . Also note that in the case of phraseness of a bigram_comma_ the equation looks similar to pointwise mutual information (Church and Hanks_comma_ 1990)_comma_ but they are different.
2029,0,Their relationship is as follows.
2030,0,a38a49a39 a9a12a11a29a9a54a43 a13a45a44a55a15 a16a22a11a2a9a54a43 a15 a11a29a9 a44a55a15a45a15 a21a31a11a29a9a54a43 a13a45a44a59a15 a1a4a3a6a5 a11a29a9a54a43 a13a45a44a59a15 a11a2a9a54a43 a15 a11a29a9 a44a55a15 a5 a6a8a7 a9 pointwise MI The pointwise KL divergence does not assign a high score to a rare phrase_comma_ whose contribution of loss is small by definition_comma_ unlike pointwise mutual information_comma_ which is known to have problems (as described in (Manning and Schutze_comma_ 1999)_comma_ e.g.).
2031,0,Word associations are extracted by setting a threshold for mutual information between words in the same language.
2032,0,The mutual information of a pair of words is defined in terms of their co-occurrence frequency and respective occurrence frequencies (Church and Hanks 1990).
2033,0,A context vector consists of associated words weighted with mutual information.
2034,0,3.2.2 Iterative calculation of correlation The correlation between the i-th translation equivalent of target word x_comma_ denoted as y(i)_comma_ and the j-th associated word x(j) is defined as ()() ( ) ()_comma_ jx'_comma_kyPLmax jx'_comma_iyPL jx'_comma_xMIjx'_comma_iyC k )()( )()( )()()( = where MI(x_comma_ x(j)) is the mutual information between x and x(j)_comma_ and PL(y(i)_comma_ x(j)) is the plausibility factor for y(i) given by x(j).
2035,0,The mutual information between the target word and the associated word is the base of the correlation between each translation equivalent of the target word and the associated word; it is multiplied by the normalized plausibility factor.
2036,0,The second component plausibility factor_comma_ PL 2_comma_ is defined as the maximum plausibility of alignment involving the translation equivalent_comma_ where the plausibility of alignment of a first-language word association with a second-language word association is defined as the mutual information of the second-language word association multiplied by the sum of correlations between the translation equivalent and the translingually alignable accompanying associated words.
2037,0,That is_comma_ the mutual information between the target word and an associated word is used as the initial value for the correlations between all translation equivalents of the target word and the associated word.
2038,0,Co-occurrence frequencies of noun pairs were counted by using a window of 13 words_comma_ excluding function words_comma_ and then noun pairs having mutual information larger than zero were extracted.
2039,0,5 Related Work Although there have been many studies on collocation extraction and mining using only statistical approaches (Church and Hanks_comma_ 1990; Ikehara et al. _comma_ 1996)_comma_ there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations.
2040,0,As (Church and Hanks_comma_ 1990)_comma_ we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence.
2041,0,This measure was normalized according to the maximal mutual information relative to the considered corpus.
2042,0,303 Wiebe_comma_ Wilson_comma_ Bruce_comma_ Bell_comma_ and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis_comma_ because they give rise to invalid or unrealistic statistical measures (Church and Hanks_comma_ 1990)_comma_ we are able to identify higher-precision collocations by including placeholders for unique words (i.e. _comma_ the ugen-n-grams).
2043,0,They can in general be classified into two types: window-based and syntax-based methods.
2044,0,The former extracts collocations within a fixed window (Church and Hanks 1990; Smadja_comma_ 1993).
2045,0,Hanks and Church (1990) proposed using pointwise mutual information to identify collocations in lexicography; however_comma_ the method may result in unacceptable collocations for low-count pairs.
2046,0,Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990_comma_ Smadja 1993_comma_ Choueka 1993_comma_ Lin 1998).
2047,0,Church and Hanks (Church and Hanks 1990) employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window.
2048,0,But the method did not extend to extract n-grams.
2049,0,The typical problems like doctor-nurse (Church and Hanks 1990) could be avoided by using such information.
2050,0,There are several basic methods for evaluating associations between words: based on frequency counts (Choueka_comma_ 1988; Wettler and Rapp_comma_ 1993)_comma_ information theoretic (Church and Hanks_comma_ 1990) and statistical significance (Smadja_comma_ 1993).
2051,0,Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words (Church and Hanks_comma_ 1990; Ruge_comma_ 1992; Pereira et al. _comma_ 1993; Grefenstette_comma_ 1994; Lee_comma_ 1997; Lin_comma_ 1998; Pantel and Lin_comma_ 2002; Weeds and Weir_comma_ 2003).
2052,0,The most widely used association weight function is (point-wise) Mutual Information (MI) (Church and Hanks_comma_ 1990; Lin_comma_ 1998; Dagan_comma_ 2000; Weeds et al. _comma_ 2004).
2053,0,A variety of methods have been applied_comma_ ranging from simple frequency (Justeson & Katz 1995)_comma_ modified frequency measures such as c-values (Frantzi_comma_ Anadiou & Mima 2000_comma_ Maynard & Anadiou 2000) and standard statistical significance tests such as the t-test_comma_ the chi-squared test_comma_ and loglikelihood (Church and Hanks 1990_comma_ Dunning 1993)_comma_ and information-based methods_comma_ e.g. pointwise mutual information (Church & Hanks 1990).
2054,0,METRIC FORMULA Frequency (Guiliano_comma_ 1964) x yf Pointwise Mutual Information [PMI] (Church & Hanks_comma_ 1990) ( )xy x y2log /P P P True Mutual Information [TMI] (Manning_comma_ 1999) ( )xy 2 xy x ylog /P P P P Chi-Squared ( 2 ) (Church and Gale_comma_ 1991) { }{ }_comma__comma_ 2( ) i X X Y Y i j i j i j j f     T-Score (Church & Hanks_comma_ 1990) 1 2 2 2 1 2 1 2 x x s s n n  + C-Values4 (Frantzi_comma_ Anadiou & Mima 2000) 2 is not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T         where is the candidate string f( ) is its frequency in the corpus T is the set of candidate terms that contain P(T ) is the number of these candidate terms 609 1_comma_700 of the three-word phrases are attested in the Lexile corpus.
2055,0,Its best performance_comma_ a score of 0.323 in the part of speech filtered condition with N=50_comma_ outdistanced METRIC POS Filtered Unfiltered RankRatio 0.323 0.196 Mutual Expectancy 0.144 0.069 TMI 0.209 0.096 PMI 0.287 0.166 Chi-sqr 0.285 0.152 T-Score 0.154 0.046 C-Values 0.065 0.048 Frequency 0.130 0.044 Table 2.
2056,0,Bigram Scores for Lexical Association Measures with N=50 METRIC POS Filtered Unfiltered RankRatio 0.218 0.125 MutualExpectation 0.140 0.071 TMI 0.150 0.070 PMI 0.147 0.065 Chi-sqr 0.145 0.065 T-Score 0.112 0.048 C-Values 0.096 0.036 Frequency 0.093 0.034 Table 3.
2057,0,Bigram Scores for Lexical Association Measures with N=5 METRIC N=50 N=10 N=5 RankRatio 0.273 0.137 0.103 PMI 0.219 0.121 0.059 TMI 0.137 0.074 0.056 Frequency 0.089 0.047 0.035 Table 5.
2058,0,After building the chunker_comma_ students were asked to 4 choose a verb and then analyze verb-argument structure (they were provided with two relevant papers (Church and Hanks_comma_ 1990; Chklovski and Pantel_comma_ 2004)).
2059,0,In the following sections_comma_ we will use 2 statistics to measure the the mutual translation likelihood (Church and Hanks_comma_ 1990).
2060,0,To this end we follow the method introduced by (Church and Hanks_comma_ 1990)_comma_ i.e. by sliding a window of a given size over some texts.
2061,0,Like (Church and Hanks_comma_ 1990)_comma_ we used mutual information to measure the cohesion between two words.
2062,0,The information content of this set is defined as mutual information I(F(w)) (Church and Hanks_comma_ 1990).
2063,0,One can also examine the distribution of character or word ngrams_comma_ e.g. Language Modeling (Croft and Lafferty_comma_ 2003)_comma_ phrases (Church and Hanks_comma_ 1990; Lewis_comma_ 1992)_comma_ and so on.
2064,0,PMI (Church and Hanks_comma_ 1990) between two phrases is de ned as: log2 prob(ph1 is near ph2)prob(ph 1)  prob(ph2) PMI is positive when two phrases tend to co-occur and negative when they tend to be in a complementary distribution.
2065,0,Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth: You shall know a word by the company it keeps. (Firth_comma_ 1957_comma_ p. 11) Context similarity has been used as a means of extracting collocations from corpora_comma_ e.g. by Church & Hanks (1990) and by Dunning (1993)_comma_ of identifying word senses_comma_ e.g. by Yarowski (1995) and by Schutze (1998)_comma_ of clustering verb classes_comma_ e.g. by Schulte im Walde (2003)_comma_ and of inducing selectional restrictions of verbs_comma_ e.g. by Resnik (1993)_comma_ by Abe & Li (1996)_comma_ by Rooth et al.(1999) and by Wagner (2004).
2066,0,Usually in 1 In our experiments_comma_ we set negative PMI values to 0_comma_ because Church and Hanks (1990)_comma_ in their seminal paper on word association ratio_comma_ show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus.
2067,0,To compute the degree of interaction between two proteins D4 BD and D4 BE_comma_ we use the information-theoretic measure of pointwise mutual information (Church and Hanks_comma_ 1990; Manning and Schutze_comma_ 1999)_comma_ which is computed based on the following quantities: 1.
2068,0,pointwise mutual information (Church and Hanks_comma_ 1990)_comma_ 3.
2069,0,We obtained correlation values for these features as shown in table 1 under V&J. These features are feature 1 frequency_comma_ feature 2 pointwise mutual information_comma_ feature 3 based on (Lin_comma_ 1999) and feature 7 LSA feature which considers the similarity of the verb-object pair with the verbal form of the object.
2070,0,Pointwise mutual information did surprisingly well on this 84% subset of the data_comma_ however the DSPROTO preferences still outperformed this feature.
2071,0,Pointwise mutual information (Fano_comma_ 1961) was used to measure strength of selection restrictions for instance by Church and Hanks (1990).
2072,0,andw2 iscomputedusinganassociationscorebased on pointwise mutual information_comma_ asdefinedbyFano (1961) and used for a similar purpose in Church and Hanks (1990)_comma_ as well as in many other studies in corpus linguistics.
2073,0,The initial vectors to be clustered are adapted with pointwise mutual information (Church and Hanks_comma_ 1990).
2074,0,We compute the centroid of the cluster by averaging the frequencies of all cluster elements except for the target element we want to reassign_comma_ and adapt the centroid with pointwise mutual information.
2075,0,We measure this association using pointwise Mutual Information (MI) (Church and Hanks_comma_ 1990).
2076,0,The MI between a 60 verb predicate_comma_ v_comma_ and its object argument_comma_ n_comma_ is: MI(v_comma_n) = log Pr(v_comma_n)Pr(v)Pr(n) = log Pr(n|v)Pr(n) (2) If MI>0_comma_ the probability v and n occur together is greater than if they were independently distributed.
2077,0,We create sets of positive and negative examples separately for each predicate_comma_ v. First_comma_ we extract all pairs where MI(v_comma_n)> as positives.
2078,0,For each positive_comma_ we create pseudo-negative examples_comma_ (v_comma_n)_comma_ by pairing v with a new argument_comma_ n_comma_ that either has MI below the threshold or did not occur with v in the corpus.
2079,0,If MI is a sparse and imperfect model of SP_comma_ what can DSP gain by training on MIs scores?
2080,0,We can regard DSP as learning a view of SP that is orthogonal to MI_comma_ in a co-training sense (Blum and Mitchell_comma_ 1998).
2081,0,MI labels the data based solely on co-occurrence; DSP uses these labels to identify other regularities  ones that extend beyond cooccurring words.
2082,0,For example_comma_ many instances of n where MI(eat_comma_n)> also have MI(buy_comma_n)> and MI(cook_comma_n)>.
2083,0,This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat_comma_n) pairs.
2084,0,We also use MI rather than frequency to define the positive pairs_comma_ ensuring that the positive pairs truly have a statistical association_comma_ and are not simply the result of parser error or noise.1 3.2 Partitioning for Efficient Training After creating our positive and negative training pairs_comma_ we must select a feature representation for our examples.
2085,0,Thus rather than a single training procedure_comma_ we can actually partition the examples by predicate_comma_ and train a 1For a fixed verb_comma_ MI is proportional to Keller and Lapata (2003)s conditional probability scores for pseudodisambiguation of (v_comma_n_comma_n) triples: Pr(v|n) = Pr(v_comma_n)/Pr(n)_comma_ which was shown to be a better measure of association than co-occurrence frequency f(v_comma_n).
2086,0,Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs.
2087,0,MI was also recently used for inference-rule SPs by Pantel et al.(2007).
2088,0,4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees_comma_ 2002) using Minipar (Lin_comma_ 1998b)_comma_ and collected verb-object and verb-subject frequencies_comma_ building an empirical MI model from this data.
2089,0,We set the MI-threshold_comma_ _comma_ to be 0_comma_ and the negative-to-positive ratio_comma_ K_comma_ to be 2.
2090,0,We convert Dagan et al.(1999)s similarity-smoothed probability to MI by replacing the empirical Pr(n|v) in Equation (2) with the smoothed PrSIM from Equation (1).
2091,0,We also test an MI model inspired by Erk (2007): MISIM(n_comma_v) = log summationdisplay nSIMS(n) Sim(n_comma_n) Pr(v_comma_n ) Pr(v)Pr(n) We gather similar words using Lin (1998a)_comma_ mining similar verbs from a comparable-sized parsed corpus_comma_ and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)s approach to obtaining web-counts.
2092,0,These are also the examples for which we would expect co-occurrence models like MI to fail.
2093,0,Unlike DSP_comma_ Resniks approach cannot learn that for warn_comma_ the property of being a person is more Seen Criteria Unseen Verb-Object Freq.All = 1 = 2 = 3 > 3 MI > 0 0.44 0.33 0.57 0.70 0.82 Freq.
2094,0,We also compare to our empirical MI model_comma_ trained on our parsed corpus.
2095,1,Although Resnik (1996) reported that 10 of the 16 plausible pairs did not occur in his training corpus_comma_ all of them occurred in ours and hence MI gives very reasonable scores on the plausible objects.
2096,0,It has no statistics_comma_ however_comma_ for many of the implausible ones.
2097,0,DSP can make finer decisions than MI_comma_ recognizing that warning an engine is more absurd than judging a climate. 4.5 Unseen Verb-Object Identification We next compare MI and DSP on a much larger set of plausible examples_comma_ and also test how well the models generalize across data sets.
2098,0,We took the MI and DSP systems trained on AQUAINT and asked them to rate observed (and thus likely plausible) verb-object pairs taken from an unseen corpus.
2099,0,DSP accepts far more pairs than MI (73% vs. 44%)_comma_ even far more than a system that accepts any previously observed verb-object combination as plausible (57%).
2100,0,Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman_comma_ 1998)_comma_ the recall of MI>0 on SJM only increases from 44.1% to 44.9%_comma_ still far below DSP.
2101,0,Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/ 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/ -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/ 2.00/-0.99 judge contest/climate 1.30/0.28 1.50/1.90* 1.70/1.70* 3.90/ 1.00/0.51 teach language/distance 1.87/1.86 2.50/1.30 3.60/2.70 3.53/ 1.86/0.19 show sample/travel 1.44/0.41 1.60/0.14 0.40/-0.82 0.53/-0.49 1.00/-0.83 expect visit/mouth 0.59/5.93* 1.40/1.50* 1.40/0.37 1.05/-0.65 1.44/-0.15 answer request/tragedy 4.49/3.88 2.70/1.50 3.10/-0.64 2.93/ 1.00/0.01 recognize author/pocket 0.50/0.50* 0.03/0.37* 0.77/1.30* 0.48/ 1.00/0.00 repeat comment/journal 1.23/1.23* 2.30/1.40 2.90/ 2.59/ 1.00/-0.48 understand concept/session 1.52/1.51 2.70/0.25 2.00/-0.28 3.96/ 2.23/-0.46 remember reply/smoke 1.31/0.20 2.10/1.20 0.54/2.60* 1.13/-0.06 1.00/-0.42 Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al._comma_ 1989).
2102,0,0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0  0.2  0.4  0.6  0.8  1 Interpolated Precision Recall DSP>T MI>T DSP>0 MI>0 Figure 2: Pronoun resolution precision-recall on MUC.
2103,0,ther evidence_comma_ if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3)_comma_ MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally_comma_ we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai_comma_ 1990; Kehler et al._comma_ 2004).
2104,0,We compare MI and DSP on this set_comma_ classifying every (v_comma_n) with MI>T (or DSP>T) as positive.
2105,1,DSP>0 has both a higher recall and higher precision than accepting every pair previously seen in text (the right-most point on MI>T).
2106,0,The DSP>T system achieves higher precision than MI>T for points where recall is greater than 60% (where MI<0).
2107,0,Interestingly_comma_ the recall of MI>0 is 66 System Acc Most-Recent Noun 17.9% Maximum MI 28.2% Maximum DSP 38.5% Table 4: Pronoun resolution accuracy on nouns in current or previous sentence in MUC.
2108,1,On the subset of pairs with strong empirical association (MI>0)_comma_ MI generally outperforms DSP at equivalent recall values.
2109,0,We next compare MI and DSP as stand-alone pronoun resolution systems (Table 4).
2110,0,If instead we choose the previous noun with the highest MI as antecedent_comma_ we get an accuracy of 28.2%_comma_ while choosing the previous noun with the highest DSP achieves 38.5%.
2111,0,DSP resolves 37% more pronouns correctly than MI.
2112,0,We leave as future work a full-scale pronoun resolution system that incorporates both MI and DSP as backed-off_comma_ interpolated_comma_ or separate semantic features.
2113,0,This task is quite common in corpus linguistics and provides the starting point to many other algorithms_comma_ e.g._comma_ for computing statistics such as pointwise mutual information (Church and Hanks_comma_ 1990)_comma_ for unsupervised sense clustering (Schutze_comma_ 1998)_comma_ and more generally_comma_ a large body of work in lexical semantics based on distributional profiles_comma_ dating back to Firth (1957) and Harris (1968).
2114,0,2 Related Works Some of the most common measures of unithood include pointwise mutual information (MI) (Church and Hanks_comma_ 1990) and log-likelihood ratio (Dunning_comma_ 1994).
2115,0,In mutual information_comma_ the co-occurrence frequencies of the constituents of complex terms are utilised to measure their dependency.
2116,0,The mutual information for two words a and b is defined as: MI(a_comma_b) = log2 p(a_comma_b)p(a)p(b) (1) where p(a) and p(b) are the probabilities of occurrence of a and b. Many measures that apply statistical techniques assuming strict normal distribution_comma_ and independence between the word occurrences (Franz_comma_ 1997) do not fare well.
2117,0,The authors define UH as: UH(ax_comma_ay) =            1 if (MI(ax_comma_ay) > MI+)  (MI+  MI(ax_comma_ay)  MI ID(ax_comma_s)  IDT  ID(ay_comma_s)  IDT  IDR+  IDR(ax_comma_ay)  IDR) 0 otherwise (2) where MI+_comma_ MI_comma_ IDT _comma_ IDR+ and IDR are thresholds for determining mergeability decisions_comma_ and MI(ax_comma_ay) is the mutual information between ax and ay_comma_ while ID(ax_comma_s)_comma_ ID(ay_comma_s) and IDR(ax_comma_ay) are measures of lexical independence of ax and ay from s. For brevity_comma_ let z be either ax or ay_comma_ and the independence measure ID(z_comma_s) is then defined as: ID(z_comma_s) = braceleftBigg log10(nz ns) if(nz > ns) 0 otherwise where nz and ns is the Google page count for z and s respectively.
2118,0,Intuitively_comma_ UH(ax_comma_ay) states that the twolexical units a x and ay can only be merged in two cases_comma_ namely_comma_ 1) if ax and ay has extremely high mutual information (i.e. higher than a certain threshold MI+)_comma_ or 2) if ax and ay achieve average mutual information (i.e. within the acceptable range of two thresholds MI+ and MI) due to both of their extremely high independence (i.e. higher than the threshold IDT ) from s.
2119,0,The thresholds required for this function are based on the values suggested by (Wong et al._comma_ 2007b)_comma_ namely_comma_ MI+ = 0.9_comma_ MI = 0.02_comma_ IDT = 6_comma_ IDR+ = 1.35_comma_ and IDR = 0.93.
2120,0,To extract such word clusters we used suffix arrays proposed in Yamamoto and Church (2001) and the pointwise mutual information measure_comma_ see Church and Hanks (1990).
2121,0,To examine the effects of including some known AMs on the performance_comma_ the following AMs had a 50% chance of being included in the initial population: pointwise mutual information (Church and Hanks_comma_ 1990)_comma_ the Dice coefficient_comma_ and the heuristic measure defined in (Petrovic et al._comma_ 2006): H(a_comma_b_comma_c) =    2log f(abc)f(a)f(c) if POS(b) = X_comma_ log f(abc)f(a)f(b)f(c) otherwise.
2122,0,183 1 2 3 4 5 6 7 8 9 100 10 20 30 40 50 60 70 80 90 100 Number of ngrams ( 105) F 1  score   Dice PMI H M13 M205 Figure 2: Comparison of association measures on a corpus of 7008 Croatian documents Figure 2 shows the comparison of AMs in terms of their F1 score obtained on the corpus of 7008 documents.
2123,0,Indeed_comma_ as Sinopalnikova and Pavel (2004) note_comma_ Deese (1965) was the first to conduct linguistic analyses of word association norms_comma_ such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of Church and Hanks (1990) notion of mutual information.
2124,0,Indeed_comma_ as Sinopalnikova and Pavel (2004) note_comma_ Deese (1965) was the first to conduct linguistic analyses of word association norms_comma_ such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of Church and Hanks (1990) notion of mutual information.
2125,0,Following Church & Hanks (1990)_comma_ Rapp (2004)_comma_ and Wettler et al.(2005) this actually seems to be successful.
2126,0,3.2.1 Co-occurrence based Similarit y Measure We focused on two popular measures : the mutual information (MI) and  2 statistics.
2127,0,Mutual Informatio n Church and Hanks (1990) discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena_comma_ ranging from semanti c relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence preferences between verbs and prepositions (content word/function word).
2128,0,The mutual information MI(O i _comma_ O j ) is dened as: MI(O i _comma_ O j ) = log S all  f(O i _comma_ O j ) S O i  S O j _comma_ (1) where S O i = summationdisplay kO all f(O i _comma_ O k )_comma_ (2) S all = summationdisplay O i O all S O i .
2129,0,For example _comma_ if it 2 When we used  2 statistic as a co-occurren ce based similarity_comma_ MI in Eq.
2130,0,& Sounds Phrase .140 .181 .677 .285 10  2 AND .170 .182 .380 .246 9 cos Phrase .100 .322 .288 .304 14 MI AND .050 .217 .282 .245 13 Newman Phrase .080 .397 .520 .451 7  2 AND .130 .212 .328 .258 9 div Phrase .090 .414 .298 .347 17 MI AND .090 .207 .325 .253 6 Phrase .160 .372 .473 .417 8  2 AND .460 .138 .644 .227 10 k-means  Phrase .110 .136 .870 .236 10 MI AND .040 .134 .599 .219 10 Co-occ.
2131,0,Phrase .150 .191 .588 .286 10  2 AND .700 .169 .415 .240 8 Newman  Phrase .190 .301 .273 .286 14 MI AND .590 .159 .537 .245 3 Phrase .140 .275 .527 .361 5 k-means  cos  .050 .145 .321 .199 10 Sounds div  .020 .126 .545 .204 10 Newman  cos  .270 .151 .365 .213 4 div  .350 .138 .408 .206 3 4.2.2 A single vs combined similarity measure To examine the effectiveness of the combined similarity measure_comma_ we used a single measur e as a quality function of the Newman clustering_comma_ and compar ed these results with those obtained by our method . As shown in Table 3_comma_ the results with combin ing similarity measur es improved overall performance.
2132,0,Surprisingly_comma_ there were no signicant difference between a combine d measure Co-occ (MI) & Sounds(cos) and a single measure Co-oc c(MI) curves_comma_ while the precision of a single measur e Sounds was constantly worse than that obtained by a combin ed measure . Another possible reason for the difference of F-score is due to product of MI and Cos in Eq.
2133,0,Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al._comma_ 1983; Church and Hanks_comma_ 1990; Smadja_comma_ 1993; Dunning_comma_ 1993; Pearce_comma_ 2002; Evert_comma_ 2004).
2134,0,These approaches use association measures to discover collocations from the word pairs in a given window.
2135,0,To avoid explosion_comma_ these approaches generally limit the window size to a small number.
2136,0,As a result_comma_ long-span collocations can not be extracted 1 . In addition_comma_ since the word pairs in the given window are regarded as potential collocations_comma_ lots of false collocations exist.
2137,0,Although these approaches used different association measures to filter those false collocations_comma_ the precision of the extracted collocations is not high.
2138,0,This does not seem to be the case_comma_ however_comma_ for common feature weighting functions_comma_ such as Point-wise Mutual Information (Church and Patrick 1990; Hindle 1990).
2139,1,Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch_comma_ Wang_comma_ and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel_comma_ Ravichandran_comma_ and Hovy 2004; Weeds_comma_ Weir_comma_ and McCarthy 2004)_comma_ dened by: weight MI (w_comma_f)=log 2 P(w_comma_f) P(w)P(f) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight MI (w_comma_f)=log 2 count(w_comma_f) nrels count(w) count(f) (2) where count(w_comma_f) is the frequency of the co-occurrence pair w_comma_f  in S_comma_ count(w)and count(f) are the independent frequencies of w and f in S_comma_andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations.
2140,0,However_comma_ a known weakness of MI and most of the other statistical weighting functions used for collocation extraction_comma_ including t-test and  2 _comma_ is their tendency to inate the weights for rare features (Dunning 1993).
2141,1,Similarity measures that utilize MI weights showed good performance_comma_ however.
2142,0,In the current work we use MI for data analysis_comma_ and for the evaluations of vector quality and word similarity performance.
2143,0,This measure behaves quite similarly to the weighted Jaccard measure (Weeds_comma_ Weir_comma_ and McCarthy 2004)_comma_ and is dened as follows: sim LIN (w_comma_v)= summationtext fF(w)F(v) (weight MI (w_comma_f)+weight MI (v_comma_f)) summationtext fF(w) weight MI (w_comma_f)+ summationtext fF(v) weight MI (v_comma_f) (5) where F(w)andF(v) are the active features ofthe two words.
2144,0,Theweight function used originally by Lin is MI (Equation 1).
2145,0,Feature weight MI Commercial bank_comma_gen_comma_ h 8.08 Destination_comma_pcomp of_comma_ m 7.97 Airspace_comma_ pcomp of_comma_ h 7.83 Landlocked_comma_mod_comma_ m 7.79 Tradebalance_comma_gen_comma_ h 7.78 Sovereignty_comma_pcomp of_comma_ h 7.78 Ambition_comma_nn_comma_ h 7.77 Bourse_comma_gen_comma_ h 7.72 Politician_comma_gen_comma_ h 7.54 Border_comma_pcomp of_comma_ h 7.53 sorted feature list_comma_ at positions 461and 832.
2146,0,weight MI =4wasfoundastheoptimalMI threshold for active feature weights (featuresincludedinthefeaturevectors)_comma_yieldinga10%precisionincreaseofsim LIN and removing over 50% of the data relative to no feature ltering.
2147,0,On the other hand_comma_ an attempt to cut off the lower ranked features of the MI weighting always resulted in a noticeable decrease in precision.
2148,0,These results show that for MI weighting many important features appear further down in the ranked vectors_comma_ while for the bootstrapped weighting adding too many features adds mostly noise_comma_ since most characteristic features are concentrated at the top ranks.
2149,0,For each of the additional measures the experiment repeats the main three steps described in Section 4: Initially_comma_ the basic similarity lists are calculated for each of the measures using MI weighting; then_comma_ the bootstrapped weighting_comma_ weight B _comma_is computed based on the initial similarities_comma_ yielding new word feature vectors; nally_comma_ the similarity values are recomputed by the same vector similarity measure using the new feature vectors.
2150,0,To assess the effectiveness of weight B we computed the four alternative output similarity lists_comma_ using the sim WJ and sim COS similarity measures_comma_ each with the weight MI 449 ComputationalLinguistics Volume35_comma_Number3 Table 5 Comparativeprecisionvalues forthetop20 similaritylistsof thethreeselected similarity measures_comma_ with MI and Bootstrappedfeatureweightingfor each.
2151,0,Table 5 summarizes the precision values achieved by LIN_comma_ WJ_comma_andCOS with both weight MI and weight B . As shown in the table_comma_ bootstrapped weighting consistently contributed between 46 points to the accuracy of each method in the top 20 similarity list.
2152,1,Analyzing the Bootstrapped Feature Vector Quality In this section we provide an in-depth analysis of the bootstrapping feature weighting quality compared tothe state-of-the-art MI weighting function.
2153,0,This behavior indicates the deciency of the MI feature weighting function in this case.
2154,0,6.3 An Empirical Assessment of the acfr-ratio Inthissubsectionwereportanempiricalcomparisonoftheacfr-ratioobtainedfortheMI and BootstrappedLIN weighting functions.
2155,0,First_comma_ we computed the average common-feature rank scores (acfr n ) (with varying valuesofn)forweight MI andforweight B overallthepairsinthetestsample.Interestingly_comma_ the mean acfr n scores for weight B range within 110264 for n = 10100_comma_ while the correspondingrangefor weight MI isbyanorderofmagnitudehigher:7801_comma_254_comma_despite the insignicant differences in vector sizes.
2156,0,As can be seen the acfr-ratio values are consistently higher for Bootstrapped LIN than for MI.
2157,0,That is_comma_ the bootstrapping method assigns much higher acfr n scores to entailing words than to non-entailing ones_comma_ whereas for MI the corresponding acfr n scores for entailing and non-entailing pairs are roughly equal.
2158,0,Then we describe how the pseudo-word sense disambiguation task_comma_ which Figure 2 Comparisonbetween the acfr-ratio for MI and Bootstrapped LIN methods_comma_when usingvarying numbersof commontop-rankedfeaturesinthewordsfeaturevectors.
2159,0,The value of fj is calculated by Mutual Information (Church and Hanks_comma_ 1990) between xi and fj.
2160,0,Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church & Hanks_comma_ 1990).
2161,0,4 Using vector-based models of semantic representation to account for the systematic variances in neural activity 4.1 Lexical Semantic Representation Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks_comma_ 1990).
2162,0,The tensor has been adapted with a straightforward extension of pointwise mutual information (Church and Hanks_comma_ 1990) for three-way cooccurrences_comma_ following equation 4.
2163,0,The first adaptation includes theswap-operation(WagnerandLowrance_comma_1975)_comma_ whilethesecondadaptationincludesphoneticsegment distances_comma_ which are generated by applying an iterative pointwise mutual information (PMI) procedure(Churchand Hanks_comma_ 1990).
2164,0,We used pointwise mutual information (PMI; Church and Hanks_comma_ 1990) to obtain these distances.
2165,0,It generates segment distances by assessing the degree of statistical dependence betweenthe segmentsx andy: PMI(x_comma_y) = log2 parenleftbigg p(x_comma_y) p(x)p(y) parenrightbigg (1) Where:  p(x_comma_y): the number of times x and y occur at the same position in two aligned strings X and Y_comma_ divided by the total number of alignedsegments(i.e.the relative occurrence of the alignedsegmentsx and y in the whole dataset).
2166,0,The segment distances can therefore be generatedbysubtractingthePMIvaluefrom0and adding the maximum PMI value (i.e. lowest distance is 0).
2167,0,Based on the PMI value and its conversion to segmentdistances_comma_we developedan iterative procedure to automatically obtain the segment distances: 1.
2168,0,Thestringalignmentsaregeneratedusingthe VC-sensitiveLevenshteinalgorithm(seesection3.1).4 4We also used the Levenshtein algorithm without the vowel-consonantrestrictionto generate the PMI values_comma_ but this had a negative effecton the performance.
2169,0,31 5.1.1 Comparisonof segmentdistances With respect to the PMI results (convergence was reached after 7 iterations_comma_ taking less than 5 CPU minutes)_comma_ we indeed found sensible results: the averagedistancebetweenidenticalsymbols was significantlylower than the distancebetween pairs of different vowels and consonants (t < 13_comma_p < .001).
2170,0,To allow a fair comparisonbetweenthePHMM probabilities and the PMI distances_comma_ we transformed the PHMM probabilities to log-odds scores (i.e. dividing the probability by the relative frequency of the segments and subsequently taking the log).
2171,1,In addition_comma_ we introduced the PMI weighting within the Levenshtein algorithm as a simple means of obtaining segment distances_comma_ and showed that it improves on the popular Levenshtein algorithm with respect to alignment accuracy.
2172,0,To support a more rigorous analysis_comma_ however_comma_ wc have followed Carletta's suggestion (1996) of using the K coettMcnt (Siegel and Castellan_comma_ 1988) as a measure of coder agreement.
2173,0,The percentage agreement for each of the features is shown in the following table: feature percent agreement form 100% intentionality 74.9% awareness 93.5% safety 90.7% As advocated by Carletta (1996)_comma_ we have used the Kappa coefficient (Siegel and Castellan_comma_ 1988) as a measure of coder agreement.
2174,0,4.5 Consistency of Annotations In order to assess the consistency of annotation_comma_ we follow Carletta (1996) in using Cohen's ~_comma_ a chancecorrected measure of inter-rater agreement.
2175,0,The statistic was developed to distinguish among levels of agreement such as 'almost perfect_comma_ substantial_comma_ moderate_comma_ fair_comma_ slight_comma_ poor' (Agresti_comma_ 1992)_comma_ and Carletta suggests that as a rule of thumb in the behavioral sciences_comma_ values of g greater than .8 indicate good replicability_comma_ with values between .67 and .8 allowing tentative conclusions to be drawn.
2176,0,All levels of the dialogue coding are described in detail in Carletta et al.(1996).
2177,0,4.2 Interpreting reliability results It has been argued elsewhere (Carletta 1996) that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test_comma_ reliability for category classifications should be measured using the kappa coefficient.
2178,0,6.1 Reader Judgments There is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms (Carletta 1996; Condon and Cech 1995).
2179,0,Proposals have recently been made for protocols for the collection of human discourse segmentation data (Nakatani et al. 1995) and for how to evaluate the validity of judgments so obtained (Carletta 1996; Isard and Carletta 1995; Ros6 1995; Passonneau and Litman 1993; Litman and Passonneau 1995).
2180,0,Carletta (1996) and Ros6 (1995) point out the importance of taking into account the expected chance agreement among judges when computing whether or not judges agree significantly.
2181,0,They suggest using the kappa coefficient (K) for this purpose.
2182,0,According to Carletta (1996)_comma_ K measures pairwise agreement among a set of coders making category judgments_comma_ correcting for expected chance agreement as follows: KP(A) -P(E) 1 -P(E) where P(A) is the proportion of times that the coders agree and P(E) is the proportion of times that they would be expected to agree by chance.
2183,0,The coefficient can be computed by making pairwise comparisons against an expert or by comparing to a group decision.
2184,0,Carletta (1996) also states that in the behavioral sciences_comma_ K > .8 signals good replicability_comma_ and .67 < K < .8 allows tentative conclusions to be drawn.
2185,0,In Hirschberg and Nakatani (1996)_comma_ average reliability (measured using the kappa coefficient discussed in Carletta \[1996\]) of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker_comma_ labeled using text and speech_comma_ is.8 or above for both read and spontaneous speech; values of at least .8 are typically viewed as representing high reliability (see Section 3.2).
2186,0,Reliability metrics (Krippendorff 1980; Carletta 1996) are designed to give a robust measure of how well distinct sets of data agree with_comma_ or replicate_comma_ one another.
2187,0,They are sensitive to the relative proportion of the different data types (e.g. _comma_ boundaries versus nonboundaries)_comma_ but insensitive to the statistical likelihood that agreements will occur.
2188,0,We then used the kappa statistic (Siegel and Castellan_comma_ 1988; Carletta_comma_ 1996) to assess the level of agreement between the three coders with respect to the 2 An agent holds the task initiative during a turn as long as some utterance during the turn directly proposes how the agents should accomplish their goal_comma_ as in utterance (3c).
2189,0,Carletta suggests that content analysis researchers consider K >.8 as good reliability_comma_ with.67< /~' <.8 allowing tentative conclusions to be drawn (Carletta_comma_ 1996).
2190,0,We will do this by examining how humans perform on summary extraction and evaluating the reliability of their performance_comma_ using the kappa statistic_comma_ a metric standardly used in the behavioral sciences (Jean Carletta_comma_ 1996; Sidney Siegel and N. John Castellan Jr. _comma_ 1988).
2191,0,Measurement of B.eliability The Kappa Statistic Following Jean Carletta (1996)_comma_ we use the kappa statistic (Sidney Siegel and N. John Castellan Jr. _comma_ 1988) to measure degree of agreement among subjects.
2192,1,As aptly pointed out in Jean Carletta (1996)_comma_ agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data.
2193,0,Its roots are the same as computational linguistics (CL)_comma_ but it has been largely ignored in CL until recently (Dunning_comma_ 1993; Carletta_comma_ 1996; Kilgarriff_comma_ 1996).
2194,0,As in much recent empirical work in discourse processing (e.g. _comma_ Arhenberg et al. 1995; Isard & Carletta 1995; Litman & Passonneau 1995; Moser & Moore 1995; Hirschberg & Nakatani 1996)_comma_ we performed an intercoder reliability study investigating agreement in annotating the times.
2195,0,Intercoder reliability was assessed using Cohen's Kappa statistic (~) (Siegel & Castellan 1988_comma_ Carletta 1996).
2196,0,A ~ value of 0.8 or greater indicates a high level of reliability among raters_comma_ with values between 0.67 and 0.8 indicating only moderate agreement (Hirschberg ~ Nakatani 1996; Carletta 1996).
2197,0,Table 1 reports values for the Kappa (K) coefficient of agreemeat (Carletta_comma_ 1996) for Forward and Backward ~netions.
2198,0,Our study is also different from these previous ones in that measuring the agreement among annotators became an issue (Carletta 1996).
2199,0,Idiom 0 0 1 1 0 2 V. Doubt 3 0 4 0 0 7 Total A 294 160 546 39 1 1_comma_040 In order to measure the agreement in a more precise way_comma_ we used the Kappa statistic (Siegel and Castellan 1988)_comma_ recently proposed by Carletta as a measure of agreement for discourse analysis (Carletta 1996).
2200,0,According to Carletta_comma_ in the field of content analysis-where the Kappa statistic originated--K > 0.8 is generally taken to indicate good reliability_comma_ whereas 0.68 < K < 0.8 allows tentative conclusions to be drawn.
2201,0,And indeed_comma_ the agreement figures went up from K = 0.63 to K = 0.68 (ignoring doubts) when we did so_comma_ i.e._comma_ within the 'tentative' margins of agreement according to Carletta (1996) (0.68 <_ x < 0.8).
2202,0,Cohen's Kappa ~ (Bakeman and Gottman_comma_ 1986; Carletta_comma_ 1996).
2203,0,The labeling agreement was 84% (n =.80; (Carletta_comma_ 1996)).
2204,0,After each step the annotations were compared using the ~ statistic as reliability measure for all classification tasks (Carletta_comma_ 1996).
2205,0,Kappa is a better measurement of agreement than raw percentage agreement (Carletta_comma_ 1996) because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders.
2206,0,We perform a statistical analysis that provides information that complements the information provided by Cohen's Kappa (Cohen_comma_ 1960; Carletta_comma_ 1996).
2207,0,The table also shows Cohen's to_comma_ an agreement measure that corrects for chance agreement (Carletta_comma_ 1996); the most important t value in the table is the value of 0.7 for the two human judges_comma_ which can be interpreted as sufficiently high to indicate that the task is reasonably well defined.
2208,0,'A CHECK move requests the partner to confirm information that the speaker has some reason to believe_comma_ but is not entirely sure about' \[Carletta et al.1996\].
2209,0,This reflects the forward looking aspect of such a dialogue act.
2210,0,'However_comma_ CHECK moves are almost always about some information which the speaker has been told' \[Carletta et al.1996\] -a description that models the backward looking functionality of a dialogue act.
2211,0,Such a coding procedure covers_comma_ for example_comma_ how segmentation of a corpus is performed_comma_ if multiple tagging is allowed and if so_comma_ is it unlimited or are there just certain combinations of tags not allowed_comma_ is look ahead permitted_comma_ etc For further information on coding procedures we want to refer to \[Dybkjmr et al.1998\] and for good examples of coding books see_comma_ for example_comma_ \[Carletta et al.1996\]_comma_ \[Alexandersson et al.1998\]_comma_ or \[Thym~-Gobbel and Levin1998\].
2212,0,It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders (Carletta_comma_ 1996).
2213,0,Although we agree with this_comma_ we believe that more experiments of the kind reported here will have to be carried out before we can produce a tagging manual that is usable by naive coders.
2214,0,k -~ P(A) P(E) (3) 1P(E) Carletta (1996) suggests that the units over which the kappa statistic is computed affects the outcome.
2215,0,The rationale for using Kappa is explained in (Carletta_comma_ 1996).
2216,0,It us widely acknowledged that word sense d~samblguatmn (WSD) us a central problem m natural language processing In order for computers to be able to understand and process natural language beyond simple keyword matching_comma_ the problem of d~samblguatmg word sense_comma_ or dlscermng the meamng of a word m context_comma_ must be effectively dealt with Advances in WSD v_comma_ ill have slgmficant Impact on apphcatlons hke information retrieval and machine translation For natural language subtasks hke part-of-speech tagging or s)ntactm parsing_comma_ there are relatlvely well defined and agreed-upon cnterm of what it means to have the 'correct' part of speech or syntactic structure assigned to a word or sentence For instance_comma_ the Penn Treebank corpus (Marcus et al_comma_ 1993) pro~ide~_comma_t large repo.~tory of texts annotated w~th partof-speech and s}ntactm structure mformatlon Tv.o independent human annotators can achieve a high rate of agreement on assigning part-of-speech tags to words m a g~ven sentence Unfortunately_comma_ th~s us not the case for word sense assignment F~rstly_comma_ it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a g~ven word Different d~ctlonanes tend to carve up the 'semantic space' m a different way_comma_ so to speak Secondly_comma_ the hst of senses for a word m a typical dmtmnar~ tend to be rather refined and comprehensive This is especmlly so for the commonly used words which have a large number of senses The sense dustmctmn between the different senses for a commonly used word m a d~ctmnary hke WoRDNET (Miller_comma_ 1990) tend to be rather fine Hence_comma_ two human annotators may genuinely dusagree m their sense assignment to a word m context The agreement rate between human annotators on word sense assignment us an Important concern for the evaluatmn of WSD algorithms One would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators The agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of WSD algorithms For instance_comma_ the SENSEVAL exerclse has performed a detaded study to find out the raterannotator agreement among ~ts lexicographers taggrog the word senses (Kllgamff_comma_ 1998c_comma_ Kllgarnff_comma_ 1998a_comma_ Kflgarrlff_comma_ 1998b) 2 A Case Study In this-paper_comma_ we examine the ~ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sense-tagged corpus of more than 30_comma_000 instances of the most frequently occurring nouns and verbs of Enghsh This corpus is the intersection of the WORDNET Semcor corpus (Miller et al_comma_ 1993) and the DSO corpus (Ng and Lee_comma_ 1996_comma_ Ng_comma_ 1997)_comma_ which has been independently tagged wlth the refined senses of WORDNET by two separate groups of human annotators The Semcor corpus us a subset of the Brown corpus tagged with ~VoRDNET senses_comma_ and consists of more than 670_comma_000 words from 352 text files Sense taggmg was done on the content words (nouns_comma_ ~erbs_comma_ adjectives and adverbs) m this subset The DSO corpus consists of sentences drawn from the Brown corpus and the Wall Street Journal For each word w from a hst of 191 frequently occurring words of Enghsh (121 nouns and 70 verbs)_comma_ sentences containing w (m singular or plural form_comma_ and m its various reflectional verb form) are selected and each word occurrence w ~s tagged w~th a sense from WoRDNET There ~s a total of about 192_comma_800 sentences in the DSO corpus m which one word occurrence has been sense-tagged m each sentence The intersection of the Semcor corpus and the DSO corpus thus consists of Brown corpus sentences m which a word occurrence w is sense-tagged m each sentence_comma_ where w Is one of.the 191 frequently oc-_comma_currmg English nouns or verbs Since this common pomon has been sense-tagged by two independent groups of human annotators_comma_ ~t serves as our data set for investigating inter-annotator agreement in this paper 3 Sentence Matching To determine the extent of inter-annotator agreement_comma_ the first step ~s to match each sentence m Semcor to its corresponding counterpart In the DSO corpus This step ~s comphcated by the following factors 1 Although the intersected portion of both corpora came from Brown corpus_comma_ they adopted different tokemzatmn convention_comma_ and segmentartan into sentences differed sometimes 2 The latest versmn of Semcor makes use of the senses from WORDNET 1 6_comma_ whereas the senses used m the DSO corpus were from WoRDNET 15 1 To match the sentences_comma_ we first converted the senses m the DSO corpus to those of WORDNET 1 6 We ignored all sentences m the DSO corpus m which a word is tagged with sense 0 or -1 (A word is tagged with sense 0 or -1 ff none of the given senses m WoRDNFT applies ) 4_comma_ sentence from Semcor is considered to match one from the DSO corpus ff both sentences are exactl) ldent~cal or ff the~ differ only m the pre~ence or absence of the characters ' (permd) or -' (hyphen) For each remaining Semcor sentence_comma_ taking into account word ordering_comma_ ff 75% or more of the words m the sentence match those in a DSO corpus sentence_comma_ then a potential match ~s recorded These i -kctua\[ly_comma_ the WORD~q'ET senses used m the DSO corpus were from a shght variant of the official WORDNE'I 1 5 release Th~s ssas brought to our attention after the pubhc release of the DSO corpus potential matches are then manually verffied to ensure that they are true matches and to ~eed out any false matches Using this method of matching_comma_ a total of 13_comma_188 sentence-palrs contasnmg nouns and 17_comma_127 sentence-pa~rs containing verbs are found to match from both corpora_comma_ ymldmg 30_comma_315 sentences which form the intersected corpus used m our present study 4 The Kappa Statistic Suppose there are N sentences m our corpus where each sentence contains the word w Assume that w has M senses Let 4 be the number of sentences which are assigned identical sense b~ two human annotators Then a simple measure to quantify the agreement rate between two human annotators Is Pc_comma_ where Pc_comma_ = A/N The drawback of this simple measure is that it does not take into account chance agreement between two annotators The Kappa statistic a (Cohen_comma_ 1960) is a better measure of rater-annotator agreement which takes into account the effect of chance agreement It has been used recently w~thm computatmnal hngu~stlcs to measure raterannotator agreement (Bruce and Wmbe_comma_ 1998_comma_ Carletta_comma_ 1996_comma_ Veroms_comma_ 1998) Let Cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 Then P~-P~ 1-P~ where M j=l and Pe measures the chance agreement between two annotators A Kappa ~alue of 0 indicates that the agreement is purely due to chance agreement_comma_ whereas a Kappa ~alue of 1 indicates perfect agreement A Kappa ~alue of 0 8 and above is considered as mdmatmg good agreement (Carletta_comma_ 1996) Table 1 summarizes the inter-annotator agreement on the mtersected corpus The first (becond) row denotes agreement on the nouns (xerbs)_comma_ wh~le the lass row denotes agreement on all words combined The a~erage ~ reported m the table is a s~mpie average of the individual ~ value of each word The agreement rate on the 30_comma_315 sentences as measured by P= is 57% This tallies with the figure reported ~n our earlier paper (Ng and Lee_comma_ 1996) where we performed a quick test on a subset of 5_comma_317 sentences_comma_n the intersection of both the Semcor corpus and the DSO corpus 10 \[\] mm m m m m m mm m m m m mm m m m Type Num of v_comma_ ords A N \[ P~ Avg Nouns 121 7_comma_676 13_comma_188 I 0 582 0 300 Verbs 70 9_comma_520 17_comma_127 I 0 555 0 347 All I 191 I 17_comma_196 30_comma_315 I 056T 0317 Table 1 Raw inter-annotator agreement 5 Algorithm Since the rater-annotator agreement on the intersected corpus is not high_comma_ we would like to find out how the agreement rate would be affected if different sense classes were in use In this section_comma_ we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators The resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible The algorithm is given m Figure 1 The algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sense-tagged by two human annotators At each Iteration of the algorithm_comma_ tt finds the pair of sense classes Ct and Cj such that merging these two sense classes results in the highest t~ value for the resulting merged group of sense classes It then proceeds to merge Cz and C~ Thin process Is repeated until the ~ value reaches a satisfactory value ~_comma_~t_comma_~_comma_ which we set as 0 8 Note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any NLP tasks in which prior human agreement rate may not be high enough Such NLP tasks could be discourse tagging_comma_ speech-act categorization_comma_ etc 6 Results For each word w from the list of 121 nouns and 70 verbs_comma_ ~e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w For a subset of 95 words (53 nouns and 42 verbs)_comma_ the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting Kappa ~alue reaches 0 8 or higher For the other 96 words_comma_ m order for the Kappa value to reach 0 8 or higher_comma_ the algorithm collapses all senses of the ~ord to a single (trivial) class Table 2 and 3 summarizes the results for the set of 53 nouns and 42 ~erbs_comma_ respectively Table 2 md~cates that before the collapse of sense classes_comma_ these 53 nouns have an average of 7 6 senses per noun There is a total of 5_comma_339 sentences in the intersected corpus containing these nouns_comma_ of which 3_comma_387 sentences were assigned the same sense by the two groups of human annotators The average Kappa statistic (computed as a simple average of the Kappa statistic of ~he mdlwdual nouns) is 0 463 After the collapse of sense classes by the greedy search algorithm_comma_ the average number of senses per noun for these 53 nouns drops to 40 Howe~er_comma_ the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5_comma_033 That is_comma_ about 94 3% of the sentences have been assigned the same coarse sense_comma_ and that the average Kappa statistic has improved to 0 862_comma_ mgmfymg high rater-annotator agreement on the derived coarse senses Table3 gl~es the analogous figures for the 42 verbs_comma_ agmn mdmatmg that high agreement is achieved on the coarse sense classes den~ed for verbs 7 Discussion Our findings on rater-annotator agreement for word sense tagging indicate that for average language users_comma_ it is quite dl~cult to achieve high agreement when they are asked to assign refned sense tags (such as those found in WORDNET) given only the scanty definition entries m the WORDNET dlctionary and a few or no example sentences for the usage of each word sense Thin observation agrees wlth that obtmned m a recent study done by (Veroms_comma_ 1998)_comma_ where the agreement on sense-tagging by naive users was also not hlgh Thus It appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very fine-grained resolutmn as formulated m a tradltlonal dmtlonary In contrast_comma_ expert lexicographers tagged the ~ ord sense in the sentences used m the SENSEVAL exerclse_comma_ where high rater-annotator agreement was reported There are also fuller dlctlonary entries m the HECTOR dlctlonary used and more e<amples showing the usage of each word sense m HECTOR These factors are likely to have contributed to the difference in rater-annotator agreement observed m the three studies conducted We also examined the coarse sense classes derived by the greedy search algorithm Vv'e found some interesting groupings of coarse senses for nouns which ~e hst in Table 4 From Table 4_comma_ it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense graz}.ulanty It Is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of WoRDNET As an example_comma_ there is a total Ii loop: let Ct_comma__comma_ C M denote the current M sense classes ~* +--oo for all z_comma_3 such that 1 <_comma_ < 3 < M let C\[_comma__comma_C~w_ 1 denote the resulting M 1 sense classes by mergmg C_comma_ and C 3 compute ~(C\[_comma__comma_ C~/_t) ff ~(C_comma__comma_ C~4_x) > ~* then ~' +~(C~_comma__comma_C~_t)_comma_ z* +~_comma_ ~* +end for merge the sense class C_comma_.
2217,0,tree-clustering In contrast_comma_ our greedb search algorithm ~s a rumple but effecuve method that makes use of the Kappa statlsUC to search the space of possible sense groupings dlrecdy 9 Conclusion In th~s paper_comma_ we examined the tssue of raterannotator agreement on word sense tagging and presented a greedy search algorithm capable of generating coarse sense classes based on the sense tags 12 Table 3 Type Avg Num of senses A N P. Avg n Before 12 8 5_comma_115 8_comma_602 0 595 0 441 After 5 6 8_comma_042 8_comma_602 0 935 0.852 Inter-annotator agreement for 42 verbs before and after the collapse of senses Noun alr board body change Coarse senses wind/gas vs aura/atmosphere committee vs plank physmal/natural object vs group/coIlectLon modfficatmn vs corns country natron vs region/countryside course class vs action vs dlrectmn field land vs subject foot human body part ~s umt vs lower part/support force strength vs personnel hght matter party Table 4 lumlnatlon vs perspectlve concern/~ssue vs substance pohtlcal party vs socml gathering vs group Coarse senses derived by the greedy search algorlthm assigned by two human annotators We found interesting groupings of word senses that correspond to human lntumve judgment of sense granularity References Rebecca Bruce and Janyce Wmbe 1998 Wordsense dlstmgmshabdlty and inter-coder agreement In Proceedings of the Thtrd Conference on Empzmcal Methods m Natural Language Processmg Jean Carletta 1996 Assessing agreement on classfficat~on tasks The kappa statlstm Computatzonal Lmgutstzcs_comma_ 22(2) 249-254 J Cohen 1960 A coeffictent of agreement for nominal scales Educatwnal and Psychological Measurement_comma_ 20 37-46 ~.dam Kllgarnff 1998a Gold standard datasets for evaluating word sense d~sambiguatlon programs Computer Speech and Language Adam Kflgamff 1998b Inter-tagger agreement In Advanced Papers of the SENSEVAL Workshop Adam Kllgarrtff 1998c SENSEVAL An exercise m evaluating word sense d~samblguatlon programs In Proceedings o/LREC Mitchell P Marcus_comma_ Beatrice Santorml_comma_ and Mary Ann Marcmkmwlcz 1993 Building a large annotated corpus of enghsh The Penn Treebank Computatzonal Lmgutst~cs_comma_ 19(2) 313-330 George A MlUer_comma_ Claudia Leaock_comma_ Randee Tengl_comma_ and Ross T Bunker 1993 A semantm concordance In Proceedings o/the ARPA Human Language Technology Workshop_comma_ pages 303-308 George A Mllter 1990 Wordnet An on-hne lexlcal database International Journal o/Lexicography_comma_ 3(4) 235-312 Hwee Tou Ng and Hlan Beng Lee 1996 Integrating multiple knowledge sources to dlsamblguate word sense An exemplar-based approach In Proceedrags of the 34th Annual Meeting of the Assoczatwn for Computatzonal Lmguzstscs (ACL)_comma_ pages 40--47 Hwee Tou Ng 1997 Exemplar-based word sense dlsambiguatlon Some recent ~mprovements In Proceedings o/ the Second Conference on Empw~cal Methods m Natural Language Processing_comma_ pages 208-213 Jean Veroms 1998 ~ study of polysemy judgemeats and rater-annotator agreement In Advanced Papers o/the SENSEVAL Workshop
2218,0,J J_comma_II Parallel Translations as Sense Discriminators Nancy IDE Department of Computer Science Vassar College 124 Raymond Avenue Poughkeepsie_comma_ NY 12604-0520 USA lde@cs vassar edu Abstract This article reports the results of a prehmlnary analysis of translation equivalents in four languages from different language famdles_comma_ extracted from an on-hne parallel corpus of George Orwell's Nmeteen Eighty-Four The goal of the study is to determine the degree to which translatmn equivalents for different meamngs of a polysemous word In Enghsh are lexlcahzed differently across a variety of languages_comma_ and to detelmme whether this information can be used to structure or create a set of sense distinctions useful in natural language processing apphcatmns A coherence Index is computed that measures the tendency for different senses o1 the same English word to be lexlcahzed differently_comma_ and flora this data a clustering algorithm is used to create sense hierat chles Introduction It ~s well known that the most nagging issue for word sense disamblguanon (WSD) Is the definmon of just what a word sense is At its base_comma_ the problem Is a philosophical and linguistic one that is far from being resolved However_comma_ work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses_comma_ at least to the degree that they are useful for natural language processing tasks such as summarization_comma_ document retrieval_comma_ and machine translataon Several criteria have been suggested and exploited to automatically determine the sense of a word m context (see Ide and V6roms_comma_ 1998)_comma_ including syntactic behavior_comma_ semantic and pragmatic knowledge_comma_ and especially in more recent empirical studies_comma_ word co-occurrence within syntactic relations (e g_comma_ Hearst_comma_ 1991_comma_ Yarowsky_comma_ 1993)_comma_ words co-occurring m global context (e g_comma_ Gale et al_comma_ 1993_comma_ Yarowsky_comma_ 1992 Schutze_comma_ 1992_comma_ 1993)_comma_ etc No clear criteria have emerged_comma_ however_comma_ and the problem continues to loom large for WSD work The notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as a basis for some recent work on WSD Foi example_comma_ Brown et al (1991)and Gale et al (1992a_comma_ 1993) used the parallel_comma_ aligned Hansard Corpus of Canadian Parhamentary debates foi WSD_comma_ and Dagan et al (1991) and Dagan and Ital (1994) used monohngual corpora of Hebrew and German and a bilingual dictionary These studies rely on the assumption that the mapping between words and word senses vanes significantly among languages For example_comma_ the word duty in English t~anslates into French as devoir m ~ts obhgatlon sense_comma_ and tmpOt m ~ts tax sense By determining the translation 52_comma_--._comma_.~.eqmvalent ot duty in a parallel French text_comma_ the correct sense of the Enghsh word is identified These studies exploit th~s lnformatmn m order to gather co-occurrence data for the different senses_comma_ which ts then used to dtsamb~guate new texts In related work_comma_ Dywk (1998) used patterns of translational relatmns in an EnghshNorwegian paralle ! corpus (ENPC_comma_ Oslo Umverslty) to define semantic propemes such as synonymy_comma_ ambtgmty_comma_ vagueness_comma_ and semantic helds and suggested a derivation otsemantic representations for signs (eg_comma_ lexemes)_comma_ captunng semantm relatmnshlps such as hyponymy etc_comma_ fiom such translatmnal relatmns Recently_comma_ Resnlk and Yarowsky (1997) suggested that fol the purposes ot WSD_comma_ the different senses of a wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally In particular_comma_ they propose that some set of target languages be ~dent~fied_comma_ and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages This idea would seem to p~ovtde an answer_comma_ at least m part_comma_ to the problem of determining different senses of a word mtumvely_comma_ one assumes that ff another language lexlcahzes a word m two or more ways_comma_ there must be a conceptual monvatmn If we look at enough languages_comma_ we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of a word However_comma_ th~s suggestmn raises several questions Fo~ instance_comma_ ~t ~s well known that many amb~gumes are preserved across languages (for example_comma_ the French tntdrYt and the Enghsh interest)_comma_ especmlly languages that are relatively closely related Assuming this problem can be overcome_comma_ should differences found m closely related languages be given lesser (or greater) weight than those found m more distantly related languages 9 More generally_comma_ which languages should be considered for this exermse 9 All languages 9 Closely related languages9 Languages from different language famlhes '~ A mixture of the two 9 How many languages_comma_ and of which types_comma_ would be 'enough' to provide adequate lnfotmanon tot this purpose~ There ts also the questmn ot the crlterm that would be used to estabhsh that a sense distinction is 'lexlcahzed cross-hngu~stmally' How consistent must the d~stlnCtlOn be 9 Does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages_comma_ or need tt only be the case that the option ot a different lexlcahzatlon exists m a certain percentage of cases 9 Another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from Using bdmgual dictionaries would be extremely tedmus and error-prone_comma_ g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make Resmk and Yalowsky (1997) suggest EutoWordNet (Vossen_comma_ 1998) as a possible somce of mformatmn_comma_ but_comma_ given that EuroWordNet ts pttmatdy a lexmon and not a corpus_comma_ ~t is subject to many of the same objections as for bl-hngual dictionaries An alternative would be to gather the reformation from parallel_comma_ ahgned corpma Unlike bilingual and muttt-hngual dictionaries_comma_ translatmn eqmvalents xn parallel texts a~e determined by experienced translatols_comma_ who evaluate each instance ot a word's use m context rather than as a part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in a dictionary However_comma_ at present very few parallel ahgned corpora exist The vast majority ot these are bl-texts_comma_ mvolwng only two languages_comma_ one of which is very often English Ideally_comma_ a serious 53 evaluation of Resnik and Yarowsky's proposal would include parallel texts m languages from several different language families_comma_ and_comma_ to maximally ensure that the word m question is used in the exact same sense across languages_comma_ ~t would be preferable that the same text were used over all languages in the study The only currently avadable parallel corpora for more than two languages are Olwell's Nmeteen Eighty-Four (Erjavec and Ide_comma_ 1998)_comma_ Plato's Repubhc (Erjavec_comma_ et al_comma_ 1998)_comma_ the MULTEXT Journal .o/ the Commt.~ston corpus (Ide and V6roms_comma_ 1994)_comma_ and the Bible (Resnlk_comma_ et al_comma_ m press) It is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions Also_comma_ ~t Is not clear how the lexlcahzatlon of sense distractions across languages Is affected by genre_comma_ domain_comma_ style_comma_ etc Thls paper attempts to provide some prehmlnary answers to the questions outhned above_comma_ In order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions_comma_ and_comma_ ff so_comma_ the ways in which th~s reformation might be used Given the lack of lalge parallel texts across multiple languages_comma_ the study is necessarily hmlted_comma_ however_comma_ close exammanon of a small sample of parallel data can_comma_ as a first step_comma_ provide the basis and dlrectmn for more extensive studies 1 Methodology I have conducted a small study using parallel_comma_ aligned versmns ot George Orwell's Nineteen Etghtv-Fo_comma_lr (Euavec and Ide_comma_ 1998)m five languages Enghsh_comma_ Slovene_comma_ Estonian_comma_ Romanlan_comma_ and Czech I The study therefole Involves languages from four language families The O~well parallel corpus also includes vers|ons o) Ntneteen-E~gho Four m Hungarian_comma_ Bulgarmn_comma_ Latwan_comma_ Llthuaman_comma_ Se~bmn_comma_ and Russmn (Germanic_comma_ Slavic_comma_ Fmno-Ugrec_comma_ and Romance)_comma_ two languages from the same family (Czech and Slovene)_comma_ as well as one non-Indo-European language (Estoman) Nmeteen Eighty-Four Is a text of about 100_comma_000 words_comma_ translated directly from the original English m each of the other languages The parallel versions of the text are sentence-aligned to the English and tagged for part of speech Although Nineteen Eighty-Four is a work of fiction_comma_ Orwell's prose IS not highly stylized and_comma_ as such_comma_ it provides a reasonable sample ot modern_comma_ ordinary language that ~s not tied to a given topic or sub-domain (such as newspapers_comma_ technical reports_comma_ etc ) Furthermore_comma_ the translations of the text seem to be relatively faithful to the original for instance_comma_ over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (Prlest-Dorman_comma_ et al_comma_ 1997) Nine ambiguous English words were considered hard_comma_ head_comma_ country_comma_ hne_comma_ promise_comma_ shght_comma_ seize_comma_ scrap_comma_ float The first four were chosen because they have been used in other dlsamb~guatlon studies_comma_ the latter five were chosen from among the words used m the Senseval dlsamblguatlon exercise (Kllgamff and Palmer_comma_ forthcoming) In all cases_comma_ the study was necessarily hmlted to words that occurred frequently enough in the Orwell text to warrant consideration F~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (Including morphological variants) of each of the nine words were extracted from the Enghsh text_comma_ together w~th the parallel sentences m which they occur m the texts ot the four comparison languages (Czech_comma_ Estonian_comma_ Romantan_comma_ Slovene) As Walks and Stevenson (1998) have pointed out_comma_ pa~t-of-speech tagging accomplishes a good portion of the work ot semantic dlsamb~guatmn_comma_ therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 The Enghsh occurrences were then grouped usmg the sense distinctions m WordNet_comma_ (version 1 6) \[Miller et al_comma_ 1990_comma_ Fellbaum_comma_ 1998\]) The sense categonzatmn was performed by the author and two student assistants_comma_ results from the three were compared and a final_comma_ mutually agreeable set of sense assignments was estabhshed For each of the four comparison languages_comma_ the corpus of sense-grouped parallel sentences were sent to a llngmst and natl_comma_ve speaker of the comparison language The hngmsts were asked to provide the lexlcal item m each parallel sentence that corresponds to the ambiguous Enghsh word If inflected_comma_ they were asked to provide both the inflected form and the root form In addttmn_comma_ the lmgmsts were asked to indicate the type of translatmn_comma_ according to the dtstmctmns given m Table 1 For over 85% of the Enghsh word occurrences (corresponding to types 1 and 2 m Table 1)_comma_ a specific lexlcal item or items could be identified as the translation equivalent for the corresponding Enghsh word For comparison purposes_comma_ each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the WordNet sense to which it corresponds In order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents_comma_ a coherence index ( Cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which a g~ven se_comma_ls_comma_z ~s translated with the same word ~ Note that the z The adJective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used m the study Note that the CI ~s similar to semanuc entropy (Melamed_comma_ 1997) However_comma_ Melamed computes CIs do not determine whether or not a sense dtstmctton can be lextcahzed in the target language_comma_ but only the degree to whmh they are lexicahzed differently m the translated text However_comma_ tt can be assumed that the CIs provide a measure of the tendency to lex~cahze different WordNet senses differently_comma_ which can m turn be seen as an mdtcatmn of the degree to which the distraction ts vahd For each ambiguous word_comma_ the CI Is computed for each pair of senses_comma_ as follows S<q t> Cl(sqS_comma_ ) = '=1 m rnrt where @ n ~s the number of comparison languages under consideration_comma_ nl~q and m_comma__comma_ are the nt~mber of occurrences olsense sqand sense s~ m the Enghsh corpus_comma_ respectively_comma_ including occurrences that have no idenufiable translation_comma_ s<~ ~>m ts the number of times that senses q and r are translated by the same lex~cal Item m language t_comma_ i e_comma_ x=y t ~tJan ~( q )_comma_ r~oan~( r ) The CI ts a value between 0 and 1_comma_ computed by examining clusters of occurrences translated by the same word In the othel languages If sense and sense ) are consistently translated w~th the same wo~d in each comparison language_comma_ then Cl(s_comma_ s~) = 1_comma_ if they are translated with a different word m every occurrence_comma_ Cl(s_comma_ ~) = 0 In general_comma_ the CI for pans of different senses provides an index of thmr relatedness_comma_ t e_comma_ the greater the value of Cl(s_comma_ sj)_comma_ the more frequently occurrences of-sense t and sense j are translated with the same lextcal item When t = j_comma_ we entropy tOl wold types_comma_ lather than word senses 55 obtain a measure of the coherence of a ~lven sense Type Meaning 1 A slngle lexlcal Item is used to translate the En@izsh equivalent (possibly a 2 The English word is translated by a phrase of two or more words or a compound_comma_ meaning as the slngle English word 3 The En@izsh word is not lexzcalized in the translation 4 A pronoun is substituted for the English word In the translation An English phrase contalnmng the ambiguous word Is translated by a single language which has a broader or more specific meanlng_comma_ or by a phrase in whl corresponding to the English word Is not explicltl~ lexlcallzed Table 1 Translation types and their trequencles % dizen whl%h h 6% 6% 6% of s p same Word # Description hard 1 1 difficult 2 head i i i 1 Table 2 1 2 _meta~horlcally hard _\] 3 not yielding to pressure_comma_ 1 4 very strong or ~lgorous_comma_ ar 2 I wlth force or vigor (adv) 3 earnestly_comma_ intently (adv) i_ ~art of the body  3 intellect 4 _r~le_!r_comma_ ch_comma_%ef 7 front_comma_ front part WoldNet senses ot hard and head CIs were also computed for each language individually as well as for different language groupings Romaman_comma_ Czech_comma_ and Estonian (three different language families) Czech and Slovene (same family)_comma_ Romaman_comma_ Czech_comma_ Slovene (Indo-European_comma_ and Estonian (nonIndo-European) To better visualize the relationship between senses_comma_ a hierarchical clustering algorithm was applied to the CI data to generate trees reflecting sense proximity 4 Finally_comma_ in order to determine the degree to which the linguistic relaUon between languages may affect coherence_comma_ a correlation was run among CIs for all pairs of the four target languages Fol example_comma_ Table 2 gives the senses of hard and head that occurred in the data s The CI data .s 'sobS' hard and head are given in Tables 3 and 4 ~uous CIs measuring the aff_comma_ mty of a sense with itself--that is_comma_ the tendency for all occurrences of that sense to be translated wlth the same word--show that all of the s_comma_x senses of ha_comma_d have greatel internal consistency tfian athmty with other senses_comma_ with senses 1 1 ('dlff|cult' CI = 56) and 13 (_comma_'not soft_comma__comma_ ci = 63) registenng the h_comma_ghest internal consistency 6 The same holds true for three of the four senses of head_comma_ while the CI for senses 1 3 ('Intellect') and 1 1 ('part of the body') is higher than the CI for 1 3/1 3 WordNet Sense 2 1 2 3 1 4 1 3 1 1 1 2 21 23 1 4 13 0 50 o 13 i ool 0 O0 0 25 i O0 0 04 0 50 0 17 0 56 0 19 0 00 0 00 0 00 0 00 0 00 0 25 0 21 Table 3 CIs for hard I i 12 0_comma__comma_63 0 00 0 50 2 Results Although the data sample is small_comma_ It gives some insight into ways m which a larger sample might contribute to sense discrimination 4 Developed by Andleas Stolcke Results tor all words m the study are avadable at http//www cs vassar edu/~~de/wsd/cross-hng html 6 Senses 2 3 and 1 4 have CIs ot 1 because each ot these senses exists m a single occurrence m the corpus_comma_ and have theretote been dlscarded horn consideration ot CIs to~ individual senses We a~e currently mvesugatmg the use oI the Kappa staUst~c (Carletta_comma_ 1996) to normahze these sparse data 56 WordNet Sense 1 1 1 3 1 4 1 7 1 1 0 69 1 3 0 53 0 45 1 4 0 12 0 07_comma_ 0 50 1 7 0 40 0 001 0 00 1 00 Table 4 CIs for head Figure 2 shows the sense clusters for hard generated from the CI data 7 The senses fall into two mare clusters_comma_ w~th the two most internally consistent senses (1 1 and 1 3) at the deepest level of each ot the respecuve groups The two adverbml forms 8 are placed in separate groups_comma_ leflectmg thmr semantic proximity to the different adjecuval meanings of hard The clusters for head (Figure 2) stmdarly show two dlstmct groupings_comma_ each anchored in the two senses with the h~ghest internal consistency and the lowest mutual CI ('part of the body' (1 1) and 'ruler_comma_ chief' (1 4)) The h~erarchtes apparent m the cluster graphs make intuitive sense Structured hke dictmnary enmes_comma_ the clusters for hard and head might appeal as m F~gure 1 This ts not dissimilar to actual dlctLonary entries for hard and head_comma_ for example_comma_ the enmes for hard in four differently constructed dlctmnanes ( Colhns Enghsh (CED)_comma_ Longman's (LDOCE)_comma_ OxJotd Advanced Learner's (OALD)_comma_ and COBUILD) all hst the ''d~fficult' and 'not soft' senses first and second_comma_ whmh_comma_ since most dictionaries hst the most common Ol frequently used senses hrst_comma_ reflects the gross dlwslon apparent m the clusters Beyond this_comma_ ~t ~s difficult to assess the 7 Foi the purposes ot the cluster analys~s_comma_ CIs of l 00 resulting from a single occurrrence were normahzed to 5 8 Because ~oot to_comma_ ms were used m the analysis_comma_ no dzstlncUon m UanslaUon eqmvalents was made tor part ot speech correspondence between the senses In the dictionary entries and the clusters The remamlng WordNet senses are scattered at various places within the entries or_comma_ m some cases_comma_ split across various senses The h~erarchlcal relatmns apparent m the clusters are not reflected m the d~cttonary enmes_comma_ smce the senses are for the most part presented in flat_comma_ hnear hsts However_comma_ It is interesting to note that the first five senses of hard In the COBUILD d~cuonary_comma_ which is the only d~cttonary in the group constructed on the bas~s of colpus examples 9 and presents senses m ruder of frequency_comma_ correspond to hve of the six WordNet senses in thls study WordNet's 'metaphorically hard' is spread over multiple senses in the COB UILD_comma_ as it.is In the other d~ctlonarles HARD HEAD I 1 dlfflcult 2 vlgorously II 1 a not soft b strong 2 a earnestly b metaphorlcally hard I 1 a part of the body b zntellect 2 front_comma_ front part II ruler_comma_ chlef Flgme 1 Clusteis tol hard and head suuctured as dlcuonary entt ~es The results tor dlftment language groupings show that the tendency to lextcahze senses differently is not aftected by language d~stance (Table 5) In fact_comma_ the mean CI fol Estonian_comma_ the only non-Indo-European language m the study_comma_ ~s lower than that for any other group_comma_ mdmatmg that WordNet sense dtstmctmns are slightly less hkely to be lexlcahzed differently m Estonian 9 Edmons ot the LDOCE (1987 vexsmn) and OALD (1985 version) dictlonalles consulted m this study ple-date edmons ol those same d~ctlonanes based on colpus evidence 57 Correlations of CIs for each language pair (Table 5) also show no relationship between the degree to which sense d~stmcuons are lexlcahzed differently and language distance This is contrary to results obtained by Resmk and Yarowsky (subm_comma_tted)_comma_ who_comma_ using a memc slmdar to the one used in this study_comma_ found that that non-Indo-European languages tended to lexlcallze English sense d~stmctlons more than Indo-European languages_comma_ especially at finergrained levels However_comma_ their translation data was generated by native speakers presented with Isolated sentences in English_comma_ who were asked to provide the translation for a given word In the sentence It is not clear how this data compares to translations generated by trained translators working with full context Lanquaqe qroup Averaqe CI ALL 0 27 RO/ES/SL 0 28 SL/CS 0 28 RO/SL/CS 0 27 ES 0 26 Table 5 Average CI values Lanqs Hard Country Llne Head Ave ES/CS 0 86 0 72 0 68 0 69 0 74 RO/SL 0 73 0 78 0 68 1 00 0 80 RO/CS 0 83 0 66 0 67 0 72 0 72 SL/CS 0 88 0 51 0 72 0 71 0 71 RO/ES 0 97 0 26 0 70 0 98 0 73 ES/SL 0 73 0 59 0 90 0 99 0 80 Table 6 CI correlauon tor the tour target languages I -I I  I I m~nlmum dlstance = 0 249399 m~nlmum d~stance = 0 434856 mlnlmum dlstance = 0 555158 mlnlmum dlstance = 0 602972 m~nlmum dlstance = 0 761327 I  >21 I  >ii I  >23 l  >13 l  >14 I  >12 (13) (23) (12) (1_comma_4) (ii) (21) (1412) (2313) ( 2 3 1 3 1 4 1 2 ) ( 2 111 ) Figure 2 Cluster tree and distance measures tor the sm senses of hard I  >14 -i I  > i i I--- 1 J  > i 3 I  >17 mlnlmum dlstance = 0 441022 mlnlmum dlstance = 0 619052 mln~mum dlstance = 0 723157 (13) (ll) (17) (1113) (111317) (14) F_comma_gure 3 Cluster tree and dmtance measures tot the tout senses ot head 58 Conclusion The small sample m this study suggests that cross-hngual lexlcahzat~on can be used to define and structure sense d~stmct~ons The cluster graphs above provide mformat~on about relations among WordNet senses that could be used_comma_ for example_comma_ to determine the granularity of sense differences_comma_ whtch m turn could be used in tasks such as machine translatton_comma_ mtormaUon retrieval_comma_ etc For example_comma_ it is hkely that as sense dtstmcttons become finer_comma_ the degree of error ~s less severe Resmk and Yarowsky (1997) suggest that confusing freer-grained sense dtstmctlons should be penahzed less severely than confusing grosser d~stmct~ons when evaluatmg the performance of sense dtsambtguatt0n systems The clusters also provide insight into the lexlcallzatlon of sense dtstmcttons related by various semantic relations (metonymy_comma_ meronymy_comma_ etc ) across languages_comma_ for instance_comma_ the 'part of the body' and 'intellect' senses of head are lex~cahzed with the same ~tem a s~gnlficant portion of the t~me across all languages_comma_ reformation that could be used m machine translatton In addtt~on_comma_ cluster data such as that presented here could be used m lexicography_comma_ to determine a mole detaded hierarchy of relations among senses in dtct~onary entries It is less clear how cross-hngual reformation can be used to determine sense d~st~nctlons independent of a pre-deflned set_comma_ such as the WordNet senses used here In an effort to explore how thts mlght be done_comma_ I have used the small sample from thts study to create word groupmgs from 'back translations' (l e_comma_ additional translations m the original language ot the translations m the target language) and developed a metric that uses th~s mformatton to determine relatedness between occurrences_comma_ whtch ~s m turn used to cluster occurrences into sense groups I have also compared sets of back translations for words representing the various WordNet senses_comma_ which provtde word groups s~mdar to WordNet synsets Interestingly_comma_ there ts virtually no overlap between the WordNet synsets and word groups generated from back translations The results show_comma_ however_comma_ that sense dlstmctlons useful for natural language processing tasks such as machme translanon could potentsally be determined_comma_ ot at least influenced_comma_ by constdeHng this mformatton The automatically generated synsets themselves may also be useful m the same apphcatlons; where WordNet synsets (and ontologtes) have been used tn the past More work needs to be done on the topic of cross-hngual sense determination_comma_ utthzmg substantially larger parallel corpora that include a variety ot language types as well as texts fiom several genres This small study explores a possible methodology to apply when such resources become avatlable Acknowledgements The author would hke to gratefully acknowledge the contrtbut~on of those who provided the translatton mfotmat~on Tomaz Eua~ec (Slovene)_comma_ Kadrt Muxschnek (Estonian)_comma_ Vladtmlr Petkevtc (Czech)_comma_ and Dan Tubs (Romanlan)_comma_ as well as Dana Fleut and Darnel Khne_comma_ who helped to transcrtbe and evaluate the data Special thanks to Dan Melamed and Hlnrtch Schutze for their helpful comments 59 \[\] \[\] in \[\] in i i Hg nn i an i am References Ca~letta_comma_ Jean (1996) Assessing Agreement on Classthcatton Tasks The Kappa Stat~st~t. Computational Lmgulstlcs_comma_ 22(2)_comma_ 249-254 Dagan_comma_ Ido and Ita~_comma_ Alon (1994) Wo~d sense dlsambxguat~on using a second language monohngual corpus Computattonal Ltngmsttcs_comma_ 20(4)_comma_ 563-596 Dagan_comma_ Ido_comma_ Ital_comma_ Alon_comma_ and Schwall_comma_ Ulnke (1991) Two languages a~e more mformattve than one Proceedings of the 29th Annual Meettng of the Assoctatton for Computattonal Ltngutsttcs_comma_ 18-21 June 1991_comma_ Berkeley_comma_ Cahfornm_comma_ 130-137 Dyvtk_comma_ Helge (1998) Translations as Semantic Mirrors Proceedmgs of Workshop W13 Multzlmguahty in the Lextcon II_comma_ The 13th Biennial European Conference on Arttftctal lntelhgence (ECA198)_comma_ Brighton_comma_ UK_comma_ 24-44 Eqavec_comma_ Tomaz and Ide_comma_ Nancy (1998) The MULTEXT-EAST Corpus Proceedlng~ of the Fltst International Conference on Language Resources and Evaluatton_comma_ 27-30 May 1998_comma_ Granada_comma_ 971-74 Erjavec_comma_ Tomaz_comma_ Lawson_comma_ Ann_comma_ and Romary_comma_ Laurent (1998) East meets West Producing Multflmgual Resources m a European Context Pioceedtngs of the Ftrst Internattonal Conference on Language Resources and Evaluation_comma_ 27-30 May 1998_comma_ Gtanada_comma_ 981-86 Fellbaum_comma_ Chttstmne (ed) (1998) WordNet An Electrontc Lexlcal Database MIT Press_comma_ Cambridge_comma_ Massachusetts Gale_comma_ Wdham A_comma_ Church_comma_ Kenneth W and Yatowsky_comma_ Davtd (1993) A method tor dlsamblguatmg word senses m a large cmpus Computers and the Humamtles_comma_ 26_comma_ 415-439_comma_ Hearst_comma_ M'attl A (1991) Noun homograph  ' dlsamblguatlon using local:'~.'0ntext m large corpora Proceedtngs of the 7th Annual Conference of the Umver~lt~ of Waterloo Centre for the New OED and Text ReaeaJch_comma_ Oxford_comma_ Umted Kingdom_comma_ 1-19 Ide_comma_ Nancy and V61oms_comma_ Jean (1998) Word sense d~samb~guat~on The state of the alt Computational Lmgut~ttc~_comma_ 24 1_comma_ 1-40 Kdgar~ttt_comma_ Adam and Palmer_comma_ Ma~tha_comma_ Eds (forthcoming) Proceedmgs ot the Senseval Word Sense D~samb~guatlon Workshop_comma_ Specml double ~ssue otComputer~ and the Humamttes_comma_ 33 4-5 Leacock_comma_ Claudia_comma_ Towell_comma_ Geoffrey and Voorhees_comma_ Ellen (1993) Corpus-based stattstlcal sense resolution Proceedtng~ of the ARPA Human Language Technology Worsl~shop_comma_ San Francisco_comma_ Morgan Kautman Melamed_comma_ I Dan (1997) Measuring Semantic Entropy ACL-SIGLEX Workshop Taggmg Tert wtth Lextcal Semanttcs Why_comma_ What_comma_ and How ~ April 4-5_comma_ 1997_comma_ Washington_comma_ D C_comma_ 41-46 Mtllet_comma_ George A_comma_ Beckwlth_comma_ Richard T Fellbaum.
2219,0,1 Introduction on measures for inter-rater reliability (Carletta_comma_ 1996)_comma_ on frameworks for evaluating spoken dialogue agents (Walker et al. _comma_ 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator_comma_ Eskenazi et al.(1999)).
2220,0,Carletta (1996) argues that the kappa statistic (a) should be adopted to judge annotator consistency for classification tasks in the area of discourse and dialogue analysis.
2221,0,Carletta argues that several incompatible measures of annotator agreement have been used in discourse analysis_comma_ making comparison impossible.
2222,0,Her solution is to look to the field of content analysis_comma_ which has already experienced these problems_comma_ and adopt their solution of using the kappa statistic.
2223,0,It has been claimed that content analysis researchers usually regard a > .8 to demonstrate good reliability and .67 < ~ < .8 alf16 lows tentative conclusions to be drawn (see Carletta (1996)).
2224,0,Carletta mentions this problem_comma_ asking what the difference would be if the kappa statistic were computed across 'clause boundaries_comma_ transcribed word boundaries_comma_ and transcribed phoneme boundaries' (Carletta_comma_ 1996_comma_ p. 252) rather than the sentence boundaries she suggested.
2225,0,len.: median length of sequences of co-specifying referring expressions with Cohen's n (Cohen_comma_ 1960; Carletta_comma_ 1996).
2226,0,It ewduato.s the pairwise agreement mnong a set; of coders making category.iudgment_comma_ correcting tbr expected chance agreement (Carletta_comma_ 1996).
2227,0,As argued in Carletta (1996)_comma_ Kappa values of 0.8 or higher are desirable for detecting associations between several coded variables; we were thus satisfied with the level of agreement achieved.
2228,0,Agreement among annotators was measured using the K statistic (Siegel and Castellan 1988; Carletta 1996).
2229,0,One of our goals was to use for our study only information that could be annotated reliably (Passonneau and Litman_comma_ 1993; Carletta_comma_ 1996)_comma_ as we believe this will make our results easier to replicate.
2230,0,The agreement on identifying the boundaries of units_comma_ using the AK statistic discussed in (Carletta_comma_ 1996)_comma_ was AK BP BMBL (for two annotators and 500 units); the agreement on features(2 annotators and at least 200 units) was follows: Attribute AK Value utype .76 verbed .9 finite .81 subject .86 NPs Our instructions for identifying NP markables derive from those proposed in the MATE project scheme for annotating anaphoric relations (Poesio et al. _comma_ 1999).
2231,0,We measured stability (the degree to which the same annotator will produce an annotation after 6 weeks) and reproducibility (the degree to which two unrelated annotators will produce the same annotation)_comma_ using the Kappa coefficient K (Siegel and Castellan_comma_ 1988; Carletta_comma_ 1996)_comma_ which controls agreement P(A) for chance agreement P(E): K = PA)-P(E) 1-P(Z) Kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution_comma_ and 1 for perfect agreement.
2232,0,The agreement was statistically significant (Kappa = 0.65.0 > 0.01 for Japanese and Kappa = 0.748_comma_0 > 0.01 for English (Carletta_comma_ 1996; Siegel-and Castellan_comma_ 1988)).
2233,0,In other words_comma_ (4b) can be used in substitution of (4a)_comma_ whereas (5b) cannot_comma_ so easily 41n (Carletta_comma_ 1996)_comma_ a value of K between .8 and I indicates good agreement; a value between .6 and .8 indicates some agreement.
2234,0,Carletta (1996) cites the convention from the domain of content analysis indicating that .67 K K < .8 indicates marginal agreement_comma_ while K > .8 is an indication of good agreement.
2235,0,The results are quite promising: our extraction method discovered 89% of the WordNet cousins_comma_ and the sense partitions in our lexicon yielded better  values (Carletta_comma_ 1996) than arbitrary sense groupings on the agreement data.
2236,0,11 This low agreement ratio is also re ected in a measure called the  statistic (Carletta_comma_ 1996;; Bruce and Wiebe_comma_ 1998;; Ng et al. _comma_ 1999).
2237,0,Normally_comma_   :8 is considered a good agreement (Carletta_comma_ 1996).
2238,0,On the one hand_comma_ even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability (a124a126a125a128a127a130a129 ) or even the level where tentative conclusions may be drawn (a127a130a131a133a132a135a134a72a124 a134 a127a130a129 ) (Carletta_comma_ 1996)_comma_ (Krippendorff_comma_ 1980).
2239,0,This information can be annotated reliably (a1a3a2a5a4a7a6a9a8 a10a12a11a14a13a16a15 and a1a17a2a5a4a19a18a20a8 a10a12a11a14a13a16a21 ).4 4Following (Carletta_comma_ 1996)_comma_ we use the a22 statistic to estimate reliability of annotation.
2240,0,Analyze resulting findings to determine a progression of competence In (Michaud et al. _comma_ 2001) we discuss the initial steps we took in this process_comma_ including the development of a list of error codes documented by a coding manual_comma_ the verification of our manual and coding scheme by testing inter-coder reliability in a subset of the corpus (where we achieved a Kappa agreement score (Carletta_comma_ 1996) of a0 a1a3a2a5a4a7a6 )2_comma_ and the subsequent tagging of the entire corpus.
2241,0,An initial concern was to put the samples on equal footing despite the fact that they covered a broad range in lengthfrom 2 to 58 sentencesand there was a danger that longer samples would tend 2We also discuss why we were satisfied with this score despite only being in the range of what Carletta calls tentative conclusions. 3Although these samples were relatively homogeneous with respect to the amount of English training and the age of the writer_comma_ we expected to see a range of demonstrated proficiency for reasons discussed above.
2242,0,Although the Kappa coefficient has a number of advantages over percentage agreement (e.g. _comma_ it takes into account the expected chance interrater agreement; see Carletta (1996) for details)_comma_ we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below_comma_ whose performance will also be reported in terms of percentage agreement.
2243,0,Other commonly used measures include kappa (Carletta 1996) and relative utility (Radev_comma_ Jing_comma_ and Budzikowska 2000)_comma_ both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract.
2244,0,421 Teufel and Moens Summarizing Scientific Articles We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and reproducibility_comma_ following Carletta (1996).
2245,0,The reliability of the annotations was checked using the kappa statistic (Carletta_comma_ 1996).
2246,0,5 Reliability of Annotations 5.1 The Kappa Statistic To measure the reliability of annotations we used the Kappa statistic (Carletta_comma_ 1996).
2247,0,To test the reliability of group segmentation within GDM-IS_comma_ we calculate the kappa coefficient (C3) 8 (Carletta_comma_ 1996; Carletta et al. _comma_ 1997; Flammia_comma_ 1998) to measure pairwise agreement between the subject and the expert.
2248,0,From (Carletta_comma_ 1996) 9 Combined metric BY BP B4AC BE B7BDB5C8CABPB4AC BE C8 B7 CAB5_comma_ from (Jurafsky and Martin_comma_ 2000_comma_ p.578)_comma_ AC BPBD.
2249,0,A detailed discussion on the use of kappa in natural language processing is presented in (Carletta_comma_ 1996).
2250,0,We chose nouns that occur a minimum of 10 times in the corpus_comma_ have no undetermined translations and at least five different translations in the six nonEnglish languages_comma_ and have the log likelihood score of at least 18; that is: LL(T T_comma_ T S ) =  = 2 1 ij n* j * j*i ij n log  18 where n ij stands for the number of times T T and T S have been seen together in aligned sentences_comma_ n i* and n *j stand for the number occurrences of T T and T S_comma_ respectively_comma_ and n ** represents the total 4 We computed raw percentages only; common measures of annotator agreement such as the Kappa statistic (Carletta_comma_ 1996) proved to be inappropriate for our two-category (yesno) classification scheme.
2251,0,The kappa value (Carletta_comma_ 1996) was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was.
2252,0,The reliability of the annotations was checked using the kappa statistic (Carletta_comma_ 1996).
2253,0,As Carletta (1996) notes_comma_ many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff_comma_ and according to Fleiss (1981)_comma_ kappa values between .4 and .75 indicate fair to good agreement anyhow.
2254,0,The resulting Kappa statistics (Carletta_comma_ 1996) over the annotated data yields a0a2a1 a3a5a4a7a6_comma_ which seems to indicate that human annotators can reliably distinguish between coherent samples (as in Example (1a)) and incoherent ones (as in Example (1b)).
2255,0,The annotation can be considered reliable (Krippendorff_comma_ 1980) with 95% agreement and a kappa (Carletta_comma_ 1996) of.88.
2256,0,Co-selection measures include precision and recall of co-selected sentences_comma_ relative utility (Radev et al. _comma_ 2000)_comma_ and Kappa (Siegel and Castellan_comma_ 1988; Carletta_comma_ 1996).
2257,0,Co-selection methods have some restrictions: they only work for extractive summarizers.
2258,0,3.1.2 Kappa Kappa (Siegel and Castellan_comma_ 1988) is an evaluation measure which is increasingly used in NLP annotation work (Krippendorff_comma_ 1980; Carletta_comma_ 1996).
2259,0,Though inter-rater reliability using the kappa statistic (Carletta 1996) may be calculated for each group_comma_ the distribution of categories in the contribution group was highly skewed and warrants further discussion.
2260,0,Overall % agreement among judges for 250 propositions 60.1 A commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic (Carletta_comma_ 1996).
2261,0,As a concession to the popularity of that statistic_comma_ we compute it in a few different ways here_comma_ though  as we will explain  we do not consider it particularly appropriate.
2262,0,The  statistic (Carletta_comma_ 1996) is recast as: (fs_comma_w)(sys_comma_sys) = agr(fs_comma_w)(sys_comma_sys) P agr(fs_comma_)(sys_comma_sys) N  P agr(fs_comma_)(sys_comma_sys) N In this modified form_comma_ (fs_comma_w) represents the divergence in relative agreement wrt f s for target noun w_comma_ relative to the mean relative agreement wrt f s over all words.
2263,0,In this sense_comma_ instead of measuring only the categorial agreement between annotators with the kappa statistic (Carletta_comma_ 1996) or the performance of a system in terms of precision/recall_comma_ we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance between two concepts such as proposed by (Hahn and Schnattinger_comma_ 1998) or (Madche et al. _comma_ 2002).
2264,0,In fact_comma_ it has been shown that the agreement of subjects annotating bridging (Poesio and Vieira_comma_ 1998) or discourse (Cimiano_comma_ 2003) relations can be too low for tentative conclusion to be drawn (Carletta_comma_ 1996).
2265,0,With the help of the kappa coefficient (Carletta_comma_ 1996) proposes to represent the dialog success independently from the task intrinsic complexity_comma_ thus opening the way to task generic comparative evaluation.
2266,0,In order to determine inter-annotator agreement for the database of annotated texts_comma_ we computed kappa statistics (Carletta_comma_ 1996).
2267,0,The reliability for the two annotation tasks (-statistics (Carletta_comma_ 1996)) was of 0.94 and 0.90 respectively.
2268,0,[KD1_comma_ 2371] 2.3 Reliability To evaluate the reliability of the annotation_comma_ we use the kappa coe cient (K) (Carletta_comma_ 1996)_comma_ which measures pairwise agreement between a set of coders making category judgements_comma_ correcting for expected chance agreement.
2269,0,The kappa statistic (Carletta_comma_ 1996) for identifying question segments is 0.68_comma_ and for linking question and answer segments given a question segment is 0.81.
2270,0,Carletta (1996) says that 0.67 a10a14a11a15a10 0.8 allows just tentative conclusions to be drawn.
2271,0,Merlo and Stevenson (2001) report inter-judge a11 values of 0.53 to 0.66 for a task we consider to be comparable to ours_comma_ that of classifying verbs into unergative_comma_ unaccusative and object-drop_comma_ and argue that Carlettas is too stringent a scale for our task_comma_ which is qualitatively quite different from content analysis (Merlo and Stevenson_comma_ 2001_comma_ 396).
2272,0,Although they are still not all above 0.8_comma_ as would be desirable according to Carletta_comma_ we consider them to be strong enough to back up both the classification and the feasibility of the task by humans.
2273,1,Carletta (1996) deserves the credit for bringing  to the attention of computational linguists.
2274,0,One of our goals was to use for this study only information that could be annotated reliably (Passonneau and Litman 1993; Carletta 1996)_comma_ as we believe this will make our results easier to replicate.
2275,0,The agreement on identifying the boundaries of units_comma_ using the kappa statistic discussed in Carletta (1996)_comma_ was  = .9 (for two annotators and 500 units); the agreement on features (two annotators and at least 200 units) was as follows: utype:  = .76; verbed:  = .9; nite:  = .81.
2276,0,The two annotators agreed on the annotations of 385/453 turns_comma_ achieving 84.99% agreement_comma_ with Kappa = 0.68.2 This inter-annotator agreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2a3a5a4a7a6a8a6a9a4a11a10a13a12a15a14a17a16a19a18a21a20a22a12a23a14a25a24a26a18 a27 a20a22a12a23a14a25a24a26a18 (Carletta_comma_ 1996).
2277,0,Inter-annotator agreement was determined for six pairs of two annotators each_comma_ resulting in kappa values (Carletta (1996)) ranging from 0.62 to 0.82 for the whole database (Carlson et al.(2003)).
2278,0,To support this claim_comma_ first_comma_ we used the  coefficient (Krippendorff_comma_ 1980; Carletta_comma_ 1996) to assess the agreement between the classification made by FLSA and the classification from the corpora  see Table 8.
2279,0,The intercoder reliability is a constant concern of everyone working with corpora to test linguistic hypotheses (Carletta_comma_ 1996)_comma_ and the more so when one is coding for semanto-pragmatic interpretations_comma_ as in the case of the analysis of connectives.
2280,0,The agreement on identifying the boundaries of units_comma_ using the  statistic discussed in (Carletta_comma_ 1996)_comma_ was  =.9 (for two annotators and 500 units); the agreement on features (2 annotators and at least 200 units) was as follows: UTYPE: =.76; VERBED: =.9; FINITE: =.81.
2281,0,6 Coding reliability The reliability of the annotation was evaluated using the kappa statistic (Carletta_comma_ 1996).
2282,0,The reliability for the two annotation tasks (-statistics (Carletta_comma_ 1996)) was of 0.94 and 0.90 respectively.
2283,0,In addition to raw inter-tagger agreement_comma_ the kappa statistic_comma_ which removes from the agreement rate the amount of agreement that is expected by chance(Carletta_comma_ 1996)_comma_ was also determined.
2284,0,Kappa coefficient is given in (1) (Carletta 1996) (1) )(1 )()( EP EPAP Kappa   = where P(A) is the proportion of times the annotators actually agree and P(E) is the proportion of times the annotators are expected to agree due to chance 3.
2285,0,An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8 (Carletta 1996_comma_ Poessio and Vieira 1988).
2286,0,The class-based kappa statistic of (Cohen_comma_ 1960; Carletta_comma_ 1996) cannot be applied here_comma_ as the classes vary depending on the number of ambiguities per entry in the lexicon.
2287,0,The a0 coefficient is computed as follows: a0 a47 a1a32a2 a9 a1 a30 a68 a9 a1a32a30 Carletta (1996) reports that content analysis researchers generally think of a0a34a33 a49a36a35a37 as good reliability_comma_ with a49a36a35a38a40a39a37a41 a0 a41a25a49a36a35a37 allowing tentative conclusions to be drawn. All that remains is to define the chance agreement probability a1 a30 . Let a1a32a41 a1 a30 a7 and a1a32a42 a1 a30 a7 be the fraction of utterances that begin or end one or more segments in segmentation a30 respectively.
2288,0,The two annotators agreed on the annotations of 385/453 turns_comma_ achieving 84.99% agreement (Kappa = 0.68 (Carletta_comma_ 1996)).
2289,0,4 Data analysis To test the reliability of the annotation_comma_ we first considered the kappa statistic (Siegel and Castellan_comma_ 1988) which is used extensively in empirical studies of discourse (Carletta_comma_ 1996).
2290,0,Much like kappa statistics proposed by Carletta (1996)_comma_ existing employments of majority class baselines assume an equal set of identical potential mark-ups_comma_ i.e. attributes and their values_comma_ for all markables.
2291,0,Therefore_comma_ they cannot be used in a straight forward manner for many tasks that involve disjunct sets of attributes and values in terms of the type and number of attributes and their values involved in the classification task.
2292,0,5.2 Results on the Newsblaster data We measured how well the models trained on DUC data perform with current news labeled using human 4http://newsblaster.cs.columbia.edu 5a20 (kappa) is a measure of inter-annotator agreement over and above what might be expected by pure chance (See Carletta (1996) for discussion of its use in NLP).a20a22a21a24a23 if there is perfect agreement between annotators anda20a25a21a27a26 if the annotators agree only as much as you would expect by chance.
2293,0,Once an acceptable rate of interjudge agreement was verified on the first nine clusters (Kappa (Carletta_comma_ 1996) of 0.68)_comma_ the remaining 11 clusters were annotated by one judge each.
2294,0,In order to determine interannotator agreement for step 2 of the coding procedure for the database of annotated texts_comma_ we calculated kappa statistics (Carletta 1996).
2295,1,Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes_comma_ there has been a general acceptance of their use within the field.
2296,0,For example_comma_ the coding manual for the Switchboard DAMSL dialogue act annotation scheme (Jurafsky_comma_ Shriberg_comma_ and Biasca 1997_comma_ page 2) states that kappa is used to assess labelling accuracy_comma_ and Di Eugenio and Glass (2004) relate reliability to the objectivity of decisions_comma_ whereas Carletta (1996) regards reliability as the degree to which we understand the judgments that annotators are asked to make.
2297,0,Although most researchers recognize that reporting agreement statistics is an important part of evaluating coding schemes_comma_ there is frequently a lack of understanding of what the figures actually mean.
2298,0,This is an unsuitable measure for inferring reliability_comma_ and it was the use of this measure that prompted Carletta (1996) to recommend chance-corrected measures.
2299,0,The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a) is probably due to a desire for a simple system that can be easily applied to a scheme.
2300,0,5To test the reliability of the annotation scheme_comma_ we had a subset of the data annotated by two annotators and found a satisfactory -agreement (Carletta_comma_ 1996) of  = 0.81.
2301,0,We use the by now standard a0 statistic (Di Eugenio and Glass_comma_ 2004; Carletta_comma_ 1996; Marcu et al. _comma_ 1999; Webber and Byron_comma_ 2004) to quantify the degree of above-chance agreement between multiple annotators_comma_ and the a1 statistic for analysis of sources of unreliability (Krippendorff_comma_ 1980).
2302,0,Labelling was carried out by three computational linguistics graduate students with 89% agreement resulting in a Kappa statistic of 0.87_comma_ which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set (Carletta_comma_ 1996).
2303,0,We evaluated annotation reliability by using the Kappa statistic (Carletta_comma_ 1996).
2304,0,In the SUMMAC experiments_comma_ the Kappa score (Carletta_comma_ 1996; Eugenio and Glass_comma_ 2004) for interannotator agreement was reported to be 0.38 (Mani et al. _comma_ 2002).
2305,0,Computational linguistics research generally attaches great value to high kappa measures (Carletta_comma_ 1996)_comma_ which indicate high human agreement on a particular task.
2306,0,The judges had an acceptable 0.74 mean  agreement (Carletta_comma_ 1996) for the assignment of the primary class_comma_ but a meaningless 0.21 for the secondary class (they did not even agree on which lemmata were polysemous).
2307,0,The table also shows the -score_comma_ which is another commonly used measure for inter-annotator agreement [Carletta_comma_ 1996].
2308,0,We then examined the inter-annotator reliability of the annotation by calculating the  score (Carletta_comma_ 1996).
2309,0,The kappa (Carletta_comma_ 1996) obtained on this feature was 0.93.
2310,0,The kappa statistic (Krippendorff_comma_ 1980; Carletta_comma_ 1996) has become the de facto standard to assess inter-annotator agreement.
2311,0,The kappa statistic (Krippendorff_comma_ 1980; Carletta_comma_ 1996) has become the de facto standard to assess inter-annotator agreement.
2312,0,In the literature on the kappa statistic_comma_ most authors address only category data; some can handle more general data_comma_ such as data in interval scales or ratio scales (Krippendorff_comma_ 1980; Carletta_comma_ 1996).
2313,0,However_comma_ none of the techniques directly apply to our data_comma_ which are ranges of durations from a lower bound to an upper bound.
2314,0,4This was a straightforward task; two annotators annotated independently_comma_ with very high agreementkappa score of over 0.95 (Carletta_comma_ 1996).
2315,0,7Following Carletta (1996)_comma_ we measure agreement in Kappa_comma_ which follows the formula K = P(A)P(E)1P(E) where P(A) is observed_comma_ and P(E) expected agreement.
2316,0,Inter-annotator agreement is typically measured by the kappa statistic (Carletta_comma_ 1996)_comma_ dekappa frequency 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8 Figure 2: Distribution of  (inter-annotator agreement) across the 54 ICSI meetings tagged by two annotators.
2317,0,Agreement is sometimes measured as percentage of the cases on which the annotators agree_comma_ but more often expected agreement is taken into account in using the kappa statistic (Cohen_comma_ 1960; Carletta_comma_ 1996)_comma_ which is given by:  = po  pe1  p e (1) where po is the observed proportion of agreement and pe is the proportion of agreement expected by chance.
2318,0,Ever since its introduction in general (Cohen_comma_ 1960) and in computational linguistics (Carletta_comma_ 1996)_comma_ many researchers have pointed out that there are quite some problems in using  (e.g.(Di Eugenio and Glass_comma_ 2004))_comma_ one of which is the discrepancy between p0 and  for skewed class distribution.
2319,0,Another is that the degree of disagreement is not taken into account_comma_ which is relevant for any non-nominal scale.
2320,0,Following the suggestions in (Carletta_comma_ 1996)_comma_ Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement.
2321,0,Inter-annotator agreement was assessed mainly using f-score and percentage agreement as well as 11 Table 1: Annotation examples of superlative adjectives example sup span det num car mod comp set The third-largest thrift institution in Puerto Rico also [] 22 def sg no ord 37 The Agriculture Department reported that feedlots in the 13 biggest ranch states held [] 910 def pl yes no 1112 The failed takeover would have given UAL employees 75 % voting control of the nation s second-largest airline [] 1717 pos sg no ord 1418 the kappa statistics (K)_comma_ where applicable (Carletta_comma_ 1996).
2322,0,Inter-annotator agreement was measured using the kappa (K) statistics (Cohen_comma_ 1960; Carletta_comma_ 1996) on 1_comma_502 instances (three Switchboard dialogues) marked by two annotators who followed specific written guidelines.
2323,0,4Following Carletta (1996)_comma_ we measure agreement in Kappa_comma_ which follows the formula K = P(A)P(E)1P(E) where P(A) is observed_comma_ and P(E) expected agreement.
2324,0,7 For the most frequent 184 expressions_comma_ on the average_comma_ the agreement rate between two human annotators is 0.93 and the Kappa value is 0.73_comma_ which means allowing tentative conclusions to be drawn (Carletta_comma_ 1996; Ng et al. _comma_ 1999).
2325,0,5.1 Agreement between translators In an attempt to quantify the agreement between the two groups of translators_comma_ we computed the Kappa coefficient for annotation tasks_comma_ as defined by Carletta (1996).
2326,0,The agreement levels are close to the coefficient suggested by Carletta as indicative of a good agreement level for discourse annotation (0.67)_comma_ and which has been adopted as a cutoff in Computational Linguistics.
2327,0,Secondly_comma_ we used the Kappa coefficient (Carletta_comma_ 1996)_comma_ which has become the standard evaluation metric and the score obtained was 0.905.
2328,0,The Kappa statistic (Carletta_comma_ 1996) is typically used to measure the human interrater agreement.
2329,0,Its values ranges from -1 (complete disagreement) to +1 (perfect agreement) and it is defined as (A-R)/(1-R)_comma_ where A is the empirical probability of agreement on a category_comma_ and R is the probability of agreement for two annotators that 36 label documents at random (with the empirically observed frequency of each label).
2330,0,The metric we used is the kappa statistic (Carletta_comma_ 1996)_comma_ which factors out the agreement that is expected by chance: )(1 )()( EP EPAP   = where P(A) is the observed agreement among the raters_comma_ and P(E) is the expected agreement_comma_ i.e._comma_ the probability that the raters agree by chance.
2331,0,Therefore_comma_ the results are more informative than a simple agreement average (Cohen_comma_ 1960; Carletta_comma_ 1996).
2332,0,6.1 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient(K)whichiswidelyused in computational linguistics for measuring agreement in category judgments (Carletta_comma_ 1996).
2333,0,It is defined as K = P(A)?P(E)1?P(E) where P(A) is the proportion of times that the annotators agree_comma_ and P(E) is the proportion of time that they would agree by chance.
2334,0,For these classications_comma_ we calculated a kappa statistic of 0.528 (Carletta_comma_ 1996).
2335,0,Obtained percent agreement of 0.988 and  coefficient (Carletta_comma_ 1996) of 0.975 suggest high convergence of both annotations.
2336,0,Annotation was highly reliable with a kappa (Carletta_comma_ 1996) of 3https://www.cia.gov/cia/publications/ factbook/index.html 4Given that the task is not about standard Named Entity Recognition_comma_ we assume that the general semantic class of the name is already known.
2337,0,While the need for annotation by multiple raters has been well established in NLP tasks (Carletta_comma_ 1996)_comma_ most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors_comma_ or to check the systems output.
2338,0,We measured inter-annotator agreement with the Kappa statistic (Carletta_comma_ 1996) using the 1_comma_391 items that two annotators scored in common.
2339,0,He uses a specic reliability statistic_comma_ _comma_ for his measurements_comma_ but Carletta (1996) implicitly assumes kappa-like metrics are similar enough in practice for the rule of thumb to apply to them as well.A detailed discussion on the differences and similarities of these_comma_ and other_comma_ measures is provided by Krippendorff (2004); in this article we will use Cohens  (1960) to investigate the value of the 0.8 reliability cut-off for computational linguistics.
2340,0,Table 1 shows the percentage of agreement in classifying words as compounds or non-compounds (Compound Classification Agreement_comma_ CCA) for each language and the Kappa score (Carletta_comma_ 1996) obtained from it_comma_ and the percentage of words for which also the decomposition provided was identical (Decompounding Agreement_comma_ DA).
2341,0,3 Analysis Results 3.1 Kappa Statistic Kappa coefficient (Carletta_comma_ 1996) is commonly used as a standard to reflect inter-annotator agreement.
2342,0,7.1 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient(K)whichiswidelyused in computational linguistics for measuring agreement in category judgments (Carletta_comma_ 1996).
2343,0,It is defined as K = P(A) P(E)1 P(E) Evaluation type P(A) P(E) K Sentence ranking .578 .333 .367 Constituent ranking .671 .333 .506 Constituent (w/identicals) .678 .333 .517 Yes/No judgments .821 .5 .642 Yes/No (w/identicals) .825 .5 .649 Table 12: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation Evaluation type P(A) P(E) K Sentence ranking .691 .333 .537 Constituent ranking .825 .333 .737 Constituent (w/identicals) .832 .333 .748 Yes/No judgments .928 .5 .855 Yes/No (w/identicals) .930 .5 .861 Table 13: Kappa coefficient values for intraannotator agreement for the different types of manual evaluation where P(A) is the proportion of times that the annotators agree_comma_ and P(E) is the proportion of time that they would agree by chance.
2344,0,3.1 Agreement for Emotion Classes The kappa coefficient of agreement is a statistic adopted by the Computational Linguistics community as a standard measure for this purpose (Carletta_comma_ 1996).
2345,0,As agreement measure we choose the Kappa coefficient  (Fleiss_comma_ 1971; Siegel and Castellan_comma_ 1988)_comma_ the agreement measure predominantly used in natural language processing research (Carletta_comma_ 1996).
2346,0,Kappa is defined as K = P(A)P(E)1P(E) (Carletta_comma_ 1996)_comma_ where P(A) is the proportion of times that the labels agree_comma_ and P(E) is the proportion of times that they may agree by chance.
2347,0,To measure interannotator agreement_comma_ we compute Cohens Kappa (Carletta_comma_ 1996) from the two sets of annotations_comma_ obtaining a Kappa value of only 0.43.
2348,0,(Carletta 1996) is another method of comparing inter-annotator agreement 0 30 60 90 120 150 1 2 3 4 5 6 7 8 9 10 11 >11 120 25 10 32 3 4 3 1 2 0 17 2 Nu mb er of an not ators Number of dialogues completed Figure 2.
2349,0,We then used Cohens Kappa () to determine the level of agreement (Carletta_comma_ 1996).
2350,0,The resulting intercoder reliability_comma_ measured with the Kappa statistic(Carletta_comma_1996)_comma_ is considered excellent (= 0.80).
2351,0,In particular_comma_ mutual information (Church and Hanks_comma_ 1990; Wu and Su_comma_ 1993) and other statistical methods such as (Smadja_comma_ 1993) and frequency-based methods such as (Justeson and Katz_comma_ 1993) exclude infrequent phrases because they tend to introduce too much noise.
2352,0,have been used in statistical machine translation (Brown et al. _comma_ 1990)_comma_ terminology research and translation aids (Isabelle_comma_ 1992; Ogden and Gonzales_comma_ 1993; van der Eijk_comma_ 1993)_comma_ bilingual lexicography (Klavans and Tzoukermann_comma_ 1990; Smadja_comma_ 1992)_comma_ word-sense disambiguation (Brown et al. _comma_ 1991b; Gale et al. _comma_ 1992) and information retrieval in a multilingual environment (Landauer and Littman_comma_ 1990).
2353,0,Most alignment work was concerned with alignment at the sentence level.
2354,0,Some methods use sentence alignment and additional statistics to find candidate translations of terms (Smadja_comma_ 1992; van der Eijk_comma_ 1993).
2355,0,3.4 Related work and issues for future research Smadja (1992) and van der Eijk (1993) describe term translation methods that use bilingual texts that were aligned at the sentence level.
2356,0,Their methods find likely translations by computing statistics on term cooccurrence within aligned sentences and selecting source-target pairs with statistically significant associations.
2357,0,One example of the 450 latter problem is the following: in (Smadja 1993) the nature of a syntactic link between two associated words is detected a posteriori.
2358,0,The performance of the system_comma_ called XTRACT_comma_ we evaluated by letting human judges compare their choice against that of the system.
2359,0,One such evaluation experiment is_comma_ in our view_comma_ questionable_comma_ since both the human judges and XTRACT make a decision outside the context of a sentence.
2360,0,The interpretation of the results then does not take into account how much XTRACT succeeds in identifying syntactic relations as they actually occurred in the test suite.
2361,0,The following Table gives a measure of ~erformance: Esl_type N P N V P N RECALL 69.1 ~Yo' N_V 55 % 67.5 % PRECISION 81.8 % 56 % V_N 86.6 % 59 % 60.5 % To fully appreciate these results_comma_ we must consider_comma_ first_comma_ that the evaluation is on a purely syntactic ground (many collocations detected by 1 It is tmclear whether Smadja considered Otis problem in his evaluation experiment the full grammar and not detected by the SSA are in fact semantically wrong)_comma_ second_comma_ that the domain is particularly complex.
2362,0,We propose a corpus-based method (Biber_comma_1993; Nagao_comma_1993; Smadja_comma_1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase.
2363,0,Further enhancement of these utilities include compiling collocation statistics (Smadja_comma_ 1993) and semi-automatic gloassary construction (Tong_comma_ 1993).
2364,0,The user can select characters by their frequencies (i.e. -f and -g options)_comma_ the top or bottom N% (i.e. -m and -n options)_comma_ their ranks (i.e. -r and -s options) and by their frequencies above two standard deviations phlS the mean (Smadja_comma_ 1993) (i.e. -z option).
2365,0,Unlike Smadja (1993)_comma_ the ke~vord rnay be part of a Chinese word.
2366,0,For instance_comma_ there is a substantial body of papers on the extraction of 'frequently co-occurring words' from corpora using statistical methods (e.g. _comma_ (Choueka et al. _comma_ 1983)_comma_ (Church and Hanks_comma_ 1989)_comma_ (Smadja_comma_ 1993) to list only a few).
2367,0,These authors focus on techniques for providing material that can be used in other processing tasks such as x The research rcpmlcd in this paper was undmtaken as the project 'Collocations and the Lexicalisation of Semantic Operations' (ET10/75).
2368,0,The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993).
2369,0,Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation (Gale_comma_ Church_comma_ and Yarowsky 1992b_comma_ 1993; Sch6tze 1992_comma_ 1993) (see Section 7 for more details and Church and Hanks 1990; Smadja 1993_comma_ for other applications of these statistics).
2370,0,It is possible to apply these methods using statistics of the target language and thus incorporate them within the framework proposed here for target word selection.
2371,0,The problem is that with such a definition of collocations_comma_ even when improved_comma_ one identifies not only collocations but freecombining pairs frequently appearing together such as lawyer-client; doctor-hospital_comma_ as pointed out by Smadja (1993).
2372,0,One of the best efforts to quantify the performance of a term-recognition system (Smadja_comma_ 1993) does so only for one processing stage_comma_ leaving unassessed the text-to-output performance of the system.
2373,0,These measures have_comma_ in fact_comma_ been used previously in measuring term recognition (Smadja_comma_ 1993; Bourigault_comma_ 1994; Lauriston_comma_ 1994).
2374,0,No study_comma_ however_comma_ adequately discusses how these measurements are applied to term recognition.
2375,0,Similarly_comma_ Smadja (1993) uses a six content word window to extract significant collocations.
2376,0,Finally_comma_ knowledge of polarity can be combined with corpus-based collocation extraction methods (Smadja_comma_ 1993) to automatically produce entries for the lexical functions used in MeaningText Theory (Mel'~uk and Pertsov_comma_ 1987) for text generation.
2377,0,Other representative collocation research can be found in Church and Hanks (1990) and Smadja (1993).
2378,0,Though all statistically-based_comma_ their definitions of collocations are different from one another.
2379,1,Unlike Church and Hanks (1990)_comma_ Smadja (1993) goes beyond the 'two-word' limitation and deals with 'collocations of arbitrary length'.
2380,0,Iegar(ling l;his l;ypu of (:olloeation_comma_ the approaches till ilOW could be divi(led inl;o t;wo groups: those thai; do uo(_comma_ refer to s'ttbstrings of colloco_comma_ l_comma_ ions as a l)arti(:ular problem_comma_ (Church and lla.nks_comma_ t99(); Kim and Cho_comma_ 1993; Nagao and Mori_comma_ 1994)_comma_ and those t.hat; do (Kita et al. _comma_ t994; Smadja_comma_ 1993; lkchara et al. _comma_ 1995; Kjelhner_comma_ 11994).
2381,0,\[towew;r_comma_ (well the lal;t_comma_ er_comma_ deal wiLh only 1)arl; of the probh;m: they l_comma_ry not to extract the mlwanl;cd substrings of collocations.
2382,0,The COlllillOil poini;s regarding collocations appear to be_comma_ as (Smadja_comma_ 1993) suggestsl: they are m'bil;rary (it is nol; clear why to 'Bill through' means to 'fail')_comma_ th('y are domain-dependent ('interest rate'_comma_ 'stock market')_comma_ t;hey are recurrenl; and cohesive lo~xical clusters: the presence of one of the.
2383,0,(Smadja_comma_ 1993; Kits et al. _comma_ 1994; Ikehara et al. _comma_ 1995)_comma_ mention about substrings of collocations.
2384,0,Smadja's Xtract produces only the biggest possible n-grams.
2385,0,From the extracted n-grams_comma_ those with a flequc'ncy of 3 or more were kept (other approaches get rid of n-grams of such low frequencies (Smadja_comma_ 1993)).
2386,0,(Smadja_comma_ 1993)_comma_ extracts uninterrupted as well as interrupted collocations (predicative relations_comma_ rigid noun phrases and phrasal templates).
2387,1,The system performs very well under two conditions: the corpus must be large_comma_ and the collocations we are interested in extracting_comma_ must have high frequencies.
2388,0,Some papers (Fung & Wu_comma_ 1994; Wang et al. _comma_ 1994) based on Smadja's paradigm (1993) learned an aided dictionary from a corpus to reduce the possibility of unknown words.
2389,0,In the past five years_comma_ important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks_comma_ 1990; Hindle_comma_ 1990; Smadja_comma_ 1993; Grei~nstette_comma_ 1994; Grishman and Sterling_comma_ 1994).
2390,0,Most of these approaches_comma_ however_comma_ need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation.
2391,0,They first extract English collocations using the Xtract systetn (Smadja_comma_ 1993)_comma_ and theu look for French coutlterparts.
2392,0,Their search strategy is an itemtive combina.tion of two elements.
2393,1,This is ha_comma_sed on the intuitive ide~ tim| 'if a set of words ('onstitutes a collocation_comma_ its subset will Mso be correla.ted'.
2394,0,Although this idea is corre~(:t_comma_ the itera|ire combination strategy generates a. mlmber ol + useless expressions.
2395,0,In fa.ct_comma_ Xtract.
2396,0,employs a. rol)ust l'_comma_nglish pa.rser to lilter out the wrong colloca.tions which form more thaal ha.If lhe candidates.
2397,0,Another drawback of their approa_comma_ ch is that only the longesl_comma_ n-gram is adopl.ed.
2398,0,There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words (Church and Hanks 1990); the distance between words (Smadja and Makeown 1990); and the number of combined words and frequency of appearance (Kita 1993_comma_ 1994).
2399,0,Thus_comma_ conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted (Smadja 1993_comma_ Shinnou and Isahara 1995).
2400,0,Smadja (1993)finds significant bigrams using an estimate of z-score (deviation from an expected mean).
2401,0,Smadja's method seems to require very large corpora_comma_ since the method needs to estimate a reliable measure of the variance of the frequencies with which words co-occur.
2402,0,This makes the method dependent on the corpus size.
2403,0,Smadja reports the use of a corpus of size 10 million words.
2404,0,'More precisely_comma_ the statistical methods we use do not seem to be effective on low frequency words (fewer than 100 occurrences)'.
2405,0,(Smadja_comma_ 1993:p.168) Kita & al.
2406,0,While several methods have been proposed to automatically extract compounds (Smadja 1993_comma_ Suet al. 1994)_comma_ we know of no successful attempt to automatically make classes of compounds.
2407,0,Therefore_comma_ sublanguage techniques such as Sager (1981) and Smadja (1993) do not work.
2408,0,Manual processes_comma_ such as lexicon development could be automated in the future using standard contextbased_comma_ word distribution methods (Smadja_comma_ 1993)_comma_ or other corpus-based techniques.
2409,1,Tools like Xtract (Smadja 1993) were based on the work of Church and others_comma_ but made a step forward by incorporating various statistical measurements like z-score and variance of distribution_comma_ as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output.
2410,0,Turning to the literature on bilingual terminology identification per se_comma_ although monolingual terminology extraction is a problem that has been previously explored_comma_ often with respect to identifying relevant multi-word terms (e.g.(Daille_comma_ 1996; Smadja_comma_ 1993))_comma_ less prior work exists for bilingual acquisition of domain-specific translations.
2411,0,For instance_comma_ one might be interested in frequencies of co-occurences of a word with other words and phrases (collocations) (Smadja_comma_ 1993)_comma_ or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus (Finch&Chater_comma_ 1993).
2412,0,There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora (Nagao and Mori_comma_ 1994)_comma_ (Ikehara et al. _comma_ 1996) (Kupiec_comma_ 1993)_comma_ (Fung_comma_ 1995)_comma_ (Kitamura and Matsumoto_comma_ 1996)_comma_ (Smadja_comma_ 1993)_comma_ (Smadja et al. _comma_ 1996)_comma_ (Haruno et al. _comma_ 1996).
2413,0,Although these approaches achieved good results for the task considered_comma_ most of them aim to extract fixed collocations_comma_ mainly noun phrases_comma_ and require the information which is dependent on each language such as dictionaries and parts of speech.
2414,0,4 Related work Algorithms for retrieving collocations has been described (Smadja_comma_ 1993) (Haruno et al. _comma_ 1996).
2415,0,(Smadja_comma_ 1993) proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3.
2416,0,In their approach_comma_ the bigrams are valid only when there are fewer than five words between them.
2417,0,This is based on the assumption that 'most of the lexical relations involving a word w can be retrieved by examining the neighborhood of w wherever it occurs_comma_ within a span of five (-5 and +5 around w) words'.
2418,0,While the assumption is reasonable for some languages such as English_comma_ it cannot be applied to all the languages_comma_ especially to the languages without word delimiters.
2419,0,MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) We can use this definition to derive an estimate of the connectedness between words_comma_ in terms of collocations (Smadja_comma_ 1993)_comma_ but also in terms of phrases and grammatical relations (Hindle_comma_ 1990).
2420,0,While bound compositions are not predictable_comma_ i.e._comma_ their reasonableness cannot be derived from the syntactic and semantic properties of the words in them(Smadja 1993).
2421,0,Now with the availability of large-scale corpus_comma_ automatic acquisition of word compositions_comma_ especially word collocations from them have been extensively studied(e.g. _comma_ Choueka et al. 1988; Church and Hanks 1989; Smadja 1993).
2422,0,The key of their methods is to make use of some statistical means_comma_ e.g._comma_ frequencies or mutual information_comma_ to quantify the compositional strength between words.
2423,0,These methods are more appropriate for retrieving bound compositions_comma_ while less appropriate for retrieving free ones.
2424,0,However morphosyntactic features alone cannot verily the terminological status of the units extracted since they can also select non terms (see Smadja 1993).
2425,0,We then replaced fi with its associated z-score kl_comma_e. kf_comma_e is the strength of code frequency f at Le_comma_ and represents the standard deviation above the average of frequency f_comma_ ve_comma_t. Referring to Smadja's definition (Smadja_comma_ 1993)_comma_ the standard deviation at at Lt and strength kLt of the code frequencies are defined as shown in formulas 1 and 2.
2426,0,In (Smadja_comma_ 1993)_comma_ automatically extracted collocations are judged by a lexicographer.
2427,0,It seems nevertheless that all 2Church and Ilanks (1989)_comma_ Smadja (1993) use statistics in their algorithms to extract collocations from texts.
2428,0,Other classes_comma_ such as the ones below (:ate be extracted using lexico-statistical tools_comma_ such as in (Smadja_comma_ 1993)_comma_ and then checked by a human.
2429,1,For the extraction problem_comma_ there have been various methods proposed to date_comma_ which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro_comma_ Matsumoto_comma_ and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997).
2430,0,In our experiments_comma_ we extracted verbs and their case frame slots (verb_comma_ slot_name_comma_ slot_value triples) from the tagged texts of the Wall Street Journal corpus (ACL/DCI CD-ROM1) consisting of 126_comma_084 sentences_comma_ using existing techniques (specifically_comma_ those in Smadja \[1993\])_comma_ then 9 There are several possible measures that one could take to address this issue_comma_ including the incorporation of absolute frequencies of the words (inside and outside the particular slot in question).
2431,0,As we remarked earlier_comma_ however_comma_ the input data required by our method (triples) could be generated automatically from unparsed corpora making use of existing heuristic rules (Brent 1993; Smadja 1993)_comma_ although for the experiments we report here we used a parsed corpus.
2432,0,For the correct identification of phrases in a Korean query_comma_ it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in Smadja (1993).
2433,0,It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus_comma_ e.g._comma_ (Choueka_comma_ 1988) and (Smadja_comma_ 1993).
2434,0,They do not_comma_ however_comma_ make a distinction between compositional and noncompositional collocations.
2435,0,Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja_comma_ 1993; Fung and Wu_comma_ 1994)_comma_ phrasal translation (Smadja et al. _comma_ 1996; Kupiec_comma_ 1993; Wu_comma_ 1995; Dagan and Church_comma_ 1994)_comma_ target word selection (Liu and Li_comma_ 1997; Tanaka and Iwasaki_comma_ 1996)_comma_ domain word translation (Fung and Lo_comma_ 1998; Fung_comma_ 1998)_comma_ sense disambiguation (Brown et al. _comma_ 1991; Dagan et al. _comma_ 1991; Dagan and Itai_comma_ 1994; Gale et al. _comma_ 1992a; Gale et al. _comma_ 1992b; Gale et al. _comma_ 1992c; Shiitze_comma_ 1992; Gale et al. _comma_ 1993; Yarowsky_comma_ 1995)_comma_ and even recently for query translation in cross-language IR as well (Ballesteros and Croft_comma_ 1998).
2436,0,Co-occurrence statistics is collected from either bilingual parallel and 334 non-parallel corpora (Smadja et al. _comma_ 1996; Kupiec_comma_ 1993; Wu_comma_ 1995; Tanaka and Iwasaki_comma_ 1996; Fung and Lo_comma_ 1998)_comma_ or monolingual corpora (Smadja_comma_ 1993; Fung and Wu_comma_ 1994; Liu and Li_comma_ 1997; Shiitze_comma_ 1992; Yarowsky_comma_ 1995).
2437,0,Based on this assumption_comma_ (Smadja_comma_ 1993) stored all bigrams of words along with their relative position_comma_ p (-5 < p _~ 5).
2438,0,He evaluated the lexical strength of a word pair using 'Z-score' and the variance of its t)osil;ion distribution using '.spread'.
2439,0,He defined ~_comma_.
2440,0,(:()\]location as an arbitary_comma_ domain dependent_comma_ recurrent_comma_ and cohesive lexical cluster.
2441,0,~F ~c ~R ~cR (2) ~\]~) continue explanations_comma_ we begin by mentioning the 'Xtrgct' tool by Smadja (Smadja_comma_ 1993).
2442,0,Our input form was designed in a similar manner with 'Xtract'.
2443,0,Smadja assumed that the components of a collocation should appear together in a relatively rigid way because of syntactic constraint.
2444,0,'Xtract' extracted the pairs whose variances are over a threshold and pulled out the interesting positions of them by standar(lizz~tion of the frequency distributions.
2445,0,Unfbrtunately_comma_ the approach for English has several limitations to work 4 on Korean structure ibr the following 'reasons: 1.
2446,0,This indicates that very few pairs were produced when 'Xtract' is applied to Korean.
2447,0,4~We ported Smadja's Xtract tool into a Korean version.
2448,0,The precision value of Korean version of Xtract was estimated to be 40.4%.
2449,0,((~))(not) (@}_comma_q))(drink)_comma_.~_(and) Noun Noun . Verb Ending Verb . . Table 1: n-grams of =l-zl(drink) by Xtract (freq: freq of sentences) dist eval -2 O -1 O -3 X -3 O -I O -3 X -4 X -2 O -2 X -1 O Thus_comma_ there are a lot of pairs with low frequency which interrupt to get reliable statistic.
2450,0,We therefore did not use_comma_ for example_comma_ tf*idf_comma_ the purely statistical technique that is the used by most information retrieval systems_comma_ or \[Smadja 1993\]_comma_ a hybrid statistical and symbolic technique for identifying collocations.
2451,0,Smadja_comma_ Frank (1993) 'Retrieving collocations from text'_comma_ Computational Linguistics 19(1):143-177.
2452,0,For colnparison~ we refer here to Smadja's method (1993) because this method and the proposed method have much in connnon.
2453,0,In both cases_comma_ technicaJ terms are retrieved from a.n untagged corpus with n-gram statistics and given syntactic categories for NI_comma_P applica.tions.
2454,0,Their detection is based on the following algorithm (cf.Smadja_comma_ 1993): 1.
2455,0,In Smadja's collocation algorithm Xtract_comma_ the lowest-frequency words are effectively discarded as well (Smadja 1993).
2456,0,The recurrence property had been utilized to extract keywords or key-phrases from text (Chien 1999_comma_ Fung 1998_comma_ Smadja 1993).
2457,0,Extract high frequency keywords in the text (Smadja 93_comma_ Chang & Su 97_comma_ Chien 99).
2458,0,Since we need knowledge-poor Daille_comma_ 1996) induction_comma_ we cannot use human-suggested filtering Chi-squared (G24 ) 2 (Church and Gale_comma_ 1991) Z-Score (Smadja_comma_ 1993; Fontenelle_comma_ et al. _comma_ 1994) Students t-Score (Church and Hanks_comma_ 1990) n-gram list in accordance to each probabilistic algorithm.
2459,0,Algorithms for the computation of first-order associations have been used in lexicography for the extraction of collocations (Smadja_comma_ 1993) and in cognitive psychology for the simulation of associative learning (Wettler & Rapp_comma_ 1993).
2460,0,Sometimes_comma_ the notion of collocation is defined in terms of syntax (by possible part-of-speech patterns) or in terms of semantics (requiring collocations to exhibit non-compositional meaning) (Smadja_comma_ 1993).
2461,0,The second method considers the means and variance of the distance between two words_comma_ and can compute flexible collocations (Smadja_comma_ 1993).
2462,0,To perform code generalization_comma_ Li adopted to Smadjas work (Smadja_comma_ 1993) and defined the code strength using a code frequency and a standard deviation in each level of the concept hierarchy.
2463,0,One aspect of VPCs that makes them dicult to extract (cited in_comma_ e.g._comma_ Smadja (1993)) is that the verb and particle can be non-contiguous_comma_ e.g. hand the paper in and battle right on.
2464,0,4 Method-2: Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles_comma_ we next look to full chunk 2Note_comma_ this is the same as the maximum span length of 5 used by Smadja (1993)_comma_ and above the maximum attested NP length of 3 from our corpus study (see Section 2.2).
2465,0,One of the earliest attempts at extracting \interrupted collocations' (i.e. non-contiguous collocations_comma_ including VPCs)_comma_ was that of Smadja (1993).
2466,0,Smadja based his method on bigrams_comma_ but unlike conventional collocation work_comma_ described bigrams by way of the triple of hword1_comma_word2_comma_posni_comma_ where posn is the number of words occurring between word1 and word2 (up to 4).
2467,0,Smadja did not attempt to evaluate his method other than anecdotally_comma_ making any comparison with our research impossible.
2468,0,Kaalep and Muischnek apply the \mutual expectation' test over a range of \positioned bigrams'_comma_ similar to those used by Smadja.
2469,0,There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches_comma_ such as mutual information (Church 1990_comma_ Sporat 1990)_comma_ t-score (Church 1991)_comma_ dice matrix (Smadja 1993_comma_ 1996).
2470,0,The approach is in the spirit of Smadja (1993) on retrieving collocations from text corpora_comma_ but is more integrated with parsing.
2471,0,3 Related work Word collocation Various collocation metrics have been proposed_comma_ including mean and variance (Smadja_comma_ 1994)_comma_ the t-test (Church et al. _comma_ 1991)_comma_ the chi-square test_comma_ pointwise mutual information (MI) (Church and Hanks_comma_ 1990)_comma_ and binomial loglikelihood ratio test (BLRT) (Dunning_comma_ 1993).
2472,0,For that purpose_comma_ syntactical (Didier Bourigault_comma_ 1993)_comma_ statistical (Frank Smadja_comma_ 1993; Ted Dunning_comma_ 1993; Gal Dias_comma_ 2002) and hybrid syntaxicostatistical methodologies (Batrice Daille_comma_ 1996; JeanPhilippe Goldman et al. 2001) have been proposed.
2473,0,On the other hand_comma_ purely statistical systems (Frank Smadja_comma_ 1993; Ted Dunning_comma_ 1993; Gal Dias_comma_ 2002) extract discriminating MWUs from text corpora by means of association measure regularities.
2474,1,As they use plain text corpora and only require the information appearing in texts_comma_ such systems are highly flexible and extract relevant units independently from the domain and the language of the input text.
2475,0,However_comma_ these methodologies can only identify textual associations in the context of their usage.
2476,0,As a consequence_comma_ many relevant structures can not be introduced directly into lexical databases as they do not guarantee adequate linguistic structures for that purpose.
2477,0,alpha 0 0.1 0.2 0.3 0.4 0.5 Freq=2 13555 13093 12235 11061 10803 10458 Freq=3 4203 3953 3616 3118 2753 2384 Freq=4 1952 1839 1649 1350 1166 960 Freq=5 1091 1019 917 743 608 511 Freq>2 2869 2699 2488 2070 1666 1307 TOTAL 23670 22603 20905 18342 16996 15620 alpha 0.6 0.7 0.8 0.9 1.0 Freq=2 10011 9631 9596 9554 9031 Freq=3 2088 1858 1730 1685 1678 Freq=4 766 617 524 485 468 Freq=5 392 276 232 202 189 Freq>2 1000 796 627 517 439 TOTAL 14257 13178 12709 12443 11805 Table 7: Number of extracted MWUs by frequency 6.2 Qualitative Analysis As many authors assess (Frank Smadja_comma_ 1993; John Justeson and Slava Katz_comma_ 1995)_comma_ deciding whether a sequence of words is a multiword unit or not is a tricky problem.
2478,0,Related Works Generally speaking_comma_ approaches to MWE extraction proposed so far can be divided into three categories: a) statistical approaches based on frequency and co-occurrence affinity_comma_ b) knowledgebased or symbolic approaches using parsers_comma_ lexicons and language filters_comma_ and c) hybrid approaches combining different methods (Smadja 1993; Dagan and Church 1994; Daille 1995; McEnery et al. 1997; Wu 1997; Wermter et al. 1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al. 2001a_comma_ 2001b; Biber et al. 2003).
2479,0,In his Xtract system_comma_ Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance_comma_ strength and spread_comma_ and then examined concordances of the bi-grams to find longer frequent multiword units.
2480,0,One of the main problems facing statistical approaches_comma_ however_comma_ is that they are unable to deal with low-frequency MWEs.
2481,0,This means that a major part of true multiword expressions are left out by statistical approaches.
2482,0,For example_comma_ Smadja (1993) suggests a basic characteristic of collocations and multiword units is recurrent_comma_ domain-dependent and cohesive lexical clusters.
2483,1,Smadja (1993)_comma_ which is the classic work on collocation extraction_comma_ uses a two-stage filtering model in which_comma_ in the first step_comma_ n-gram statistics determine possible collocations and_comma_ in the second step_comma_ these candidates are submitted to a syntactic valida7Of course_comma_ lexical material is always at least partially dependent on the domain in question.
2484,0,Previous work in collocation acquisition varies in the kinds of collocations they detect.
2485,0,These range from twoword to multi-word_comma_ with or without syntactic structure (Smadja 1993; Lin_comma_ 1998; Pearce_comma_ 2001; Seretan et al. 2003).
2486,0,Some studies have been done for acquiring collocation translations using parallel corpora (Smadja et al_comma_ 1996; Kupiec_comma_ 1993; Echizen-ya et al. _comma_ 2003).
2487,0,The former extracts collocations within a fixed window (Church and Hanks 1990; Smadja_comma_ 1993).
2488,0,Smadja (1993) also detailed techniques for collocation extraction and developed a program called XTRACT_comma_ which is capable of computing flexible collocations based on elaborated statistical calculation.
2489,0,The group of collocations and compounds should be delimited using statistical approaches_comma_ such as Xtract (Smadja_comma_ 1993) or LocalMax (Silva et al. _comma_ 1999)_comma_ so that only the most relevantthose of higher frequency are included in the database.
2490,1,Many efficient techniques exist to extract multiword expressions_comma_ collocations_comma_ lexical units and idioms (Church and Hanks_comma_ 1989; Smadja_comma_ 1993; Dias et al. _comma_ 2000; Dias_comma_ 2003).
2491,0,Unfortunately_comma_ very few have been applied to information retrieval with a deep evaluation of the results.
2492,0,Comparing to a collocation extraction system based on Xtract_comma_ we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords_comma_ almost 50% improvement from precision rate of 30% in the Xtract system.
2493,0,Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990_comma_ Smadja 1993_comma_ Choueka 1993_comma_ Lin 1998).
2494,0,The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993_comma_ Lin 1997 and Lu et al. 2003).
2495,0,Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength.
2496,1,This method successfully extracted both adjacent and distant bi-grams and n-grams.
2497,0,However_comma_ the method failed to extract bi-grams with lower frequency.
2498,0,Our research group has further applied the Xtract system to Chinese (Lu et al. 2003) by adjusting the parameters to optimize the algorithm for Chinese and a new weighted algorithm was developed based on mutual information to acquire word bigrams with one higher frequency word and one lower frequency word.
2499,0,The result has achieved an estimated 5% improvement in recall rate and a 15% improvement in precision comparing to the Xtract system.
2500,0,All of the above techniques do not take advantage of the wide range of lexical resources available including synonym information.
2501,0,The extraction of bi-gram collocation is based on the English Xtract(Smaja 1993) with improvements.
2502,0,3.3.1 Bi-gram Extraction Based on the lexical statistical model proposed by Smadja in Xtract on extracting English collocations_comma_ an improved algorithm was developed for Chinese collocation by our research group and the system is called CXtract.
2503,0,According to Xtract_comma_ word cooccurence is denoted by a tripplet (w h_comma_ w i_comma_ d) where w h is a given headword_comma_ w i is a co-word appeared in the corpus in a distance d within the window of [-5_comma_ 5].
2504,0,The average frequency of f i_comma_ denoted by i f_comma_ is given by 10/ 5 5_comma_ = = j jii ff (5) Then_comma_ the average frequency f_comma_ and the standard deviation  are defined by  = = n i i f n f 1 1 ; 2 1 )( 1  = = n i i ff n  (6) The Strength of the co-occurrence for the pair (w h_comma_ w i_comma_)_comma_ denoted by k i_comma_ is defined by  ff k i i  =  (7) Furthermore_comma_ the Spread of (w h_comma_ w i_comma_)_comma__comma_ denoted as U i_comma_ which characterizes the distribution of w i around w h is define as: 10 )( 2_comma_  = iji i ff U ; (8) To eliminate the bi-grams with unlikely cooccurrence_comma_ the following sets of threshold values is defined: 0 :1 K ff kC i i   =  (9) 0 :2 UUC i  (10) )(:3 1_comma_ iiji UKffC + (11) However_comma_ the above statistical model given by Smadja fails to extract the bi-grams with a much higher frequency of w h but a relatively low frequency word of w i_comma__comma_ For example_comma_ in the bigram_comma_ freq ( ) is much lower than the freq ( ).
2505,0,According to Smadja_comma_ U 0 defined in the formula (8) represents the co-occurrence distribution of the candidate collocation (w h_comma_ w c ) in the position of d (-5  d  5).
2506,0,There are several basic methods for evaluating associations between words: based on frequency counts (Choueka_comma_ 1988; Wettler and Rapp_comma_ 1993)_comma_ information theoretic (Church and Hanks_comma_ 1990) and statistical significance (Smadja_comma_ 1993).
2507,0,It is true that various term extraction systems have been developed_comma_ such as Xtract (Smadja 1993)_comma_ Termight (Dagan & Church 1994)_comma_ and TERMS (Justeson & Katz 1995) among others (cf.Daille 1996_comma_ Jacquemin & Tzoukermann 1994_comma_ Jacquemin_comma_ Klavans_comma_ & Toukermann 1997_comma_ Boguraev & Kennedy 1999_comma_ Lin 2001).
2508,0,Such systems typically rely on a combination of linguistic knowledge and statistical association measures.
2509,0,The linguistic filters used in typical term extraction systems have no obvious connection with the criteria that linguists would argue define a phrasal term (noncompositionality_comma_ fixed order_comma_ nonsubstitutability_comma_ etc.).
2510,0,They function_comma_ instead_comma_ to reduce the number of a priori improbable terms and thus improve precision.
2511,0,The association measure does the actual work of distinguishing between terms and plausible nonterms.
2512,0,3 Schone & Jurafsky's results indicate similar results for log-likelihood & T-score_comma_ and strong parallelism among information-theoretic measures such as ChiSquared_comma_ Selectional Association (Resnik 1996)_comma_ Symmetric Conditional Probability (Ferreira and Pereira Lopes_comma_ 1999) and the Z-Score (Smadja 1993).
2513,0,In the work of Smadja (1993) on extracting collocations_comma_ preference was given to constructions whose constituents appear in a fixed order_comma_ a similar (and more generally implemented) version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones.
2514,0,Baron and Hirst (2004) extracted collocations with Xtract (Smadja_comma_ 1993) and classified the collocations using the orientations of the words in the neighboring sentences.
2515,0,Most previous work on compositionality of MWEs either treat them as collocations (Smadja_comma_ 1993)_comma_ or examine the distributional similarity between the expression and its constituents (McCarthy et al. _comma_ 2003; Baldwin et al. _comma_ 2003; Bannard et al. _comma_ 2003).
2516,0,Syntactic analysis has long since been recognized as a prerequisite for collocation extraction (for instance_comma_ by Smadja2)_comma_ but the traditional systems simply ignored it because of the lack_comma_ at that time_comma_ of efficient and robust parsers required for processing large corpora.
2517,0,This fact is being seriously challenged by current research ()_comma_ and might not be true in the near future (Smadja_comma_ 1993_comma_ 151).
2518,0,Parsing has been also used after extraction (Smadja_comma_ 1993) for filtering out invalid results.
2519,0,We believe that this is not enough and that parsing is required prior to the application of statistical tests_comma_ for computing a realistic frequency profile for the pairs tested.
2520,0,We can mentionhere only part of this work: (Berry-Rogghe_comma_ 1973; Church et al. _comma_ 1989; Smadja_comma_1993;Lin_comma_1998;KrennandEvert_comma_2001) for monolingualextraction_comma_ and (Kupiec_comma_ 1993; Wu_comma_1994;Smadjaetal._comma_1996;KitamuraandMat40 sumoto_comma_1996; Melamed_comma_1997)for bilingualextractionviaalignment.
2521,1,Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults (Breidt_comma_ 1993; Smadja_comma_ 1993; Zajac et al. _comma_ 2003).
2522,0,Morphologicaltoolssuch as lemmatizers andPOStaggersarebeingcommonlyusedin extractionsystems;they areemployedbothfordealingwithtext variationandfor validatingthe candidatepairs: combinationsof functionwordsare typicallyruledout (Justesonand Katz_comma_ 1995)_comma_as are the ungrammaticalcombinationsin the systemsthatmake useofparsers(ChurchandHanks_comma_ 1990;Smadja_comma_1993;Basilietal._comma_1994;Lin_comma_1998; Goldmanetal._comma_2001;Seretanetal._comma_2004).
2523,0,Given the motivations for performing a linguistically-informedextraction whichwere also put forth_comma_ among others_comma_ by Church and Hanks(1990_comma_25)_comma_ Smadja(1993_comma_151) and Heid (1994)  and given the recent developmentof linguisticanalysistools_comma_itseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems.
2524,0,3 OverviewofExtractionWork 3.1 English As one mightexpect_comma_the bulk of the collocation extractionwork concernsthe English language: (Choueka_comma_1988;Churchet al. _comma_1989;Churchand Hanks_comma_1990; Smadja_comma_1993; Justesonand Katz_comma_ 1995;Kjellmer_comma_ 1994;Sinclair_comma_ 1995;Lin_comma_1998)_comma_ amongmany others1.
2525,0,Smadja(1993)employsthez-scoreinconjunction with several heuristics(e.g. _comma_ the systematic occurrenceof two lexical items at the same distanceintext)andextractspredicativecollocations_comma_ 1E.g._comma_(Frantziet al. _comma_2000;Pearce_comma_2001;Goldmanet al. _comma_ 2001;ZaiuInkpenandHirst_comma_2002;Dias_comma_2003;Seretanetal._comma_ 2004;Pecina_comma_2005)_comma_andthelistcanbecontinued.
2526,0,It was followedby (Basiliet al. _comma_ 1994)_comma_ thatmadeuseofparsinginformation;  Korean:(Shimohataetal._comma_1997)usedanadjacencyn-grammodel_comma_and(Kimetal._comma_1999) reliedonPOS-tagging;  Chinese:(Huanget al. _comma_ 2005)usedPOSinformation_comma_while(Luetal._comma_2004)appliedextractiontechniquessimilarto Xtractsystem (Smadja_comma_1993);  Japanese:(Ikeharaetal._comma_1995)wasbasedon animprovedn-grammethod.
2527,0,2 Related Work The issue of MWE processing has attracted much attention from the Natural Language Processing (NLP) community_comma_ including Smadja_comma_ 1993; Dagan and Church_comma_ 1994; Daille_comma_ 1995; 1995; McEnery et al. _comma_ 1997; Wu_comma_ 1997; Michiels and Dufour_comma_ 1998; Maynard and Ananiadou_comma_ 2000; Merkel and Andersson_comma_ 2000; Piao and McEnery_comma_ 2001; Sag et al. _comma_ 2001; Tanaka and Baldwin_comma_ 2003; Dias_comma_ 2003; Baldwin et al. _comma_ 2003; Nivre and Nilsson_comma_ 2004 Pereira et al_comma_.
2528,0,2004; Piao et al. _comma_ 2005.
2529,0,We argue that linguistic knowledge could not only improve results (Krenn_comma_ 2000b; Smadja_comma_ 1993) but is essential when extracting collocations from certain languages: this knowledge provides other applications (or a lexicon user_comma_ respectively) with a ne-grained description of how the extracted collocations are to be used in context.
2530,0,Extracting word combinations using relational patterns (represented by part of speech (PoS) tags or dependency rules) offers a higher level of abstraction and improves the results (cf.(Krenn_comma_ 2000b; Smadja_comma_ 1993)).
2531,0,Baron and Hirst (2004) extracted collocations with Xtract (Smadja_comma_ 1993) and classified the collocations using the orientations of the words in the neighboring sentences.
2532,0,There are some existing corpus linguistic researches on automatic extraction of collocations from electronic text (Smadja 1993; Lin 1998; Xu and Lu 2006).
2533,0,These techniques are mainly based on statistical techniques and syntactic analysis.
2534,0,However_comma_ the performances of automatic collocation extraction systems are not satisfactory (Pecina 2005).
2535,0,Lastly_comma_ collocations are domain-dependent (Smadja 1993) and language-dependent.
2536,0,Future work will include: (i) applying the method to retrieve other types of collocations (Smadja_comma_ 1993)_comma_ and (ii) evaluating the method using Internet directories.
2537,0,Since Odds = P/(1  P)_comma_ we multiply both sides of Definition 3 by (1P(U|E))1 to obtain_comma_ P(U|E) 1P(U|E) = P(E|U)P(U) P(E)(1P(U|E)) (7) By substituting Equation 6 in Equation 7 and later_comma_ applying the multiplication rule P(U|E)P(E) = P(E|U)P(U) to it_comma_ we will obtain: P(U|E) P(U|E) = P(E|U)P(U) P(E|U)P(U) (8) We proceed to take the log of the odds in Equation 8 (i.e. logit) to get: log P(E|U)P(E|U) = log P(U|E)P(U|E) log P(U)P(U) (9) While it is obvious that certain words tend to cooccur more frequently than others (i.e. idioms and collocations)_comma_ such phenomena are largely arbitrary (Smadja_comma_ 1993).
2538,0,Since ROUGE computed similarity on surface word level_comma_ stemmed version allowed ROUGE to perform more lenient matches.
2539,0,The skip-bigram-based ROUGE-S* (without skip distance restriction) had the best Pearson's  correlation of 0.95 in adequacy when all words were lower case and stemmed.
2540,0,ROUGE-L_comma_ ROUGE-W_comma_ ROUGE-S*_comma_ ROUGE-S4_comma_ and ROUGE-S9 were equal performers to BLEU in measuring fluency.
2541,1,The evaluation results of ROUGE-L_comma_ ROUGEW_comma_ and ROUGE-S in machine translation evaluation are very encouraging.
2542,1,ROUGE-L_comma_ ROUGE-W_comma_ and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
2543,0,In Lin and Och (2004)_comma_ we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
2544,1,According to the results reported in that paper_comma_ ROUGE-L_comma_ ROUGE-W_comma_ and ROUGE-S also outperformed BLEU and NIST.
2545,0,The results of the comparison with ROUGE-N (Lin and Hovy_comma_ 2003; Lin_comma_ 2004a; Lin_comma_ 2004b)_comma_ ROUGE-S(U) (Lin_comma_ 2004b; Lin and Och_comma_ 2004) and ROUGE-L (Lin_comma_ 2004a; Lin_comma_ 2004b) show that our method correlates more closely with human evaluations and is more robust.
2546,0,One is the longest common subsequence (LCS) based approach (Hori et al. _comma_ 2003; Lin_comma_ 2004a; Lin_comma_ 2004b; Lin and Och_comma_ 2004).
2547,0,a0 subsequence S1 S2 a0 subsequence S1 S2 a0 subsequence S1 S2 Becoming 1 1 Becoming-is a1 a2 a1 a2 astronaut-DREAM 0 a1 a2 DREAM 1 1 Becoming-my a1a4a3a5a1a4a3 astronaut-ambition 0 a1 a2 SPACEMAN 1 1 SPACEMAN-DREAM a1a4a3a5a1 a2 astronaut-is 0 1 a 1 0 SPACEMAN-ambition 0 a1 a2 astronaut-my 0 a1 ambition 0 1 SPACEMAN-dream a1 a3 0 cosmonaut-DREAM a1 a3 0 1 an 0 1 SPACEMAN-great a1 a2 0 cosmonaut-dream a1 a3 0 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great a1 a2 0 cosmonaut 1 0 SPACEMAN-my a1a6a1 cosmonaut-is 1 0 dream 1 0 a-DREAM a1 a7 0 cosmonaut-my a1 0great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0 is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0 my 1 1 a-dream a1 a7 0 is-DREAM a1 a2 a1 Becoming-DREAM a1a4a8a5a1 a7 a-great a1 a3 0 is-ambition 0 a1 Becoming-SPACEMAN a1a6a1 a-is a1 0 is-dream a1 a2 0 Becoming-a 1 0 a-my a1 a2 0 is-great a1 0 Becoming-ambition 0 a1 a7 an-DREAM 0 a1 a3 is-my 1 1 2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM a1 1 Becoming-astronaut 0 a1 an-ambition 0 a1 a3 my-ambition 0 1 Becoming-cosmonaut a1 0 an-astronaut 0 1 my-dream a1 0 Becoming-dream a1a4a8 0 an-is 0 a1 my-great 1 0 Becoming-great a1 a7 0 an-my 0 a1 a2 2002; Lin and Hovy_comma_ 2003; Lin_comma_ 2004a; Lin_comma_ 2004b; Soricut and Brill_comma_ 2004).
2548,0,Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L.
2549,0,They applied ROUGE-L to the evaluation of summarization and machine translation.
2550,0,In particular_comma_ ROUGE-1_comma_ i.e._comma_ unigram matching_comma_ provides the best correlation with human evaluation.
2551,0,Therefore_comma_ Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation.
2552,0,148 4.3 Compared Automatic Evaluation Methods We compared our method with ROUGE-N and ROUGE-L described below.
2553,0,We used only content words to calculate the ROUGE scores because the correlation coefficient decreased if we did not remove functional words.
2554,0,ROUGE-N ROUGE-N is an N-gram-based evaluation measure defined as follows (Lin_comma_ 2004b): ROUGE-Na59a61a146a31a62a90a147a49a65a68a67 a215 a77a83a216 a209a68a217a61a173 a172a27a218 a77 a215 a219a27a220a158a221a183a222a85a223 a172 a173a78a224a76a225a164a226 a59a136a227a158a228 a152a130a150a104a229 a65 a215 a77a29a216 a209a68a217a76a173 a172 a218 a77 a215 a219a27a220a159a221a183a222a85a223 a59a136a227a158a228 a152a130a150 a229 a65 (10) Here_comma_ a230a66a231a37a232a21a233a27a234a118a28a78a235a37a236a25a237a37a238a11a239a168a36 is the number of an N-gram and a230a66a231a37a232a21a233a27a234 a196 a197a29a240a98a241a243a242a244a28a78a235a37a236a25a237a37a238a49a239a168a36 denotes the number of ngram co-occurrences in a system output and the reference.
2555,0,ROUGE-S ROUGE-S is an extension of ROUGE-2 defined as follows (Lin_comma_ 2004b): ROUGE-Sa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 (11) Where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows: a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2a23a147 (12) a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 # of skip bigram a2 a146 (13) Here_comma_ function Skip2 returns the number of skipbi-grams that are common to a141 and a139 . ROUGE-SU ROUGE-SU is an extension of ROUGE-S_comma_ which includes unigrams as a feature defined as follows (Lin_comma_ 2004b): ROUGE-SUa59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 (14) Where a166 a169a8a7 and a171 a169a8a7 are defined as follows: a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 (# of skip bigrams + # of unigrams) a2 a147 (15) a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 (# of skip bigrams + # of unigrams) a2 a146 (16) Here_comma_ function SU returns the number of skip-bigrams and unigrams that are common to a141 and a139 . ROUGE-L ROUGE-L is an LCS-based evaluation measure defined as follows (Lin_comma_ 2004b): ROUGE-La59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 (17) where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows: a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 LCSa17a244a59a61a156 a88 a62a90a146a21a65 (18) a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 LCSa17 a59a61a156a34a88a78a62a98a146a21a65 (19) Here_comma_ LCSa19a244a28a78a144a183a114a93a32a93a139a102a36 is the LCS score of the union longest common subsequence between reference sentences a144a25a114 and a139 . a115 and a122 are the number of words contained in a141_comma_ and a139_comma_ respectively.
2556,0,a23 indicates an automatic evaluation function such as a26a138a167a132a169a90a170_comma_ a26a15a27a21a169a78a170_comma_ ROUGE-N_comma_ ROUGE-S_comma_ ROUGE-SU and ROUGE-L.
2557,0,short long a202 a72 a202 a2 a202 a3 a202 a173a164a208a93a209 a202 a72 a202 a2 a202 a3 a202 a173a164a208a93a209 stop case stop case stop case stop case stop case stop case stop case stop case ROUGE-1 .965 .884 .931 .888 .937 .879 .956 .906 .906 .876 .919 .916 .897 .891 .918 .948 ROUGE-2 .943 .960 .836 .880 .861 .906 .904 .937 .886 .930 .788 .941 .834 .616 .856 .929 ROUGE-3 .906 .936 .759 .814 .786 .846 .862 .900 .873 .909 .717 .849 .826 .431 .844 .885 ROUGE-4 .878 .914 .725 .752 .729 .794 .837 .871 .850 .890 .651 .787 .836 .292 .836 .865 ROUGE-L .919 .777 .789 .683 .875 .867 .898 .852 .917 .840 .861 .812 .847 .829 .910 .848 ROUGE-S(a34 ) .934 .914 .805 .888 .872 .938 .867 .917 .812 .863 .744 .954 .709 .547 .757 .900 ROUGE-S(9) .929 .935 .783 .899 .808 .917 .856 .939 .840 .903 .735 .951 .730 .617 .787 .927 ROUGE-S(4) .936 .943 .802 .891 .839 .917 .877 .940 .876 .920 .778 .945 .814 .663 .840 .932 ROUGE-SU(a34 ) .934 .914 .805 .887 .872 .937 .867 .917 .811 .864 .743 .954 .707 .547 .756 .900 ROUGE-SU(9) .926 .938 .765 .890 .789 .906 .845 .936 .829 .904 .705 .948 .701 .586 .766 .925 ROUGE-SU(4) .930 .945 .772 .865 .810 .889 .861 .927 .868 .921 .730 .928 .785 .620 .818 .925 a160 a58 a70 a2 a133a98a134a61a135 a59a61a162a138a67 a1 a65 .942 .927 .921 .957 .941 .957 .967 .969 a160 a58 a70 a2 a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .929 .943 .928 .965 .939 .962 .959 .967 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67 a1 a65 .939 .923 .919 .962 .926 .954 .953 .966 a160 a58 a70 a3a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .927 .933 .920 .964 .920 .947 .904 .949 a160 a58 a70 a7 a133a98a134a61a135 a59a61a162a138a67 a1 a65 .921 .900 .897 .955 .900 .932 .890 .946 a160 a58 a70 a7 a133a98a134a61a135 a59a61a162a138a67a36a35a159a65 .909 .900 .888 .950 .892 .921 .819 .922 a160 a58 a70 a2 a37 a134a136a135 a59a61a162a138a67 a1 a65 .939 .900 .897 .942 .931 .923 .936 .939 a160 a58 a70 a2 a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .928 .921 .909 .958 .932 .939 .950 .950 a160 a58 a70 a3a37 a134a136a135 a59a61a162a138a67 a1 a65 .938 .902 .886 .947 .924 .921 .934 .944 a160 a58 a70 a3a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .928 .922 .895 .960 .920 .929 .919 .942 a160 a58 a70 a7 a37 a134a136a135 a59a61a162a138a67 a1 a65 .929 .896 .873 .947 .910 .913 .908 .938 a160 a58 a70 a7 a37 a134a136a135 a59a61a162a138a67a36a35a159a65 .918 .915 .879 .956 .903 .913 .865 .925 bles show results obtained with and without stop word exclusion for the entire ROUGE family.
2558,0,For ROUGE-S and ROUGE-SU_comma_ we use three variations following (Lin_comma_ 2004b): the maximum skip distances are 4_comma_ 9 and infinity 7.
2559,0,The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection.
2560,0,It was also included in the DUC 2004 evaluation plan where summary quality was automatically judged using a set of n-gram word overlap metrics called ROUGE (Lin and Hovy_comma_ 2003).
2561,0,A more detailed description of these rules can be found in Dorr et al.(2003) and Zajic et al.(2004) In this example_comma_ we can see that after compression the lead sentence reads 156 more like a headline.
2562,0,The headline-style summaries created by each system were evaluated against a set of human generated (or model) summaries using the ROUGE metrics.
2563,0,The format of the evaluation was based on six scoring metrics: ROUGE-1_comma_ ROUGE-2_comma_ ROUGE-3_comma_ ROUGE-4_comma_ ROUGE-LCS and ROUGE-W.
2564,0,The first four metrics are based on the average n-gram match between a set of model summaries and the system-generated summary for each document in the corpus.
2565,0,ROUGE-LCS calculated the longest common 2 Details of our official DUC 2004 headline generation system can be found in Doran et al.(2004b).
2566,0,+= ))_comma_(*)(()( jirelrepsrepschainScore ji (1) 158 sub-string between the system summaries and the models_comma_ and ROUGE-W is a weighted version of the LCS measure.
2567,0,So for all ROUGE metrics_comma_ the higher the ROUGE value the better the performance of the summarisation system_comma_ since high ROUGE scores indicate greater overlap between the system summaries and their respective models.
2568,0,In the official DUC 2004 evaluation all summary words were stemmed before the ROUGE metrics were calculated; however_comma_ stopwords were not removed.
2569,0,5.1 ROUGE Evaluation Results Table 1 shows the results of our headline generation experiments on the DUC 2004 collection.
2570,1,Since the DUC 2004 evaluation_comma_ Lin (2004) has concluded that certain ROUGE metrics correlate better with human judgments than others_comma_ depending on the summarisation task being evaluated_comma_ i.e. single document_comma_ headline_comma_ or multi-document summarisation.
2571,1,In the case of headline generation_comma_ Lin found that ROUGE-1_comma_ ROUGE-L and ROUGE-W scores worked best and so only these scores are included in Table 1.
2572,0,ROUGE scores for headline generation systems As the results show the best performing topic labeling techniques are the TF and Hybrid systems.
2573,0,The performance of the baseline TFTrim system and the HybridTrim system are very similar for all Rouge metrics; however_comma_ both systems outperform the Topiary headline generator.
2574,0,c2005 Association for Computational Linguistics A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?
2575,0,We found positive correlations between ROUGE scores and two different summary types_comma_ where only weak or negative correlations were found using other agreement measures.
2576,0,However_comma_ we show that ROUGE may be sensitive to the choice of summarization style.
2577,0,Section 5 presents the results of correlation between task usefulness and the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric (Lin and Hovy_comma_ 2003).1 While we show that ROUGE correlates with task usefulness (using our Relevance-Prediction measure)_comma_ we detect a slight difference between informative_comma_ extractive headlines (containing words from the full document) and less informative_comma_ non-extractive eye-catchers (containing words that might not appear in the full document_comma_ and intended to entice a reader to read the entire document).
2578,0,1ROUGE has been previously used as the primary automatic evaluation metric by NIST in the 2003 and 2004 DUC Evaluations.
2579,0,Automatic intrinsic measures such as ROUGE use n-gram scoring to produce rankings of summarization methods.
2580,0,5 Correlation with Intrinsic Evaluation Metric: ROUGE We now turn to the task of correlating our extrinsic task performance with scores produced by an intrinsic evaluation measure.
2581,0,We used the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric version 1.2.1.
2582,0,In previous studies (Dorr et al. _comma_ 2004) ROUGE was shown to have a very low correlation with the LDC-Agreement measurement results of the extrinsic task.
2583,0,Our goal was to test whether our new RelevancePrediction technique would allow us to induce higher correlations with ROUGE.
2584,0,5.1 Extrinsic Agreement Data To reduce the effect of outliers on the correlation between ROUGE and the human judgments_comma_ we averaged over all judgments for each subject (20 judgments3 events) to produce 60 data points.
2585,0,5.2 Intrinsic ROUGE Score To correlate the partitioned agreement scores above with our intrinsic measure_comma_ we first ran ROUGE on all 120 surrogates in our experiment (i.e. _comma_ the HUM and HEAD surrogates for each of the 60 event/document pairs) and then averaged the ROUGE scores for all surrogates belonging to the same partitions (for each of the three partition sizes).
2586,0,These partitioned ROUGE values were then used for detecting correlations with the corresponding partitioned agreement scores described above.
2587,0,Table 4 shows the ROUGE scores_comma_ based on 3 reference summaries per document_comma_ for partitions P1P15 used in the previous tables.8 For brevity_comma_ we include 8We commissioned a total of 180 human-generated reference summaries (3 for each of 60 documents) (in addition to the human generated summaries used in the experiment).
2588,0,5 Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 HEAD 80% 80% 85% 70% 73% 60% 80% 75% 60% 75% 88% 68% 80% 93% 83% HUM 83% 88% 85% 68% 75% 75% 93% 75% 98% 90% 75% 70% 80% 90% 78% Table 2: Relevance-Prediction Rates for HEAD and HUM Surrogates (Representative Partition of Size 4) Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 HEAD 70% 73% 85% 70% 63% 60% 60% 85% 50% 73% 70% 78% 65% 63% 73% HUM 68% 75% 58% 68% 75% 70% 68% 80% 88% 58% 63% 55% 55% 60% 78% Table 3: LDC-Agreement Rates for HEAD and HUM Surrogates (Representative Partition of Size 4) Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 Avg HEAD.10 .23 .13 .27 .20 .24 .26 .22 .13 .08 .30 .16 .26 .27 .30 .211 HUM .16 .22 .17 .23 .19 .36 .39 .29 .28 .25 .37 .22 .22 .39 .27 .269 Table 4: Average Rouge-1 Scores for HEAD and HUM Surrogates (Representative Partition of Size 4) only ROUGE 1-gram measurement (R1).9 The ROUGE scores for HEAD surrogates were slightly lower than those for HUM surrogates.
2589,0,Because ROUGE measures whether a particular summary has the same words (or n-grams) as a reference summary_comma_ a more constrained choice of words (as found in the extractive HUM surrogates) makes it more likely that the summary would match the reference.
2590,0,This raises the concern that ROUGE may be sensitive to the style of summarization that is used.
2591,0,5.3 Intrinsic and Extrinsic Correlation To test whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement_comma_ we calculated the correlation for the results of both techniques using Pearsons r (Siegel and Castellan_comma_ 1988): summationtextn i=1(rir)(sis)radicalbigsummationtext n i=1(rir)2 radicalbigsummationtextn i=1(sis)2 where ri is the ROUGE score of surrogate i_comma_ r is the average ROUGE score of all data points_comma_ si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement)_comma_ and s is the average agreement score.
2592,0,Pearsons statistics is commonly used in summarization and machine translation evaluation_comma_ see e.g.(Lin_comma_ 2004; Lin and Och_comma_ 2004).
2593,0,As one might expect_comma_ there is some variability in the correlation between ROUGE and human judgments for 9We also computed ROUGE 2-gram_comma_ ROUGE L and ROUGE W_comma_ but the trend for these did not differ from ROUGE1.
2594,0,Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement.
2595,0,For LDC-Agreement_comma_ we observed no correlation (or a minimally negative one) with ROUGE-1 scores_comma_ for both the HEAD and HUM surrogates.
2596,0,We conclude that ROUGE correlates more highly with the Relevance-Prediction measurement than the LDCAgreement measurement_comma_ although we should add that none of the correlations in Table 5 were statistically significant atp< 0.05.
2597,0,The low LDC-Agreement scores are consistent with previous studies where poor correlations 6 Surrogate P = 1 P = 2 P = 4 HEAD (RP) 0.1270 0.1943 0.3140 HUM (RP) 0.0632 0.1096 0.1391 HEAD (LDC) -0.0968 -0.0660 -0.0099 HUM (LDC) -0.0395 -0.0236 -0.0187 Table 5: Pearson Correlations with ROUGE-1 for Relevance-Prediction (RP) and LDC-Agreement (LDC)_comma_ where Partition size (P) = 1_comma_ 2_comma_ and 4 were attributed to low interannotator agreement rates.
2598,0,6 Discussion Our results suggest that ROUGE may be sensitive to the style of summarization that is used.
2599,0,In addition_comma_ these same surrogates often did not use a high percentage of words that were actually from the story_comma_ resulting in low ROUGE scores.
2600,0,There were three consequences of this difference between HEAD and HUM: (1) The rate of agreement was lower for HEAD than for HUM; (2) The average ROUGE score was lower for HEAD than for HUM; and (3) The correlation of ROUGE scores with agreement was higher for HEAD than for HUM.
2601,0,A further analysis supports the (somewhat counterintuitive) third point above.
2602,0,Although the ROUGE scores of true positives (and true negatives) were significantly lower for HEAD surrogates (0.2127 and 0.2162) than for HUM surrogates (0.2696 and 0.2715)_comma_ the number of false negatives was substantially higher for HEAD surrogates than for HUM surrogates.
2603,0,These cases corresponded to much lower ROUGE scores for HEAD surrogates (0.1996) than for HUM (0.2586) surrogates.
2604,0,A summary of this analysis is given in Table 6_comma_ where true positives and negatives are indicated by Rel/Rel and NonRel/NonRel_comma_ respectively_comma_ and false positives and negatives are indicated by Rel/NonRel and NonRel/Rel_comma_ respectively.10 The numbers in parentheses after each ROUGE score refer to the standard deviation for that 10We also included (average) elapsed times for summary judgments in each of the four categories.
2605,0,= radicalBiggsummationtext N i=1(xix)2 N where N is the number of surrogates in a particular judgment category (e.g. _comma_ N = 245 for the HEAD-based NonRel/Rel judgments)_comma_ xi is the ROUGE score for the ith surrogate_comma_ and r is the average of all ROUGE scores in that category.
2606,0,Because the false negatives were associated with the lowest average ROUGE score (0.1996)_comma_ we speculate that_comma_ if a correlation exists between RelevancePrediction and ROUGE_comma_ the false negatives may be a major contributing factor.
2607,0,Based on this experiment_comma_ we conjecture that ROUGE may not be a good method for measuring the usefulness of summaries when the summaries are not extractive.
2608,0,That is_comma_ if someone intentionally writes summaries that contain different words than the story_comma_ the summaries will also likely contain different words than a reference summary_comma_ resulting in low ROUGE scores.
2609,0,Using this new method_comma_ we were able to find positive correlations between relevance assessments and ROUGE scores for HUM and HEAD surrogates_comma_ where only 7 Judgment HEAD HUM (Surr/Doc) Raw R1-Avg Avg Time Raw R1-Avg Avg Time Rel/Rel 211 (35%) 0.2127 (0.120) 4.6 251 (42%) 0.2696 (0.130) 4.2 Rel/NonRel 27 (5%) 0.2115 (0.110) 7.1 35 (6%) 0.2725 (0.131) 4.6 NonRel/Rel 117 (19%) 0.1996 (0.127) 8.5 77 (13%) 0.2586 (0.120) 13.8 NonRel/NonRel 245 (41%) 0.2162 (0.126) 2.5 237 (39%) 0.2715 (0.131) 1.9 TOTAL 600 (100%) 0.2115 (0.124) 4.6 600 (100%) 0.2691 (0.129) 4.6 Table 6: Subjects Judgments and Corresponding Average ROUGE 1 Scores negative correlations were found using LDC-Agreement scores.
2610,0,We found that both the Relevance-Prediction and the ROUGE-1 scores were higher for human-generated summaries than for the original headlines.
2611,0,It appears that most of the difference is induced by surrogates that are eye-catchers (rather than true summaries)_comma_ where both agreement and ROUGE scores are low.
2612,0,We also plan to test for correlations between ROUGE and human task performance with automatic summaries_comma_ to further investigate whether ROUGE is a good predictor of human task performance.
2613,0,3.1 Similarity metrics For this work_comma_ we have considered the following similarity metrics: ROUGE based metrics (R): ROUGE (Lin and Hovy_comma_ 2003) estimates the quality of an automatic summary on the basis of the n-gram coverage related to a set of human summaries (models).
2614,0,Although ROUGE is an evaluation metric_comma_ we can adapt it to behave as a similarity metric between pairs of summaries if we consider only one model in the computation.
2615,0,There are different kinds of ROUGE metrics such as ROUGE-W_comma_ ROUGE-L_comma_ ROUGE1_comma_ ROUGE-2_comma_ ROUGE-3_comma_ ROUGE-4_comma_ etc.(Lin_comma_ 2004b).
2616,0,Each of these metrics has been applied over summaries with three preprocessing options: with stemming and stopword removal (type c); only with stopwords removal (type b); or without any kind of preprocessing (type a).
2617,0,All these combinations give 24 similarity metrics based on ROUGE.
2618,0,Inverted ROUGE based metrics (Rpre): ROUGE metrics are recall oriented.
2619,0,If we reverse the directionofthesimilaritycomputation_comma_ weobtain precision oriented metrics (i.e. Rpre(a_comma_b) = R(b_comma_a)).
2620,0,In this way_comma_ we generate another 24 metrics based on inverted ROUGE.
2621,0,After the clustering process_comma_ the 48 ROUGE metrics are grouped in 7 sets_comma_ and the 9 TVM metrics are grouped in 3 sets.
2622,0,In each cluster_comma_ the metric with highest KING has been marked in boldface.
2623,0,Note that the ROUGE-c metrics (with stemming)withhighestKINGarethosebasedonrecall whereas the ROUGE-a/b metrics (without stemming)arethosebasedonprecision.
2624,1,Rpre-W.1.2.b (inverted ROUGE measure_comma_ using non-contiguous word sequences_comma_ removing stopwords_comma_ without stemming) obtains the highest individual KING for task 2_comma_ and is one of the best in task 5_comma_ confirming that ROUGEbased metrics are a robust way of evaluating summaries_comma_ and indicating that non-contiguous word sequences can be more useful for evaluation purposes than n-grams.
2625,0,5 Related work The methodology which is closest to our framework is ORANGE (Lin_comma_ 2004a)_comma_ which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set.
2626,0,For instance_comma_ ROUGE can be applied directly in the ORANGE framework without any reformulation.
2627,0,For both tasks_comma_ metric sets with higher KING values slightly outperforms the best ROUGE evaluation measure.
2628,0,3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin_comma_ 2004) is also used for evaluating results.
2629,0,ROUGE F-measure scores are given for ROUGE2 (bigram)_comma_ ROUGE-3 (trigram)_comma_ and ROUGE-SU4 (skip-bigram)_comma_ using the model average (average score across all references) metric.
2630,0,ROUGE metrics_comma_ however_comma_ do not show any significant improvement.
2631,0,Using class models (Class and Mixed)_comma_ for all ROUGE metrics_comma_ relative improvements range from 3.5% to 13.4% for the 10% summarisation ratio_comma_ and from 8.6% to 16.5% on the 30% summarisation ratio.
2632,0,the TRS_comma_ LiM adaptation showed improvements in terms of SumACCY_comma_ but ROUGE metrics do not corroborate those results for the 10% summarisation ratio.
2633,0,Using class models_comma_ for all ROUGE metrics_comma_ relative improvements range from 6.0% to 22.2% and from 7.4% to 20.0% for the 10% and 30% summarisation ratios_comma_ respectively.
2634,0,6 Discussion Compared to previous experiments using only word models_comma_ improvements obtained using class models are larger and more significant for both ROUGE and SumACCY metrics.
2635,0,Rougea1 Rougea2 Extractive Lead10 458 114 9.9 20.8 11.1 HedgeTrimmera3 399 104 7.4 18.1 9.9 Topiarya4 576 115 9.9 26.2 12.5 Abstractive Keywords 585 22 9.9 26.6 5.5 Webcl 311 76 7.3 14.1 7.5 WIDL-Aa5 562 126 10.0 25.5 12.9 Table 1: Headline generation evaluation.
2636,0,We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGEa129 (Lin_comma_ 2004).
2637,0,The interpolation weights a65 (Equation 2) are trained using discriminative training (Och_comma_ 2003) using ROUGEa129 as the objective function_comma_ on the development set.
2638,0,(Donaway et al. _comma_ 2000_comma_ Hirao et al. _comma_ 2005_comma_ Lin et al. _comma_ 2003_comma_ Lin_comma_ 2004_comma_ Hori et al. _comma_ 2003) and manual methods
2639,0,We tested several measures_comma_ such as ROUGE (Lin_comma_ 2004) and the cosine distance.
2640,0,As a similarity measure_comma_ we tested cosine distance and 11 kinds of ROUGE.
2641,0,ROUGE-N (Lin_comma_ 2004) This measure compares n-grams of two summaries_comma_ and counts the number of matches.
2642,0,The measure is defined by Equation 3.
2643,0,= RSSgram N RSSgram Nmatch N N gramCount gramCount NROUGE )( )( (3) where Count(gram N ) is the number of an N-gram and Count match (gram N ) denotes the number of ngram co-occurrences in two summaries.
2644,0,ROUGE-L (Lin_comma_ 2004) This measure evaluates summaries by longest common subsequence (LCS) defined by Equation 4.
2645,0,m CrLCS LROUGE u ii i =  = )_comma_( (4) where LCS U (r i_comma_C) is the LCS score of the unions longest common subsequence between reference sentences r i and the summary to be evaluated_comma_ and m is the number of words contained in a reference summary.
2646,0,605 ROUGE-S (Lin_comma_ 2004) Skip-bigram is any pair of words in their sentence order_comma_ allowing for arbitrary gaps.
2647,0,ROUGE-S measures the overlap of skip-bigrams in a candidate summary and a reference summary.
2648,0,Several variations of ROUGE-S are possible by limiting the maximum skip distance between the two in-order words that are allowed to form a skip-bigram.
2649,0,In the following_comma_ ROUGE-SN denotes ROUGE-S with maximum skip distance N. ROUGE-SU (Lin_comma_ 2004) This measure is an extension of ROUGE-S; it adds a unigram as a counting unit.
2650,0,In the following_comma_ ROUGE-SUN denotes ROUGE-SU with maximum skip distance N. 4.3 Evaluation Methods In the following_comma_ we elaborate on the evaluation methods for each experiment.
2651,0,As a similarity measure in scr(x)_comma_ we tested ROUGE and the cosine distance.
2652,0,using Spearmans rank correlation coefficient and Pearsons rank correlation coefficient (Lin et al. _comma_ 2003_comma_ Lin_comma_ 2004_comma_ Hirao et al. _comma_ 2005).
2653,0,The two summaries were also ranked by ROUGE and by cosine distance_comma_ and both Precision values were calculated.
2654,0,Finally_comma_ the Precision value of Kazawas method was compared with those of ROUGE and cosine distance.
2655,0,Instead of those methods_comma_ we 3 http://www.nist.gov/speech/tests/mt/mt2001/resource/ 606 tested ROUGE and cosine distance_comma_ both of which have been used for summary evaluation.
2656,0,Two summaries were also ranked by ROUGE and by cosine distance and both Precision values were calculated.
2657,0,Finally_comma_ the Precision value of Yasudas method was compared with those of ROUGE and cosine distance.
2658,0,We also tested other automatic methods: content-based evaluation_comma_ BLEU (Papineni et al. _comma_ 2001) and ROUGE-1 (Lin_comma_ 2004)_comma_ and compared their results with that of evaluation by revision as reference.
2659,1,Of the 12 measures_comma_ unigram-based methods_comma_ such as cosine distance and ROUGE-1_comma_ produced good results.
2660,0,However_comma_ there were no significant differences between measures except for when ROUGE-L was used.
2661,0,We ranked summaries by Kazawas method_comma_ ROUGE and cosine distance_comma_ calculated using Precision.
2662,0,We also plotted the Precision values of ROUGE2 as dotted lines.
2663,1,ROUGE-2 was superior to the other 11 measures in terms of Ranking.
2664,0,From the result shown in Figure 1_comma_ we found that Kazawas method outperformed ROUGE-2_comma_ when the threshold value was greater than 0.968.
2665,0,608 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 threshold value pr e c i s i o n Kazawa's method R-2 Figure 1 Comparison of Kazawas method and ROUGE-2 (ratio: 40%) 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 threshold value p r e c is io n Kazawa's method R-2 Figure 2 Comparison of Kazawas method and ROUGE-2 (ratio: 20%) Exp-3: An experiment for Point 3 based on Yasudas method For Point 2 in Section 3.2_comma_ we also examined Yasudas method.
2666,0,When the ratio is 20%_comma_ ROUGE-SU4 is the best.
2667,0,In particular_comma_ the Gap values of 0.023 by ROUGE-2 and by ROUGE-3 are smaller than those produced by Kazawas method with a threshold value of 0.9 (Tables 1 and 2).
2668,1,We evaluated 10 systems by Yasudas method with ROUGE-3_comma_ which produced the best results in Exp-3.
2669,0,Of the automatic methods compared_comma_ ROUGE-4 was the best.
2670,0,609 As evaluation scores by Yasudas method were calculated based on ROUGE-3_comma_ there were no striking differences between Yasudas method and the others except for the integration process of evaluation scores for each summary.
2671,0,Yasudas method using ROUGE-3 outperformed the original ROUGE-3 for both ratios_comma_ 20% and 40%.
2672,1,We also investigated an automatic method based on Yasudas method and found that the method using ROUGE-2 and -3 could accurately estimate manual scores_comma_ and could outperform Kazawas method and the other automatic methods tested.
2673,1,From these results_comma_ we can conclude that the automatic method performed the best when ROUGE-2 or 3 is used as a similarity measure_comma_ and a regression analysis is used for combining manual method.
2674,0,ROUGE (Lin_comma_ 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks.
2675,0,ROUGE-N uses average N-gram recall_comma_ and ROUGE-1 is word recall.
2676,0,ROUGE-L uses the length of the longest common subsequence (LCS) of the original and summarized sentences.
2677,0,In our model_comma_ the length of the LCS is equal to the number of common words_comma_ and ROUGE-L is equal to the unigram F-measure because words are not rearranged.
2678,0,ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener853 ation task (Lin_comma_ 2004).
2679,0,Additionally_comma_ automatic evaluation of content coverage using ROUGE (Lin_comma_ 2004) was explored in 2004.
2680,0,We also computed ROUGE scores as was done in DUC 2004.
2681,0,4.2.2 ROUGE We computed two ROUGE scores: ROUGE-2 and ROUGE-SU4 recall_comma_ both with stemming and implementing jackknifing for each [peer_comma_ topic] pair so that human and automatic peers could be compared.
2682,0,Since the number of ROUGE evaluations per topic varied depending on the number of reference summaries_comma_ we computed a macroaverage of each score for each peer_comma_ where the macro-average score is the mean over all topics of the mean per-topic score for the peer.
2683,1,Unlike responsiveness and linguistic quality scores_comma_ which are ordinal data and are best suited for non-parametric analyses_comma_ ROUGE scores_comma_ can be measured on an interval scale and are suitable for parametric analysis.
2684,0,Analysis of variance showed significant effects from peer and topic (p = 0 for each factor) for both ROUGE-2 and ROUGE-SU4 recall.
2685,0,To see which peers were different_comma_ a multiple comparison of population marginal means (PMM) was performed for each type of ROUGE score.
2686,0,ROUGE-2 and ROUGE-SU4 both distinguish human peers from automatic ones.
2687,0,The difference in the ROUGE-2 5 10 15 20 25 30 35 10 15 20 25 30 35 Average scaled responsiveness (primary) Average scaled responsiveness (secondary) Figure 1: Primary vs. secondary average scaled responsiveness score of the best system and worst human is not considered significant (possibly due to the very conservative nature of the multiple comparison test) but is still relatively large.
2688,0,On the other hand_comma_ ANOVA of ROUGE-2 found more significant differences between the automatic peers than did Friedmans test of responsiveness.
2689,0,Intrinsic measures like ROUGE rely on multiple model summaries to take into account human variation (although Pyramids add another level of human variation in the manual pyramid and peer annotation).
2690,0,Table 4 shows that the correlation between the primary and sec53 RunID PMM of R2 C 0.1172 A A 0.1156 A B I 0.1023 A B C B 0.1014 A B C J 0.1012 A B C E 0.1009 A B C D 0.0986 A B C G 0.0970 B C F 0.0947 C H 0.0897 C D 15 0.0725 D E 17 0.0717 E 10 0.0698 E F 8 0.0696 E F 4 0.0686 E F G 5 0.0675 E F G 11 0.0643 E F G H 14 0.0635 E F G H I 16 0.0633 E F G H I 19 0.0632 E F G H I 7 0.0628 E F G H I J 9 0.0625 E F G H I J 29 0.0609 E F G H I J K 25 0.0609 E F G H I J K 6 0.0609 E F G H I J K 24 0.0597 E F G H I J K 28 0.0594 E F G H I J K 3 0.0594 E F G H I J K 21 0.0573 E F G H I J K 12 0.0563 F G H I J K 18 0.0553 F G H I J K L 26 0.0547 F G H I J K L 27 0.0546 F G H I J K L 32 0.0534 G H I J K L 20 0.0515 H I J K L 13 0.0497 H I J K L 30 0.0496 H I J K L 31 0.0487 I J K L 2 0.0478 J K L 22 0.0462 K L 1 0.0403 L M 23 0.0256 M Table 3: Multiple comparison of all peers based on ANOVA of ROUGE-2 recall 54 Spearman Pearson All peers 0.900 0.976 [0.960_comma_ 1.000] Auto peers 0.775 0.822 [0.695_comma_ 1.000] Table 4: Correlation between primary and secondary average scaled responsiveness (20 topics)_comma_ with 95% confidence intervals for Pearsons r. ondary average scaled responsiveness scores is respectable despite the low number of topics.
2691,0,Table 5 shows that there is high correlation between macro-average ROUGE scores (intrinsic measures) and average scaled responsiveness (a pseudo-extrinisic measure).
2692,0,Metric Spearman Pearson ROUGE-2 (all) 0.951 0.972 [0.953_comma_ 1.000] ROUGE-SU4 (all) 0.942 0.958 [0.930_comma_ 1.000] ROUGE-2 (auto) 0.901 0.928 [0.872_comma_ 1.000] ROUGE-SU4 (auto) 0.872 0.919 [0.855_comma_ 1.000] Table 5: Correlation between average scaled responsiveness and macro-average ROUGE recall over all topics and either all peers or only automatic peers.
2693,1,We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE (Lin and Hovy_comma_ 2003; Lin_comma_ 2004) which allow quick measurement of systems during development and enable evaluation of larger amounts of data.
2694,1,Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau_comma_ 2004b) and ROUGE (Lin_comma_ 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies_comma_ which achieves 91.3% of human performance in Pyramid score_comma_ and outperforms our best-performing non-sequential model by 3.9%.
2695,1,Two metrics have become quite popular in multi-document summarization_comma_ namely the Pyramid method (Nenkova and Passonneau_comma_ 2004b) and ROUGE (Lin_comma_ 2004).
2696,0,Pyramid and ROUGE are techniques looking for content units repeated in different model summaries_comma_ i.e._comma_summarycontentunits(SCUs)suchasclauses and noun phrases for the Pyramid method_comma_ and ngrams for ROUGE.
2697,0,For completeness_comma_ we also included standard ROUGE (1_comma_ 2_comma_ and L) scores in Table 7_comma_ which were obtained using parameters defined for the FEATURE SET P lexical .471 IR .415 lexical + IR .497 acoustic .407 structural/durational .478 acoustic + structural/durational .476 all features .507 selected features (S) .515 Table 5: Pyramid score for each feature set.
2698,0,SUMMARIZER P R-1 R-2 R-L baseline .188 .501 .210 .495 skip-chain CRF (transcript) .554 .715 .442 .709 skip-chain CRF (ASR) .504 .714 .42 .706 human .607 .720 .477 .715 optimal 1 .791 .648 .788 Table 7: Pyramid_comma_ and average ROUGE scores for summaries produces by a baseline (lead summarizer)_comma_ our best system_comma_ humans_comma_ and the optimal summarizer.
2699,0,Since system summaries have on average approximately the same length as references_comma_ we only report recall measures of ROUGE (precision and F averages are within  .002).3 It may come as a surprise that our best system (both with ASR and true words) performs almost as well as humans; it seems more reasonable to conclude that_comma_ in our case_comma_ ROUGE has trouble discriminating between systems with moderately close performance.
2700,0,3Human performance with ROUGE was assessed by cross-validating reference summaries of each meeting (i.e. _comma_ n references for a given meeting resulted in n evaluations against the other references).
2701,0,The best ROUGE-1 measure reported in (Murray et al. _comma_ 2005) is .69 recall_comma_ which is significantly lower than ours according to confidence intervals.
2702,0,In recent years_comma_ many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al. _comma_ 2000; Akiba et al. _comma_ 2001; Papineni et al. _comma_ 2002; NIST_comma_ 2002; Leusch et al. _comma_ 2003; Turian et al. _comma_ 2003; Babych and Hartley_comma_ 2004; Lin and Och_comma_ 2004; Banerjee and Lavie_comma_ 2005; Gimenez et al. _comma_ 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently.
2703,0,Many methods for calculating the similarity have been proposed (Niessen et al. _comma_ 2000; Akiba et al. _comma_ 2001; Papineni et al. _comma_ 2002; NIST_comma_ 2002; Leusch et al. _comma_ 2003; Turian et al. _comma_ 2003; Babych and Hartley_comma_ 2004; Lin and Och_comma_ 2004; Banerjee and Lavie_comma_ 2005; Gimenez et al. _comma_ 2005).
2704,0,In our research_comma_ 23 scores_comma_ namely BLEU (Papineni et al. _comma_ 2002) with maximum n-gram lengths of 1_comma_ 2_comma_ 3_comma_ and 4_comma_ NIST (NIST_comma_ 2002) with maximum n-gram lengths of 1_comma_ 2_comma_ 3_comma_ 4_comma_ and 5_comma_ GTM (Turian et al. _comma_ 2003) with exponents of 1.0_comma_ 2.0_comma_ and 3.0_comma_ METEOR (exact) (Banerjee and Lavie_comma_ 2005)_comma_ WER (Niessen et al. _comma_ 2000)_comma_ PER (Leusch et al. _comma_ 2003)_comma_ and ROUGE (Lin_comma_ 2004) with n-gram lengths of 1_comma_ 2_comma_ 3_comma_ and 4 and 4 variants (LCS_comma_ S_comma_SU_comma_ W-1.2)_comma_ were used to calculate each similarity S i . Therefore_comma_ the value of m in Eq.
2705,0,The weight or importance of a sentence S in GENERIC FOCUSED Frequency 0.11972 0.11795 (0.11168??.12735) (0.11010??.12521) LLR 0.11223 0.11600 (0.10627??.11873) (0.10915??.12281) LLR(C) 0.11949 0.12201 (0.11249??.12724) (0.11507??.12950) LLR(CQ) not app 0.12546 (.11884??13247) Table 1: SU4 ROUGE recall (and 95% confidence intervals) for runs on the entire input (GENERIC) and on relevant sentences (FOCUSED).
2706,0,For evaluation we use ROUGE (Lin_comma_ 2004) SU4 recall metric1_comma_ which was among the official automatic evaluation metrics for DUC.
2707,0,No automatic method for query expansion can be expected to give more accurate results_comma_ since the content of the human summaries is a direct indication of what information in the input was important and relevant and_comma_ moreover_comma_ the ROUGE evaluation metric is based on direct n-gram comparison with these human summaries.
2708,1,ROUGE measures: considering the impact of ngram overlap metrics in textual entailment_comma_ we believethattheideaofintegratingthesemeasures1 into our system is very appealing.
2709,0,We have implemented them as defined in (Lin_comma_ 2004).
2710,0,Eachmeasureisappliedtothewords_comma_ lemmasand stems belonging to the text-hypothesis pair.
2711,0,Within the entire set of measures_comma_ each one of them is considered as a feature for the training and test stages of a machine learning algorithm.
2712,0,2.3 Syntactic Module The syntactic module we have built is composed of few submodules that operate collaboratively in order 1The considered measures were ROUGE-N with n=2 and n=3_comma_ ROUGE-L_comma_ ROUGE-W and ROUGE-S with s=2 and s=3.
2713,0,The analyses show that while ROUGE has very strong correlation with responsiveness for both human and automatic summaries_comma_ there is a significant gap in responsiveness between humans and systems which is not accounted for by the ROUGE metrics.
2714,0,We apply this new evaluation method_comma_ which we call ROSE (ROUGE Optimal Summarization Evaluation)_comma_ to find the optimal linear combination of ROUGE scores to maximize correlation with human responsiveness.
2715,0,1 Introduction ROUGE (Lin_comma_ 2004) and its linguisticallymotivated descendent_comma_ Basic Elements (BE) (Hovy et al._comma_ 2005)_comma_ evaluate a summary by computing its overlap with a set of model (human) summaries; ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries_comma_ while Basic Elements uses larger units of comparison based on the output of syntactic parsers.
2716,1,The ROUGE/BE toolkit has become the standard automatic method for evaluating the content of c2008.
2717,0,Using the results of the Document Understanding Conference main task for 20052007 we explore the correlation between variants of ROUGE and the human metrics of responsiveness and linguistic quality.
2718,0,In particular_comma_ while ROUGE has very strong correlation with responsiveness for both human and system summaries_comma_ there is a significant gap in responsiveness between humans and systems which is not accounted for by the ROUGE metrics.
2719,0,One cause of the gap is that many automatic summarizers truncate the last sentence of their summary_comma_ which shows significant reduction in the responsiveness score but does not result in a statistically significant drop in ROUGE scores.
2720,0,We apply this new evaluation method_comma_ which we call ROSE (ROUGE Optimal Summarization Evaluation)_comma_ to find the optimal linear combination of ROUGE metrics to maximize correlation with human responsiveness.
2721,0,Responsiveness differs from other measures of summary content such as SEE coverage (Lin and Hovy_comma_ 2002) and Pyramid scores (Nenkova and Passonneau_comma_ 2004) in that it does not compare a peer summary against a set of known human summaries.
2722,0,In addition to the human assessment of responsiveness_comma_ NIST computed three official automatic scores using ROUGE and Basic Elements: ROUGE-2_comma_ ROUGE-SU4_comma_ and ROUGE-BE recall.
2723,0,This line could be used to extrapolate the system performance if ROUGE scores were to increase.
2724,0,As seen in Figure 1_comma_ while both the manual and the automatic ROUGE scores of the human summarizers remained relatively constant over the years_comma_ the systems made significant progress in their automatic scores_comma_ with the top systems performing within statistical confidence of the human summarizers in the ROUGE metrics as reported by Conroy et al.(2007).
2725,0,Thus_comma_ there is not only a gap in performance between humans and systems on this task as measured manually by content responsiveness_comma_ but there is also a metric gap in using any single variant of ROUGE to predict content responsiveness.
2726,0,This metric gap becomes more pronounced as system performance improves to the point where ROUGE is unable to distinguish between systems and humans.
2727,0,Figure 1: Scatter plot of average manual content responsiveness vs. automatic ROUGE scores (ROUGE-BE_comma_ ROUGE-2_comma_ and ROUGE-SU4) for humans (filled points) and systems (unfilled points)_comma_ for DUC 2005-2007.
2728,0,We tested the hypothesis by comparing the average grammaticality (Q1)_comma_ content responsiveness_comma_ and ROUGE scores of the 15 systems in DUC 2007 that ended their summaries with a complete sentence_comma_ against the 17 systems whose summaries ended with a sentence fragment.
2729,0,As measured by a Student Ttest_comma_ systems that ended their summaries with a complete sentence had significantly higher content responsiveness scores than those that did not; however_comma_ there was no significant difference in ROUGE scores.
2730,1,We hypothesize that different variants of ROUGE may capture different qualities of a summary; for example_comma_ ROUGE-1 may be a good indicator of the relevance of summary content_comma_ but ROUGE variants that take into account larger contexts may capture linguistic qualities of the summary.
2731,0,next section_comma_ we present a new evaluation metric that finds a linear combination of ROUGE metrics which_comma_ in general_comma_ has stronger correlation with content responsiveness than any of the current ROUGE metrics.
2732,0,4 ROSE: Un Melange de ROUGEs We developed an automatic content evaluation model which combines multiple ROUGE scores using canonical correlation (Hotelling_comma_ 1935).
2733,0,Canonical correlation finds the linear combination of ROUGE scores that has maximum correlation with human responsiveness on a given data set.
2734,0,As this family of models is a blend of ROUGE scores we call this metric ROSE_comma_ for ROUGE Optimal Summarization Evaluation.
2735,0,4.1 Blending ROUGE Scoring with a Canonical Correlation Model Suppose we are given a set of ROUGE scores and the corresponding content responsiveness scores.
2736,0,149 We let aij_comma_ for i = 1_comma__comma_m and j = 1_comma__comma_n_comma_ be the ROUGE score of type j for the summarizer i_comma_ and bi the human content evaluation metric.
2737,0,To this end_comma_ we included in our optimization 7 ROUGE automatic metrics: ROUGE-1_comma_2_comma_3_comma_4_comma_L_comma_SU4_comma_ and BE to predict content responsiveness and (for DUC 2006) overall responsiveness.
2738,0,While the linguistic questions evaluation scores are manually generated we combine them with the automatic methods of ROUGE in an attempt see to what extent these non-content scores can better model both content and overall responsiveness.
2739,0,The canonical variate with the highest estimated median correlation is then compared with the best performing ROUGE method.
2740,0,We compare the best of 504=511-7 canonical variates with the best of the 7 ROUGE variants by using the Mann-Whitney Utest_comma_ which tests for equal medians.
2741,0,In each case the best canonical variate and the estimated median correlation are reported over the set of ROUGE scores and the ROUGE scores in union with the linguistic questions.
2742,0,A * by a variant indicates that it differs significantly from the best single ROUGE correlation with a p-value of 107 or less as measured by a Mann Whitney U-test.
2743,0,Year Metric Summarizer Best ROUGE Corr.
2744,0,ROSEROUGE Corr.
2745,0,The ROSE models based on just ROUGE for the automatic summarizers are an appropriate method to use to compare systems that did not compete in DUC with those that did.
2746,0,Here_comma_ for simplicity_comma_ we restrict the ROSE model to use only the official ROUGE metrics to build a model based on a givenyearandthenevaluatethatmodelonasubsequent year.
2747,0,The ROSE models built from only ROUGE scores had mixed results_comma_ sometimes performing worse than a single ROUGE score (e.g._comma_ the ROSE model trained on DUC 2005 and evaluated on DUC 2006)_comma_ but in other cases performing as well as or better than single ROUGE scores.
2748,0,As the automatic ROUGE scores of system summaries approaches that of human summaries_comma_ the disparity between automatic and manual measures of summary content becomes a more important concern.
2749,0,We found that a blending of ROUGE scores using canonical correlation gave higher correlations with content and overall responsiveness.
2750,0,We report on ROUGE-1 (unigrams)_comma_ ROUGE-2 (bigrams)_comma_ ROUGE W-1.2 (weighted LCS)_comma_ and ROUGE-S* (skip bigrams) as they appear to correlate well with human judgments for longer multi-document summaries_comma_ particularly ROUGE-1 (Lin_comma_ 2004).
2751,0,Table 2 shows the results of this ROUGE-based evaluation including recall (R)_comma_ precision (P)_comma_ and balanced fmeasure (F).
2752,0,Quality evaluation results (5% word compression)  COGENT consistently outperforms the Random and Default baselines based on all four reported ROUGE measures.
2753,0,For this aspect of the evaluation_comma_ we have used ROUGE-L_comma_ an LCS metric computed using ROUGE.
2754,0,The ROUGE-L computation examines the union LCS between each reference sentence and all the sentences in the candidate summary.
2755,1,We believe this metric may be well-suited to reflect the degree of linguistic surface structure similarity between summaries.
2756,1,We postulate that ROUGE-L may be able to account for the explicitly copy-pasted concepts and to detect the more subtle similarities with paraphrased concepts in the expert-generated domain knowledge map.
2757,0,Content-based evaluation results (word compression in parentheses)  COGENT consistently outperforms the Random and Default baselines on both the ROUGEL and cosine measures.
2758,0,The ROUGE-L scores also consistently indicate that the COGENT summary may be closer to the reference summary in relative word order181 ing than either the Random or Default configurations.
2759,0,First_comma_ the ROUGE-L recall score for COGENT (R=0.
2760,0,Second_comma_ we would have expected the ROUGE-L precision score for the Best Case configuration to be closer to 1.0.
2761,0,Since the sentences in the Best Case summary come directly from the digital library resources_comma_ we hypothesize that experts may have used extensive linguistic transformations for paraphrased concepts_comma_ resulting in structures that ROUGE-L could not identify as similar.
2762,0,Given the difference in word compression for the Best Case summary_comma_ we have performed an incremental analysis using the ROUGE-L measure shown in Figure 2.
2763,0,ROUGE-L COGENT Evaluation 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 0 5 10 15 20 25 30 MEAD Word Percent Compression Recall Precision F-Measure Figure 2.
2764,0,COGENT ROUGE-L results at different word compression rates  This graph shows improved COGENT performance in ROUGE-L recall as the length of the summary increases_comma_ while both precision and fmeasure degrade.
2765,0,To evaluate the quality of our generated summaries_comma_ we choose to use the ROUGE3 (Lin_comma_ 2004) evaluation toolkit_comma_ that has been found to be highly correlated with human judgments.
2766,0,In our experiments ROUGE-1_comma_ ROUGE-2 and ROUGE-SU4 will be computed.
2767,0,Another interesting result is the high performance of the non-update specific method (cosine+ JWe) that could be due to the small size 3ROUGE is available at http://haydn.isi.edu/ROUGE/.
2768,0,ROUGE-1 ROUGE-2 ROUGE-SU4 Baseline 0.26232 0.04543 0.08247 3rd system 0.35715 0.09622 0.13245 2nd system 0.36965 0.09851 0.13509 cosine+ JWe 0.35905 0.10161 0.13701 NR 0.36207 0.10042 0.13781 SMMR 0.36323 0.10223 0.13886 1st system 0.37032 0.11189 0.14306 Worst human 0.40497 0.10511 0.14779 Table 1: ROUGE average recall scores computed on the DUC 2007 update corpus.
2769,0,(Toutanova et al._comma_ 2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle_comma_ (b) Pyramid-derived_comma_ and (c) Model Frequency.
2770,0,The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin_comma_ 2004).
2771,0,Given two sentences X and Y_comma_ the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in (Lin_comma_ 2004).
2772,0,We computed the LCS and WLCS-based F-measure following (Lin_comma_ 2004) using both the query pool and the sentence pool as in the previous section.
2773,0,Following (Lin_comma_ 2004)_comma_ we computed the skip bi-gram score using both the sentence pool and the query pool.
2774,0,We evaluate the summaries using the automatic evaluation tool ROUGE (Lin_comma_ 2004) (described in Section 6) and the ROUGE value works as the feedback to our learning loop.
2775,0,Our learning system tries to maximize the ROUGE score in every step by changing the weights individually by a specific step size (i.e. 0.01).
2776,0,For each weight wi_comma_ the algorithm achieves the local maximum of ROUGE value.
2777,0,Input: Stepsize l_comma_ Weight Initial Value v Output: A vector vectorw of learned weights Initialize the weight values wi to v. for i 1 ton do rg1 = rg2 = prev = 0 while (true) do scoreSentences(vectorw) generateSummaries() rg2 = evaluateROUGE() if rg1rg2 thenprev = w i wi+ = l rg1 = rg2 else break end end end return vectorwAlgorithm 1: Tuning weights using Local Search technique Once we have learned the feature-weights_comma_ our empirical method computes the final scores for the sentences using the formula: scorei = vectorxi.vectorw (1) Where_comma_ vectorxi is the feature vector for i-th sentence_comma_ vectorw is the weight vector and scorei is the score of i-th sentence.
2778,1,We carried out automatic evaluation of our summaries using ROUGE (Lin_comma_ 2004) toolkit_comma_ which has been widely adopted by DUC for automatic summarization evaluation.
2779,0,It measures summary quality by counting overlapping units such as the n-grams (ROUGE-N)_comma_ word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary.
2780,0,ROUGE parameters were set as the same as DUC 2007 evaluation setup.
2781,0,As Table 1 shows_comma_ in k-means_comma_ SYS2 gets 0-21%_comma_ SYS3 gets 4-32% and ALL gets 3-36% improvement in ROUGE-2 scores over the SYS1 system.
2782,0,We get best ROUGE-W (Table 2) scores for SYS2 (i.e. including BE) but SYS3 and ALL do not perform well in this case.
2783,0,SYS2 improves the ROUGE-W F-score by 1% over SYS1.
2784,0,We do not get any improvement in ROUGE-SU (Table 3) scores when we include any kind of syntactic/semantic structures.
2785,0,Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.074 0.077 0.086 0.075 0.075 0.078 0.077 P 0.081 0.084 0.093 0.081 0.098 0.107 0.110 F 0.078 0.080 0.089 0.078 0.085 0.090 0.090 Table 1: ROUGE-2 measures in k-means learning Table 10 shows the F-scores of the ROUGE measures for one baseline system_comma_ the best system in DUC 2007 and our three scoring techniques considering all features.
2786,0,The baseline system gener6SYS2_comma_ SYS3 and ALL systems show the impact of BE_comma_ syntactic and semantic features respectively 7R stands for Recall_comma_ P stands for Precision and F stands for F-score 311 Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.098 0.097 0.101 0.099 0.101 0.097 0.097 P 0.195 0.194 0.200 0.237 0.233 0.241 0.237 F 0.130 0.129 0.134 0.140 0.141 0.139 0.138 Table 2: ROUGE-W measures in k-means learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.131 0.127 0.139 0.136 0.135 0.135 0.135 P 0.155 0.152 0.162 0.176 0.171 0.174 0.174 F 0.142 0.139 0.150 0.153 0.151 0.152 0.152 Table 3: ROUGE-SU in k-means learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.089 0.080 0.087 0.085 0.085 0.089 0.091 P 0.096 0.087 0.094 0.092 0.095 0.116 0.138 F 0.092 0.083 0.090 0.088 0.090 0.101 0.109 Table 4: ROUGE-2 measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.103 0.096 0.101 0.102 0.101 0.102 0.101 P 0.205 0.193 0.200 0.203 0.218 0.222 0.223 F 0.137 0.128 0.134 0.136 0.138 0.139 0.139 Table 5: ROUGE-W measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.146 0.128 0.138 0.143 0.144 0.145 0.144 P 0.171 0.153 0.162 0.168 0.177 0.186 0.185 F 0.157 0.140 0.149 0.154 0.159 0.163 0.162 Table 6: ROUGE-SU measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.086 0.080 0.087 0.087 0.090 0.095 0.099 P 0.093 0.087 0.094 0.094 0.112 0.115 0.116 F 0.089 0.083 0.090 0.090 0.100 0.104 0.107 Table 7: ROUGE-2 in empirical approach Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.102 0.096 0.101 0.102 0.102 0.104 0.105 P 0.203 0.193 0.200 0.204 0.239 0.246 0.247 F 0.135 0.128 0.134 0.137 0.143 0.147 0.148 Table 8: ROUGE-W in empirical approach Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.144 0.129 0.138 0.145 0.146 0.149 0.150 P 0.169 0.153 0.162 0.171 0.182 0.195 0.197 F 0.155 0.140 0.150 0.157 0.162 0.169 0.170 Table 9: ROUGE-SU in empirical approach ates summaries by returning all the leading sentences (up to 250 words) in the TEXT field of the most recent document(s).
2787,0,Comparing with the DUC 2007 participants our systems achieve top scores and for some ROUGE measures there is no statistically significant difference between our system and the best DUC 2007 system.
2788,0,System ROUGE1 ROUGE2 ROUGEW ROUGESU Baseline 0.335 0.065 0.114 0.113 Best 0.438 0.122 0.153 0.174 k-means 0.390 0.090 0.138 0.152 EM 0.399 0.109 0.139 0.162 Empirical 0.413 0.107 0.148 0.170 Table 10: F-measures for different systems 7 Conclusion and Future Work Our experiments show the following: (a) our approaches achieve promising results_comma_ (b) empirical approach outperforms the other two learning and EM performs better than the k-means algorithm for this particular task_comma_ and (c) our systems achieve better results when we include BE_comma_ syntactic and semantic features.
2789,0,We experimented with two independent_comma_ arguably complementary techniques for clustering and aligning  a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE (Lin_comma_ 2004) based 265 approach that can extract templates containing multiple verbs.
2790,1,As we will see below_comma_ both approaches show promise.
2791,0,To our knowledge_comma_ the ROUGE metric has not been used for automatic extraction of templates.
2792,0,Second_comma_ we used ROUGE_comma_ a summarization evaluation metric that is generally used to compare machine generated and human written summaries.
2793,0,We uniquely used this metric for clustering similar summary items_comma_ after abstracting the surface forms to a representation that facilitates comparison of a pair of sentences.
2794,0,The following subsections detail both the techniques.
2795,0,4.2 A ROUGE Based Approach ROUGE (Lin_comma_ 2004) is the standard automatic evaluation metric in the Summarization community.
2796,0,It is derived from the BLEU (Papineni et al._comma_ 2001) score which is the evaluation metric used in the Machine Translation community.
2797,0,The underlying idea in the metric is comparing the candidate and the reference sentences (or summaries) based on their token co-occurrence statistics.
2798,0,Thus_comma_ intuitively_comma_ we may use the ROUGE score as a measure for clustering the sentences.
2799,1,Amongst the various ROUGE statistics_comma_ the most appealing is Weighted Longest Common Subsequence(WLCS).
2800,0,WLCS favors contiguous LCS which corresponds to the intuition of finding the common template.
2801,1,We experimented with other ROUGE statistics but we got better and easily interpretable results using WLCS and so we chose it as the final metric.
2802,0,In the following subsections_comma_ we describe the various clustering techniques that we tried using the ROUGE score followed by the alignment technique.
2803,0,4.2.1 Clustering Unsupervised Clustering: As the ROUGE score defines a distance metric_comma_ we can use this score for doing unsupervised clustering.
2804,0,We obtain the similarity matrix in our domain by thresholding the ROUGE similarity score matrix.
2805,0,This technique also did not give us good clusters_comma_ evaluated empirically.
2806,0,269 We created a similarity matrix by thresholding the ROUGE score.
2807,0,In the event_comma_ the clusters obtained by this approach were also not good_comma_ evaluated empirically.
2808,0,We factored the ROUGE-WLCS score_comma_ which is an F-measure score_comma_ into its component Precision and Recall scores and experimented with various combinations of using the Precision and Recall scores.
2809,0,It is based on using the ROUGE-WLCS metric to determine a match between the preprocessed (baseNP and NE tagged) test corpora with the proposed set of correct templates_comma_ a set determined by taking an intersection of only the relevant templates marked by each judge.
2810,0,For the ROUGE based method_comma_ the test corpus consists of 262 sentences_comma_ while for the predicate-argument based method it consists of a set of 263 propositions extracted from the 262 sentences using ASSERT followed by a filtering of invalid propositions (e.g. ones starting with a verb).
2811,0,Amongst different ROUGE scores (precision/recall/f-measure)_comma_ we consider precision as the criterion for deciding a match and experimented with different thresholding values.
2812,0,For 0.9 precision in ROUGEWLCS_comma_ the recall is 0.3 which shows that there is a 30% near exact coverage over propositions_comma_ while for 0.6 precision in ROUGE-WLCS_comma_ the recall is an encouraging 81%.
2813,0,0  0.2  0.4  0.6  0.8  1  0.4 0.5 0.6 0.7 0.8 0.9 Recall Precision Threshold forMatching Test Sentences ROUGESRL Figure 8: Automated Recall based on ROUGEWLCS measure comparing the test corpora with the set of templates extracted by the PredicateArgument (SRL) and the ROUGE based method.
2814,0,5.3 Results: ROUGE based approach Various precision and recall thresholds for ROUGE were considered for clustering.
2815,0,The approaches seem complementary as the ROUGE based technique does not use the structure of the sentence at all whereas the predicate-argument approach is heavily dependent on it.
2816,0,Moreover_comma_ the predicate argument based approach gives general templates with one predicate while ROUGE based approach 271 can extract templates containing multiple verbs.
2817,0,It would also be desirable to establish the generality of the techniques_comma_ by using other domains such as newswire_comma_ medical reports and others.
2818,0,We have also used ROUGE evaluation approach (Lin_comma_ 2004) which is based on n-gram co-occurrences between machine summaries and ideal human summaries.
2819,0,3.2 Evaluation of  a summary 486 In this paper_comma_ we have applied ROUGE-1 and ROUGE-2 which are simple n-gram measures.
2820,0,MAP_comma_ F-measure and ROUGE scores.
2821,1,The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy_comma_ 2003a; Lin_comma_ 2004).
2822,0,The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy_comma_ 2000) in the context of single document summarization_comma_ and was later used in several multi-document summarization systems (Conroy et al._comma_ 2006; Lacatusu et al._comma_ 2004; Gupta et al._comma_ 2007).
2823,0,We carried out automatic evaluation of our summaries using ROUGE (Lin_comma_ 2004) toolkit_comma_ which has been widely adopted by DUC for automatic summarization evaluation.
2824,0,It measures summary quality by counting overlapping units such as the n-gram (ROUGE-N)_comma_ word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary.
2825,0,ROUGE parameters were set as the same as DUC 2007 evaluation setup.
2826,0,All the ROUGE measures were calculated by running ROUGE-1.5.5 with stemming but no removal of stopwords.
2827,0,The ROUGE run-time parameters are: ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a 11 The purpose of our experiments is to study the impact of the syntactic and semantic representation for complex question answering task.
2828,0,The SYN system improves the ROUGE-1_comma_ ROUGE-L and ROUGEW scores over the TF*IDF system by 2.84%_comma_ 0.53% and 2.14% respectively.
2829,0,The SEM system improves the ROUGE-1_comma_ ROUGE-L_comma_ ROUGE-W_comma_ and ROUGE-SU scores over the TF*IDF system by 8.46%_comma_ 6.54%_comma_ 6.56%_comma_ and 11.68%_comma_ and over the SYN system by 5.46%_comma_ 5.98%_comma_ 4.33%_comma_ and 12.97% respectively.
2830,0,The SYNSEM system improves the ROUGE-1_comma_ ROUGE-L_comma_ ROUGE-W_comma_ and ROUGESU scores over the TF*IDF system by 4.64%_comma_ 1.63%_comma_ 2.15%_comma_ and 4.06%_comma_ and over the SYN system by 1.74%_comma_ 1.09%_comma_ 0%_comma_ and 5.26% respectively.
2831,0,The SEM system improves the ROUGE-1_comma_ ROUGEL_comma_ ROUGE-W_comma_ and ROUGE-SU scores over the SYNSEM system by 3.65%_comma_ 4.84%_comma_ 4.32%_comma_ and 7.33% respectively which indicates that including syntactic feature with the semantic feature degrades the performance.
2832,0,6 Conclusion In this paper_comma_ we have introduced the syntactic and shallow semantic structures and discussed their imSystems ROUGE 1 ROUGE L ROUGE W ROUGE SU TF*IDF 0.359458 0.334882 0.124226 0.130603 SYN 0.369677 0.336673 0.126890 0.129109 SEM 0.389865 0.356792 0.132378 0.145859 SYNSEM 0.376126 0.340330 0.126894 0.135901 Table 1: ROUGE F-scores for different systems pacts in measuring the similarity between the sentences in the random walk framework for answering complex questions.
2833,0,We use ROUGE (Lin_comma_ 2004) to assess summary quality using common n-gram counts and longest common subsequence (LCS) measures.
2834,0,We report on ROUGE-1 (unigrams)_comma_ ROUGE-2 (bigrams)_comma_ ROUGE W-1.2 (weighted LCS)_comma_ and ROUGE-S* (skip bigrams) as they have been shown to correlate well with human judgments for longer multidocument summaries (Lin_comma_ 2004).
2835,0,We use ROUGE-L to examine the union LCS between the reference and candidate summaries_comma_ thus capturing their linguistic surface structure similarity.
2836,0,Content evaluation results (word compression) The ROUGE-L scores consistently indicate that the COGENT summary may be closer to the reference in linguistic surface structure than either the Random or Default summaries.
2837,0,Since the COGENT ROUGE-L recall score (R=0.
2838,0,Given the difference in word compression for the Best Case summary_comma_ we have performed an 19 incremental analysis using the ROUGE-L measure shown in Figure 2.
2839,0,Incremental COGENT ROUGE-L analysis Figure 2 indicates that COGENT can match the Best Case recall (R=0.9669) by generating a longer summary.
2840,0,In this paper_comma_ we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization.
2841,1,ROUGE (Lin_comma_ 2004) has been widely used for summarization evaluation.
2842,1,In the news article domain_comma_ ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin_comma_ 2004).
2843,0,However_comma_ there are many differences between written texts (e.g._comma_ news wire) and spoken documents_comma_ especially in the meeting domain_comma_ for example_comma_ the presence of disfluencies and multiple speakers_comma_ and the lack of structure in spontaneous utterances.
2844,0,The question of whether ROUGE is a good metric for meeting summarization is unclear.
2845,0,(Murray et al._comma_ 2005) have reported that ROUGE-1 (unigram match) scores have low correlation with human evaluation in meetings.
2846,0,In this paper we investigate the correlation between ROUGE and human evaluation of extractive meeting summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers.
2847,0,Our analysis shows that by integrating meeting characteristics into ROUGE settings_comma_ better correlation can be achieved between the ROUGE scores and human evaluation based on Spearmans rho in the meeting domain.
2848,0,Different approaches have been proposed to measure matches using words or more meaningful semantic units_comma_ for example_comma_ ROUGE (Lin_comma_ 2004)_comma_ factoid analysis (Teufel and Halteren_comma_ 2004)_comma_ pyramid method (Nenkova and Passonneau_comma_ 2004)_comma_ and Basic Element (BE) (Hovy et al._comma_ 2006).
2849,0,However (Zhu and Penn_comma_ 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g._comma_ RU_comma_ ROUGE) on a subset of the Switchboard data.
2850,0,ROUGE has been used in meeting summarization evaluation (Murray et al._comma_ 2005; Galley_comma_ 2006)_comma_ yet the question remained whether ROUGE is a good metric for the meeting domain.
2851,1,(Murray et al._comma_ 2005) showed low correlation of ROUGE and human evaluation in meeting summarization evaluation; however_comma_ they 201 simply used ROUGE as is and did not take into account the meeting characteristics during evaluation.
2852,0,In this paper_comma_ we ask the question of whether ROUGE correlates with human evaluation of extractive meeting summaries and whether we can modify ROUGE to account for the meeting style for a better correlation with human evaluation.
2853,0,Thus_comma_ intotalwehave36humansummariesand 24 system summaries on the 6 test meetings_comma_ on which the correlation between ROUGE and human evaluation is calculated and investigated.
2854,0,All the experiments in this paper are based on human transcriptions_comma_ with a central interest on whether some characteristics of the meeting recordings affect the correlation between ROUGE and human evaluations_comma_ without the effect from speech recognition or automatic sentence segmentation errors.
2855,0,3.2 Automatic ROUGE Evaluation ROUGE(Lin_comma_ 2004)measuresthen-grammatchbetween system generated summaries and human summaries.
2856,0,In most of this study_comma_ we used the same options in ROUGE as in the DUC summarization evaluation (NIST_comma_ 2007)_comma_ and modify the input to ROUGE to account for the following two phenomena.
2857,0,To study the impact of disfluencies_comma_ we run ROUGE using two different inputs: summaries based on the original transcription_comma_ and the summaries with disfluencies removed.
2858,0,We associate the speaker ID with each word_comma_ treat them together as a new word in the input to ROUGE.
2859,0,4 Results 4.1 Correlation between Human Evaluation and Original ROUGE Score Similar to (Murray et al._comma_ 2005)_comma_ we also use Spearmans rank coefficient (rho) to investigate the correlation between ROUGE and human evaluation.
2860,0,For each of the human summaries_comma_ the ROUGE scores are generated using the other 5 humansummariesasreferences.
2861,0,Forsystemgeneratedsummaries_comma_ we calculate the ROUGE score using 5 human references_comma_ and then obtain the average from 6 such setups.
2862,0,Correlation on Human Summaries H AVG H IS H IC H IRV H IRD R-1 0.09 0.22 0.21 0.03 -0.20 R-SU4 0.18 0.33 0.38 0.04 -0.30 Correlation on System Summaries R-1 -0.07 -0.02 -0.17 -0.27 -0.02 R-SU4 0.08 0.05 0.01 -0.15 0.14 Table 1: Spearmans rho between human evaluation (H) and ROUGE (R) with basic setting.
2863,0,This is consistent with what ROUGE is designed for_comma_ recall oriented understudy gisting evaluation  we expect it to model IS and IC well by ngram and skip-bigram matching but not relevancy (IRV) and redundancy (IRD) effectively.
2864,0,4.2 Impacts of Disfluencies on Correlation Table 2 shows the correlation results between ROUGE (R-SU4) and human evaluation on the original and cleaned up summaries respectively.
2865,0,For human summaries_comma_ after removing disfluencies_comma_ the correlation between ROUGE and human evaluation improves on the whole_comma_ but degrades on information structure (IS) and information coverage (IC) categories.
2866,0,4.3 Incorporating Speaker Information We further incorporated speaker information in ROUGE setting using the summaries with disfluencies removed.
2867,1,This suggests that by leveraging speaker information_comma_ ROUGE can assign better credits or penalties to system generated summaries (same words from different speakers will not be counted as a match)_comma_ and thus yield better correlation with human evaluation; whereas for human summaries_comma_ this may not happen often.
2868,0,5 Conclusion and Future Work In this paper_comma_ we have made a first attempt to systematically investigate the correlation of automatic ROUGE scores with human evaluation for meeting summarization.
2869,0,Adaptations on ROUGE setting based on meeting characteristics are proposed and evaluated using Spearmans rank coefficient.
2870,0,Our experimental results show that in general the correlation between ROUGE scores and human evaluation is low_comma_ with ROUGE SU4 score showing better correlation than ROUGE-1 score.
2871,1,There is significant improvement in correlation when disfluencies are removed and speaker information is leveraged_comma_ especiallyforevaluatingsystem-generatedsummaries.
2872,0,In addition_comma_weobservethatthecorrelationisaffecteddifferently by those factors for human summaries and systemgenerated summaries.
2873,0,In our future work we will examine the correlation between each statement and ROUGE scores to better represent human evaluation results instead of using simply the average over all the statements.
2874,0,With our best performing features_comma_ we get ROUGE-2 (Lin_comma_ 2004) scores of 0.11 and 0.0925 on 2007 and 2006 5This threshold was derived experimentally with previous data.
2875,0,Baseline FastSum_comma_ 6 Top Systems and generic baseline for DUC 2007 ROUGE2 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Figure 1: ROUGE-2 results including 95%-confidence intervals for the top 6 systems_comma_ FastSum and the generic baseline for DUC 2007 DUC data_comma_ respectively.
2876,0,Table 1 shows the ROUGE score (recall) of the summaries generated when we used each of the features by themselves on 2006 and 2007 DUC data_comma_ trained on the data from the respective previous year.
2877,0,Feature name 2007 2006 Title word frequency 0.096 0.0771 Topic word frequency 0.0996 0.0883 Content word frequency 0.1046 0.0839 Document frequency 0.1061 0.0903 Headline frequency 0.0938 0.0737 Sentence length 0.054 0.0438 Sentence position(binary) 0.0522 0.0484 Sentence position (real-valued) 0.0544 0.0458 Table 1: ROUGE-2 scores of individual features We chose a so-called model selection algorithm to find a minimal set of features.
2878,1,Although it cannot replace the finesse of human evaluation_comma_ it can provide a crude idea of progress which can later be validated.
2879,0,e.g. BLEU (Papineni et al._comma_ 2001) for machine translation_comma_ ROUGE (Lin_comma_ 2004) for summarization.
2880,0,There are also automatic methods for summary evaluation_comma_ such as ROUGE (Lin_comma_ 2004)_comma_ which gives a score based on the similarity in the sequences of words between a human-written model summary  and  the  machine  summary.
2881,0,When automatically evaluating generated output_comma_ the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality.
2882,0,Such metrics have been introduced in other fields_comma_ including PARADISE (Walker et al._comma_ 1997) for spoken dialogue systems_comma_ BLEU (Papineni et al._comma_ 2002) for machine translation_comma_1 and ROUGE (Lin_comma_ 2004) for summarisation.
2883,0,Many automated generation evaluations measure the similarity between the generated output and a corpus of gold-standard target outputs_comma_ often using measures such as precision and recall.
2884,0,Such measures of corpus similarity are straightforward to compute and easy to interpret; however_comma_ they are not always appropriate for generation systems.
2885,0,In what concerns the evaluation process_comma_ although ROUGE (Lin_comma_ 2004) is the most common evaluation metric for the automatic evaluation of summarization_comma_ since our approach might introduce in the summary information that it is not present in the original input source_comma_ we found that a human evaluation was more adequate to assess the relevance of that additional information.
2886,0,We use SUMMA (Saggion and Gaizauskas_comma_ 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin_comma_ 2004) relative to human generated summaries.
2887,0,5 Results The model summaries were compared against 24 summaries generated automatically using SUMMA by calculating ROUGE-1 to ROUGE4_comma_ ROUGE-L and ROUGE-W-1.2 recall metrics (Lin_comma_ 2004).
2888,0,For all these metrics ROUGE compares each automatically generated summary s pairwise to every model summary mi from the set of M model summaries and takes the maximum ROUGEScore value among all pairwise comparisons as the best ROUGEScore score: ROUGEScore = argmaxiROUGEScore(mi_comma_s) ROUGE repeats this comparisonM times.
2889,0,The column GenericToModel for example shows ROUGE results for generic summaries relative to model summaries.
2890,0,All ROUGE metrics except R-1 and R-L indicate higher agreement in human image-related summaries than in DUC document summaries.
2891,0,The ROUGE metrics most indicative of agreement between human summaries are those that best capture words occurring in longer sequences of words immediately following each other (R-2_comma_ R-3_comma_ R-4 and R-W).
2892,0,All ROUGE results of the query-based summaries are greater than the generic summary scores.
2893,0,The agreement between the query-based and model summaries gets lower for ROUGE-3 and ROUGE-4 indicating that the query-based summaries contain very little information in common with the participants results.
2894,0,This indication is supported by the ROUGE-L (35%) and the low ROUGE-W (12%) agreement which are substantially lower compared to the UserToUser ROUGEL (40%) and ROUGE-W (15%) and the low ROUGE scores in column QueryToCPOfModel.
2895,0,For comparison with automated summaries in a different domain_comma_ we include ROUGE scores of query based SUMMA used in DUC 2004 (Saggion and Gaizauskas_comma_ 2005) as shown in the last column of Table 3.
2896,0,We considered a variety of tools like ROUGE (Lin_comma_ 2004) and METEOR (Lavie and Agarwal_comma_ 2007) but decided they were unsuitable for this task.
2897,0,ROUGE and METEOR were developed to compare larger stretches of text  they are usually used to compare paragraphs rather than sentences.
2898,0,We decided developing our own metric would be simpler than trying to adapt one of these existing tools.
2899,0,Finally_comma_ in order to formally evaluate the method and the different heuristics_comma_ a large-scale evaluation on the BioMed Corpus is under way_comma_ based on computing the ROUGE measures (Lin_comma_ 2004).
2900,0,Here_comma_ we use the more established ROUGE-W measure (Lin_comma_ 2004) instead.
2901,0,ROUGEW and other versions of ROUGE have been used in summarization to measure how close a machineauthored summary is to multiple human summaries of the same input.
2902,0,We use ROUGE-W in a similar setting_comma_ to measure how close a training window is to multiple encyclopedia definitions of the same term.
2903,0,A further difference from our previous work is that we also use ROUGE-W when computing the features of the windows to be classified.
2904,0,The indicative phrases are selected automatically during training_comma_ but now the corresponding features are not Boolean; their values are the ROUGEW similarity scores between an indicative phrase and the context of the target term in the window.
2905,1,Theuseofthe MAXENT classifierbyitselfimprovedslightlyourresults_comma_ buttheimprovements come mostly from using ROUGE-W.
2906,1,The latter is particularly interesting_comma_ because it is well published_comma_ it includes both an alternative_comma_ centroid-based technique to automatically tag training examples and a soft-matching classifier_comma_ 1We also experimented with other similarity measures (e.g._comma_ edit distance) and ROUGE variants_comma_ but we obtained the best results with ROUGE-W.
2907,0,Section 2 discusses ATTW with ROUGE-W_comma_ Cui et al.s centroid-based method to tag training examples_comma_ and experiments showing that ATTW is better.
2908,0,2.1 ATTW with ROUGE-W similarity To tag a training window w of a training term t with ATTW and ROUGE-W_comma_ we obtain a set Ct of definitions of t from encyclopedias.5 Stop-words_comma_ punctuation_comma_ and non-alphanumeric characters are removed from Ct and w_comma_ and a stemmer is applied; the testing windows undergo the same preprocessing.6 For each definition d  Ct_comma_ we find the longest common word subsequence of w and d. If w is the word sequence A_comma_B_comma_F_comma_C_comma_D_comma_E 3See http://www.cuihang.com/software.html.
2909,0,WLCS(w_comma_d) =summationtextmi=0 f(ki) We then compute the following quantities_comma_ where || is word length_comma_ and f1 is the inverse of f. P(w_comma_d) = f1(WLCS(w_comma_d)f(|w|) ) R(w_comma_d) = f1(WLCS(w_comma_d)f(|d|) ) F(w_comma_d) = (1+2)R(w_comma_d)P(w_comma_d)R(w_comma_d)+2P(w_comma_d) In effect_comma_ P(w_comma_d) examines how close the longest common substring is to w and R(w_comma_d) how close it is to d. Following Lin (2004)_comma_ we use  = 8_comma_ assigninggreaterimportancetoR(w_comma_d).
2910,0,The ROUGE-W similarity sim(w_comma_Ct) between w and Ct is the maximum F(w_comma_d)_comma_ for all d  Ct. Training windows with sim(w_comma_Ct) > T+ are tagged as positive; if sim(w_comma_Ct) < T_comma_ they are tagged as negative; and if T  sim(w_comma_Ct)  T+_comma_ they are discarded.
2911,0,However_comma_ ATTW is significantly better when tagging positive examples_comma_ as shown in figure 1; hence_comma_ it is better than the centroid method.8 8We tried different values of ROUGE-Ws a parameter in When using ATTW in practice_comma_ we need to select T+ and T.
2912,0,The value of each feature is the ROUGE-W score between a pattern and the left or right context of the target term in the window.
2913,0,The third baseline (centroid baseline) creates a centroid of the r  f windows_comma_ as in section 2.2_comma_ and returns the five windows with the highest cosine similarity to the centroid.11 11We also reimplemented the definitions component of Chu-Carroll et al.(2004; 2005)_comma_ but its performance was worse than our centroid baseline.
2914,1,Additional experiments we conducted with the old system replacing the SVM by the MAXENT classifier (without using ROUGE-W) indicate that the use of MAXENT by itself also improved slightly the results_comma_butthedifferencesaretoominortoshow; the improvement is mostly due to the use of ROUGEW instead of our previous measure.
2915,0,5 Related work Xu et al.(2004) use an information extraction engine to extract linguistic features from documents relevant to the target term.
2916,1,We use ROUGE-W to generate training examples from Web snippets and encyclopedias_comma_ a method that outperforms an alternative centroid-based one.
2917,0,The performance of our approach is evaluated against 300 unseen news articles and shows that use of these features results in statistically significant improvements over a provenly robust baseline_comma_ as measured using metrics such as precision_comma_ recall and ROUGE.
2918,0,More recently_comma_ the work of Svore et al.(2007) is closely related to our approach as it has also exploited the CNN Story Highlights_comma_ although their focus was on summarization and using ROUGE as an evaluation and training measure.
2919,0,System ROUGE-1 ROUGE-2 Baseline-fixed 47.73 15.98 AURUM-fixed 49.20 (+3.09%) 16.53 (+3.63%) Baseline-thresh 55.11 19.31 AURUM-thresh 56.73 (+2.96%) 19.66 (+1.87%) Table 6: ROUGE scores for AURUM-fixed_comma_ returning 4 sentences_comma_ and AURUM-thresh_comma_ returning between 3 and 6 sentences.
2920,0,ROUGE (Lin_comma_ 2004)_comma_ a recall-oriented evaluation package for automatic summarization.
2921,0,ROUGE operates essentially by comparing n-gram cooccurrences between a candidate summary and a number of reference summaries_comma_ and comparing that number in turn to the total number ofn-grams in the reference summaries: ROUGE-n = summationdisplay SReferences summationdisplay ngramnS Match(ngramn) summationdisplay SReferences summationdisplay ngramnS Count(ngramn) Where n is the length of the n-gram_comma_ with lengths of 1 and 2 words most commonly used in current evaluations.
2922,0,ROUGE has become the standard tool for evaluating automatic summaries_comma_ though it is not the optimal system for this experiment.
2923,0,This is due to the fact that it is geared towards a different taskas ours is not automatic summarization per seand that ROUGE works best judging between a number of candidate and model summaries.
2924,0,The ROUGE scores are shown in Table 6.
2925,0,Furthermore_comma_ in 76.2 percent of the documents_comma_ our systems ROUGE-1 score is higher than or equal to that of the baseline_comma_ and likewise for 85.2 percent of ROUGE-2 scores.
2926,0,Our ROUGE scores and their improvements over the baseline are comparable to the results of Svore et al.(2007)_comma_ who optimized their approach towards ROUGE and gained significant improvements from using third-party data resources_comma_ both of which our approach does not require.5 Table 7 shows the unique sentences extracted by every system_comma_ which are the number of sentences one system extracted correctly while the other did not; this is thus an intuitive measure of how much two systems differ.
2927,0,We use only the words that are content words (nouns_comma_ verbs_comma_ or adjectives) and not in the stopword list used in ROUGE (Lin_comma_ 2004).
2928,0,4.6 Weakly-constrained algorithms In evaluation with ROUGE (Lin_comma_ 2004)_comma_ summaries are truncated to a target length K. Yih et al.(2007)usedastackdecodingwithaslightmodication_comma_ which allows the last sentence in a summary to be truncated to a target length.
2929,1,ROUGE version 1.5.5 (Lin_comma_ 2004) was used for evaluation.2 Among others_comma_ we focus on ROUGE-1 in the discussion of the result_comma_ because ROUGE-1 has proved to have strong correlation with human annotation (Lin_comma_ 2004; Lin and Hovy_comma_ 2003).
2930,0,Wilcoxon signed rank test for paired samples with signicance level 0.05 was used for the signicance test of the difference in ROUGE1.
2931,0,The columns 1_comma_ 2_comma_ and SU4 in the tables respectively refer to ROUGE-1_comma_ ROUGE-2_comma_ and ROUGE-SU4.
2932,0,Table 1: ROUGE of MCKP with interpolated weights.
2933,0,Underlined ROUGE-1 scores are significantly different from the score of exact.
2934,0,ROUGE time 1 2 SU4 (sec) greedy 0.283 0.083 0.123 <0.01 g-greedy 0.294 0.080 0.121 0.01 rand100k 0.300 0.079 0.119 1.88 stack30 0.304 0.078 0.120 4.53 exact 0.305 0.081 0.121 4.04 Table 2: ROUGE of MCKP with trained weights.
2935,0,Underlined ROUGE-1 scores are signicantly differentfromthescoreof exact.
2936,0,ROUGE time 1 2 SU4 (sec) greedy 0.283 0.080 0.121 < 0.01 g-greedy 0.310 0.077 0.118 0.01 rand100k 0.299 0.077 0.117 1.93 stack30 0.309 0.080 0.120 4.23 exact 0.307 0.078 0.119 4.56 ence between exact and the other algorithms except for greedy and rand100k.
2937,0,The result suggests that approximate fast algorithms can yield results comparable to the exact method in terms of ROUGE-1 score.
2938,0,This table shows that the ROUGE1 value does not increase as the stacksize does; ROUGE-1 for stack with interpolated weights does not change much with the stacksize_comma_ and the peak of ROUGE-1 for trained weights is at the stacksize of 20.
2939,0,We also counted the number of the document clusters for which an approximate algorithm with interpolated weights yielded the same solution as 785 Table 3: ROUGE of stack with various stacksizes size 10 20 30 50 100 inter 0.304 0.304 0.304 0.304 0.303 train 0.308 0.310 0.309 0.308 0.307 Table 4: Search errors of MCKP with interpolated weights solution same search error ROUGE (=) =   greedy 0 1 35 14 g-greedy 0 5 26 19 rand100k 6 5 25 14 stack30 16 11 8 11 exact (same solution column in Table 4).
2940,0,If the approximate algorithm failed to yield the exact solution (search error column)_comma_ we checked whether the search error made ROUGE score unchanged (= column)_comma_ decreased ( column)_comma_ or increased ( column) compared with ROUGE score of exact.
2941,0,Table 4 shows that (i) stack30 is a better optimizer than other approximate algorithms_comma_ (ii) when the search error occurs_comma_ stack30 increases ROUGE-1 more often than it decreases ROUGE-1 compared with exact in spite of stack30s inaccurate solution_comma_ (iii) approximate algorithms sometimes achieved better ROUGE scores.
2942,0,We also empirically showed that our method outperforms McDonald (2007)s method in experiments on DUC02_comma_ where our method achieved 0.354 ROUGE-1 score with interpolated weights and 0.359 with trained weights when the optimal  is given_comma_ while McDonald (2007)s method yielded at most 0.348.
2943,0,However_comma_ this very point can also 786 Table 5: ROUGE-1 of MCKP-Rel with interpolated weights.
2944,0,Specically_comma_ we conducted experiments on DUC03 with different  ( {0.0_comma_0.1_comma__comma_1.0}) and simply selected the one with the highest ROUGE-1 value.
2945,0,Only ROUGE-1 values are shown.
2946,0,All methods except for greedy yielded signicantly better ROUGE values compared with the corresponding results in Tables 1 and 2.
2947,0,Figures 1 and 2 show ROUGE-1 for different values of .
2948,0,0.28  0.29  0.3  0.31  0.32  0.33  0.34  0  0.2  0.4  0.6  0.8  1 ROUGE-1 lambda exactstack30 rand100kg-greedy greedy Figure 1: MCKP-Rel with interpolated weights  0.28  0.29  0.3  0.31  0.32  0.33  0.34  0  0.2  0.4  0.6  0.8  1 ROUGE-1 lambda exactstack30 rand100kg-greedy greedy Figure 2: MCKP-Rel with trained weights 6.2.2 Experiments with the optimal  In the experiments above_comma_ we found that  = 0.2 is the optimal value for exact with interpolated weights.
2949,0,We obtained Table 6_comma_ which shows that search errors in MCKP-Rel counterintuitively increase () ROUGE-1 score less often than MCKP did in Table 4.
2950,0,solution same search error ROUGE (=) =   greedy 0 2 42 6 g-greedy 1 0 34 15 rand100k 3 6 33 8 stack30 14 13 14 10 6.2.3 Comparison with DUC results In Section 6.2.1_comma_ we empirically showed that the augmented model MCKP-Rel is better than MCKP_comma_ whose optimization problem is used also in one of the state-of-the-art methods by Yih et al.(2007).
2951,0,We compared the MCKP-Rel with peer65 (Conroy et al._comma_ 2004) of DUC04_comma_ which performed best in terms of ROUGE-1 in the competition.
2952,0,Tables 7 and 8 are the ROUGE-1 scores_comma_ respectively evaluatedwithoutandwithstopwords.
2953,0,Table 7: ROUGE-1 of MCKP-Rel with byte constraints_comma_ evaluated without stopwords.
2954,0,interpolated train greedy 0.289 (0.1) 0.284 (0.8) g-greedy 0.297 (0.4) 0.323 (0.3) rand100k 0.315 (0.2) 0.308 (0.4) stack30 0.324 (0.2) 0.323 (0.3) exact 0.325 (0.3) 0.326 (0.5) exactopt 0.325 (0.3) 0.329 (0.4) peer65 0.309 In Table 7_comma_ MCKP-Rel with stack30 and exact yielded signicantly better ROUGE-1 scores than peer65.
2955,0,Although stack30 and exact yielded greater ROUGE-1 scores than peer65 also in Table 8_comma_ the difference was not signicant.
2956,0,Only greedy was signicantly worse than peer65.3 One 3We actually succeeded in greatly improving the ROUGE-1 value of MCKP-Rel evaluated with stopwords by using all the words including stopwords as conceptual units.
2957,0,However_comma_ we ignore those results in this paper_comma_ because it Table 8: ROUGE-1 of MCKP-Rel with byte constraints_comma_ evaluated with stopwords.
2958,0,At the task of producing generic DUC-style summaries_comma_ HIERSUM yields state-of-the-art ROUGE performance and in pairwise user evaluation stronglyoutperformsToutanovaetal.(2007)s state-of-the-art discriminative system.
2959,0,Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin_comma_ 2004) which representsrecallovervariousn-gramsstatisticsfrom asystem-generatedsummaryagainstasetofhumangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries.
2960,0,Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin_comma_ 2004).
2961,0,Despite its simplicity_comma_ SUMBASIC yields 5.3 R-2 without stop words on DUC 2006 (see table 1).8 By comparison_comma_ the highest-performing ROUGE system at the DUC 2006 evaluation_comma_ SUMFOCUS_comma_ was built on top of SUMBASIC and yielded a 6.0_comma_ which is not a statistically significant improvement (Vanderwende et al._comma_ 2007).9 Intuitively_comma_ SUMBASIC is trying to select a summary which has sentences where most words have high likelihood under the document set unigram distribution.
2962,0,8This result is presented as 0.053 with the official ROUGE scorer (Lin_comma_ 2004).
2963,0,9To be fair obtaining statistical significance in ROUGE scores is quite difficult.
2964,0,System ROUGE-stop ROUGEall R-1 R-2 R-SU4 R-1 R-2 R-SU4 SUMBASIC 29.6 5.3 8.6 36.1 7.1 12.3 KLSUM 30.6 6.0 8.9 38.9 8.3 13.7 TOPICSUM 31.7 6.3 9.1 39.2 8.4 13.6 HIERSUM 30.5 6.4 9.2 40.1 8.6 14.3 Table 1: ROUGE results on DUC2006 for models presented in section 3.
2965,0,While not a statistically significant improvementinROUGEover TOPICSUM_comma_wefoundthe summaries to be noticeably improved.
2966,0,These parameters wereminimallytuned(withoutreferencetoROUGE results) in order to ensure that all topic distribution behaved as intended.
2967,0,Although the differences in ROUGE between HIERSUM and TOPICSUM were minimal_comma_ we found HIERSUM summary quality to be stronger.
2968,0,In order to provide a reference for ROUGE and manual evaluation results_comma_ we compare against PYTHY_comma_ a state-of-the-art supervised sentence extraction summarization system.
2969,0,PYTHY uses humangenerated summaries in order to train a sentence ranking system which discriminatively maximizes 22Since the ROUGE evaluation metric is recall-oriented_comma_ it is always advantageous with respect to ROUGE to use all 250 words.
2970,0,367 System ROUGE w/o stop ROUGE w/ stop R-1 R-2 R-SU4 R-1 R-2 R-SU4 HIERSUM unigram 34.6 7.3 10.4 43.1 9.7 15.3 HIERSUM bigram 33.8 9.3 11.6 42.4 11.8 16.7 PYTHY w/o simp 34.7 8.7 11.8 42.7 11.4 16.5 PYTHY w/ simp 35.7 8.9 12.1 42.6 11.9 16.8 Table 3: Formal ROUGE experiment results on DUC 2007 document set collection (see section 5.1).
2971,0,ROUGE scores.
2972,0,At DUC 2007_comma_ PYTHY wasrankedfirstoverallinautomaticROUGE evaluation and fifth in manual content judgments.
2973,0,5.1 ROUGE Evaluation ROUGEresultscomparingvariantsof HIERSUM and PYTHY are given in table 3.
2974,0,We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance.
2975,0,The reason for this might be that PYTHY is discriminatively trained to maximize ROUGE which does not directly penalize redundancy.
2976,0,We apply four different summarization techniques to data in the ACL Anthol584 ogy and evaluate our results using both automatic (ROUGE) and human-mediated (nugget-based pyramid) measures.
2977,0,Our aim is not only to determine the utility of citation texts for survey creation_comma_ but also to examine the quality distinctions between this form of input and others such as abstracts and full textscomparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman_comma_ 2006; Nenkova and Passonneau_comma_ 2004; Lin_comma_ 2004).
2978,0,For this we evaluated each of the automatically generated surveys using two separate approaches: nugget-based pyramid evaluation and ROUGE (described in the two subsections below).
2979,0,6.1.1 Nugget-Based Pyramid Evaluation For our first approach we used a nugget-based evaluation methodology (Lin and Demner-Fushman_comma_ 2006; Nenkova and Passonneau_comma_ 2004; Hildebrandt et al._comma_ 2004; Voorhees_comma_ 2003).
2980,0,0.1550 0.1259 0.1200 0.1654 0.1416 Table 4: ROUGE-2 scores obtained for each of the manually created surveys by using the other three as reference.
2981,0,6.1.2 ROUGE evaluation Table 4 presents ROUGE scores (Lin_comma_ 2004) of each of human-generated 250-word surveys against each other.
2982,0,Table 5 lists ROUGE scores of surveys when the manually created 250-word survey of the QA citation texts_comma_ survey of the QA abstracts_comma_ and the survey of the DP citation texts_comma_ were used as gold standard.
2983,0,When we use manually created citation text surveys as reference_comma_ then the surveys generated from citation texts obtained significantly better ROUGE scores than the surveys generated from abstracts and full papers (p < 0.05) [RESULT 1].
2984,0,When we use manually created abstract surveys as reference_comma_ then the surveys generated from abstracts obtained significantly better ROUGE scores than the surveys generated from citation texts and full papers (p < 0.05) [RESULT 3].
2985,0,This suggests that Trim590 SystemPerformance:ROUGE-2 Random C-LexRank C-RR LexRank Trimmer Input:QAcitationsurveys QACTrefs.
2986,0,0.03770 0.02511 0.03433 * 0.04554 Table 5: ROUGE-2 scores of automatic surveys of QA and DP data.
2987,0,We then used two separate approaches_comma_ nugget-based pyramid and ROUGE_comma_ to evaluate the surveys.
2988,0,The difference between DEPEVAL(summ) and BE is that in DEPEVAL(summ) the dependency extraction is accomplished through an LFG annotation of Cahill et al.(2004) applied to the output of the reranking parser of Charniak and Johnson (2005)_comma_ whereas in BE (in the version presented here) dependencies are generated by the Minipar parser (Lin_comma_ 1995).
2989,0,Despite relying on a the same concept_comma_ our approach outperforms BE in most comparisons_comma_ and it often achieves higher correlations with human judgments than the string-matching metric ROUGE (Lin_comma_ 2004).
2990,0,A more detailed description of BE and ROUGE is presented in Section 2_comma_ which also gives an account of manual evaluation methods employed at TAC 2008.
2991,0,In TAC 2008 Summarization track_comma_ all submitted runs were scored with the ROUGE (Lin_comma_ 2004) and Basic Elements (BE) metrics (Hovy et al._comma_ 2005).
2992,0,ROUGE is a collection of string-comparison techniques_comma_ based on matching n-grams between a candidate string and a reference string.
2993,0,We compute the similarity of a large number of articles produced by our system and several baselines to the original human-authored articles using ROUGE_comma_ a standard metric for summary quality.
2994,0,These domains have been commonly used in prior work on summarization (Weischedel et al._comma_ 2004; Zhou et al._comma_ 2004; Filatova and Prager_comma_ 2005; DemnerFushman and Lin_comma_ 2007; Biadsy et al._comma_ 2008).
2995,0,We use ROUGE_comma_ an evaluation metric employed at the Document Understanding Conferences (DUC)_comma_ which assumes that proximity to human-authored text is an indicator of summary quality.
2996,0,We use the publicly available ROUGE toolkit (Lin_comma_ 2004)tocomputerecall_comma_ precision_comma_ andF-scorefor ROUGE-1.
2997,0,As such_comma_ we quantify success based on ROUGE (Lin_comma_ 2004) scores.
2998,0,Figure 2 shows ROUGE scores for our acoustics-only system_comma_ as depicted by horizontal lines_comma_ as well as those for the extractive summaries given automatic transcripts having different WERs_comma_ as depicted by points.
2999,0,The consistency of this trend is shown across both ROUGE-2 and ROUGE-SU4_comma_ which are the official measures used in the DUC evaluation.
3000,0,Figure 2 also shows the ROUGE scores achievable by selecting utterances uniformly at random for extractive summarization_comma_ which are significantly lower than all other presented methods and corroborate the usefulness of acoustic information.
3001,0,Although our acoustics-based method performs similarly to automatic transcripts with 33-37% WER_comma_ the errors observed are not the same_comma_ which 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=10% Rand=0.197 ROUGESU4 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=20%_comma_ Rand=0.340 ROUGESU4 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=30%_comma_ Rand=0.402 ROUGESU4 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=10%_comma_ Rand=0.176 ROUGE2 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=20%_comma_ Rand=0.324 ROUGE2 Word error rate 0 0.1 0.2 0.3 0.4 0.50.7 0.75 0.8 0.85 0.9 0.95 1 Len=30%_comma_ Rand=0.389 ROUGE2 Word error rate Figure 2: ROUGE scores and 95% confidence intervals for the MMR-based extractive summaries produced from our acoustics-only approach (horizontal lines)_comma_ and from ASR-generated transcripts having varying WER (points).
3002,0,Automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields: widely used measures include BLEU (Papineni et al._comma_ 2002) for machine translation and ROUGE (Lin_comma_ 2004) for summarisation_comma_ for example.
3003,0,When employing any such metric_comma_ it is crucial to verify that the predictions of the automated evaluation process agree with human judgements of the important aspects of the system output.
3004,0,ROUGE (Lin_comma_ 2004) is an evaluation metric designed to evaluate automatically generated summaries.
3005,0,It comprises a number of string comparison methods including ngram matching and skip-ngrams.
3006,0,We use the default ROUGE-L longest common subsequence f-score measure.2 GTM General Text Matching (Melamed et al._comma_ 2003) calculates word overlap between a reference and a solution_comma_ without double counting duplicate words.
3007,0,It places less importance on word order than BLEU.
3008,0,Experiment A Experiment B GOLD LM LL LL human A (rank 13) 1.4 2.55 2.05 human B (scale 15) 3.92 BLEU 1.0 0.67 0.72 0.79 ROUGE-L 1.0 0.85 0.78 0.85 GTM 1.0 0.55 0.60 0.74 SED 1.0 0.54 0.61 0.71 WER 0.0 48.04 39.88 28.83 TER 0.0 0.16 0.14 0.11 DEP 100 82.60 87.50 93.11 WDEP 1.0 0.70 0.82 0.90 Table 1: Average scores of each metric for Experiment A data Sentence Corpus corr p-value corr p-value BLEU -0.615 <0.001 -1 0.3333 ROUGE-L -0.644 <0.001 -0.5 1 GTM -0.643 <0.001 -1 0.3333 SED -0.628 <0.001 -1 0.3333 WER 0.623 <0.001 1 0.3333 TER 0.608 <0.001 1 0.3333 Table 2: Correlation between human judgements for experiment A (rank 13) and automatic metrics for attempting to account for different but equivalent translations of a given source word_comma_ typically by integrating a lexical resource such as WordNet.
3009,0,Interestingly_comma_ the ROUGE-L metric is the only one that does not rank the output of the three systems in the same order as the judges.
3010,0,It ranks the strings chosen by the language model higher than the strings chosen by the log-linear model.
3011,1,However_comma_ at the level of the individual sentence_comma_ the ROUGE-L metric correlates best with the human judgements.
3012,0,The GTM metric correlates at about the same level_comma_ but in general there seems to be little difference between the metrics.
3013,0,For the Experiment B data we use the Pearson correlation coefficient to measure the correlation between the human judgements and the automatic 98 Sentence Correlation P-Value BLEU 0.095 0.5048 ROUGE-L 0.207 0.1417 GTM 0.424 0.0017 SED 0.168 0.2344 WER -0.188 0.1817 TER -0.024 0.8646 Table 3: Correlation between human judgements for experiment B (naturalness scale 15) and automatic metrics metrics.
3014,0,This does not correspond to related work on English Summarisation evaluation (Owczarzak_comma_ 2009) which shows that a metric based on an automatically induced LFG parser for English achieves comparable or higher correlation with human judgements than ROUGE and Basic Elements (BE).4 Parsers of German typically do not achieve as high performance as their English counterparts_comma_ and further experiments including alternative parsers are needed to see if we can improve performance of this metric.
3015,1,We found that for our first experiment_comma_ all metrics were correlated to roughly the same degree (with ROUGE-L achieving the highest correlation at an individual sentence level and the GTM tool not far behind).
3016,0,At a corpus level all except ROUGE were in agreement with the human judgements.
3017,0,Table 2 shows various systems with their ranks based on ROUGE-2 and the average log-likelihood scores.
3018,1,The ROUGE (Lin_comma_ 2004) suite of metrics are n-gram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness.
3019,0,ROUGE-2 and ROUGE-SU4 are the official ROUGE metrics for evaluating query-focused multi-document summarization task since DUC 2005.
3020,0,ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2 1 31 -1.9842 0.06039 J -3.9465 0.13904 24 4 -5.8451 0.11793 C -2.1387 0.15055 E -3.9485 0.13850 9 12 -5.9049 0.10370 16 32 -2.2906 0.03813 10 28 -4.0723 0.07908 14 14 -5.9860 0.10277 27 30 -2.4012 0.06238 21 22 -4.2460 0.08989 5 23 -6.0464 0.08784 6 29 -2.5536 0.07135 G -4.3143 0.13390 4 3 -6.2347 0.11887 12 25 -2.9415 0.08505 25 27 -4.4542 0.08039 20 6 -6.3923 0.10879 I -3.0196 0.13621 B -4.4655 0.13992 29 2 -6.4076 0.12028 11 24 -3.0495 0.08678 19 26 -4.6785 0.08453 3 9 -7.1720 0.10660 28 16 -3.1932 0.09858 26 21 -4.7658 0.08989 8 11 -7.4125 0.10408 2 18 -3.2058 0.09382 23 7 -5.3418 0.10810 17 15 -7.4458 0.10212 D -3.2357 0.17528 30 10 -5.4039 0.10614 13 5 -7.7504 0.11172 H -3.4494 0.13001 7 8 -5.6291 0.10795 32 17 -8.0117 0.09750 A -3.6481 0.13254 18 19 -5.6397 0.09170 22 13 -8.9843 0.10329 F -3.8316 0.13395 15 1 -5.7938 0.12448 31 20 -9.0806 0.09126 Table 2: Rank_comma_ Averaged log-likelihood score based on binomial model_comma_ true ROUGE-2 score for the summaries of various systems in DUC07 query-focused multi-document summarization task.
3021,0,ID rank LL ROUGE-2 ID rank LL ROUGE-2 ID rank LL ROUGE-2 1 31 -4.6770 0.06039 10 28 -8.5004 0.07908 5 23 -14.3259 0.08784 16 32 -4.7390 0.03813 G -9.5593 0.13390 9 12 -14.4732 0.10370 6 29 -5.4809 0.07135 E -9.6831 0.13850 22 13 -14.8557 0.10329 27 30 -5.5110 0.06238 26 21 -9.7163 0.08989 4 3 -14.9307 0.11887 I -6.7662 0.13621 J -9.8386 0.13904 18 19 -15.0114 0.09170 12 25 -6.8631 0.08505 19 26 -10.3226 0.08453 14 14 -15.4863 0.10277 2 18 -6.9363 0.09382 B -10.4152 0.13992 20 6 -15.8697 0.10879 C -7.2497 0.15055 25 27 -10.7693 0.08039 32 17 -15.9318 0.09750 H -7.6657 0.13001 29 2 -12.7595 0.12028 7 8 -15.9927 0.10795 11 24 -7.8048 0.08678 21 22 -13.1686 0.08989 17 15 -17.3737 0.10212 A -7.8690 0.13254 24 4 -13.2842 0.11793 8 11 -17.4454 0.10408 D -8.0266 0.17528 30 10 -13.3632 0.10614 31 20 -17.5615 0.09126 28 16 -8.0307 0.09858 23 7 -13.7781 0.10810 3 9 -19.0495 0.10660 F -8.2633 0.13395 15 1 -14.2832 0.12448 13 5 -19.3089 0.11172 Table 3: Rank_comma_ Averaged log-likelihood score based on multinomial model_comma_ true ROUGE-2 score for the summaries of various systems in DUC07 query-focused multi-document summarization task.
3022,0,Table 3 shows various systems with their ranks based on ROUGE-2 and the average loglikelihood scores.
3023,0,3.3 Correlation of ROUGE and log-likelihood scores Tables 2 and 3 display log-likelihood scores of various systems in the descending order of log-likelihood scores along with their respective ROUGE-2 scores.
3024,0,We computed the pearson correlation coefficient () of ROUGE-2 and log-likelihood and ROUGE-SU4 and log-likelihood.
3025,0,This clearly indicates that there is a strong negative correlation between likelihood of occurrence of a non-query-term and ROUGE-2 score.
3026,0,That is_comma_ a strongpositivecorrelationbetweenlikelihoodofoccur107 rence of a query-term and ROUGE-2 score.
3027,0,Similarly_comma_ for human summarizers there is a weak negative correlation between likelihood of occurrence of a queryterm and ROUGE-2 score.
3028,0,The same correlation analysis applies to ROUGE-SU4 scores: r1 = -0.66 and r2 = 0.38.
3029,0,Tables 4 and 5 show the correlation among ROUGE-2 and log-likelihood scores for systems2 and humans3.
3030,0,ROUGE-2 ROUGE-SU4 binomial -0.66 -0.66 multinomial -0.73 -0.73 Table 4: Correlation of ROUGE measures with loglikelihood scores for automated systems  ROUGE-2 ROUGE-SU4 binomial 0.39 0.38 multinomial 0.15 0.09 Table 5: Correlation of ROUGE measures with loglikelihood scores for humans 4 Conclusions and Discussion Our results underscore the differences between human and machine generated summaries.
3031,0,We further confirm based on the likelihood of emitting non query-biased sentence_comma_ that there is a strong (negative) correlation among systems likelihood score and ROUGE score_comma_ which suggests that systems are trying to improve performance based on ROUGE metrics by being biased towards the query terms.
3032,0,Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries_comma_ using sentence compression on the extractive summaries improves their ROUGE scores; however_comma_ the best performance is still quite low_comma_ suggesting the need of language generation for abstractive summarization.
3033,0,Our experiments using the ICSI meeting corpus show that compressing extractive summaries can improve human readability and the ROUGE scores against the reference abstractive summaries.
3034,0,For a comparison_comma_ we also include the ROUGE-1 Fscores (Lin_comma_ 2004) of each system output against the human compressed sentences.
3035,0,Also shown is the ROUGE-1 (unigram match) F-score of different systems compared to human compression.
3036,0,The ROUGE-1 results along with the word compression ratio for each compression approach are shown in Table 2.
3037,0,We can see that all of the compression algorithms yield better ROUGE score than the original extractive summaries.
3038,0,We also observe some different patterns between the ROUGE scores and the human evaluation results in Table 1.
3039,0,For example_comma_ Markov (S2) has the highest ROUGE result_comma_ but worse human evaluation score than other methods.
3040,0,Approach Word ratio (%) P(%) R(%) F(%) P(%) R(%) F(%) Original extractive summary 100 7.58 66.06 12.99 29.98 34.29 31.83 Human compression 65.58 10.43 63.00 16.95 34.35 37.39 35.79 Markov (S1) 67.67 10.15 61.98 16.41 34.24 36.88 35.46 Markov (S2) 53.28 11.90 58.14 18.37 32.23 34.96 33.49 FP + IP 76.38 9.11 59.85 14.78 31.82 35.62 33.57 Table 2: Compression ratio of different systems and ROUGE-1 scores compared to human abstractive summaries.
3041,0,the extractive summaries more like abstractive summaries_comma_ we conduct an oracle experiment: we compute the ROUGE score for each of the extractive summary sentences (the original sentence or the compressed sentence) against the abstract_comma_ and select the sentences with the highest scores until the number of selected words is about the same as that in the abstract.6 The ROUGE results using these selected top sentences are shown in the right part of Table 2.
3042,0,Moreover_comma_ the current ROUGE score is not very high.
3043,0,In this paper_comma_ we use ROUGE similarity measure_comma_ Basic Element (BE) overlap_comma_ syntactic similarity measure_comma_ semantic similarity measure_comma_ and Extended String Subsequence Kernel (ESSK) to automatically label the corpora of sentences (DUC-2006 data) into extract summary or non-summary categories in correspondence with the document abstracts.
3044,0,2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N (N=1_comma_2_comma_3_comma_4)_comma_ ROUGE-L_comma_ ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram_comma_ word-sequences_comma_ and word-pairs between the extract and the abstract summaries (Lin_comma_ 2004).
3045,0,We assume each individual document sentence as the extract summary and calculate its ROUGE similarity scores with the corresponding abstract summaries.
3046,0,Thus an average ROUGE score is assigned to each sentence in the document.
3047,0,Supervised Systems For SVM we use second order polynomial kernel for the ROUGE and ESSK labeled training.
3048,0,We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE (Lin_comma_ 2004).
3049,1,We report the three widely adopted important ROUGE metrics in the results: ROUGE-1 (unigram)_comma_ ROUGE-2 (bigram) and ROUGE-SU (skip bi-gram).
3050,0,Figure 1 shows the ROUGE F-measures for SVM_comma_ HMM_comma_ CRF and MaxEnt systems.
3051,0,The X-axis containing ROUGE_comma_ BE_comma_ Synt (Syntactic)_comma_ Sem (Semantic)_comma_ and ESSK stands for the annotation scheme used.
3052,0,The Y-axis shows the ROUGE1 scores at the top_comma_ ROUGE-2 scores at the bottom and ROUGE-SU scores in the middle.
3053,0,From the figure_comma_ we can see that the ESSK labeled SVM system is having the poorest ROUGE 1 score whereas the Sem labeled system performs 8http://www.cs.ualberta.ca/lindek/downloads.htm 331 Figure 1: ROUGE F-scores for different supervised systems best.
3054,0,The other annotation methods impact is almost similar here in terms of ROUGE-1.
3055,0,Analyzing ROUGE-2 scores_comma_ we find that the BE performs the best for SVM_comma_ on the other hand_comma_ Sem achieves top ROUGE-SU score.
3056,0,As for the two measures Sem annotation is performing the best_comma_ we can typically conclude that Sem annotation is the most suitable method for the SVM system.
3057,0,ESSK works as the best for HMM and Sem labeling performs the worst for all ROUGE scores.
3058,0,Synt and BE labeled HMMs perform almost similar whereas ROUGE labeled system is pretty close to that of ESSK.
3059,0,Again_comma_ we see that the CRF performs best with the ESSK annotated data in terms of ROUGE -1 and ROUGE-SU scores and Sem has the highest ROUGE-2 score.
3060,0,But BE and Synt labeling work bad for CRF whereas the ROUGE labeling performs decently.
3061,0,We can also see that ROUGE_comma_ Synt and Sem labeled MaxEnt systems perform almost similar.
3062,0,The summaries from the above algorithm for the QF-MDS were evaluated based on ROUGE metrics (Lin_comma_ 2004).
3063,0,The average4 recall scores are reported for ROUGE-2 and ROUGE-SU4 in Table 1.
3064,0,system ROUGE-2 ROUGE-SU4 first x words baseline 0.06039 0.10507 generic baseline 0.09382 0.14641 SPP algorithm 0.06913 0.12492 system 15 (top system) 0.12448 0.17711 Table 1: ROUGE 2_comma_ SU4 Recall scores for two baselines_comma_ the SPP algorithm and a top performing system at Query-Focused Multi-Document Summarization task_comma_ DUC 2007.
3065,0,Summaries from the above algorithm for the Query Focused Update Summarization task were evaluated based on ROUGE metrics.
3066,0,The rouge scores suggest that this algorithm is well above the median for cluster A and among the top 5 systems for cluster B. It must be noted that consistent performance across clusters (both A and B) shows the robustness of the SPP algorithm at the update summarization task.
3067,0,These surprisingly high scores on ROUGE metrics prompted us to evaluate the summaries based on PyramidEvaluation(Nenkovaetal._comma_2007).
3068,0,System 43 was adjudged best system based on ROUGE metrics_comma_ and system 11 was top performer based on pyramid evaluations at TAC 2008.
3069,0,ROUGE-2 ROUGE-SU4 pyramid cluster A 0.08987 0.1213 0.3432 cluster B 0.09319 0.1283 0.3576 Table 2: Cluster wise ROUGE 2_comma_ SU4 Recall scores and modifiedPyramidScoresforSPPalgorithmattheUpdate Summarization task.
3070,0,50 system ROUGE-2 ROUGE-SU4 pyramid first x words baseline 0.05896 0.09327 0.166 SPP algorithm 0.09153 0.1245 0.3504 System 43 (top in ROUGE) 0.10395 0.13646 0.289 System 11 (top in pyramid) 0.08858 0.12484 0.336 Table 3: Average ROUGE 2_comma_ SU4 Recall scores and modified Pyramid Scores for baseline_comma_ SPP algorithm and two top performing systems at TAC 2008.
3071,0,Note that bigrams gave consistently better performance than unigrams or trigrams for a variety of ROUGE measures.
3072,0,Normalizing by document frequency measured over a generic set (TFIDF weighting) degraded ROUGE performance.
3073,0,The summaries produced by the two systems have been evaluated automatically with ROUGE and manually with the Pyramid metric.
3074,0,In particular_comma_ ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin_comma_ 2004).
3075,0,Nonetheless_comma_ our system based on word bigram concepts_comma_ similarly primitive_comma_ performed at least as well as any in the TAC evaluation_comma_ according to two-tailed t-tests comparing ROUGE_comma_ Pyramid_comma_ and manually evaluated content responsiveness (Dang and Owczarzak_comma_ 2008) of our system and the highest scoring system in each category.
3076,0,System ROUGE-2 Pyramid Baseline 0.058 0.186 McDonald 0.072 0.295 Concepts 0.110 0.345 Table 2: Scores for both systems and a baseline on TAC 2008 data (Set A) for ROUGE-2 and Pyramid evaluations.
3077,0,A greedy approximation of our objective function gives 10% lower ROUGE scores than the exact solution_comma_ a gap that separates the highest scoring systems from the middle of the pack in the TAC evaluation.
3078,0,in ROUGE-2 score (see Table 3)_comma_ but a reduction in Pyramid score.
3079,0,In ROUGE and Pyramid evaluation_comma_ our system significantly outperformed McDonalds ILP system.
3080,0,4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans_comma_ which is the view taken by existing summarization evaluation schemes such as ROUGE (Lin_comma_ 2004) and the Pyramid method (Nenkova et al._comma_ 2007).
3081,0,This view is supported by Lin (2004a)_comma_ who concludes that correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative.
3082,0,For our experiments_comma_ we used data from the Summarization track at the Text Analysis Conference (TAC) 2008_comma_ where participating systems were assessed on their summarization of 48 topics_comma_ and the automatic metrics ROUGE and BE_comma_ as well as the manual Pyramid evaluation method_comma_ had access to 4 human models.
3083,0,Despite the lack of full Pyramid evaluation in DUC 2007_comma_ we look at the remaining metrics applied that year (ROUGE_comma_ BE_comma_ and Content Responsiveness)_comma_ in order to see whether they confirm the insights gained from the TAC 2008 data.
3084,0,All submitted runs were evaluated with the automatic metrics: ROUGE (Lin_comma_ 2004b)_comma_ which calculates the proportion of n-grams shared between the candidate summary and the reference summaries_comma_ and Basic Elements (Hovy et al._comma_ 2005)_comma_ which compares the candidate to the models in terms of head-modifier pairs.
3085,0,The stability_comma_ similarly to the method employed by Voorhees and Buckley (2002) for Information Retrieval_comma_ is determined by how well a system ranking based on a small number of models/topics cor24 Models Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.8839 0.8032 0.7842 0.7680 2 0.8943 0.8200 0.7957 0.7983 3 0.8974* 0.8258 0.7999* 0.8098 4 (bootstr) 0.8972* 0.8310 0.8023* 0.8152 4 (actual) 0.8997 0.8302 0.8033 0.8171 Table 1: Mean correlations of Responsiveness and other metrics using 1_comma_ 2_comma_ 3_comma_ or 4 models for TAC 2008 initial summaries.
3086,0,Models Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.9315 0.8861 0.8874 0.8716 2 0.9432 0.9013 0.8961 0.8978 3 0.9474* 0.9068* 0.8994 0.9076 4 (bootstr) 0.9481* 0.9079* 0.9023 0.9114 4 (actual) 0.9492 0.9103 0.9020 0.9132 Table 2: Mean correlations of Responsiveness and other metrics using 1_comma_ 2_comma_ 3_comma_ or 4 models for TAC 2008 update summaries.
3087,0,Values in each row are significantly different from each other at 95% level except ROUGE-2 and ROUGE-SU4 in 1-model category.
3088,0,Models ROUGE-2 ROUGE-SU4 BE 1 0.8789 0.8671 0.8553 2 0.8972 0.8803 0.8917 3 0.9036 0.8845 0.9048 4 (bootstr) 0.9082 0.8874 0.9107 4 (actual) 0.9077 0.8877 0.9123 Table 3: Mean correlations of 4-model Pyramid score and other metrics using 1_comma_ 2_comma_ 3_comma_ or 4 models for TAC 2008 initial summaries.
3089,0,Values in each row are significantly different from each other at 95% level except ROUGE-2 and BE in 4-model category.
3090,0,Models ROUGE-2 ROUGE-SU4 BE 1 0.9179 0.9110 0.9016 2 0.9336 0.9199 0.9284 3 0.9392 0.9233 0.9383 4 (bootstr) 0.9443 0.9277 0.9436 4 (actual) 0.9429 0.9263 0.9446 Table 4: Mean correlations of 4-model Pyramid score and other metrics using 1_comma_ 2_comma_ 3_comma_ or 4 models for TAC 2008 update summaries.
3091,0,Values in each row are significantly different from each other at 95% level except ROUGE-2 and BE in 4-model category.
3092,0,2.2 Automatic metrics Similarly to the Pyramid method_comma_ ROUGE (Lin_comma_ 2004b) and Basic Elements (Hovy et al._comma_ 2005) require multiple topics and model summaries to produce optimal results.
3093,0,ROUGE is a collection of automatic n-gram matching metrics_comma_ ranging from unigram to four-gram.
3094,0,It also includes measurements of the longest common subsequence_comma_ weighted or unweighted_comma_ and the option to compare stemmed versions of words and omit stopwords.
3095,0,The skip-n-grams together with stemming are the only ways ROUGE can accomodate alternative forms of expression and match concepts even though they might differ in terms of their syntactic or lexical form.
3096,0,These methods are necessarily limited_comma_ and so ROUGE relies on using multiple parallel model summaries which serve as a source of lexical/syntactic variation in the comparison process.
3097,0,Our question here is not only what this relation looks like (as it was examined on the basis of Document Understanding Conference data in Lin (2004a))_comma_ but also how it compares to the reliability of other metrics.
3098,0,TAC 2008 and DUC 2007 evaluations used ROUGE-2 and ROUGE-SU4_comma_ which refer to the recall of bigram and skip-bigram (with up to 4 intervening words) matches on stemmed words_comma_ respectively_comma_ as well as a BE score calculated on the basis of stemmed head-modifier pairs without relation labels.
3099,0,The increase in correlation mean is most dramatic for BE_comma_ which in all cases starts as the lowest26 correlating metric in the single-model category_comma_ but by the 4-model point it outperforms one or both versions of ROUGE.
3100,0,Of the two ROUGE versions_comma_ ROUGE-2 seems consistently a better predictor of both Responsiveness and the full 4model Pyramid score than ROUGE-SU4.
3101,0,ROUGE is situated somewhere between the two_comma_ noting small but often significant increases.
3102,0,In a process which mirrored that described in Section 3_comma_ we created 1000 random samples in each of the n-topics category: 1_comma_ 3_comma_ 6_comma_ 12_comma_ 24_comma_ 36_comma_ 27 Topics Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.4219 0.4276 0.4375 0.3506 3 0.6204 0.5980 0.9016 0.5108 6 0.7274 0.6901 0.6836 0.6233 12 0.8159 0.7618 0.7456 0.7117 24 0.8679 0.8040 0.7809 0.7762 36 0.8890* 0.8208* 0.7951* 0.8017* 39 0.8927* 0.8231* 0.7967* 0.8063* 42 0.8954* 0.8258* 0.7958* 0.8102* 45 0.8977* 0.8274* 0.8008* 0.8132 48 (bootstr) 0.8972* 0.8302* 0.8046 0.8138 48 (actual) 0.8997 0.8302 0.8033 0.8171 Table 7: Mean correlations of 48 topic Responsiveness and other metrics using from 1 to 48 topics for TAC 2008 initial summaries.
3103,0,Values in each row are significantly different from each other at 95% level except: ROUGE-2_comma_ ROUGE-SU4 and BE in 1-topic category_comma_ ROUGE-2 and ROUGE-SU4 in 3and 6-topic category.
3104,0,Topics Pyramid ROUGE-2 ROUGE-SU4 BE 1 0.5005 0.4882 0.5609 0.4011 3 0.7053 0.6862 0.7340 0.6097 6 0.8080 0.7850 0.8114 0.7274 12 0.8812 0.8498 0.8596 0.8188 24 0.9250 0.8882 0.8859 0.8774 36 0.9408* 0.9023* 0.8960* 0.8999* 39 0.9433* 0.9045* 0.8973* 0.9037* 42 0.9455* 0.9061* 0.8987* 0.9068* 45 0.9474 0.9078* 0.8996* 0.9094 48 (bootstr) 0.9481 0.9101 0.9015* 0.9111 48 (actual) 0.9492 0.9103 0.9020 0.9132 Table 8: Mean correlations of 48 topic Responsiveness and other metrics using from 1 to 48 topics for TAC 2008 update summaries.
3105,0,Values in each row are significantly different from each other at 95% level except: Pyramid and ROUGE-2 in 1topic category_comma_ Pyramid and ROUGE-SU4 in 6-topic category_comma_ ROUGE-2 and BE in 39-_comma_ 42-_comma_ and 48-topic category.
3106,0,Topics ROUGE-2 ROUGE-SU4 BE 1 0.4693 0.4856 0.3888 3 0.6575 0.6684 0.5732 6 0.7577 0.7584 0.6960 12 0.8332 0.8245 0.7938 24 0.8805 0.8642 0.8684 36 0.8980* 0.8792* 0.8966* 39 0.9008* 0.8812* 0.9017* 42 0.9033* 0.8839* 0.9058 45 0.9052* 0.8853* 0.9093 48 (bootstr) 0.9074 0.8877 0.9107 48 (actual) 0.9077 0.8877 0.9123 Table 9: Mean correlations of 48 topic Pyramid score and other metrics using from 1 to 48 topics for TAC 2008 initial summaries.
3107,0,Values in each row are significantly different from each other at 95% level except: ROUGE-2 and ROUGE-SU4 in the 6-topic category_comma_ ROUGE-2 and BE in 39and 48-topic category.
3108,0,Topics ROUGE-2 ROUGE-SU4 BE 1 0.5026 0.5729 0.4094 3 0.7106 0.7532 0.6276 6 0.8130 0.8335 0.7512 12 0.8806 0.8834 0.8475 24 0.9196 0.9092 0.9063 36 0.9343* 0.9198* 0.9301* 39 0.9367* 0.9213* 0.9341* 42 0.9386* 0.9227* 0.9376* 45 0.9402* 0.9236* 0.9402 48 (bootstr) 0.9430 0.9280 0.9444 48 (actual) 0.9429 0.9263 0.9446 Table 10: Mean correlations of 48 topic Pyramid score and other metrics using from 1 to 48 topics for TAC 2008 update summaries.
3109,0,Values in each row are significantly different from each other at 95% level except: ROUGE-2 and ROUGE-SU4 in 12-topic category_comma_ ROUGE-2 and BE in 45-topic category.
3110,0,Topics ROUGE-2 ROUGE-SU4 BE 1 0.6157 0.6378 0.5756 3 0.7597 0.7511 0.7323 6 0.8168 0.7904 0.7957 12 0.8493 0.8123 0.8306 24 0.8690 0.8249* 0.8517* 36 0.8751* 0.8287* 0.8580* 39 0.8761* 0.8295* 0.8592 42 0.8768* 0.8299* 0.8602 45 (bootstr) 0.8761* 0.8305 0.8627 45 (actual) 0.8795 0.8301 0.8609 Table 11: Mean correlations of 45 topic Content Responsiveness and other metrics using from 1 to 45 topics for DUC 2007 summaries.
3111,0,Some of these values_comma_ especially for update summaries_comma_ are even lower than those obtained by ROUGE in the same category_comma_ despite the fact that 1and 3-topic Responsiveness or Pyramid is a proper subset of the 48-topic Responsiveness/Pyramid.
3112,0,On the other hand_comma_ ROUGE achieves considerably worse correlations with Responsiveness than Pyramid when there are many topics available.
3113,0,Its still possible to use MSA if_comma_ for example_comma_ the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)).
3114,0,Very timely_comma_ the acquisition of paraphrase patterns has been actively studied in recent years: AF Manual collection of paraphrases in the context of language generation_comma_ e.g.(Robin and McKeown_comma_ 1996)_comma_ AF Derivation of paraphrases through existing lexical resources_comma_ e.g.(Kurohashi et al. _comma_ 1999)_comma_ AF Corpus-based statistical methods inspired by the work on information extraction_comma_ e.g.(Jacquemin_comma_ 1999; Lin and Pantel_comma_ 2001)_comma_ and AF Alignment-based acquisition of paraphrases from comparable corpora_comma_ e.g.(Barzilay and McKeown_comma_ 2001; Shinyama et al. _comma_ 2002; Barzilay and Lee_comma_ 2003).
3115,0,One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context.
3116,0,For this reason_comma_ paraphrase poses a great challenge for many Natural Language Processing (NLP) tasks_comma_ just as ambiguity does_comma_ notably in text summarization and NL generation (Barzilay and Lee_comma_ 2003; Pang et al. _comma_ 2003).
3117,0,Similar to the work of Barzilay and Lee (2003)_comma_ who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event_comma_ we are currently attempting to solve the data sparseness problem by extending our approach to non-parallel corpora.
3118,0,1 Introduction The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization_comma_ search_comma_ and dialog has been highlighted by a number of recent efforts (Barzilay & McKeown 2001; Shinyama et al. 2002; Lee & Barzilay 2003; Lin & Pantel 2001).
3119,0,While several different learning methods have been applied to this problem_comma_ all share a need for large amounts of data in the form of pairs or sets of strings that are likely to exhibit lexical and/or structural paraphrase alternations.
3120,0,Lee & Barzilay (2003)_comma_ for example_comma_ use MultiSequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts.
3121,0,Their goal is to extract sentential templates that can be used in high-precision generation of paraphrase alternations within a limited domain.
3122,0,While the idea of exploiting multiple news reports for paraphrase acquisition is not new_comma_ previous efforts (for example_comma_ Shinyama et al. 2002; Barzilay and Lee 2003) have been restricted to at most two news sources.
3123,0,Mean number of instances of paraphrase phenomena per sentence (such as Multiple Sequence Alignment_comma_ as employed by Barzilay & Lee 2003).
3124,0,However_comma_ there is a disparity between the kinds of paraphrase alternations that we need to be able to align and those that we can already align well using current SMT techniques.
3125,0,But because we want the insertion state a1a16a20 to model digressions or unseen topics_comma_ we take the novel step of forcing its language model to be complementary to those of the other states by setting a2 a3a27a38 a21 a8 a8 a4 a8 a24 a26a11a28a30a29a6 a39a41a40a43a42a45a44a16a46 a1a48a47a1a50a49 a20 a2 a3 a26a17a21 a8a9a8 a4 a8 a24 a51a53a52a55a54a57a56 a21 a39a58a40a43a42a45a44a16a46 a1a59a47a1a50a49 a20 a2 a3a27a26a11a21a50a60 a4 a8 a24a30a24 a17 4Following Barzilay and Lee (2003)_comma_ proper names_comma_ numbers and dates are (temporarily) replaced with generic tokens to help ensure that clusters contain sentences describing the same event type_comma_ rather than same actual event.
3126,0,Although a large number of studies have been made on learning paraphrases_comma_ for example (Barzilay and Lee_comma_ 2003)_comma_ there are only a few studies which address the connotational difference of paraphrases.
3127,0,There are several works that try to learn paraphrase pairs from parallel or comparable corpora (Barzilay and McKeown_comma_ 2001; Shinyama et al. _comma_ 2002; Barzilay and Lee_comma_ 2003; Pang et al. _comma_ 2003).
3128,0,Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003).
3129,0,Barzilay & Lee (2003) also identify paraphrases in their paraphrased sentence generation system.
3130,0,They first find different paraphrasing rules by clustering sentences in comparable corpora using n-gram word-overlap.
3131,0,Then for each cluster_comma_ they use multi-sequence alignment to find intra-cluster paraphrasing rules: either morphosyntactic or lexical patterns.
3132,0,To identify intercluster paraphrasing_comma_ they compare the slot values without considering word ordering.
3133,0,Similarly_comma_ (Barzilay and Lee_comma_ 2003) and (Shinyanma et al. _comma_ 2002) learn sentence level paraphrase templates from a corpus of news articles stemming from different news source.
3134,0,Such machine learning approaches have known pros and cons.
3135,1,On the one hand_comma_ they produce large scale resources at little man labour cost.
3136,0,On the other hand_comma_ the degree of descriptive abstraction offered by the list of inference or paraphrase rules they output is low.
3137,0,A growing body of recent research has focused on the problems of identifying and generating paraphrases_comma_ e.g._comma_ Barzilay & McKeown (2001)_comma_ Lin & Pantel (2002)_comma_ Shinyama et al_comma_ (2002)_comma_ Barzilay & Lee (2003)_comma_ and Pang et al.(2003).
3138,0,Barzilay & Lee (2003) employ Multiple Sequence Alignment (MSA_comma_ e.g._comma_ Durbin et al. _comma_ 1998) to align strings extracted from closely related news articles.
3139,0,Although the MSA approach can produce dramatic results_comma_ it is chiefly effective in extracting highly templatic data_comma_ and appears to be of limited extensibility to broad domain application (Quirk et al. 2004).
3140,0,The word-based edit distance heuristic yields pairs that are relatively clean but offer relatively minor rewrites in generation_comma_ especially when compared to the MSA model of (Barzilay & Lee_comma_ 2003).
3141,0,Barzilay & Lee (2003) and Quirk et al.(2004) use human evaluations of end-to-end generation_comma_ but these are not very useful here_comma_ since they add an additional layer of uncertainty into the evaluation_comma_ and depend to a significant extent on the quality and functionality of the decoder.
3142,1,g2 2 Motivation The success of Statistical Machine Translation (SMT) has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem (e.g. _comma_ Barzilay & Lee_comma_ 2003; Pang et al. _comma_ 2003; Quirk et al. _comma_ 2004; Finch et al. _comma_ 2004).
3143,0,However_comma_ a lack of standardly-accepted corpora on which to train and evaluate models is a major stumbling block to the successful application of SMT models or other machine learning algorithms to paraphrase tasks.
3144,0,Likewise_comma_ the data made available by (Barzilay & Lee_comma_ 2003: http://www.cs.cornell.edu/ Info/Projects/NLP/statpar.html)_comma_ while invaluable in understanding and evaluating their results_comma_ is too limited in size and domain coverage to serve as either training or test data.
3145,0,Some studies exploit topically related articles derived from multiple news sources (Barzilay and Lee_comma_ 2003; Shinyama and Sekine_comma_ 2003; Quirk et al. _comma_ 2004; Dolan et al. _comma_ 2004).
3146,0,There are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (Agichtein et al. _comma_ 2001; Florence et al. _comma_ 2003; Rinaldi et al. _comma_ 2003; Tomuro_comma_ 2003; Lin and Pantel_comma_ 2001;)_comma_ information extraction (Shinyama et al. _comma_ 2002; Shinyama and Sekine_comma_ 2003)_comma_ machine translation (Hiroshi et al. _comma_ 2003; Zhang and Yamamoto_comma_ 2003)_comma_ multi-document (Barzilay et al. _comma_ 2003).
3147,0,Generation of paraphrase examples was also investigated (Barzilay and Lee_comma_ 2003; Quirk et al. _comma_ 2004).
3148,0,Such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation (BARZILAY and LEE_comma_ 2003).
3149,0,Past work (Barzilay and McKeown_comma_ 2001; Barzilay and Lee_comma_ 2003; Pang et al. _comma_ 2003; Ibrahim et al. _comma_ 2003) has examined the use of monolingual parallel corpora for paraphrase extraction.
3150,0,Examples of monolingual parallel corpora that have been used are multiple translations of classical French novels into English_comma_ and data created for machine translation evaluation methods such as Bleu (Papineni et al. _comma_ 2002) which use multiple reference translations.
3151,0,While the results reported for these methods are impressive_comma_ their usefulness is limited by the scarcity of monolingual parallel corpora.
3152,0,Small data sets mean a limited number of paraphrases can be extracted.
3153,0,Furthermore_comma_ the narrow range of text genres available for monolingual parallel corpora limits the range of contexts in which the paraphrases can be used.
3154,0,2 Extracting paraphrases Much previous work on extracting paraphrases (Barzilay and McKeown_comma_ 2001; Barzilay and Lee_comma_ 2003; Pang et al. _comma_ 2003) has focused on finding identifying contexts within aligned monolingual sentences from which divergent text can be extracted_comma_ and treated as paraphrases.
3155,0,Such transformations are typically denoted as paraphrases in the literature_comma_ where a wealth of methods for their automatic acquisition were proposed (Lin and Pantel_comma_ 2001; Shinyama et al. _comma_ 2002; Barzilay and Lee_comma_ 2003; Szpektor et al. _comma_ 2004).
3156,0,Previous attempts have used_comma_ for instance_comma_ the similarities between case frames (Lin and Pan57 tel_comma_ 2001)_comma_ anchor words (Barzilay and Lee_comma_ 2003; Shinyama et al. _comma_ 2002; Szepektor et al. _comma_ 2004)_comma_ and a web-based method(Szepektor et al. _comma_ 2004;Geffet and Dagan_comma_ 2005).
3157,0,2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing_comma_ in particular_comma_ to sentence level paraphrasing (Barzilay and Lee_comma_ 2003; Pang et al. _comma_ 2003; Quirk et al. _comma_ 2004).
3158,0,Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora.
3159,0,Instances of such corpora include multiple English translations of the same source text written in a foreign language_comma_ and different news articles about the same event.
3160,0,Our approach differs from traditional work on automatic paraphrasing in goal and methodology.
3161,0,Unlike previous approaches_comma_ we are not aiming to produce any paraphrase of a given sentence since paraphrases induced from a parallel corpus do not necessarily produce a rewriting that makes a reference closer to the system output.
3162,0,2This can explain why previous attempts to use WordNet for generating sentence-level paraphrases (Barzilay and Lee_comma_ 2003; Quirk et al. _comma_ 2004) were unsuccessful.
3163,0,5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corporawith multiple descriptionsof the same events (Barzilay and McKeown_comma_ 2001; Barzilay and Lee_comma_ 2003).
3164,0,Classi er Training Set Precision Recall F-Measure Linear 10K pairs 0.837 0.774 0.804 Maximum Entropy 10K pairs 0.881 0.851 0.866 Maximum Entropy 450K pairs 0.902 0.944 0.922 Table 4: Performance of Alignment Classi er 3.2 Paraphrase Acquisition Much recent work on automatic paraphrasing (Barzilay and Lee_comma_ 2003) has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora.
3165,0,Since sentence-level paraphrases are generally assumed to contain information about the same event_comma_ these approaches have generally assumed that all of the available paraphrases for a given sentence will include at least one pair of entities which can be used to extract sets of paraphrases from text.
3166,0,In order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example_comma_ extracted sentences were clustered using complete-link clustering using a technique proposed in (Barzilay and Lee_comma_ 2003).
3167,0,Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier_comma_ 1998; Barzilay and Lee_comma_ 2003; Sudo et al. _comma_ 2003).
3168,0,The procedure of substituting named entities with their respective tags previously proved to be useful for various tasks (Barzilay and Lee_comma_ 2003; Sudo et al. _comma_ 2003; Filatova and Prager_comma_ 2005).
3169,0,If we consider these probabilities as a vector_comma_ the similarities of two English words can be obtained by computing the dot product of their corresponding vectors.2 The formula is described below: similarity(ei_comma_ ej) = Nsummationdisplay k=1 p(ei|fk)p(ej|fk) (3) Paraphrasing methods based on monolingual parallel corpora such as (Pang et al. _comma_ 2003; Barzilay and Lee_comma_ 2003) can also be used to compute the similarity ratio of two words_comma_ but they dont have as rich training resources as the bilingual methods do.
3170,0,Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee_comma_ 2003)_comma_ a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases.
3171,0,Nevertheless_comma_ this approach has a drawback of over-generating ungrammatical sentences due to its almost-free alignment.
3172,0,In our approach_comma_ we work on comparable texts (not necessarily equivalent in their semantic meanings) as Barzilay and Lee did.
3173,0,However_comma_ we use local syntactic similarity (as opposed to lexical similarity) in doing the alignment on the raw sentences instead of on their parse trees.
3174,0,Because of the semantic discrepancies among the inputs_comma_ applying syntactic features in the alignment has a larger impact on the grammaticality and fidelity of the generated unseen sentences.
3175,0,While previous work positions the primary focus on the quality of paraphrases and/or translations_comma_ we are more interested in the relation between the use of syntactic features and the correctness of the sentences being generated_comma_ including those that are not paraphrases of the original input.
3176,0,2 Related work Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee_comma_ 2003) and (Pang et al. _comma_ 2003).
3177,0,Both of these papers describe how multiple sequence alignment can be used for extracting paraphrases from clustered texts.
3178,0,Barzilay and Lee_comma_ on the other hand_comma_ make use of classic techniques in biological sequence analysis to identify paraphrases from comparable texts (news from different sources on the same event).
3179,0,In summary_comma_ Pang et al. use syntactic alignment of parallel texts while Barzilay and Lee use comparable (not parallel) input but ignore syntax.
3180,0,Our work differs from the two in that we apply syntactic information on aligning comparable texts and that the syntactic clues we use are drawn from Chunklink ilk.uvt.nl/ sabine/homepage/software.html output_comma_ which is further analysis from the syntactic parse trees.
3181,0,Our experience suggests that disjunctive LFs are an important capability_comma_ especially as one seeks to make grammars reusable across applications_comma_ and to employ domain-specific_comma_ sentence-level paraphrases (Barzilay and Lee_comma_ 2003).
3182,0,Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional_comma_ sentence-level PR.
3183,0,Given multiple articles on a certain type of event_comma_ sentence clusters are rst generated.
3184,0,Sentences within the same cluster_comma_ presumably similar in structure and content_comma_ are then used to construct a lattice with backbone nodes corresponding to words shared by the majority and slots corresponding to different realization of arguments.
3185,0,If sentences from different clusters have shared arguments_comma_ the associated lattices are claimed to be paraphrase.
3186,0,It should be noted that the latter two approaches are geared towards acquiring paraphrases rather than detecting them_comma_ and as such have the disadvantage of requiring a certain level of repetition among candidates for paraphrases to be recognized.
3187,0,All past approaches invariably aim at a proper similarity measure that accounts for all of the words in the sentences in order to make a judgment for PR.
3188,0,This is suitable for PR where input sentences are precisely equivalent semantically.
3189,0,However_comma_ for many people the notion of paraphrases also covers cases in which minor or irrelevant information is added or omitted in candidate sentences_comma_ as observed in the earlier example.
3190,0,Such extraneous content should not be a barrier to PR if the main concepts are shared by the sentences.
3191,0,Approaches that focus only on the similarity of shared contents may fail when the (human) criteria for PR include whether the unmatched content is signi cant or not.
3192,0,Correctly addressing this problem should increase accuracy.
3193,0,2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years_comma_ ranging from distributional similarity to finding shared contexts (Lin and Pantel_comma_ 2001; Ravichandran and Hovy_comma_ 2002; Shinyama et al. _comma_ 2002; Barzilay and Lee_comma_ 2003; Szpektor et al. _comma_ 2004; Sekine_comma_ 2005).
3194,0,However_comma_ there is still no common accepted framework for their evaluation.
3195,0,Furthermore_comma_ all these methods learn rules as pairs of templates {L_comma_R} in a symmetric manner_comma_ without addressing rule directionality.
3196,0,Accordingly_comma_ previous works (except (Szpektor et al. _comma_ 2004)) evaluated the learned rules under the paraphrase criterion_comma_ which underestimates the practical utility of the learned rules (see Section 2.1).
3197,0,Indeed_comma_ the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel_comma_ 2001; Shinyama et al. _comma_ 2002; Barzilay and Lee_comma_ 2003; Pang et al. _comma_ 2003; Szpektor et al. _comma_ 2004; Sekine_comma_ 2005).
3198,0,In this evaluation scheme_comma_ termed here the rule-based approach_comma_ a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not.
3199,0,The criterion for correctness is not explicitly described in most previous works.
3200,0,Indeed_comma_ only few earlier works reported inter-judge agreement level_comma_ and those that did reported rather low Kappa values_comma_ such as 0.54 (Barzilay and Lee_comma_ 2003) and 0.55 0.63 (Szpektor et al. _comma_ 2004).
3201,0,To conclude_comma_ the prominent rule-based methodology for entailment rule evaluation is not sufficiently well defined.
3202,0,It results in low inter-judge agreement which prevents reliable and consistent assessments of different algorithms.
3203,0,Most previous work on paraphrase has focused on high quality rather than coverage (Barzilay and Lee_comma_ 2003; Quirk et al. _comma_ 2004)_comma_ but generating artificial references for MT parameter tuning in our setting has two unique properties compared to other paraphrase applications.
3204,0,At the sentence level_comma_ (Barzilay and Lee_comma_ 2003) employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora.
3205,0,Their technique produces a paraphrase only if the input sentence matches any of the extracted lattice pairs_comma_ leading to a bias strongly favoring quality over coverage.
3206,0,They were able to generate paraphrases for 59 sentences (12%) out of a 484-sentence test set_comma_ generating no paraphrases at all for the remainder.
3207,0,Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel_comma_ 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy_comma_ 2002; Shinyama et al. _comma_ 2002; Barzilay and Lee_comma_ 2003; Sudo et al. _comma_ 2003; Szpektor et al. _comma_ 2004; Satoshi_comma_ 2005).
3208,0,Most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other (Bannard and Callison-Burch_comma_ 2005; Barzilay and Lee_comma_ 2003; Barzilay and McKeown_comma_ 2001; Callison-Burch et al. _comma_ 2006; Dolan et al. _comma_ 2004; Ibrahim et al. _comma_ 2003; Lin and Pantel_comma_ 2001; Pang et al. _comma_ 2003; Quirk et al. _comma_ 2004; Shinyama et al. _comma_ 2002).
3209,0,The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee_comma_ 2003; Brockett and Dolan_comma_ 2005).
3210,0,Despite the results previous work has achieved_comma_ no system that robustly recognizes and generates paraphrases is established.
3211,0,We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning.
3212,0,Second_comma_ we discuss the work done by (Barzilay & Lee_comma_ 2003) who use clustering of paraphrases to induce rewriting rules.
3213,0,We will see_comma_ through classical visualization methodologies (Kruskal & Wish_comma_ 1977) and exhaustive experiments_comma_ that clustering may not be the best approach for automatic pattern identification.
3214,0,Sentence Compression takes an important place for Natural Language Processing (NLP) tasks where specific constraints must be satisfied_comma_ such as length in summarization (Barzilay & Lee_comma_ 2002; Knight & Marcu_comma_ 2002; Shinyama et al. _comma_ 2002; Barzilay & Lee_comma_ 2003; Le Nguyen & Ho_comma_ 2004; Unno et al. _comma_ 2006)_comma_ style in text simplification (Marsi & Krahmer_comma_ 2005) or sentence simplification for subtitling (Daelemans et al. _comma_ 2004).
3215,0,Second_comma_ we will discuss the work done by (Barzilay & Lee_comma_ 2003) who use clustering of paraphrases to induce rewriting rules.
3216,0,2 Related Work Two different approaches have been proposed for Sentence Compression: purely statistical methodologies (Barzilay & Lee_comma_ 2003; Le Nguyen & Ho_comma_ 2004) and hybrid linguistic/statistic methodologies (Knight & Marcu_comma_ 2002; Shinyama et al. _comma_ 2002; Daelemans et al. _comma_ 2004; Marsi & Krahmer_comma_ 2005; Unno et al. _comma_ 2006).
3217,0,As our work is based on the first paradigm_comma_ we will focus on the works proposed by (Barzilay & Lee_comma_ 2003) and (Le Nguyen & Ho_comma_ 2004).
3218,0,(Barzilay & Lee_comma_ 2003) present a knowledge-lean algorithm that uses multiple-sequence alignment to 177 learn generate sentence-level paraphrases essentially from unannotated corpus data alone.
3219,0,In contrast to (Barzilay & Lee_comma_ 2002)_comma_ they need neither parallel data nor explicit information about sentence semantics.
3220,0,Rather_comma_ they use two comparable corpora.
3221,0,Their approach has three main steps.
3222,0,First_comma_ working on each of the comparable corpora separately_comma_ they compute lattices compact graph-based representations to find commonalities within groups of structurally similar sentences.
3223,0,Next_comma_ they identify pairs of lattices from the two different corpora that are paraphrases of each other.
3224,0,Finally_comma_ given an input sentence to be paraphrased_comma_ they match it to a lattice and use a paraphrase from the matched lattices mate to generate an output sentence.
3225,0,Comparatively_comma_ (Barzilay & Lee_comma_ 2003) propose to use the N-gram Overlap metric to capture similarities between sentences and automatically create paraphrase corpora.
3226,0,However_comma_ this choice is arbitrary and mainly leads to the extraction of quasi-exact or exact matching pairs.
3227,1,Unlike (Le Nguyen & Ho_comma_ 2004)_comma_ one interesting idea proposed by (Barzilay & Lee_comma_ 2003) is to cluster similar pairs of paraphrases to apply multiplesequence alignment.
3228,0,However_comma_ once again_comma_ this choice is not justified and we will see by classical visualization methodologies (Kruskal & Wish_comma_ 1977) and exhaustive experiments by applying different clustering algorithms_comma_ that clustering may not be the best approach for automatic pattern identification.
3229,0,3.1 Paraphrase Identification A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay & Lee_comma_ 2003; Dolan & Brockett_comma_ 2004).
3230,0,However_comma_ these unsupervised methodologies show a major drawback by extracting quasi-exact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan & Brockett_comma_ 2004) and word N-gram overlap for (Barzilay & Lee_comma_ 2003).
3231,0,Such pairs are clearly useless.
3232,0,In particular_comma_ it shows systematically better F-Measure and Accuracy measures over all other metrics showing an improvement of (1) at least 2.86% in terms of F-Measure and 3.96% in terms of Accuracy and (2) at most 6.61% in terms of FMeasure and 6.74% in terms of Accuracy compared to the second best metric which is also systematically the word N-gram overlap similarity measure used by (Barzilay & Lee_comma_ 2003).
3233,0,On one hand_comma_ as (Barzilay & Lee_comma_ 2003) evidence_comma_ clusters of paraphrases can lead to better learning of text-totext rewriting rules compared to just pairs of paraphrases.
3234,0,However_comma_ as (Barzilay & Lee_comma_ 2003) do not propose any evaluation of which clustering algorithm should be used_comma_ we experiment a set of clustering algorithms and present the comparative results.
3235,0,Table 2: Figures about clustering algorithms Algorithm # Sentences/# Clusters S-HAC 6_comma_23 C-HAC 2_comma_17 QT 2_comma_32 EM 4_comma_16 In fact_comma_ table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by (Barzilay & Lee_comma_ 2003) who only keep the clusters that contain more than 10 sentences.
3236,0,These results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction_comma_ contrarily to what (Barzilay & Lee_comma_ 2003) suggest.
3237,0,Experiments_comma_ by using 4 algorithms and through visualization techniques_comma_ revealed that clustering is a worthless effort for paraphrase corpora construction_comma_ contrary to the literature claims (Barzilay & Lee_comma_ 2003).
3238,0,Multiple translations of the same text (Barzilay and McKeown_comma_ 2001)_comma_ corresponding articles from multiple news sources (Barzilay and Lee_comma_ 2003; Quirk et al._comma_ 2004; Dolan et al._comma_ 2004)_comma_ and bilingual corpus (Bannard and Callison-Burch_comma_ 2005) have been utilized.
3239,0,Unfortunately_comma_ this approach produces only a low coverage because the size of the parallel/comparable corpora is limited.
3240,0,Some works focused on learning rules from comparable corpora_comma_ containing comparable documents such as different news articles from the same date on the same topic (Barzilay and Lee_comma_ 2003; Ibrahim et al._comma_ 2003).
3241,1,Such corpora are highly informative for identifying variations of the same meaning_comma_ since_comma_ typically_comma_ when variable instantiations are shared across comparable documents the same predicates are described.
3242,0,However_comma_ it is hard to collect broad-scale comparable corpora_comma_ as the majority of texts are non-comparable.
3243,0,This is related to the wellstudied problem of identifying paraphrases (Barzilay and Lee_comma_ 2003; Pang et al._comma_ 2003) and the more general variant of recognizing textual entailment_comma_ which explores whether information expressed in a hypothesis can be inferred from a given premise.
3244,0,The other utilizes a sort of parallel texts_comma_ such as multiple translation of the same text (Barzilay and McKeown_comma_ 2001; Pang et al._comma_ 2003)_comma_ corresponding articles from multiple news sources (Barzilay and Lee_comma_ 2003; Dolan et al._comma_ 2004)_comma_ and bilingual corpus (Wu and Zhou_comma_ 2003; Bannard and Callison-Burch_comma_ 2005).
3245,0,This approach is_comma_ however_comma_ limited by the difficulty of obtaining parallel/comparable corpora.
3246,0,Recently_comma_ some work has been done on corpusbased paraphrase extraction (Lin and Pantel_comma_ 2001; Barzilay and Lee_comma_ 2003).
3247,0,The basic idea of their methods is that two words with similar meanings are used in similar contexts.
3248,0,Although their methods can obtain broad-coverage paraphrases_comma_ the obtained paraphrases are not accurate enough to be utilized 1This word usually means  water supply . 787 for achieving precise matching since they contain synonyms_comma_ near-synonyms_comma_ coordinate terms_comma_ hypernyms_comma_ and inappropriate synonymous expressions.
3249,0,Dolan et al.(2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases.
3250,0,All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size.
3251,0,In paraphrase generation_comma_ a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. Avarietyofmethodshavebeenproposedonparaphrase patterns extraction (Lin and Pantel_comma_ 2001; Ravichandran and Hovy_comma_ 2002; Shinyama et al._comma_ 2002; Barzilay and Lee_comma_ 2003; Ibrahim et al._comma_ 2003; Pang et al._comma_ 2003; Szpektor et al._comma_ 2004).
3252,0,However_comma_ these methods have some shortcomings.
3253,0,Especially_comma_ the precisions of the paraphrase patterns extracted with these methods are relatively low.
3254,0,The preci781 start Palestinian suicide bomberblew himself up in SLOT1 on SLOT2 killing SLOT3 other people and injuring wounding SLOT4 end detroit the *e* a s *e* building buildingin detroit flattened ground levelled to blasted leveled *e* was reduced razed leveled to down rubble into ashes *e* to *e* (1) (2) Figure 1: Examples of paraphrase patterns extracted by Barzilay and Lee (2003) and Pang et al.(2003).
3255,0,Barzilay and Lee (2003) applied multi-sequence alignment (MSA) to parallel news sentences and induced paraphrase patterns for generating new sentences (Figure 1 (1)).
3256,0,However_comma_ it is difficult to find methods that are suitable for comparison.
3257,0,Some methods only extract paraphrase patternsusingnewsarticlesoncertaintopics(Shinyama et al._comma_ 2002; Barzilay and Lee_comma_ 2003)_comma_ while some others need seeds as initial input (Ravichandran and Hovy_comma_ 2002).
3258,0,Different news articles reporting on the same event are commonly used as monolingual comparable corpora_comma_ from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al._comma_ 2002; Barzilay and Lee_comma_ 2003; Quirk et al._comma_ 2004).
3259,0,For example_comma_ Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences.
3260,0,The pattern-based methods can generate complex paraphrases that usually involve syntactic variation.
3261,0,However_comma_ the methods were demonstrated to be of limited generality (Quirk et al._comma_ 2004).
3262,0,3 Monolingual comparable corpus: Similar to the methods in (Shinyama et al._comma_ 2002; Barzilay and Lee_comma_ 2003)_comma_ we construct a corpus of comparable documents from a large corpus D of news articles.
3263,0,While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy_comma_ sentencelevel paraphrasingis moredifficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee_comma_ 2003).
3264,0,There exist many different string similarity measures: word overlap (Tomuro and Lytinen_comma_ 2004)_comma_ longest common subsequence (Islamand Inkpen_comma_2007)_comma_ Levenshteinedit distance (Dolan et al._comma_ 2004)_comma_ word n-gramoverlap (Barzilay and Lee_comma_ 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words containedin the sentencesbeing compared.
3265,0,In order to be able to compare the edit distance with the other metrics_comma_ we have used the following formula(Wen et al._comma_ 2002)whichnormalisesthe minimum edit distance by the length of the longest questionand transformsit into a similaritymetric: normalisededitdistance = 1 edit dist(q1_comma_q2)max(| q 1 |_comma_| q2 |) Word Ngram Overlap This metric compares the word n-gramsin both questions: ngramoverlap = 1N Nsummationdisplay n=1 | Gn(q1)  Gn(q2) | min(| Gn(q1) |_comma_| Gn(q2) |) where Gn(q) is the set of n-grams of length n in question q and N usually equals 4 (Barzilay and Lee_comma_ 2003;Cordeiroet al._comma_ 2007).
3266,0,Barzilay and Lee (Barzilay and Lee_comma_ 2003) learned paraphrasing patterns as pairs of word lattices_comma_ which are then used to produce sentence level paraphrases.
3267,0,Their corpus contained news agency articles on the same events_comma_ which allows precise sentence paraphrasing_comma_ but on a small sets of phenomena and for a limited domain.
3268,0,Note that previous researches on entailment acquisition focused on templates with variables or word-lattices (Lin and Pantel_comma_ 2001; Szpektor and Dagan_comma_ 2008; Barzilay and Lee_comma_ 2003; Shinyama 1 Verb entailment pairs are described as v 1  v 2 (v 1 is the entailing verb and v 2 is the entailed one) henceforth.
3269,0,2 Related Work Previous studies on entailment_comma_ inference rules_comma_ and paraphrase acquisition are roughly classified into those that require comparable corpora (Shinyama et al._comma_ 2002; Barzilay and Lee_comma_ 2003; Ibrahim et al._comma_ 2003) and those that do not (Lin and Pantel_comma_ 2001; Weeds and Weir_comma_ 2003; Geffet and Dagan_comma_ 2005; Pekar_comma_ 2006; Bhagat et al._comma_ 2007; Szpektor and Dagan_comma_ 2008).
3270,0,Barzilay and Lee (2003) also used newspaper articles on the same event as comparable corpora to acquire paraphrases.
3271,0,They induced paraphrasing patterns by sentence clustering.
3272,0,Matusov et al.(2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system com2Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm.
3273,0,Unlike our approach_comma_ MSA does not allow reordering of inputs.
3274,0,The use of Profile HMMs for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries (Barzilay and Lee_comma_ 2002) and sentence-level paraphrasing (Barzilay and Lee_comma_ 2003).
3275,0,For natural language engineers_comma_ the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee_comma_ 2003)_comma_ question answering modules (Marsi and Krahmer_comma_ 2005) and machine translation (Callison-Burch et al._comma_ 2006).
3276,0,Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel_comma_ 2001; Barzilay and Lee_comma_ 2003; Zhao et al._comma_ 2008b)_comma_ which facilitates the rule-based PG methods.
3277,0,However_comma_ it has been shown that the coverage of the paraphrase patterns is not high enough_comma_ especially when the used paraphrase patterns are long or complicated (Quirk et al._comma_ 2004).
3278,0,For instance_comma_ automatic summary can be seen as a particular paraphrasing task (Barzilay and Lee_comma_ 2003) with the aim of selecting the shortest paraphrase.
3279,0,In another generation approach_comma_ Barzilay and Lee (2002; 2003) look for pairs of slotted word lattices that share many common slot fillers; the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events.
3280,0,Again_comma_ a paraphrase recognizer could be embedded in all of these methods_comma_ to filter out erroneous generated patterns.
3281,0,3.4 Perspectives for automatic paraphrase extraction There is a growing amount of work on automatic extraction of paraphrases from text corpora (Lin and Pantel_comma_ 2001; Barzilay and Lee_comma_ 2003; Ibrahim et al._comma_ 2003; Dolan et al._comma_ 2004).
3282,0,A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay & Lee_comma_ 2003; Dolan et al._comma_ 2004).
3283,0,However_comma_ these unsupervised methodologies show a major drawback by extracting quasi-exact or even exact match pairs of sentences as they rely on classical string similarity measures such as the Edit Distance in the case of (Dolan et al._comma_ 2004) and Word N-gram Overlap for (Barzilay & Lee_comma_ 2003).
3284,0,Such pairs are useless for our purpose_comma_ since we aim to identify asymmetrical paraphrase pairs to be used for sentence compression rule induction_comma_ as explained in (Cordeiro et al._comma_ Oct 2007).
3285,0,Pang et al.(2002) reported that a similar experiment produced their best results on a related classification task.
3286,0,Some work identifies inflammatory texts (e.g. _comma_ (Spertus_comma_ 1997)) or classifies reviews as positive or negative ((Turney_comma_ 2002; Pang et al. _comma_ 2002)).
3287,0,For example_comma_ (Spertus_comma_ 1997) developed a system to identify inflammatory texts and (Turney_comma_ 2002; Pang et al. _comma_ 2002) developed methods for classifying reviews as positive or negative.
3288,0,For example_comma_ (Pang et al. _comma_ 2002) collected reviews from a movie database and rated them as positive_comma_ negative_comma_ or neutral based on the rating (e.g. _comma_ number of stars) given by the reviewer.
3289,0,Pang et al.(2002) adopted a more direct approach_comma_ using supervised machine learning with words and n-grams as features to predict orientation at the document level with up to 83% precision.
3290,0,Our focus is on the sentence level_comma_ unlike (Pang et al. _comma_ 2002) and (Turney_comma_ 2002); we employ a significantly larger set of seed words_comma_ and we explore as indicators of orientation words from syntactic classes other than adjectives (nouns_comma_ verbs_comma_ and adverbs).
3291,0,Although Naive Bayes can be outperformed in text classification tasks by more complex methods such as SVMs_comma_ Pang et al.(2002) report similar performance for Naive Bayes and other machine learning techniques for a similar task_comma_ that of distinguishing between positive and negative reviews at the document level.
3292,0,2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Turney_comma_ 2002; Pang et al. _comma_ 2002) where a document is assumed to have only a single sentiment_comma_ thus these studies are not applicable to our goal.
3293,0,Recent computational work either focuses on sentence subjectivity (Wiebe et al. 2002; Riloff et al. 2003)_comma_ concentrates just on explicit statements of evaluation_comma_ such as of films (Turney 2002; Pang et al. 2002)_comma_ or focuses on just one aspect of opinion_comma_ e.g._comma_ (Hatzivassiloglou and McKeown 1997) on adjectives.
3294,0,Automatic subjectivity analysis would also be useful to perform flame recognition (Spertus 1997; Kaufer 2000)_comma_ e-mail classification (Aone_comma_ Ramos-Santacruze_comma_ and Niehaus 2000)_comma_ intellectual attribution in text (Teufel and Moens 2000)_comma_ recognition of speaker role in radio broadcasts (Barzialy et al. 2000)_comma_ review mining (Terveen et al. 1997)_comma_ review classification (Turney 2002; Pang_comma_ Lee_comma_ and Vaithyanathan 2002)_comma_ style in generation (Hovy 1987)_comma_ and clustering documents by ideological point of view (Sack 1995).
3295,0,Three studies classify reviews as positive or negative (Turney 2002; Pang_comma_ Lee_comma_ and Vaithyanathan 2002; Dave_comma_ Lawrence_comma_ Pennock 2003).
3296,0,Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents_comma_ e.g. Pang et al.(2002)_comma_ Dave et al.(2003).
3297,0,By learning models from labeled data_comma_ one can apply familiar_comma_ powerful techniques directly; however_comma_ in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately.
3298,0,4 Data Analysis 4.1 Accuracy of Unsupervised Coefficients By means of a Perl script that uses the Lynx browser_comma_ Version 2.8.3rel.1_comma_ we download AltaVista hit counts for queries of the form target NEAR anchor. The initial list of targets consists of 44_comma_321 word types extracted from the Pang corpus of 1400 labeled movie reviews.
3299,0,Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data.
3300,0,Further_comma_ with 5In the labeled Pang corpus_comma_ intense appears in 38 positive reviews and only 6 negative ones.
3301,0,a larger number of labeled documents_comma_ its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al. _comma_ 2002).
3302,0,Although such approaches have been employed effectively (Pang et al. _comma_ 2002)_comma_ there appears to remain considerable room for improvement.
3303,0,Pang et al.(2002)s treatment of the task as analogous to topic-classification underscores the difference between the two tasks.
3304,0,The way they were added is similar to incorporating the negation effect described by Pang et al.(2002).
3305,0,When pre-processing text for classification_comma_ following the method of Pang et al. _comma_ we attached the tag MORE to all words between the more-words and the following punctuation mark_comma_ and the tag LESS to the words after the less-words.
3306,0,Pang et al.(2002) did not compare the result of using and not using the negation context effect_comma_ so it is not clear how much it improved their result.
3307,0,Subjective phrases are used by (Turney_comma_ 2002; Pang and Vaithyanathan_comma_ 2002; Kushal et al. _comma_ 2003; Kim and Hovy_comma_ 2004) and others in order to classify reviews or sentences as positive or negative.
3308,0,Much of this research explores sentiment classification_comma_ a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g. _comma_ Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
3309,0,Part-of-speech features Based on the lexical categories produced by GATE (Cunningham et al. _comma_ 2002)_comma_ each token xi is classified into one of a set of coarse part-of-speech tags: noun_comma_ verb_comma_ adverb_comma_ wh-word_comma_ determiner_comma_ punctuation_comma_ etc. We do the same for neighboring words in a [ 2_comma_ +2] window in order to assist noun phrase segmentation.
3310,0,Errors from the sentence boundary detector in GATE (Cunningham et al. _comma_ 2002) were especially problematic because they caused the Collins parser to fail_comma_ resulting in no dependency tree information.
3311,0,1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002_comma_ Pang et al. 2004_comma_ Turney 2002_comma_ Turney and Littman 2002_comma_ Wiebe et al. 2001_comma_ Bai et al. 2004_comma_ Yu and Hatzivassiloglou 2003 and many others).
3312,0,Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002_comma_ Pang et al. 2004_comma_ Turney 2002_comma_ Turney and Littman 2002).
3313,0,So far research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al. _comma_ 2004; Riloff et al. _comma_ 2003; Riloff and Wiebe_comma_ 2003)_comma_ identifying opinionated documents (Yu and Hatzivassiloglou_comma_ 2003) and sentences (Yu and Hatzivassiloglou_comma_ 2003; Riloff et al. _comma_ 2003; Riloff and Wiebe_comma_ 2003)_comma_ and discriminating between positive and negative language (Yu and Hatzivassiloglou_comma_ 2003; Turney and Littman_comma_ 2003; Pang et al. _comma_ 2002; Dave et al. _comma_ 2003; Nasukawa and Yi_comma_ 2003; Morinaga et al. _comma_ 2002).
3314,0,There are studies on learning subjective language (Wiebe et al. _comma_ 2004)_comma_ identifying opinionated documents (Yu and Hatzivassiloglou_comma_ 2003) and sentences (Riloff et al. _comma_ 2003; Riloff and Wiebe_comma_ 2003)_comma_ and discriminating between positive and negative language (Turney and Littman_comma_ 2003; Pang et al. _comma_ 2002; Dave et al. _comma_ 2003; Nasukawa and Yi_comma_ 2003; Morinaga et al. _comma_ 2002).
3315,0,Pang et al. proposed a method of classifying movie reviews into positive and negative ones (Pang et al. _comma_ 2002).
3316,0,In these applications_comma_ one of the most important issue is how to determine the polarity (or semantic orientation) of a given text.
3317,0,So far_comma_ this approach has been taken by a lot of researchers (Pang et al. _comma_ 2002; Dave et al. _comma_ 2003; Wilson et al. _comma_ 2005).
3318,0,In these previous works_comma_ polarity-tagged corpus was built in either of the following two ways.
3319,0,However_comma_ both of the two approaches are not appropriate for building large polarity-tagged corpus.
3320,0,The method that relies on review sites can not be applied to domains in which large amount of reviews are not available.
3321,0,Consider the following example (Pang et al. _comma_ 2002): This film should be brilliant.
3322,0,Negation was processed in a similar way as previous works (Pang et al. _comma_ 2002).
3323,0,Document level sentiment classification is mostly applied to reviews_comma_ where systems assign a positive or negative sentiment for a whole review document (Pang et al. _comma_ 2002; Turney_comma_ 2002).
3324,0,Building on this work_comma_ more sophisticated problems in the opinion domain have been studied by many researchers.
3325,0,2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g. _comma_ Pang et al.(2002)_comma_ Turney (2002))_comma_ a sentence (e.g. _comma_ Liu et al.(2003)_comma_ Yu and Hatzivassiloglou (2003)_comma_ Kim and Hovy (2004)_comma_ Gamon et al.(2005))_comma_ a phrase (e.g. _comma_ Wilson et al.(2005))_comma_ and a specific object (such as a product) mentioned in a document (e.g. _comma_ Morinaga et al.(2002)_comma_ Yi et al.(2003)_comma_ Popescu and Etzioni (2005)).
3326,0,Two notable exceptions are the work of Dave et al.(2003) and Pang et al.(2002).
3327,1,Interestingly_comma_ while Dave et al. report good performance on classifying reviews using bigrams or trigrams alone_comma_ Pang et al. show that bigrams are not useful features for the task_comma_ whether they are used in isolation or in conjunction with unigrams.
3328,0,The 2000 reviews are taken from Pang et al.s polarity dataset (version 2.0)3_comma_ which consists of an equal number of positive and negative reviews.
3329,0,All learning parameters are set to their default values.6 Each document is first tokenized and downcased_comma_ and then represented as a vector of unigrams with length normalization.7 Following Pang et al.(2002)_comma_ we use frequency as presence.
3330,0,Our neutral reviews are randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews8 that have either a rating of 2 (on a 4-point scale) or 2.5 (on a 5-point scale).
3331,0,As a sanity check_comma_ we duplicated Pang et al.s (2002) baseline in which all unigrams that appear four or more times in the training documents are used as features.
3332,0,The negative results that Pang et al.(2002) obtained when using bigrams as features for their polarity classifier seem to suggest that high-order n-grams are not useful for polarity classification.
3333,0,Similar results were also obtained by Pang et al.(2002).
3334,0,Work focusses on analysing subjective features of text or speech_comma_ such as sentiment_comma_ opinion_comma_ emotion or point of view (Pang et al. _comma_ 2002; Turney_comma_ 2002; Dave et al. _comma_ 2003; Liu et al. _comma_ 2003; Pang and Lee_comma_ 2005; Shanahan et al. _comma_ 2005).
3335,0,For instance_comma_ both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative?
3336,0,A richer set of features besides n-grams should be checked_comma_ and we should not ignore the potential effectiveness of unigrams in this task (Pang et al. _comma_ 2002).
3337,0,Identifying subjectivity helps separate opinions from fact_comma_ which may be useful in question answering_comma_ summarization_comma_ etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown_comma_ 1997; Turney_comma_ 2002; Esuli and Sebastiani_comma_ 2005)_comma_ phrases and sentences (Kim and Hovy_comma_ 2004; Wilson et al. _comma_ 2005)_comma_ or documents (Pang et al. _comma_ 2002; Turney_comma_ 2002).
3338,0,Building on this work_comma_ more sophisticated problems such as opinion holder identification have also been studied.
3339,0,Most of the annotation approaches tackling these issues_comma_ however_comma_ are aimed at performing classifications at either the document level (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ or the sentence or word level (Wiebe et al. _comma_ 2004; Yu and Hatzivassiloglou_comma_ 2003).
3340,0,In addition_comma_ these approaches focus primarily on sentiment classification_comma_ and use the same for getting at the classification of facts vs. opinions.
3341,0,In contrast to these approaches_comma_ the focus here is on marking attribution on more analytic semantic units_comma_ namely the Abstract Objects (AOs) associated with predicate-argument discourse relations annotated in the PDTB_comma_ with the aim of providing a compositional classification of the factuality of AOs.
3342,0,In analyzing opinions (Cardie et al. _comma_ 2003; Wilson et al. _comma_ 2004)_comma_ judging document-level subjectivity (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ and answering opinion questions (Cardie et al. _comma_ 2003; Yu and Hatzivassiloglou_comma_ 2003)_comma_ the output of a sentence-level subjectivity classification can be used without modification.
3343,0,In particular_comma_ since we treat each individual speech within a debate as a single document_comma_ we are considering a version of document-level sentiment-polarity classification_comma_ namely_comma_ automatically distinguishing between positive and negative documents (Das and Chen_comma_ 2001; Pang et al. _comma_ 2002; Turney_comma_ 2002; Dave et al. _comma_ 2003).
3344,0,3.2 Classifying speech segments in isolation In our experiments_comma_ we employed the well-known classifier SVMlight to obtain individual-document classification scores_comma_ treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al. _comma_ 2002)_comma_ the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors.
3345,0,A number of studies have investigated sentiment classification at document level_comma_ e.g._comma_ (Pang et al. _comma_ 2002; Dave et al. _comma_ 2003)_comma_ and at sentence level_comma_ e.g._comma_ (Hu and Liu_comma_ 2004; Kim and Hovy_comma_ 2004; Nigam and Hurst_comma_ 2005); however_comma_ the accuracy is still less than desirable.
3346,0,This negation handling is similar to that used in (Das and Chen_comma_ 2001; Pang et al. _comma_ 2002).
3347,0,For example_comma_ Pang et al.(2002) found that unigrams outperformed bigrams_comma_ and unigrams outperformed the combination of unigrams plus bigrams.
3348,0,Lexical cues of differing complexities have been used_comma_ including single words and Ngrams (e.g. _comma_ (Mullen and Collier_comma_ 2004; Pang et al. _comma_ 2002; Turney_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Wiebe et al. _comma_ 2004))_comma_ as well as phrases and lexico-syntactic patterns (e.g_comma_ (Kim and Hovy_comma_ 2004; Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005; Riloff and Wiebe_comma_ 2003; Whitelaw et al. _comma_ 2005)).
3349,0,While many of these studies investigate combinations of features and feature selection_comma_ this is the rst work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classi cation.
3350,0,So far_comma_ research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al. _comma_ 2004; Riloff et al. _comma_ 2003)_comma_ identifying opinionated documents (Yu and Hatzivassiloglou_comma_ 2003) and sentences (Yu and Hatzivassiloglou_comma_ 2003; Riloff et al. _comma_ 2003)_comma_ and discriminating between positive and negative language (Pang et al. _comma_ 2002; Morinaga et al. _comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Turney and Littman_comma_ 2003; Dave et al. _comma_ 2003; Nasukawa and Yi_comma_ 2003; Popescu and Etzioni_comma_ 2005; Wilson et al. _comma_ 2005).
3351,0,Research on the automatic classification of movie or product reviews as positive or negative (e.g. _comma_ (Pang et al. _comma_ 2002; Morinaga et al. _comma_ 2002; Turney and Littman_comma_ 2003; Nasukawa and Yi_comma_ 2003; Mullen and Collier_comma_ 2004; Beineke et al. _comma_ 2004; Hu and Liu_comma_ 2004)) is perhaps the most similar to our work.
3352,0,1 Introduction In the past few years_comma_ there has been an increasing interest in mining opinions from product reviews (Pang_comma_ et al_comma_ 2002; Liu_comma_ et al_comma_ 2004; Popescu and Etzioni_comma_ 2005).
3353,0,However_comma_ due to the lack of editorial and quality control_comma_ reviews on products vary greatly in quality.
3354,0,Pang et al (2002) considered the same problem and presented a set of supervised machine learning approaches to it.
3355,0,This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al. _comma_ 2002; Turney_comma_ 2002_comma_ etc.).
3356,1,We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al. _comma_ 2002; Pang and Lee_comma_ 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive_comma_ all written before 2002 by a total of 312 authors_comma_ with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable.
3357,0,5 Experimental Procedures 5.1 Feature extraction Although this dataset seems to demand discourselevel features that contextualize bits of praise and criticism_comma_ we exactly follow Pang et al.(2002) and Pang and Lee (2004) in merely using binary unigram features_comma_ corresponding to the 17_comma_744 unstemmed word or punctuation types with count  4 in the full 2000-document corpus.
3358,0,1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al. _comma_ 2002; Turney_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003).
3359,0,2 Related Work Sentiment Classi cation Traditionally_comma_ categorization of opinion texts has been cast as a binary classication task (Pang et al. _comma_ 2002; Turney_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Dave et al. _comma_ 2003).
3360,0,3.4 Feature Representation Ranking Models Following previous work on sentiment classi cation (Pang et al. _comma_ 2002)_comma_ we represent each review as a vector of lexical features.
3361,0,Sentiment analysis includes a variety of different problems_comma_ including: sentiment classification techniques to classify reviews as positive or negative_comma_ based on bag of words (Pang et al. _comma_ 2002) or positive and negative words (Turney_comma_ 2002; Mullen and Collier_comma_ 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe_comma_ 2003; Pang and Lee_comma_ 2004); identifying or classifying appraisal targets (Nigam and Hurst_comma_ 2004); identifying the source of an opinion in a text (Choi et al. _comma_ 2005)_comma_ whether the author is expressing the opinion_comma_ or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. _comma_ 2005; Popescu and Etzioni_comma_ 2005).
3362,0,Much of this work has utilized the fundamental concept of semantic orientation_comma_ (Turney_comma_ 2002); however_comma_ sentiment analysis still lacks a unified field theory.
3363,0,Finally_comma_ other approaches rely on reviews with numeric ratings from websites (Pang and Lee_comma_ 2002; Dave et al. _comma_ 2003; Pang and Lee_comma_ 2004; Cui et al. _comma_ 2006) and train (semi-)supervised learning algorithms to classify reviews as positive or negative_comma_ or in more fine-grained scales (Pang and Lee_comma_ 2005; Wilson et al. _comma_ 2006).
3364,0,Implicitly_comma_ the supervised learning techniques assume that numeric ratings fully encapsulate the sentiment of the review.416 In this paper_comma_ we take a different approach and instead consider the economic context in which an opinion is evaluated.
3365,0,Previous workonsentimentanalysishascoveredawiderange of tasks_comma_ including polarity classification (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ opinion extraction (Pang and Lee_comma_ 2004)_comma_ and opinion source assignment (Choi et al. _comma_ 2005; Choi et al. _comma_ 2006).
3366,0,Furthermore_comma_ these systems have tackled the problem at different levels of granularity_comma_ from the document level (Pang et al. _comma_ 2002)_comma_ sentence level (Pang and Lee_comma_ 2004; Mao and Lebanon_comma_ 2006)_comma_ phrase level (Turney_comma_ 2002; Choi et al. _comma_ 2005)_comma_ as well as the speaker level in debates (Thomas et al. _comma_ 2006).
3367,0,1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al. _comma_ 2002; Turney_comma_ 2002; Goldberg and Zhu_comma_ 2004).
3368,0,After this conversion_comma_ we had 1000 positive and 1000 negative examples for each domain_comma_ the same balanced composition as the polarity dataset (Pang et al. _comma_ 2002).
3369,0,On the polarity dataset_comma_ this model matches the results reported by Pang et al.(2002).
3370,0,7 Related Work Sentiment classification has advanced considerably since the work of Pang et al.(2002)_comma_ which we use as our baseline.
3371,0,2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications_comma_ such as tracking sentiment timelines in online forums and news (Lloyd et al. _comma_ 2005; Balog et al. _comma_ 2006)_comma_ review classification (Turney_comma_ 2002; Pang et al. _comma_ 2002)_comma_ mining opinions from product reviews (Hu and Liu_comma_ 2004)_comma_ automatic expressive text-to-speech synthesis (Alm et al. _comma_ 2005)_comma_ text semantic analysis (Wiebe and Mihalcea_comma_ 2006; Esuli and Sebastiani_comma_ 2006)_comma_ and question answering (Yu and Hatzivassiloglou_comma_ 2003).
3372,0,This approach builds a subjectivity-annotated corpus for the target language through projection_comma_ and then trains a statistical classifier on the resulting corpus (numerous statistical classifiers have been trained for subjectivity or sentiment classification_comma_ e.g._comma_ (Pang et al. _comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003)).
3373,0,Similar to_comma_ e.g._comma_ (Pang et al. _comma_ 2002)_comma_ we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications.
3374,0,Researchers extracted opinions from words_comma_ sentences_comma_ and documents_comma_ and both rule-based and statistical models are investigated (Wiebe et al. _comma_ 2002; Pang et al. _comma_ 2002).
3375,0,[subjective] So far_comma_ none of the studies in sentiment detection (e.g. Wilson et al. _comma_ 2005; Pang et al. _comma_ 2002) or opinion extraction (e.g. Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005) have specifically looked at the role of superlatives in these areas.
3376,0,Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al. _comma_ 2002) andsubjectivityanalysis(Wiebeetal._comma_ 2004)_comma_ butassessing the usefulness of analysis algorithms leveragingtheAppraisalframeworkwillrequiretestdata.
3377,0,One major focus is sentiment classification and opinion mining (e.g._comma_ Pang et al 2002; Turney 2002; Hu and Liu 2004; Wilson et al 2004; Kim and Hovy 2004; Popescu and Etzioni 2005)   2008.
3378,0,However_comma_ these studies mainly center on direct opinions or sentiments expressed on entities.
3379,0,Sentiment classification at the document level investigates ways to classify each evaluative document (e.g._comma_ product review) as positive or negative (Pang et al 2002; Turney 2002).
3380,0,2 Related Work There has been extensive research in opinion mining at the document level_comma_ for example on product and movie reviews (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Dave et al._comma_ 2003; Popescu and Etzioni_comma_ 2005).
3381,0,80 8.0% Positive child education Positive cost Negative SUBJECT increase Figure 3: An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification_comma_ ranging from the use of co-occurrence with typical positive and negative words (Turney_comma_ 2002) to bag of words (Pang et al._comma_ 2002) and dependency structure (Kudo and Matsumoto_comma_ 2004).
3382,0,We use the same set of binary features as in previous work on this dataset (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Zaidan et al._comma_ 2007).
3383,0,Since the work of Pang et al.(2002)_comma_ various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee_comma_ 2004; Mullen and Collier_comma_ 2004; Wilson et al._comma_ 2005a; Read_comma_ 2005).
3384,1,2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al._comma_ 2002; Matsumoto et al._comma_ 2005; Kudo and Matsumoto_comma_ 2004; Mullen and Collier_comma_ 2004; Gamon_comma_ 2004).
3385,1,One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al._comma_ 2005; Kudo and Matsumoto_comma_ 2004; Pang et al._comma_ 2002).
3386,0,1 Introduction Automatic classification of sentiment has been a focus of a number of recent research efforts (e.g.(Turney_comma_ 2002; Pang et al._comma_ 2002; Dave at al._comma_ 2003).
3387,0,2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment (Turney_comma_ 2002; Pang et al._comma_ 2002; Dave at al._comma_ 2003).
3388,1,Unigram models have been previously shown to give good results in sentiment classification tasks (Kennedy and Inkpen_comma_ 2006; Pang et al._comma_ 2002): unigram representations can capture a variety of lexical combinations and distributions_comma_ including those of emotion words.
3389,1,SVM has been shown to be useful for text classification tasks (Joachims_comma_ 1998)_comma_ and has previously given good performance in sentiment classification experiments (Kennedy and Inkpen_comma_ 2006; Mullen and Collier_comma_ 2004; Pang and Lee_comma_ 2004; Pang et al._comma_ 2002).
3390,0,(HICSS-35</booktitle> <contexts> <context>documents_comma_ genres also work on an intra-document_comma_ or page segment level because a single document can contain instances of multiple genres_comma_ e.g._comma_ contact information_comma_ list of publications_comma_ C.V._comma_ see (Rehm_comma_ 2002; Rehm_comma_ 2007; Mehler et al._comma_ 2007).
3391,0,But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al._comma_ 2002).
3392,0,291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon_comma_ 2005; Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Riloff et al._comma_ 2006; Turney_comma_ 2002; Turney and Littman_comma_ 2003) or at the sentence levels (Gamon and Aue_comma_ 2005; Hu and Liu_comma_ 2004; Kim and Hovy_comma_ 2005; Riloff et al._comma_ 2006).
3393,0,It should be noted that each of these levels presents different challenges for sentiment annotation.
3394,0,Sentiment classification is a well studied problem (Wiebe_comma_ 2000; Pang et al._comma_ 2002; Turney_comma_ 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007).
3395,1,Three approaches are dominating_comma_ i.e. knowledge-based approach (Kim and Hovy_comma_ 2004)_comma_ information retrieval-based approach (Turney and Littman_comma_ 2003) and machine learning approach (Pang et al._comma_ 2002)_comma_ in which the last approach is found very popular.
3396,0,Pang et al.(2002) adopt the VSM model to represent product reviews and apply text classification algorithms such as Nave Bayes_comma_ maximum entropy and support vector machines to predict sentiment polarity of given product review.
3397,0,1 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of_comma_ or sentiment toward a given subject (e.g._comma_ if an opinion is supported or not) (Pang et al._comma_ 2002).
3398,1,This task has created a considerable interest due to its wide applications.
3399,0,2 Related Work Sentiment classification has become a hot topic since the publication work that discusses classification of movie reviews by Pang et al.(2002).
3400,0,This was followed by a great many studies into sentiment classification focusing on many domains besides that of movie.
3401,1,Experiment Implementation: We apply SVM algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms (Pang et al._comma_ 2002).
3402,0,Sentiment summarization has been well studied in the past decade (Turney_comma_ 2002; Pang et al._comma_ 2002; Dave et al._comma_ 2003; Hu and Liu_comma_ 2004a_comma_ 2004b; Carenini et al._comma_ 2006; Liu et al._comma_ 2007).
3403,0,There are many research directions_comma_ e.g._comma_ sentiment classification (classifying an opinion document as positive or negative) (e.g._comma_ Pang_comma_ Lee and Vaithyanathan_comma_ 2002; Turney_comma_ 2002)_comma_ subjectivity classification (determining whether a sentence is subjective or objective_comma_ and its associated opinion) (Wiebe and Wilson_comma_ 2002; Yu and Hatzivassiloglou_comma_ 2003; Wilson et al_comma_ 2004; Kim and Hovy_comma_ 2004; Riloff and Wiebe_comma_ 2005)_comma_ feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni_comma_ 2005; Carenini et al._comma_ 2005; Ku et al._comma_ 2006; Kobayashi_comma_ Inui and Matsumoto_comma_ 2007; Titov and McDonald.
3404,0,One of the main directions is sentiment classification_comma_ which classifies the whole opinion document (e.g._comma_ a product review) as positive or negative (e.g._comma_ Pang et al_comma_ 2002; Turney_comma_ 2002; Dave et al_comma_ 2003; Ng et al. 2006; McDonald et al_comma_ 2007).
3405,0,Despite the large amount of recent work on sentiment analysis and opinion mining_comma_ much of it has focused on supervised methods (e.g._comma_ Pang et al.(2002)_comma_ Kim and Hovy (2004)_comma_ Mullen and Collier (2004)).
3406,0,One weakness of these existing supervised polarity classification systems is that they are typically domainand language-specific.
3407,1,We use five sentiment classification datasets_comma_ including the widely-used movie review dataset [MOV] (Pang et al._comma_ 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO)_comma_ DVDs (DVD)_comma_ electronics (ELE)_comma_ and kitchen appliances (KIT)] (Blitzer et al._comma_ 2007).
3408,0,Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives).
3409,0,The former is a task of identifying positive and negative sentiments from a text which can be a passage_comma_ a sentence_comma_ a phrase and even a word (Somasundaran et al._comma_ 2008; Pang et al._comma_ 2002; Dave et al._comma_ 2003; Kim and Hovy_comma_ 2004; Takamura et al._comma_ 2005).
3410,0,Amount of works have been done on sentimental classification in different levels (Zhang et al._comma_ 2009; Somasundaran et al._comma_ 2008; Pang et al._comma_ 2002; Dave et al._comma_ 2003; Kim and Hovy_comma_ 2004; Takamura et al._comma_ 2005).
3411,0,Such a lexicon can be used_comma_ e.g._comma_ to classify individual sentences or phrases as subjective or not_comma_ and as bearing positive or negative sentiments (Pang et al._comma_ 2002; Kim and Hovy_comma_ 2004; Wilson et al._comma_ 2005a).
3412,0,Automatic identification of subjective content often relies on word indicators_comma_ such as unigrams (Pang et al._comma_ 2002) or predetermined sentiment lexica (Wilson et al._comma_ 2005).
3413,0,2 Related Work There has been a large and diverse body of research in opinion mining_comma_ with most research at the text (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Popescu and Etzioni_comma_ 2005; Ounis et al._comma_ 2006)_comma_ sentence (Kim and Hovy_comma_ 2005; Kudo and Matsumoto_comma_ 2004; Riloff et al._comma_ 2003; Yu and Hatzivassiloglou_comma_ 2003) or word (Hatzivassiloglou and McKeown_comma_ 1997; Turney and Littman_comma_ 2003; Kim and Hovy_comma_ 2004; Takamura et al._comma_ 2005; Andreevskaia and Bergler_comma_ 2006; Kaji and Kitsuregawa_comma_ 2007) level.
3414,0,W(S_comma_T) = summationdisplay uS_comma_vT w(u_comma_v) Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice_comma_ using the maximum flow algorithm (Pang and Lee_comma_ 2004; Cormen et al._comma_ 2002).
3415,0,Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al._comma_ 2002; Wiebe and Riloff_comma_ 2005)_comma_ sometimes connected to extrinsic values like prediction markets (Lerman et al._comma_ 2008).
3416,0,Applications have included the categorization of documents by topic (Joachims_comma_ 1998)_comma_ language (Cavnar and Trenkle_comma_ 1994)_comma_ genre (Karlgren and Cutting_comma_ 1994)_comma_ author (Bosch and Smith_comma_ 1998)_comma_ sentiment (Pang et al._comma_ 2002)_comma_ and desirability (Sahami et al._comma_ 1998).
3417,0,The research of opinion mining began in 1997_comma_ the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al._comma_ 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al._comma_ 2003; Pang et al._comma_ 2002; Zagibalov et al._comma_ 2008;).
3418,0,There has been a great deal of previous work in sentiment analysis that worked with reviews_comma_ but they were typically restricted to using reviews extracted from one or two well-known sources_comma_ bypassing automatic review detection.
3419,0,Examples of such early work include (Turney_comma_ 2002; Pang et al._comma_ 2002; Dave et al._comma_ 2003; Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005).
3420,0,1   Introduction In the community of sentiment analysis (Turney 2002; Pang et al._comma_ 2002; Tang et al._comma_ 2009)_comma_ transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work_comma_ because sentiment expression often behaves with strong domain-specific nature.
3421,0,Since the work of Pang et al.(2002)_comma_ various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee_comma_ 2004; Mullen and Collier_comma_ 2004; Wilson et al._comma_ 2005; Read_comma_ 2005).
3422,1,In their seminal work_comma_ (Pang et al._comma_ 2002) demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms.
3423,0,Most semi-automated approaches have met with limited success (Ng et al._comma_ 2006) and supervised learning models have tended to outperform dictionary-based classi cation schemes (Pang et al._comma_ 2002).
3424,1,Movies Reviews: This is a popular dataset in sentiment analysis literature (Pang et al._comma_ 2002).
3425,0,It consists of 1000 positive and 1000 negative movie reviews drawn from the IMDB archive of the rec.arts.movies.reviews newsgroups.
3426,1,In particular_comma_ the use of SVMs in (Pang et al._comma_ 2002) initially sparked interest in using machine learning methods for sentiment classi cation.
3427,0,Note that none of these competing methods utilizes lexical knowledge.
3428,0,Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al._comma_ 2002).
3429,0,Pang et al.(2002) employed n-gram and POS features for ML methods to classify movie-review data.
3430,1,Pang et al.(2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task.
3431,0,Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks_comma_ that_comma_ unlike fact-based text analysis tasks_comma_ repetition of terms does not imply a significance on the overall sentiment.
3432,0,Specifically_comma_ we explore the statistical term weighting features of the word generation model with Support Vector machine (SVM)_comma_ faithfully reproducing previous work as closely as possible (Pang et al._comma_ 2002).
3433,0,Accuracy Features Movie-review MPQA PRESENCE 82.6 76.8 TF 71.1 76.5 VS.TF 81.3 76.7 BM25.TF 81.4 77.9 IDF 61.6 61.8 VS.IDF 83.6 77.9 BM25.IDF 83.6 77.8 VS.TFVS.IDF 83.8 77.9 BM25.TFBM25.IDF 84.1 77.7 BM25.TFVS.IDF 85.1 77.7 first introduced by Pang et al.(2002) to test various ML-based methods for sentiment classification.
3434,0,To closely reproduce the experiment with the best performance carried out in (Pang et al._comma_ 2002) using SVM_comma_ we use unigram with the presence feature.
3435,0,As observed by Pang et al.(2002)_comma_ using the raw tf drops the accuracy of the sentiment classification (-13.92%) of movie-review data.
3436,0,Recently_comma_ sentiment classification has become popular because of its wide applications (Pang et al._comma_ 2002).
3437,0,Among these methods_comma_ SVM is shown to perform better than other methods (Yang and Pedersen_comma_ 1997; Pang et al._comma_  1 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/  2002).
3438,1,4 Evaluation 4.1 Experimental Setup For evaluation_comma_ we use five sentiment classification datasets_comma_ including the widely-used movie review dataset [MOV] (Pang et al._comma_ 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO)_comma_ DVDs (DVD)_comma_ electronics (ELE)_comma_ and kitchen appliances (KIT)] (Blitzer et al._comma_ 2007).
3439,0,Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives).
3440,1,While the NASA researchers have applied a heuristic method for labeling a report with shapers (Posse 1http://kdd.ics.uci.edu/databases/20newsgroups/ 2Of course_comma_ the fact that sentiment classification requires a deeper understanding of a text also makes it more difficult than topic-based text classification (Pang et al._comma_ 2002).
3441,0,1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews_comma_ product reviews_comma_ news and blog reviews (Pang et al._comma_ 2002; Turney_comma_ 2002).
3442,1,In most cases_comma_ supervised learning methods can perform well (Pang et al._comma_ 2002).
3443,0,But when training data and test data are drawn from different domains_comma_ supervised learning methods always produce disappointing results.
3444,0,This formulation is similar to the energy minimization framework_comma_ which is commonly used in image analysis (Besag_comma_ 1986; Boykov et al. _comma_ 1999) and has been recently applied in natural language processing (Pang and Lee_comma_ 2004).
3445,0,7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment_comma_ for example determining whether a review is positive or negative (e.g. _comma_ (Turney_comma_ 2002; Dave et al. _comma_ 2003; Pang and Lee_comma_ 2004; Beineke et al. _comma_ 2004)).
3446,0,Much of this research explores sentiment classification_comma_ a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g. _comma_ Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
3447,0,Other research efforts analyze opinion expressions at the sentence level or below to recognize opinions_comma_ their polarity_comma_ and their strength (e.g. _comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)_comma_ Wilson et al.(2004)_comma_ Yu and Hatzivassiloglou (2003)_comma_ Wiebe and Riloff (2005)).
3448,0,Many NLP problems address attitudinal meaning distinctions in text_comma_ e.g. detecting subjective opinion documents or expressions_comma_ e.g.(Wiebe et al_comma_ 2004)_comma_ measuring strength of subjective clauses (Wilson_comma_ Wiebe and Hwa_comma_ 2004)_comma_ determining word polarity (Hatzivassiloglou and McKeown_comma_ 1997) or texts attitudinal valence_comma_ e.g.(Turney_comma_ 2002)_comma_ (Bai_comma_ Padman and Airoldi_comma_ 2004)_comma_ (Beineke_comma_ Hastie and Vaithyanathan_comma_ 2003)_comma_ (Mullen and Collier_comma_ 2003)_comma_ (Pang and Lee_comma_ 2003).
3449,1,Recently_comma_ graph-based methods have proved useful for a number of NLP and IR tasks such as document re-ranking in ad hoc IR (Kurland and Lee_comma_ 2005) and analyzing sentiments in text (Pang and Lee_comma_ 2004).
3450,0,Much of the relevant research explores sentiment classification_comma_ a text categorization task in which the goal is to assign to a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
3451,0,Recent work_comma_ for example_comma_ indicates that systems can be trained to recognize opinions_comma_ their polarity_comma_ their source_comma_ and their strength to a reasonable degree of accuracy (e.g. Dave et al.(2003)_comma_ Riloff and Wiebe (2003)_comma_ Bethard et al.(2004)_comma_ Pang and Lee (2004)_comma_ Wilson et al.(2004)_comma_ Yu and Hatzivassiloglou (2003)_comma_ Wiebe and Riloff (2005)).
3452,0,In the document classification approach_comma_ researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g.(Dave et al. _comma_ 2003; Pang and Lee_comma_ 2004; Turney_comma_ 2002)).
3453,1,All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classi cation (Pang and Lee_comma_ 2004).
3454,1,Interestingly_comma_ previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee_comma_ 2004).
3455,1,A later study (Pang and Lee_comma_ 2004) found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective.
3456,0,1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002_comma_ Pang et al. 2004_comma_ Turney 2002_comma_ Turney and Littman 2002_comma_ Wiebe et al. 2001_comma_ Bai et al. 2004_comma_ Yu and Hatzivassiloglou 2003 and many others).
3457,0,Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002_comma_ Pang et al. 2004_comma_ Turney 2002_comma_ Turney and Littman 2002).
3458,0,Results on the movie domain We also performed a small set of experiments on the movie domain using Pang and Lees 2004 data set.
3459,0,This set consists of 2000 reviews_comma_ 1000 each of very positive and very negative reviews.
3460,0,Since this data set is balanced and the task is only a two-way classification between positive and negative reviews_comma_ we only report accuracy numbers here.
3461,0,accuracy Training data Turney (2002) 66% unsupervised Pang & Lee (2004) 87.15% supervised Aue & Gamon (2005) 91.4% supervised SO 73.95% unsupervised SM+SO to increase seed words_comma_ then SO 74.85% weakly supervised Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web.
3462,0,Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection.
3463,0,This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee_comma_ 2004; Yu and Hatzivassiloglou_comma_ 2003); 2.
3464,0,determining document orientation (or polarity)_comma_ as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee_comma_ 2004; Turney_comma_ 2002); 3.
3465,0,Inspired by the idea of graph based algorithms to collectively rank and select the best candidate_comma_ research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau_comma_ 2004)_comma_ text summarization (Erkan and Radev_comma_ 2004; Mihalcea_comma_ 2004)_comma_ word sense disambiguation (Mihalcea et al. _comma_ 2004; Mihalcea_comma_ 2005)_comma_ sentiment analysis (Pang and Lee_comma_ 2004)_comma_ and sentence retrieval for question answering (Otterbacher et al. _comma_ 2005).
3466,0,There are also research work on automatically classifying movie or product reviews as positive or negative (Nasukawa and Yi_comma_ 2003; Mullen and Collier_comma_ 2004; Beineke et al. _comma_ 2004; Pang and Lee_comma_ 2004; Hu and Liu_comma_ 2004).
3467,0,The third exploits automatic subjectivity analysis in applications such as review classification (e.g. _comma_ (Turney_comma_ 2002; Pang and Lee_comma_ 2004))_comma_ mining texts for product reviews (e.g. _comma_ (Yi et al. _comma_ 2003; Hu and Liu_comma_ 2004; Popescu and Etzioni_comma_ 2005))_comma_ summarization (e.g. _comma_ (Kim and Hovy_comma_ 2004))_comma_ information extraction (e.g. _comma_ (Riloff et al. _comma_ 2005))_comma_ 1Note that sentiment_comma_ the focus of much recent work in the area_comma_ is a type of subjectivity_comma_ specifically involving positive or negative opinion_comma_ emotion_comma_ or evaluation.
3468,0,Next_comma_ we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets_comma_ one assembled by Pang and Lee (2004) and the other by ourselves.
3469,1,Indeed_comma_ recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g_comma_ Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g. _comma_ Pang and Lee (2004)).
3470,0,4.1 Experimental Setup Like several previous work (e.g. _comma_ Mullen and Collier (2004)_comma_ Pang and Lee (2004)_comma_ Whitelaw et al.(2005))_comma_ we view polarity classification as a supervised learning task.
3471,0,The first one_comma_ which we will refer to as Dataset A_comma_ is the Pang et al. polarity dataset (version 2.0).
3472,0,Reviews in Dataset B were randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews (see Section 3) that have either a positive or a negative rating.
3473,0,We followed exactly Pang et al.s guideline when determining whether a review is positive or negative.14 Also_comma_ we took care to ensure that reviews included in Dataset B do not appear in Dataset A. We applied to these reviews the same four pre-processing steps that we did to the neutral reviews in the previous section.
3474,0,14The guidelines come with their polarity dataset.
3475,0,Note that our result on Dataset A is as strong as that obtained by Pang and Lee (2004) via their subjectivity summarization algorithm_comma_ which retains only the subjective portions of a document.
3476,0,The resulting classifier achieves an accuracy of 87.2% and 82.7% for Datasets A and B_comma_ respectively.
3477,0,To construct the lexicon_comma_ we take Pang et al.s pool of unprocessed documents (see Section 3)_comma_ remove those that appear in either Dataset A or Dataset B17_comma_ and compile a list of adjectives from the remaining documents.
3478,0,In comparison to the classifier that uses only n-grams and dependency-based features (row 3)_comma_ accuracy increases significantly (p =.1) from 89.2% to 90.4% for Dataset A_comma_ and from 84.7% to 86.2% for Dataset B. These results suggest that the classifier has benefited from the 17We treat the test documents as unseen data that should not be accessed for any purpose during system development.
3479,0,Due to space limitations_comma_ we will simply present results on Dataset A below_comma_ and show results on Dataset B only in cases where a different trend is observed.
3480,0,For Dataset A_comma_ the classifier achieves an accuracy of 87.1%_comma_ which is statistically indistinguishable from our baseline result.
3481,0,The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document eitherpositive(thumbsup)ornegative(thumbs down) polarity (e.g. Das and Chen (2001)_comma_ Pang et al.(2002)_comma_ Turney (2002)_comma_ Dave et al.(2003)_comma_ Pang and Lee (2004)).
3482,0,Recent work has shown that systems can be trained to recognize opinions_comma_ their polarity_comma_ and their strength at a reasonable degree of accuracy (e.g. Dave et al.(2003)_comma_ Riloff and Wiebe (2003)_comma_ Bethard et al.(2004)_comma_ Pang and Lee (2004)_comma_ Wilson et al.(2004)_comma_ Yu and Hatzivassiloglou (2003)_comma_ Wiebe and Riloff (2005)).
3483,0,For process (2)_comma_ existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy_comma_ 2004; Pang and Lee_comma_ 2004; Riloff and Wiebe_comma_ 2003).
3484,0,For process (3)_comma_ machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al. _comma_ 2003; Beineke et al. _comma_ 2004; Hu and Liu_comma_ 2004; Pang and Lee_comma_ 2004) or multipoint scale categories (Kim and Hovy_comma_ 2004; Pang and Lee_comma_ 2005).
3485,0,The focus of much of the automatic sentiment analysis research is on identifying the affect bearing words (words with emotional content) and on measurement approaches for sentiment (Turney & Littman_comma_ 2003; Pang & Lee_comma_ 2004; Wilson et al. _comma_ 2005).
3486,0,New text is automatically classified by comparing the feature vectors with the training set.
3487,0,(Pang & Lee_comma_ 2004; Aue & Gamon_comma_ 2005).
3488,0,This methodology generally requires a large amount of training data and is domain dependent.
3489,0,As has been previously observed and exploited in the NLP literature (Pang and Lee_comma_ 2004; Agarwal and Bhattacharyya_comma_ 2005; Barzilay and Lapata_comma_ 2005)_comma_ the above optimization function_comma_ unlike many others that have been proposed for graph or set partitioning_comma_ can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs.
3490,0,In contrast to the opinion extracts produced by Pang and Lee (2004)_comma_ our summaries are not text extracts_comma_ but rather explicitly identify and 337 characterize the relations between opinions and their sources.
3491,0,Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe_comma_ 2003; Pang and Lee_comma_ 2004)_comma_ which is two-fold classification into subjective and objective sentences.
3492,0,3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al. _comma_ 2004)_comma_ the Polarity data set5 created by (Pang and Lee_comma_ 2004)_comma_ and the MPQA data set created by (Wiebe et al. _comma_ 2005).6 The OP and Polarity data sets involve document-level opinion classi cation_comma_ while the MPQA data set involves 5Version v2.0_comma_ which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classi cation.
3493,0,The Polarity data consists of 700 positive and 700 negative reviews from the Internet Movie Database (IMDb) archive.
3494,0,The bottom table in Figure 6 shows feature pairs identi ed for their behavioral differences on the Polarity data set_comma_ where the task is to distinguish positive reviews from negative reviews.
3495,0,Features Base =.0005 =.001 =.002 1Gram 79.8 1+2Gram 81.2 81.0 81.3 81.0 1+EP 81.7 81.4 81.4 82.0 1+2+EP 81.7 82.3 82.3 82.7 Figure 7: Accuracies on Polarity Data Features Base =.0005 =.001 =.002 1Gram 97.5 1+2Gram 98.0 98.7 98.6 98.7 1+EP 97.2 97.8 97.9 97.9 1+2+EP 97.8 98.6 98.7 98.7 Figure 8: Accuracies on OP Data Features Base =.0005 =.001 =.002 1Gram 74.8 1+2Gram 74.3 74.9 74.6 74.8 1+EP 74.4 74.6 74.6 74.6 1+2+EP 74.4 74.9 74.7 74.6 Figure 9: Accuracies on MPQA Data We also observed that subsumption had a dramatic effect on the F-measure scores on the OP data_comma_ which are shown in Figure 10.
3496,0,78 78.5 79 79.5 80 80.5 81 81.5 82 82.5 83 83.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 12: Feature Selection on Polarity Data 72 72.5 73 73.5 74 74.5 75 75.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.0005 Feature Selection Subsumption d=0.0005 + Feature Selection Figure 13: Feature Selection on MPQA Data just a single data point (accuracy value)_comma_ but we drew that value as a line across the graph for the sake of comparison.
3497,0,The best accuracy results are 99.0% on the OP data_comma_ 83.1% on the Polarity data_comma_ and 75.4% on the MPQA data.
3498,0,For other work see also Dave et al.(2003)_comma_ Pang and Lee (2004_comma_ 2005).
3499,0,For examples_comma_ see (Erkan and Radev_comma_ 2004; Mihalcea and Tarau_comma_ 2004; Pang and Lee_comma_ 2004).
3500,0,2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee_comma_ 2004)_comma_ random walks (Mihalcea_comma_ 2005; Otterbacher et al. _comma_ 2005)_comma_ graph matching (Haghighi et al. _comma_ 2005)_comma_ and label propagation (Niu et al. _comma_ 2005).
3501,1,We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al. _comma_ 2002; Pang and Lee_comma_ 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive_comma_ all written before 2002 by a total of 312 authors_comma_ with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable.
3502,0,Pang and Lee have divided the 2000 documents into 10 folds_comma_ each consisting of 100 positive reviews and 100 negative reviews.
3503,0,The dataset is arguably artificial in that it keeps only reviews where the reviewer provided a rather high or rather low numerical rating_comma_ allowing Pang and Lee to designate the review as positive or negative.
3504,0,The annotators classification accuracies in Tasks 1 and 3 (against Pang & Lees labels) ranged from 92%97%_comma_ with 4-way agreement on the class for 89% of the documents_comma_ and pairwise agreement also ranging from 92%97%.
3505,0,5 Experimental Procedures 5.1 Feature extraction Although this dataset seems to demand discourselevel features that contextualize bits of praise and criticism_comma_ we exactly follow Pang et al.(2002) and Pang and Lee (2004) in merely using binary unigram features_comma_ corresponding to the 17_comma_744 unstemmed word or punctuation types with count  4 in the full 2000-document corpus.
3506,0,Sentiment analysis includes a variety of different problems_comma_ including: sentiment classification techniques to classify reviews as positive or negative_comma_ based on bag of words (Pang et al. _comma_ 2002) or positive and negative words (Turney_comma_ 2002; Mullen and Collier_comma_ 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe_comma_ 2003; Pang and Lee_comma_ 2004); identifying or classifying appraisal targets (Nigam and Hurst_comma_ 2004); identifying the source of an opinion in a text (Choi et al. _comma_ 2005)_comma_ whether the author is expressing the opinion_comma_ or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. _comma_ 2005; Popescu and Etzioni_comma_ 2005).
3507,0,Much of this work has utilized the fundamental concept of semantic orientation_comma_ (Turney_comma_ 2002); however_comma_ sentiment analysis still lacks a unified field theory.
3508,0,Finally_comma_ other approaches rely on reviews with numeric ratings from websites (Pang and Lee_comma_ 2002; Dave et al. _comma_ 2003; Pang and Lee_comma_ 2004; Cui et al. _comma_ 2006) and train (semi-)supervised learning algorithms to classify reviews as positive or negative_comma_ or in more fine-grained scales (Pang and Lee_comma_ 2005; Wilson et al. _comma_ 2006).
3509,0,Implicitly_comma_ the supervised learning techniques assume that numeric ratings fully encapsulate the sentiment of the review.416 In this paper_comma_ we take a different approach and instead consider the economic context in which an opinion is evaluated.
3510,0,Previous workonsentimentanalysishascoveredawiderange of tasks_comma_ including polarity classification (Pang et al. _comma_ 2002; Turney_comma_ 2002)_comma_ opinion extraction (Pang and Lee_comma_ 2004)_comma_ and opinion source assignment (Choi et al. _comma_ 2005; Choi et al. _comma_ 2006).
3511,0,Furthermore_comma_ these systems have tackled the problem at different levels of granularity_comma_ from the document level (Pang et al. _comma_ 2002)_comma_ sentence level (Pang and Lee_comma_ 2004; Mao and Lebanon_comma_ 2006)_comma_ phrase level (Turney_comma_ 2002; Choi et al. _comma_ 2005)_comma_ as well as the speaker level in debates (Thomas et al. _comma_ 2006).
3512,1,In fact_comma_ it has already been established that sentence level classification can improve document level analysis (Pang and Lee_comma_ 2004).
3513,0,This line of reasoning suggests that a cascaded approach would also be insufficient.
3514,0,Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004).
3515,0,In that work an initial model classified each sentence as being subjective or objective using a global mincut inference algorithm that considered local labeling consistencies.
3516,0,The current work differs from that in Pang and Lee through the use of a single joint structured model for both sentence and document level analysis.
3517,0,For instance_comma_ in Pang and Lee (2004)_comma_ yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective.
3518,0,The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm.
3519,0,However_comma_ jointly modeling the document label and allowing for non-binary labelscomplicatesmin-cut stylesolutionsasinference becomes intractable.
3520,0,In both cases there 1Alternatively_comma_ decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee_comma_ 2004).
3521,1,First_comma_ even when sentiment is the desired focus_comma_ researchers in sentiment analysis have shown that a two-stage approach is often beneficial_comma_ in which subjective instances are distinguished from objective ones_comma_ and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou_comma_ 2003; Pang and Lee_comma_ 2004; Wilson et al. _comma_ 2005; Kim and Hovy_comma_ 2006).
3522,0,It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by Pang and Lee (2004) and Eriksson (2006).
3523,0,Unlike previous annotations of sentiment or subjectivity (Wiebe et al. _comma_ 2005; Pang and Lee_comma_ 2004)_comma_ which typically relied on binary 0/1 annotations_comma_ we decided to use a finer-grained scale_comma_ hence allowing the annotators to select different degrees of emotional load.
3524,1,Sentence-level subjectivity detection_comma_ where training data is easier to obtain than for positive vs. negative classification_comma_ has been successfully performed using supervised statistical methods alone (Pang and Lee_comma_ 2004) or in combination with a knowledgebased approach (Riloff et al. _comma_ 2006).
3525,0,Since the extant literature does not provide clear evidence for the choice between supervised machine learning methods and unsupervised knowledgebased approaches for the task of ternary sentiment classification of sentences or headlines_comma_ we developed two systems for the Affective Text task at SemEval-2007.
3526,1,3 CLaC-NB System: Nave Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level: on movie review texts they reach an accuracy of 85-90% (Aue and Gamon_comma_ 2005; Pang and Lee_comma_ 2004) and up to 92% accuracy on classifying movie review snippets into subjective and objective using both Nave Bayes and SVM (Pang and Lee_comma_ 2004).
3527,0,6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al._comma_ 2003; Pang and Lee_comma_ 2004) and adjacency pair information has been used to predict congressional votes (Thomas et al._comma_ 2006).
3528,0,2 Related Work There has been extensive research in opinion mining at the document level_comma_ for example on product and movie reviews (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Dave et al._comma_ 2003; Popescu and Etzioni_comma_ 2005).
3529,0,Movie-domainSubjectivityDataSet(Movie): Pang and Lee (2004) used a collection of labeled subjective and objective sentences in their work on review classification.5 The data set contains 5000 subjective sentences_comma_ extracted from movie reviews collected from the Rotten Tomatoes web formed best.
3530,0,Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts.
3531,0,Within NLP_comma_ applications include sentiment-analysis problems (Pang and Lee_comma_ 2004; Agarwal and Bhattacharyya_comma_ 2005; Thomas et al._comma_ 2006) and content selection for text generation (Barzilay and Lapata_comma_ 2005).
3532,0,We collect substring rationales for a sentiment classification task (Pang and Lee_comma_ 2004) and use them to obtain significant accuracy improvements for each annotator.
3533,0,Our new generative approach exploits the rationales more effectively than our previous masking SVM approach.
3534,0,4 Experimental Data: Movie Reviews In Zaidan et al.(2007)_comma_ we introduced the Movie Review Polarity Dataset Enriched with Annotator Rationales.8 It is based on the dataset of Pang and Lee (2004)_comma_9 which consists of 1000 positive and 1000 negative movie reviews_comma_ tokenized and divided into 10 folds (F0F9).
3535,0,We use the same set of binary features as in previous work on this dataset (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Zaidan et al._comma_ 2007).
3536,0,Since the work of Pang et al.(2002)_comma_ various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee_comma_ 2004; Mullen and Collier_comma_ 2004; Wilson et al._comma_ 2005a; Read_comma_ 2005).
3537,0,Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents.
3538,0,5 Evaluation 5.1 Datasets We used two datasets_comma_ customer reviews 1 (Hu and Liu_comma_ 2004) and movie reviews 2 (Pang and Lee_comma_ 2005) to evaluate sentiment classification of sentences.
3539,0,Both of these two datasets are often used for evaluation in sentiment analysis researches.
3540,0,However_comma_ on movie review dataset_comma_ the proposed method did not outperform 3gram.
3541,0,We evaluated three models: sentence-wise_comma_ 3gram model and hybrid 3gram on both customer review and movie review.
3542,0,Figures 1 and 2 show the results on customer review and movie review respectively.
3543,0,When the size of the training data is small_comma_ sentence-wise outper301 Figure 1: Experimental results on customer review Figure 2: Experimental results on movie review forms 3gram on both datasets.
3544,0,There has also been previous work on determining whether a given text is factual or expresses opinion (Yu& Hatzivassiloglu_comma_ 2003; Pang & Lee_comma_ 2004); again this work uses a binary distinction_comma_ and supervised rather than unsupervised approaches.
3545,1,SVM has been shown to be useful for text classification tasks (Joachims_comma_ 1998)_comma_ and has previously given good performance in sentiment classification experiments (Kennedy and Inkpen_comma_ 2006; Mullen and Collier_comma_ 2004; Pang and Lee_comma_ 2004; Pang et al._comma_ 2002).
3546,0,291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon_comma_ 2005; Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Riloff et al._comma_ 2006; Turney_comma_ 2002; Turney and Littman_comma_ 2003) or at the sentence levels (Gamon and Aue_comma_ 2005; Hu and Liu_comma_ 2004; Kim and Hovy_comma_ 2005; Riloff et al._comma_ 2006).
3547,1,Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon_comma_ 2005; Pang and Lee_comma_ 2004).
3548,1,These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon_comma_ 2005).
3549,0,It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks (Pang and Lee_comma_ 2004).
3550,0,Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative.
3551,0,In many applications_comma_ it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee_comma_ 2004)(Esuli and Sebastiani_comma_ 2006).
3552,0,So far_comma_ linguistic cues have played an important role in research of subjectivity recognition (e.g.(Wilson et al._comma_ 2006))_comma_ sentiment analysis (e.g.(Wilson et al._comma_ 2005; Pang and Lee_comma_ 2004))_comma_ and emotion studies (e.g.(Pennebaker et al._comma_ 2001)).
3553,0,5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al._comma_ 2003; Pang and Lee_comma_ 2004)) and adjacency pair information has been used to predict congressional votes (Thomas et al._comma_ 2006).
3554,0,However_comma_ these methods do not explicitly model the relations between opinions.
3555,0,With this model_comma_ we can provide not only qualitative textual summarization such as good food and bad service_comma_ but also a numerical scoring of sentiment_comma_ i.e._comma_ how good the food is and how bad the service is. 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee_comma_ 2004_comma_ 2005; Gamon et al._comma_ 2005; Popescu and Etzioni_comma_ 2005; Liu et al._comma_ 2005; Zhuang et al._comma_ 2006; Kim and Hovy_comma_ 2006).
3556,0,Others use sentence cohesion (Pang and Lee_comma_ 2004)_comma_ agreement/disagreement between speakers (Thomas et al._comma_ 2006; Bansal et al._comma_ 2008)_comma_ or structural adjacency.
3557,0,et al._comma_ 2007)) and unigrams (used by many researchers_comma_ e.g._comma_ (Pang and Lee_comma_ 2004)).
3558,0,Second_comma_ benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou_comma_ 2003; Pang and Lee_comma_ 2004; Wilson et al._comma_ 2005a; Kim and Hovy_comma_ 2006).
3559,0,2 Literature Survey The task of sentiment analysis has evolved from document level analysis (e.g._comma_ (Turney._comma_ 2002); (Pang and Lee_comma_ 2004)) to sentence level analysis (e.g._comma_ (Hu and Liu._comma_ 2004); (Kim and Hovy._comma_ 2004); (Yu and Hatzivassiloglou_comma_ 2003)).
3560,0,These researchers first set priors on words using a prior polarity lexicon.
3561,0,Mincuts have been used 4As of this writing_comma_ WordNet is available for more than 40 world languages (http://www.globalwordnet.org) Figure 2: Semi-supervised classification using mincuts in semi-supervised learning for various tasks_comma_ including document level sentiment analysis (Pang and Lee_comma_ 2004).
3562,0,2 Related Work There has been a large and diverse body of research in opinion mining_comma_ with most research at the text (Pang et al._comma_ 2002; Pang and Lee_comma_ 2004; Popescu and Etzioni_comma_ 2005; Ounis et al._comma_ 2006)_comma_ sentence (Kim and Hovy_comma_ 2005; Kudo and Matsumoto_comma_ 2004; Riloff et al._comma_ 2003; Yu and Hatzivassiloglou_comma_ 2003) or word (Hatzivassiloglou and McKeown_comma_ 1997; Turney and Littman_comma_ 2003; Kim and Hovy_comma_ 2004; Takamura et al._comma_ 2005; Andreevskaia and Bergler_comma_ 2006; Kaji and Kitsuregawa_comma_ 2007) level.
3563,0,Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee_comma_ 2004; Agarwal and Bhattacharyya_comma_ 2005; Thomas et al._comma_ 2006)_comma_ instead of aiming at dictionary annotation as we do.
3564,0,We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences_comma_ used in Pang and Lee (2004)) at the word sense level.
3565,0,W(S_comma_T) = summationdisplay uS_comma_vT w(u_comma_v) Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice_comma_ using the maximum flow algorithm (Pang and Lee_comma_ 2004; Cormen et al._comma_ 2002).
3566,0,In fact_comma_ researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou_comma_ 2003; Pang and Lee_comma_ 2004; Wilson et al._comma_ 2005; Kim and Hovy_comma_ 2006).
3567,0,The description of the minimum cut framework in Section 4.1 was inspired by Pang and Lee (2004).
3568,0,3.1 Data and Experimental Setup The data set by Pang and Lee (2004) consists of 2000 movie reviews (1000-pos_comma_ 1000-neg) from the IMDb review archive.
3569,0,Since the work of Pang et al.(2002)_comma_ various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee_comma_ 2004; Mullen and Collier_comma_ 2004; Wilson et al._comma_ 2005; Read_comma_ 2005).
3570,1,A two-tier scheme (Pang and Lee_comma_ 2004) where sentences are  rst classi ed as subjective versus objective_comma_ and then applying the sentiment classi er on only the subjective sentences further improves performance.
3571,0,Results in these papers also suggest that using more sophisticated linguistic models_comma_ incorporating parts-of-speech and n-gram language models_comma_ do not improve over the simple unigram bag-of-words representation.
3572,1,Moreover_comma_ Ng et al.(2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%_comma_ which is higher than the result reported by Pang and Lee (2004) with the same dataset.
3573,1,And 20NG is a collection of approximately 20_comma_000 20-category documents 1 . In sentiment text classification_comma_ we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee_comma_ 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al._comma_ 2007).
3574,0,Among these methods_comma_ SVM is shown to perform better than other methods (Yang and Pedersen_comma_ 1997; Pang et al._comma_  1 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 3 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/  2002).
3575,0,For instance_comma_ Pang and Lee (2004) train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification.
3576,0,Pang & Lee (2004) propose the use of language models for sentiment analysis task and subjectivity extraction.
3577,0,3.3 Language Model (LM) As a second baseline we use the classification based on the language model using overlapping ngram sequences (n was set to 8) as suggested by Pang & Lee (2004_comma_ 2005) for the English language.
3578,0,Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al._comma_ 2004)_comma_ and also on accurate sentiment polarity assignment (Turney_comma_ 2002; Yi et al._comma_ 2003; Pang and Lee_comma_ 2004; Sindhwani and Melville_comma_ 2008; Melville et al._comma_ 2009).
3579,0,There is also work in sentiment analysis relying on optimization or clustering-based approaches.
3580,0,Pang and Lee (2004) frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences.
3581,0,They produce compressed versions of movie reviews using just the subjective sentences_comma_ which retain the polarity information of the review.
