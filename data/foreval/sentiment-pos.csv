id,sentiment,review
1,1,SCL addresses both of these issues simultaneously by aligning features from the two domains.
2,1,Thus although [S+T+] is easier to handle_comma_ [S+T-] is of higher practical importance.
3,1,For [S+T-]_comma_ the resulting algorithm is a balanced bootstrapping algorithm_comma_ which was shown to outperform the standard bootstrapping algorithm.
4,1,In this paper_comma_ we assume the [S+T-] settings_comma_ and we show that the approach proposed in this paper_comma_ domain adaptive bootstrapping (DAB)_comma_ outperforms the balanced bootstrapping algorithm on NER.
5,1,So far_comma_ SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007).
6,1,We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains.
7,1,Similarly_comma_ Structural Correspondence Learning (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007; Blitzer_comma_ 2008) has proven to be successful for the two tasks examined_comma_ PoS tagging and Sentiment Classification.
8,1,We examine the effectiveness of Structural Correspondence Learning (SCL) (Blitzer et al._comma_ 2006) for this task_comma_ a recently proposed adaptation technique shown to be effective for PoS tagging and Sentiment Analysis.
9,1,Thus_comma_ our first instantiation of SCL for parse disambiguation indeed shows promising results.
10,1,The empirical results show that our instantiation of SCL to parse disambiguation gives promising initial results_comma_ even without the many additional extensions on the feature level as done in Blitzer et al.(2006).
11,1,On the three examined datasets_comma_ SCL slightly but constantly outperformed the baseline.
12,1,c 2009 Association for Computational Linguistics Improving SCL Model for Sentiment-Transfer Learning  Songbo Tan Institute of Computing Technology Beijing_comma_ China tansongbo@software.ict.ac.cn Xueqi Cheng Institute of Computing Technology Beijing_comma_ China cxq@ict.ac.cn  ABSTRACT In recent years_comma_ Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning.
13,1,Among these techniques_comma_ SCL (Structural Correspondence Learning) (Blitzer et al._comma_ 2006) is regarded as a promising method to tackle transfer-learning problem.
14,1,A preliminary evaluation favors the use of SCL over the simpler self-training techniques.
15,1,2 Previous Work So far_comma_ Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al._comma_ 2006; Blitzer et al._comma_ 2007).
16,1,A recent attempt (Plank_comma_ 2009) shows promising results on applying SCL to parse disambiguation.
17,1,SCL and Self-training results The results for SCL (Table 3) show a small_comma_ but consistent increase in absolute performance on all testsets over the baselines (up to +0.27 absolute CA or 7.34% relative error reduction_comma_ which is significant at p < 0.05 according to sign test).
18,1,In contrast_comma_ basic self-training (Table 3) achieves roughly only baseline accuracy and lower performance than SCL_comma_ with one exception.
19,1,However_comma_ the improvements of both SCL and self-training are not significant on this rather 39 small testset.
20,1,The main conclusion is that in contrast to SCL_comma_ none of the self-training instantiations achieves a significant improvement over the baseline.
21,1,The more indirect exploitation of unlabeled data through SCL is more fruitful than pure self-training.
22,1,In all cases SCL still performs best.
23,1,Again_comma_ cursory investigations have thus far supported Turneys conclusion that the former are the appropriate terms to use for this task.
24,1,Turney also reported good result without domain customization (Turney_comma_ 2002).
25,1,(Turney_comma_ 2002) is one of the most famous work that discussed learning polarity from corpus.
26,1,SO-PMI is an unsupervised approach proposed by Turney that has been shown to work well for English.
27,1,This approach has previously been successfully used on English.
28,1,An important potential application of such work is in business intelligence: brands and company image are valuable property_comma_so organizations want to know how they are viewed by the media (what the 'spin' is on news stories_comma_ and editorials)_comma_ business analysts (as expressed in stock market reports)_comma_ customers (for example on product review sites) and their own employees.
29,1,Another important application is to help people find out others' views about products they have purchased(e.g. consumer electronics)_comma_ services and entertainment (e.g. movies)_comma_ stocks and shares (from investor bulletin boards)_comma_ and so on.
30,1,Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification.
31,1,Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification.
32,1,The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al._comma_ 2003)_comma_ or quality annotation data for statistical training.
33,1,Recently_comma_ researchers have explored additional knowledge sources that could enhance automatic evaluation.
34,1,We see that METEOR_comma_ as the other metric sitting in the middle of n-gram based metrics and loose sequence metrics_comma_ achieves improvement over BLEU in both adequacy and uency evaluation.
35,1,SSCN2 and SSCN u are also competitive to the state-of-art MT metrics such as METEOR and SIA.
36,1,It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision (Banerjee and Lavie_comma_ 2005).
37,1,However_comma_ the METEOR tuning yields extremely high TER and low BLEU scores.
38,1,The pair of graphs show_comma_ especially in the case of the larger feature set_comma_ that a large improvement in classification accuracy does not bring proportional improvement in its corresponding metricss correlation; with an accuracy of near 90%_comma_ its correlation coefficient is 0.362_comma_ well below METEOR.
39,1,Interestingly_comma_ next to METEOR boosted with WordNet_comma_ it is the dependency-based method_comma_ and especially the predicates-only version_comma_ that shows the least bias towards the phrase-based translation.
40,1,Similarly to METEOR_comma_ the dependency-based method shows on the whole lower bias than other metrics.
41,1,As to the correlation with human evaluation of translation accuracy_comma_ our method currently falls short of METEOR and even NIST.
42,1,The WordNet-boosted dependencybased method scores only slightly lower than METEOR with WordNet.
43,1,Nevertheless_comma_ none of these measures or extensions takes into account linguistic knowledge about actual translation errors_comma_ for example what is the contribution of verbs in the overall error rate_comma_ how many full forms are wrong whereas their base forms are correct_comma_ etc. A framework for human error analysis has been proposed in (Vilar et al. _comma_ 2006) and a detailed analysis of the obtained results has been carried out.
44,1,In an experiment on 16_comma_800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project_comma_ we compare the LFG-based evaluation method with other popular metrics like BLEU_comma_ NIST_comma_ General Text Matcher (GTM) (Turian et al. _comma_ 2003)_comma_ Translation Error Rate (TER) (Snover et al. _comma_ 2006)1_comma_ and METEOR (Banerjee and Lavie_comma_ 2005)_comma_ and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment.
45,1,As to the correlation with human evaluation of translation accuracy_comma_ our method currently falls 108 short of METEOR.
46,1,It flexibly matches words using stemming and WordNet synonyms.
47,1,We might find better suited metrics_comma_ such as METEOR (Banerjee and Lavie_comma_ 2005)_comma_ which is oriented towards word selection8.
48,1,c2007 Association for Computational Linguistics Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Alon Lavie and Abhaya Agarwal Language Technologies Institute Carnegie Mellon University Pittsburgh_comma_ PA_comma_ 15213_comma_ USA {alavie_comma_abhayaa}@cs.cmu.edu Abstract Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality_comma_ significantly outperforming the more commonly used Bleu metric.
49,1,The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish_comma_ French and German_comma_ in addition to English.
50,1,Interestingly_comma_ METEOR obtains a higher correlation_comma_ which_comma_ in the case of French-to-English_comma_ rivals the top-scoring metrics based on deeper linguistic features.
51,1,Other well-known metrics are WER (Nieen et al._comma_ 2000)_comma_ NIST (Doddington_comma_ 2002)_comma_ GTM (Melamed et al._comma_ 2003)_comma_ ROUGE (Lin and Och_comma_ 2004a)_comma_ METEOR (Banerjee and Lavie_comma_ 2005)_comma_ and TER (Snover et al._comma_ 2006)_comma_ just to name a few.
52,1,The results show that_comma_ as compared to BLEU_comma_ several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez_comma_ 2007)_comma_ ParaEval-recall (Zhou et al._comma_ 2006)_comma_ and METEOR (Banerjee and Lavie_comma_ 2005) achieve higher correlation.
53,1,During the workshop_comma_ only three automatic metrics (Semantic-role overlap_comma_ ParaEval-recall_comma_ and METEOR) achieve higher correlation than BLEU.
54,1,Many researchers (Banerjee and Lavie_comma_ 2005; Liu and Gildea_comma_ 2006)_comma_ have observed consistent gains by using more flexible matching criteria.
55,1,We compare this metric against a combination metric of four state-of-theart scores (BLEU_comma_ NIST_comma_ TER_comma_ and METEOR) in two different settings.
56,1,Since the computation of RTER takes considerably more resources than METEORR_comma_ it is interesting to compare the predictions of RTER against METEORR.
57,1,1: Among individual metrics_comma_ METEORR and TERR do better than BLEUR and NISTR.
58,1,This result supports the intuition in (Banerjee and Lavie_comma_ 2005) that correlation at segment level is necessary to ensure the reliability of metrics in different situations.
59,1,We plan to assess the additional benefit of the full entailment feature set against the TRADMT feature set extended by a proper lexical similarity metric_comma_ such as METEOR.
60,1,For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al._comma_ 2008).
61,1,However_comma_ we trust the METEOR results more due to their better correlation with human judgements.
62,1,Semantic collocations are harder to extract than cooccurrence patterns--the state of the art does not enable us to find semantic collocations automatically t. This paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon_comma_ we can at least achieve semi-automatic extraction of semantic collocations (see also Calzolari and Bindi (1990) I But note the important work by Hindle \[HindlegO\] on extracting semantically similar nouns based on their substitutability in certain verb contexts.
63,1,His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words (such as in (Hindle_comma_ 1990)).
64,1,While full automatic extraction of semantic collocations is not yet feasible_comma_ some recent research in related areas is promising.
65,1,Hindle (1990) reports interesting results of this kind based on literal collocations_comma_ where he parses the corpus (Hindle 1983) into predicate-argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments.
66,1,The sets he extracts are promising; for example_comma_ the ten most similar nouns to treaty in his corpus are agreement_comma_ plan_comma_ constitution_comma_ contract_comma_ proposal_comma_ accord_comma_ amendment_comma_ rule_comma_ law_comma_ and legislation.
67,1,His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words (such as in Hindle 1990).
68,1,The realization of such an automatic construction method would make it possible to a) save the cost of constructing a thesaurus by hand_comma_ b) do away with subjectivity inherent in a hand made thesaurus_comma_ and c) make it easier to adapt a natural language processing system to a new domain.
69,1,In the past five years_comma_ important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks_comma_ 1990; Hindle_comma_ 1990; Smadja_comma_ 1993; Grei~nstette_comma_ 1994; Grishman and Sterling_comma_ 1994).
70,1,Yet_comma_ similarity measures that utilize MI showed good performance.
71,1,Similarity-based smoothing (Hindle 1990; Brown et al. 1992; Dagan_comma_ Marcus_comma_ and Markovitch 1993; Pereira_comma_ Tishby_comma_ and Lee 1993; Dagan_comma_ Lee_comma_ and Pereira 1999) provides an intuitively appealing approach to language modeling.
72,1,A wide range of contextual information_comma_ such as surrounding words (Lowe and McDonald_comma_ 2000; Curran and Moens_comma_ 2002a)_comma_ dependency or case structure (Hindle_comma_ 1990; Ruge_comma_ 1997; Lin_comma_ 1998)_comma_ and dependency path (Lin and Pantel_comma_ 2001; Pado and Lapata_comma_ 2007)_comma_ has been utilized for similarity calculation_comma_ and achieved considerable success.
73,1,This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.
74,1,Recent work_comma_ (McClosky et al. _comma_ 2006)_comma_ has shown that adding many millions of words of machine parsed and reranked LA Times articles does_comma_ in fact_comma_ improve performance of the parser on the closely related WSJ data.
75,1,Finally_comma_ the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al.(2006) find that the self-trained models help considerably when parsing WSJ.
76,1,While (McClosky et al. _comma_ 2006) showed that this technique was effective when testing on WSJ_comma_ the true distribution was closer to WSJ so it made sense to emphasize it.
77,1,We should keep in mind that (1) a treebank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al.(2006)_comma_ and (2) that our treebank PCFG is binarized as in Klein and Manning (2005) to make results comparable.
78,1,Recently there have been some improvements to the Charniak parser_comma_ use n-best re-ranking as reported in (Charniak and Johnson_comma_ 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al. _comma_ 2006a; McClosky et al. _comma_ 2006b).
79,1,Recently_comma_ (McClosky et al. _comma_ 2006a; McClosky et al. _comma_ 2006b) have successfully applied self-training to various parser adaptation scenarios using the reranking parser of (Charniak and Johnson_comma_ 2005).
80,1,We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al. _comma_ 2006).
81,1,David McClosky_comma_ Eugene Charniak_comma_ and Mark Johnson Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence_comma_ RI 02912 {dmcc|ec|mj}@cs.brown.edu Abstract Self-training has been shown capable of improving on state-of-the-art parser performance (McClosky et al._comma_ 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak_comma_ 1997; Steedman et al._comma_ 2003).
82,1,Studies prior to McClosky et al.(2006) failed to show a benefit to parsing from self-training (Charniak_comma_ 1997; Steedman et al._comma_ 2003).
83,1,The significant predictors from McClosky et al.(2006) such as the number of conjunctions or sentence length continue to be helpful whereas unknown words are a weak predictor at best.
84,1,Its success stories range from parsing (McClosky et al._comma_ 2006) to machine translation (Ueffing_comma_ 2006).
85,1,There are only a few successful studies_comma_ such as (Ando and Zhang_comma_ 2005) for chunking and (McClosky et al._comma_ 2006a; McClosky et al._comma_ 2006b) on constituency parsing.
86,1,Tighter integration of semantics into the parsing models_comma_ possibly in the form of discriminative reranking models (Collins and Koo_comma_ 2005; Charniak and Johnson_comma_ 2005; McClosky et al._comma_ 2006)_comma_ is a promising way forward in this regard.
87,1,McClosky et al.(2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data.
88,1,However more recent results have shown that it can indeed improve parser performance (Bacchiani et al._comma_ 2006; McClosky et al._comma_ 2006a; McClosky et al._comma_ 2006b).
89,1,This paper is based on the C/J parser and thus its results are much more in line with modern expectations.
90,1,For English_comma_ self-training contributes 0.83% absolute improvement to the PCFG-LA parser_comma_ which is comparable to the improvement obtained from using semi-supervised training with the twostage parser in (McClosky et al._comma_ 2006).
91,1,The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al._comma_ 2008)_comma_ parse re-ranking (McClosky et al._comma_ 2006)_comma_ pronoun resolution (Nguyen and Kim_comma_ 2008)_comma_ and semantic role labeling (Liu and Sarkar_comma_ 2007).
92,1,Performance with Charniak parser enhanced by re-ranking plus self-training 5.7 Comparison with Other State-of-the-art Results Table 11 and table 12 compare our method with the other state-of-the-art methods; we use I_comma_ B_comma_ R_comma_ S and C to denote individual model (Charniak 2000; Collins 2000; Bod 2003; Petrov and Klein 2007)_comma_ bilingual-constrained model (Burkett and Klein 2008) 1 _comma_ re-ranking model (Charniak and Johnson 2005_comma_ Huang 2008)_comma_ self-training model (David McClosky 2006) and combination model (Sagae and Lavie 2006) respectively.
93,1,They show that together with a re-ranker_comma_ improvements 37 are obtained.
94,1,4.3 Using Unlabeled Data for Parsing Recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the WSJ data_comma_ even when labeled data is relatively large (McClosky et al._comma_ 2006a; Reichart and Rappoport_comma_ 2007).
95,1,Such approaches have shown promise in applications such as web page classification (Blum and Mitchell_comma_ 1998)_comma_ named entity classification (Collins and Singer_comma_ 1999)_comma_ parsing (McClosky et al._comma_ 2006)_comma_ and machine translation (Ueffing_comma_ 2006).
96,1,Improvements are obtained (McClosky et al._comma_ 2006; McClosky and Charniak_comma_ 2008)_comma_ showing that a reranker is necessary for successful self-training in such a high-resource scenario.
97,1,Dirichlet priors can be used to bias HMMs toward more skewed distributions (Goldwater and Griffiths_comma_ 2007; Johnson_comma_ 2007)_comma_ which is especially useful in the weakly supervised setting consideredhere.
98,1,For an HMM with a set of states T and a set of output symbols V : t  T t  Dir(1_comma_|T|) (1) t  T t  Dir(1_comma_|V |) (2) ti|ti1_comma_ ti1  Multi(ti1) (3) wi|ti_comma_ ti  Multi(ti) (4) One advantage of the Bayesian approach is that the prior allows us to bias learning toward sparser structures_comma_ by setting the Dirichlet hyperparameters _comma_ to a value less than one (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007).
99,1,There is evidence that this leads to better performance on some part-of-speech induction metrics (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007).
100,1,We use maximum marginal decoding_comma_ which Johnson (2007) reports performs better than Viterbi decoding.
101,1,Although this results in a situation where multiple induced tags may share a single gold tag_comma_ it does not punish a system for providing tags of a finer granularity than the gold standard.
102,1,Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy_comma_ confirming the results reported by Johnson (2007).
103,1,Recent work (Johnson_comma_ 2007; Goldwater and Griffiths_comma_ 2007; Gao and Johnson_comma_ 2008) explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results.
104,1,Given the parameters{pi0_comma_pi_comma__comma_K}of the HMM_comma_ the joint distribution over hidden states s and observationsy can be written (with s0 = 0): p(s_comma_y|pi0_comma_pi_comma__comma_K) = Tproductdisplay t=1 p(st|st1)p(yt|st) As Johnson (2007) clearly explained_comma_ training the HMM with EM leads to poor results in PoS tagging.
105,1,In the supervised setting_comma_ a recent paper by Daume III (2007) shows that_comma_ using a very simple feature augmentation method coupled with Support Vector Machines_comma_ he is able to effectively use both labeled target and source data to provide the best results in a number of NLP tasks.
106,1,5.1 The AUGMENT technique for Domain Adaptation The AUGMENT technique introduced by Daume III (2007) is a simple yet very effective approach to performing domain adaptation.
107,1,Despite its relative simplicity_comma_ this AUGMENT technique has been shown to outperform other domain adaptation techniques on various tasks such as named entity recognition_comma_ part-of-speech tagging_comma_ etc. 5.2 Experimental Results As mentioned in Section 4_comma_ training our WSD system on SEMCOR examples gave a relatively low accuracy of 76.2%_comma_ as compared to the 89.1% accuracy obtained from training on the OntoNotes section 0221 examples.
108,1,Although we observe that in this scenario_comma_ ON performs better than SC+ON_comma_ SC+ON Augment continues to perform better than ON (where the improvement is statistically significant) till the result for sections 02-09.
109,1,This means that the SC+ON Augment strategy_comma_ besides giving good performance when one has few in-domain examples_comma_ does continue to perform well even when one has a large number of in-domain examples.
110,1,6 Active Learning with AUGMENT Technique So far in this paper_comma_ we have seen that when we have access to some in-domain examples_comma_ a good strategy is to combine the out-of-domain and in-domain examples via the AUGMENT technique.
111,1,Also_comma_ since we have found that the AUGMENT technique is useful in increasing WSD accuracy_comma_ we will apply the AUGMENT technique during each iteration of active learning to combine the SEMCOR examples and the selected adaptation examples.
112,1,Based on these results_comma_ we propose that when there is a need to apply a previously trained WSD system to a different domain_comma_ one can apply the AUGMENT technique with active learning on the most frequent word types_comma_ to greatly reduce the annotation effort required while obtaining a substantial improvement in accuracy.
113,1,We show that by combining the AUGMENT domain adaptation technique with active learning_comma_ we are able to effectively reduce the amount of annotation effort required for domain adaptation.
114,1,In the supervised setting_comma_ a recent paper by Daume III (2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks.
115,1,His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu_comma_ 2006; Chelba and Acero_comma_ 2004).
116,1,Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only_comma_ target only_comma_ or the union of the data) achieve a relatively high performance level and are surprisingly difficult to beat (Daume III_comma_ 2007).
117,1,Thus_comma_ one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data_comma_ it is often more fruitful to either just use that_comma_ or to apply simple adaptation techniques (Daume III_comma_ 2007; Plank and van Noord_comma_ 2008).
118,1,For instance_comma_ (Daume III_comma_ 2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks.
119,1,His method improves or equals over previously explored more sophisticated methods (Daume III and Marcu_comma_ 2006; Chelba and Acero_comma_ 2004).
120,1,Nonetheless_comma_ such global properties can improve the accuracy of a model_comma_ so recent NLP papers have considered practical techniques for decoding with them.
121,1,However_comma_ other types of nonlocal information have also been shown to be effective (Finkel et al. _comma_ 2005) and we will examine the effectiveness of other non-local information which can be embedded into label information.
122,1,Global information is known to be useful in other NLP tasks_comma_ especially in the named entity recognition task_comma_ and several studies successfully used global features (Chieu and Ng_comma_ 2002; Finkel et al. _comma_ 2005).
123,1,Wecanseethatthenalperformanceofour algorithm was worse than that of the related work.
124,1,The resulting performance of the proposed algorithm with non-local features is higher than that of Finkel et al.(2005) and comparable with that of Krishnan and Manning (2006).
125,1,However_comma_ the achieved accuracy was not better than that of related work (Finkel et al. _comma_ 2005; Krishnan and Manning_comma_ 2006) based on CRFs.
126,1,The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al._comma_ 2005).
127,1,Lins (1998) information-theoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD (McCarthy et al._comma_ 2004).
128,1,The best accuracies are observed when the labelsarecreatedfromdistributionallysimilarwords using Lins (1998) dependency-based similarity measure (Depend).
129,1,Among these measures_comma_ the most important are Wu & Palmers (Wu and Palmer_comma_ 1994)_comma_ Resniks (Resnik_comma_ 1995) and Lins (Lin_comma_ 1998).
130,1,Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)s information-theoretic metric work best.
131,1,They have been successfully applied in several tasks_comma_ such as information retrieval (Salton et al._comma_ 1975) and harvesting thesauri (Lin_comma_ 1998).
132,1,This similarity score is computed as a max over a number of component scoring functions_comma_ some based on external lexical resources_comma_ including:  various string similarity functions_comma_ of which most are applied to word lemmas  measures of synonymy_comma_ hypernymy_comma_ antonymy_comma_ and semantic relatedness_comma_ including a widelyused measure due to Jiang and Conrath (1997)_comma_ based on manually constructed lexical resources such as WordNet and NomBank  a function based on the well-known distributional similarity metric of Lin (1998)_comma_ which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources both manually and automatically constructedis critical to the success of MANLI.
133,1,A wide range of contextual information_comma_ such as surrounding words (Lowe and McDonald_comma_ 2000; Curran and Moens_comma_ 2002a)_comma_ dependency or case structure (Hindle_comma_ 1990; Ruge_comma_ 1997; Lin_comma_ 1998)_comma_ and dependency path (Lin and Pantel_comma_ 2001; Pado and Lapata_comma_ 2007)_comma_ has been utilized for similarity calculation_comma_ and achieved considerable success.
134,1,3.1 Context Extraction We adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies (Ruge_comma_ 1997; Lin_comma_ 1998).
135,1,Whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems (Pad and Lapata_comma_ 2007; Lin_comma_ 1998)_comma_ for the task of categorisation simple pattern based spaces have been shown to perform equally good if not better (Poesio and Almuhareb_comma_ 2005b; Almuhareb and Poesio_comma_ 2005b).
136,1,However_comma_ to be more expressive and flexible_comma_ it is often easier to start with a general SCFG or tree-transducer (Galley et al. _comma_ 2004).
137,1,We denote (t) to be the root symbol of tree t. When writing these rules_comma_ we avoid notational overhead by introducing a short-hand form from Galley et al.(2004) that integrates the mapping into the tree_comma_ which is used throughout Section 1.
138,1,1 Introduction Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (Galley et al._comma_ 2004; Liu et al._comma_ 2006_comma_ 2007; Zhang et al._comma_ 2007_comma_ 2008a; Mi et al._comma_ 2008; Mi and Huang 2008; Zhang et al._comma_ 2009).
139,1,This algorithm is referred to as GHKM (Galley et al._comma_ 2004) and is widely used in SSMT systems (Galley et al._comma_ 2006; Liu et al._comma_ 2006; Huang et al._comma_ 2006).
140,1,By this means_comma_ GHKM proves to be able to extract all valid tree-to-string rules from training instances.
141,1,This approach has been shown to be accurate_comma_ relatively efficient_comma_ and robust using both generative and discriminative models (Roark_comma_ 2001; Roark_comma_ 2004; Collins and Roark_comma_ 2004).
142,1,1 Introduction Statistical parsing models have been shown to be successful in recovering labeled constituencies (Collins_comma_ 2003; Charniak and Johnson_comma_ 2005; Roark and Collins_comma_ 2004) and have also been shown to be adequate in recovering dependency relationships (Collins et al. _comma_ 1999; Levy and Manning_comma_ 2004; Dubey and Keller_comma_ 2003).
143,1,Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore_comma_ 2005)_comma_ sequence analysis (Daume and Marcu_comma_ 2005; McDonald et al. _comma_ 2005a) and phrase-structure parsing (Collins and Roark_comma_ 2004).
144,1,2.2 Perceptron-based training To tune the parameters w of the model_comma_ we use the averaged perceptron algorithm (Collins_comma_ 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark_comma_ 2004; Roark et al. _comma_ 2004).
145,1,Successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark_comma_ 2004; Henderson_comma_ 2004; Taskar et al. _comma_ 2004).
146,1,This combination of the perceptron algorithm with beam-search is similar to that described by Collins and Roark (2004).5 The perceptron algorithm is a convenient choice because it converges quickly  usually taking only a few iterations over the training set (Collins_comma_ 2002; Collins and Roark_comma_ 2004).
147,1,Successful discriminative parsers have used generative models to reduce training time and raise accuracy above generative baselines (Collins & Roark_comma_ 2004; Henderson_comma_ 2004; Taskar et al. _comma_ 2004).
148,1,Our work fuses this idea of selective hierarchical updates with the simplicity of the perceptron algorithm and the flexibility of arbitrary feature sharing inherent in the ranking framework.
149,1,These methods alleviate this problem by using some approximation in perceptron-type learning.
150,1,Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark_comma_ 2004)_comma_ chunking (Daume and Marcu_comma_ 2005)_comma_ and machine translation (Cowan et al. _comma_ 2006).
151,1,Variants of this method have been successfully used in many NLP tasks_comma_ like shallow processing (Daume III and Marcu_comma_ 2005)_comma_ parsing (Collins and Roark_comma_ 2004; Shen and Joshi_comma_ 2005) and word alignment (Moore_comma_ 2005).
152,1,Beam-search has been successful in many NLP tasks (Koehn et al._comma_ 2003; 562 Inputs: training examples (xi_comma_yi) Initialization: set vectorw = 0 Algorithm: // R training iterations; N examples for t = 1R_comma_ i = 1N: zi = argmaxyGEN(xi) (y) vectorw if zi negationslash= yi: vectorw = vectorw + (yi)(zi) Outputs: vectorw Figure 1: The perceptron learning algorithm Collins and Roark_comma_ 2004)_comma_ and can achieve accuracy that is close to exact inference.
153,1,Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses (Roark_comma_ 2001; Henderson_comma_ 2003; Collins and Roark_comma_ 2004)_comma_ and we will use such rich models to derive our scores.
154,1,It is an online training algorithm and has been successfully used in many NLP tasks_comma_ such as POS tagging (Collins_comma_ 2002)_comma_ parsing (Collins and Roark_comma_ 2004)_comma_ Chinese word segmentation (Zhang and Clark_comma_ 2007; Jiang et al._comma_ 2008)_comma_ and so on.
155,1,To achieve efficient parsing_comma_ we use a beam search strategy like the previous methods (Collins and Roark_comma_ 2004; Roark_comma_ 2001; Roark_comma_ 2004).
156,1,Albeit simple_comma_ the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark_comma_ 2004; Collins_comma_ 2004; Zettlemoyer and Collins_comma_ 2005; Zettlemoyer and Collins_comma_ 2007).
157,1,We are already using the extracted semantic forms in parsing new text with robust_comma_ wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II treebank (Cahill et al. _comma_ 2004a).
158,1,Even robust parsers using linguistically sophisticated formalisms_comma_ such as TAG (Chiang_comma_ 2000)_comma_ CCG (Clark and Curran_comma_ 2004b; Hockenmaier_comma_ 2003)_comma_ HPSG (Miyao et al. _comma_ 2004) and LFG (Riezler et al. _comma_ 2002; Cahill et al. _comma_ 2004)_comma_ often use training data derived from the Penn Treebank.
159,1,Inspired by (Cahill et al. _comma_ 2004)s methodology which was originally designed for English and Penn-II treebank_comma_ our approach to Chinese non-local dependency recovery is based on Lexical-Functional Grammar (LFG)_comma_ a formalism that involves both phrase structure trees and predicate-argument structures.
160,1,Inspired by (Cahill et al. _comma_ 2004; Burke et al. _comma_ 2004)_comma_ we have implemented an f-structure annotation algorithm to automatically obtain f-structures from CFG-trees in the CTB5.1.
161,1,We keep the traces inserted by the C04 algorithm and abandon those inserted by our algorithm in case of conflict_comma_ as the results in Section 5.2 suggest that C04 has a higher precision than ours.
162,1,They achieved around 80% f-score for fstructures parsing on the WSJ part of the Penn-II treebank_comma_ a score comparable to the ones of the state-ofthe-art hand-crafted grammars.
163,1,Hockenmaier and Steedman (2002)_comma_ Miyao et al.(2004) and Cahill et al.(2004) show fairly good results on the Penn Treebank (for CCG_comma_ HPSG and LFG_comma_ respectively): these parsers achieve accuracies on predicate-argument relations between 80% and 87%_comma_ which show the feasibility and scalability of this approach.
164,1,One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al. _comma_ 1992).
165,1,An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data.
166,1,The Xerox tagger attempts to avoid the need for a hand-tagged training corpus as far as possible.
167,1,The results of the Xerox experiment appear very encouraging.
168,1,The kind of biasing Cutting et al. describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not.
169,1,3.1 Training The Xerox tagger is claimed (Cutting el al. _comma_ 1992) to be adaptable and easily trained; only a lexicon and suitable amount of untagged text is required.
170,1,A number of part-of-speech taggers are readily available and widely used_comma_ all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993).
171,1,Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill_comma_ 1992; Cutting et al. _comma_ 1992).
172,1,Recent comparisons of approaches that can be trained on corpora (van Halteren et al. _comma_ 1998; Volk and Schneider_comma_ 1998) have shown that in most cases statistical aproaches (Cutting et al. _comma_ 1992; Schmid_comma_ 1995; Ratnaparkhi_comma_ 1996) yield better results than finite-state_comma_ rule-based_comma_ or memory-based taggers (Brill_comma_ 1993; Daelemans et al. _comma_ 1996).
173,1,(Cutting et al. _comma_ 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.
174,1,This impressive result triggered several follow-up studies in which the effect of hand tuning the tag dictionary was quantified as a combination of labeled and unlaPierre/NNP Vinken/NNP NP will/MD join/VB the/DT board/NN NP as/IN a/DT nonexecutive/JJ director/NN NP PP VP VP S Figure 1: An example of the kind of output expected from a statistical parser.
175,1,Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead.
176,1,It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model (Cutting et al. _comma_ 1992).
177,1,4.1 Complete ambiguity classes Ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.4 And ambiguity classes have been shown to be successfully employed_comma_ in a variety of ways_comma_ to improve POS tagging (e.g._comma_ Cutting et al._comma_ 1992; Daelemans et al._comma_ 1996; Dickinson_comma_ 2007; Goldberg et al._comma_ 2008; Tseng et al._comma_ 2005).
178,1,The results of these studies have important applications in lexicography_comma_ to detect lexicosyntactic regularities (Church and Hanks_comma_ 19901 (Calzolari and Bindi_comma_1990)_comma_ such as_comma_ for example~ support verbs (e.g. 'make-decision') prepositional verbs (e.g. 'rely-upon') idioms_comma_ semantic relations (e.g. 'part_of') and fixed expressions (e.g. 'kick the bucket').
179,1,INTRODUCTION Using mutual information for measuring word association has become popular since \[Church and Hanks_comma_ 1990\] defined word association ratio as mutual information between two words.
180,1,These tools are important in that the strongest collocational associations often represent different word senses_comma_ and thus 'they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags' (Church and Hanks 1990_comma_ p. 28).
181,1,Collocation has been applied successfully to many possible applications (Church et aal._comma_ 1989)_comma_ e.g_comma_ lexicography (Church and Hanks_comma_ 1990)_comma_ information retrieval (Salton_comma_ 1986a)_comma_ text input (Yamashina and Obashi_comma_ 1988)_comma_ etc. This paper will touch on its feasibility in topic identification.
182,1,In the past five years_comma_ important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks_comma_ 1990; Hindle_comma_ 1990; Smadja_comma_ 1993; Grei~nstette_comma_ 1994; Grishman and Sterling_comma_ 1994).
183,1,Since the goal of disambiguation is to select the best pair among many alternatives as described above_comma_ the mutual information statistic is a natural choice in judging the degree to which two words co-occur within a certain text boundary.
184,1,The performance of our implicit ambiguity resolution method for all translations (tall_rerank) shows 8.63% improvement compared with that of ambiguity resolution based on mutual information (tone_base).
185,1,Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics_comma_ MI and Residual IDF (RIDF)_comma_ and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases.
186,1,Although Resnik (1996) reported that 10 of the 16 plausible pairs did not occur in his training corpus_comma_ all of them occurred in ours and hence MI gives very reasonable scores on the plausible objects.
187,1,DSP>0 has both a higher recall and higher precision than accepting every pair previously seen in text (the right-most point on MI>T).
188,1,On the subset of pairs with strong empirical association (MI>0)_comma_ MI generally outperforms DSP at equivalent recall values.
189,1,Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch_comma_ Wang_comma_ and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel_comma_ Ravichandran_comma_ and Hovy 2004; Weeds_comma_ Weir_comma_ and McCarthy 2004)_comma_ dened by: weight MI (w_comma_f)=log 2 P(w_comma_f) P(w)P(f) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight MI (w_comma_f)=log 2 count(w_comma_f) nrels count(w) count(f) (2) where count(w_comma_f) is the frequency of the co-occurrence pair w_comma_f  in S_comma_ count(w)and count(f) are the independent frequencies of w and f in S_comma_andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations.
190,1,Similarity measures that utilize MI weights showed good performance_comma_ however.
191,1,Analyzing the Bootstrapped Feature Vector Quality In this section we provide an in-depth analysis of the bootstrapping feature weighting quality compared tothe state-of-the-art MI weighting function.
192,1,In addition_comma_ we introduced the PMI weighting within the Levenshtein algorithm as a simple means of obtaining segment distances_comma_ and showed that it improves on the popular Levenshtein algorithm with respect to alignment accuracy.
193,1,As aptly pointed out in Jean Carletta (1996)_comma_ agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data.
194,1,Carletta (1996) deserves the credit for bringing  to the attention of computational linguists.
195,1,Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes_comma_ there has been a general acceptance of their use within the field.
196,1,Unlike Church and Hanks (1990)_comma_ Smadja (1993) goes beyond the 'two-word' limitation and deals with 'collocations of arbitrary length'.
197,1,The system performs very well under two conditions: the corpus must be large_comma_ and the collocations we are interested in extracting_comma_ must have high frequencies.
198,1,This is ha_comma_sed on the intuitive ide~ tim| 'if a set of words ('onstitutes a collocation_comma_ its subset will Mso be correla.ted'.
199,1,Tools like Xtract (Smadja 1993) were based on the work of Church and others_comma_ but made a step forward by incorporating various statistical measurements like z-score and variance of distribution_comma_ as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output.
200,1,For the extraction problem_comma_ there have been various methods proposed to date_comma_ which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro_comma_ Matsumoto_comma_ and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997).
201,1,As they use plain text corpora and only require the information appearing in texts_comma_ such systems are highly flexible and extract relevant units independently from the domain and the language of the input text.
202,1,Smadja (1993)_comma_ which is the classic work on collocation extraction_comma_ uses a two-stage filtering model in which_comma_ in the first step_comma_ n-gram statistics determine possible collocations and_comma_ in the second step_comma_ these candidates are submitted to a syntactic valida7Of course_comma_ lexical material is always at least partially dependent on the domain in question.
203,1,Many efficient techniques exist to extract multiword expressions_comma_ collocations_comma_ lexical units and idioms (Church and Hanks_comma_ 1989; Smadja_comma_ 1993; Dias et al. _comma_ 2000; Dias_comma_ 2003).
204,1,This method successfully extracted both adjacent and distant bi-grams and n-grams.
205,1,Morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults (Breidt_comma_ 1993; Smadja_comma_ 1993; Zajac et al. _comma_ 2003).
206,1,The evaluation results of ROUGE-L_comma_ ROUGEW_comma_ and ROUGE-S in machine translation evaluation are very encouraging.
207,1,ROUGE-L_comma_ ROUGE-W_comma_ and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
208,1,According to the results reported in that paper_comma_ ROUGE-L_comma_ ROUGE-W_comma_ and ROUGE-S also outperformed BLEU and NIST.
209,1,Since the DUC 2004 evaluation_comma_ Lin (2004) has concluded that certain ROUGE metrics correlate better with human judgments than others_comma_ depending on the summarisation task being evaluated_comma_ i.e. single document_comma_ headline_comma_ or multi-document summarisation.
210,1,In the case of headline generation_comma_ Lin found that ROUGE-1_comma_ ROUGE-L and ROUGE-W scores worked best and so only these scores are included in Table 1.
211,1,Rpre-W.1.2.b (inverted ROUGE measure_comma_ using non-contiguous word sequences_comma_ removing stopwords_comma_ without stemming) obtains the highest individual KING for task 2_comma_ and is one of the best in task 5_comma_ confirming that ROUGEbased metrics are a robust way of evaluating summaries_comma_ and indicating that non-contiguous word sequences can be more useful for evaluation purposes than n-grams.
212,1,Of the 12 measures_comma_ unigram-based methods_comma_ such as cosine distance and ROUGE-1_comma_ produced good results.
213,1,ROUGE-2 was superior to the other 11 measures in terms of Ranking.
214,1,We evaluated 10 systems by Yasudas method with ROUGE-3_comma_ which produced the best results in Exp-3.
215,1,We also investigated an automatic method based on Yasudas method and found that the method using ROUGE-2 and -3 could accurately estimate manual scores_comma_ and could outperform Kazawas method and the other automatic methods tested.
216,1,From these results_comma_ we can conclude that the automatic method performed the best when ROUGE-2 or 3 is used as a similarity measure_comma_ and a regression analysis is used for combining manual method.
217,1,Unlike responsiveness and linguistic quality scores_comma_ which are ordinal data and are best suited for non-parametric analyses_comma_ ROUGE scores_comma_ can be measured on an interval scale and are suitable for parametric analysis.
218,1,We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE (Lin and Hovy_comma_ 2003; Lin_comma_ 2004) which allow quick measurement of systems during development and enable evaluation of larger amounts of data.
219,1,Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau_comma_ 2004b) and ROUGE (Lin_comma_ 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies_comma_ which achieves 91.3% of human performance in Pyramid score_comma_ and outperforms our best-performing non-sequential model by 3.9%.
220,1,Two metrics have become quite popular in multi-document summarization_comma_ namely the Pyramid method (Nenkova and Passonneau_comma_ 2004b) and ROUGE (Lin_comma_ 2004).
221,1,ROUGE measures: considering the impact of ngram overlap metrics in textual entailment_comma_ we believethattheideaofintegratingthesemeasures1 into our system is very appealing.
222,1,The ROUGE/BE toolkit has become the standard automatic method for evaluating the content of c2008.
223,1,We hypothesize that different variants of ROUGE may capture different qualities of a summary; for example_comma_ ROUGE-1 may be a good indicator of the relevance of summary content_comma_ but ROUGE variants that take into account larger contexts may capture linguistic qualities of the summary.
224,1,We believe this metric may be well-suited to reflect the degree of linguistic surface structure similarity between summaries.
225,1,We postulate that ROUGE-L may be able to account for the explicitly copy-pasted concepts and to detect the more subtle similarities with paraphrased concepts in the expert-generated domain knowledge map.
226,1,We carried out automatic evaluation of our summaries using ROUGE (Lin_comma_ 2004) toolkit_comma_ which has been widely adopted by DUC for automatic summarization evaluation.
227,1,As we will see below_comma_ both approaches show promise.
228,1,Amongst the various ROUGE statistics_comma_ the most appealing is Weighted Longest Common Subsequence(WLCS).
229,1,We experimented with other ROUGE statistics but we got better and easily interpretable results using WLCS and so we chose it as the final metric.
230,1,The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy_comma_ 2003a; Lin_comma_ 2004).
231,1,ROUGE (Lin_comma_ 2004) has been widely used for summarization evaluation.
232,1,In the news article domain_comma_ ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin_comma_ 2004).
233,1,(Murray et al._comma_ 2005) showed low correlation of ROUGE and human evaluation in meeting summarization evaluation; however_comma_ they 201 simply used ROUGE as is and did not take into account the meeting characteristics during evaluation.
234,1,This suggests that by leveraging speaker information_comma_ ROUGE can assign better credits or penalties to system generated summaries (same words from different speakers will not be counted as a match)_comma_ and thus yield better correlation with human evaluation; whereas for human summaries_comma_ this may not happen often.
235,1,There is significant improvement in correlation when disfluencies are removed and speaker information is leveraged_comma_ especiallyforevaluatingsystem-generatedsummaries.
236,1,Although it cannot replace the finesse of human evaluation_comma_ it can provide a crude idea of progress which can later be validated.
237,1,Theuseofthe MAXENT classifierbyitselfimprovedslightlyourresults_comma_ buttheimprovements come mostly from using ROUGE-W.
238,1,The latter is particularly interesting_comma_ because it is well published_comma_ it includes both an alternative_comma_ centroid-based technique to automatically tag training examples and a soft-matching classifier_comma_ 1We also experimented with other similarity measures (e.g._comma_ edit distance) and ROUGE variants_comma_ but we obtained the best results with ROUGE-W.
239,1,Additional experiments we conducted with the old system replacing the SVM by the MAXENT classifier (without using ROUGE-W) indicate that the use of MAXENT by itself also improved slightly the results_comma_butthedifferencesaretoominortoshow; the improvement is mostly due to the use of ROUGEW instead of our previous measure.
240,1,We use ROUGE-W to generate training examples from Web snippets and encyclopedias_comma_ a method that outperforms an alternative centroid-based one.
241,1,ROUGE version 1.5.5 (Lin_comma_ 2004) was used for evaluation.2 Among others_comma_ we focus on ROUGE-1 in the discussion of the result_comma_ because ROUGE-1 has proved to have strong correlation with human annotation (Lin_comma_ 2004; Lin and Hovy_comma_ 2003).
242,1,However_comma_ at the level of the individual sentence_comma_ the ROUGE-L metric correlates best with the human judgements.
243,1,We found that for our first experiment_comma_ all metrics were correlated to roughly the same degree (with ROUGE-L achieving the highest correlation at an individual sentence level and the GTM tool not far behind).
244,1,The ROUGE (Lin_comma_ 2004) suite of metrics are n-gram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness.
245,1,We report the three widely adopted important ROUGE metrics in the results: ROUGE-1 (unigram)_comma_ ROUGE-2 (bigram) and ROUGE-SU (skip bi-gram).
246,1,On the one hand_comma_ they produce large scale resources at little man labour cost.
247,1,g2 2 Motivation The success of Statistical Machine Translation (SMT) has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem (e.g. _comma_ Barzilay & Lee_comma_ 2003; Pang et al. _comma_ 2003; Quirk et al. _comma_ 2004; Finch et al. _comma_ 2004).
248,1,Unlike (Le Nguyen & Ho_comma_ 2004)_comma_ one interesting idea proposed by (Barzilay & Lee_comma_ 2003) is to cluster similar pairs of paraphrases to apply multiplesequence alignment.
249,1,Such corpora are highly informative for identifying variations of the same meaning_comma_ since_comma_ typically_comma_ when variable instantiations are shared across comparable documents the same predicates are described.
250,1,Interestingly_comma_ while Dave et al. report good performance on classifying reviews using bigrams or trigrams alone_comma_ Pang et al. show that bigrams are not useful features for the task_comma_ whether they are used in isolation or in conjunction with unigrams.
251,1,We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al. _comma_ 2002; Pang and Lee_comma_ 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive_comma_ all written before 2002 by a total of 312 authors_comma_ with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable.
252,1,2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al._comma_ 2002; Matsumoto et al._comma_ 2005; Kudo and Matsumoto_comma_ 2004; Mullen and Collier_comma_ 2004; Gamon_comma_ 2004).
253,1,One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al._comma_ 2005; Kudo and Matsumoto_comma_ 2004; Pang et al._comma_ 2002).
254,1,Unigram models have been previously shown to give good results in sentiment classification tasks (Kennedy and Inkpen_comma_ 2006; Pang et al._comma_ 2002): unigram representations can capture a variety of lexical combinations and distributions_comma_ including those of emotion words.
255,1,SVM has been shown to be useful for text classification tasks (Joachims_comma_ 1998)_comma_ and has previously given good performance in sentiment classification experiments (Kennedy and Inkpen_comma_ 2006; Mullen and Collier_comma_ 2004; Pang and Lee_comma_ 2004; Pang et al._comma_ 2002).
256,1,Three approaches are dominating_comma_ i.e. knowledge-based approach (Kim and Hovy_comma_ 2004)_comma_ information retrieval-based approach (Turney and Littman_comma_ 2003) and machine learning approach (Pang et al._comma_ 2002)_comma_ in which the last approach is found very popular.
257,1,This task has created a considerable interest due to its wide applications.
258,1,Experiment Implementation: We apply SVM algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms (Pang et al._comma_ 2002).
259,1,We use five sentiment classification datasets_comma_ including the widely-used movie review dataset [MOV] (Pang et al._comma_ 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO)_comma_ DVDs (DVD)_comma_ electronics (ELE)_comma_ and kitchen appliances (KIT)] (Blitzer et al._comma_ 2007).
260,1,In their seminal work_comma_ (Pang et al._comma_ 2002) demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms.
261,1,Movies Reviews: This is a popular dataset in sentiment analysis literature (Pang et al._comma_ 2002).
262,1,In particular_comma_ the use of SVMs in (Pang et al._comma_ 2002) initially sparked interest in using machine learning methods for sentiment classi cation.
263,1,Pang et al.(2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task.
264,1,4 Evaluation 4.1 Experimental Setup For evaluation_comma_ we use five sentiment classification datasets_comma_ including the widely-used movie review dataset [MOV] (Pang et al._comma_ 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO)_comma_ DVDs (DVD)_comma_ electronics (ELE)_comma_ and kitchen appliances (KIT)] (Blitzer et al._comma_ 2007).
265,1,While the NASA researchers have applied a heuristic method for labeling a report with shapers (Posse 1http://kdd.ics.uci.edu/databases/20newsgroups/ 2Of course_comma_ the fact that sentiment classification requires a deeper understanding of a text also makes it more difficult than topic-based text classification (Pang et al._comma_ 2002).
266,1,In most cases_comma_ supervised learning methods can perform well (Pang et al._comma_ 2002).
267,1,Recently_comma_ graph-based methods have proved useful for a number of NLP and IR tasks such as document re-ranking in ad hoc IR (Kurland and Lee_comma_ 2005) and analyzing sentiments in text (Pang and Lee_comma_ 2004).
268,1,All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classi cation (Pang and Lee_comma_ 2004).
269,1,Interestingly_comma_ previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee_comma_ 2004).
270,1,A later study (Pang and Lee_comma_ 2004) found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective.
271,1,Indeed_comma_ recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g_comma_ Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g. _comma_ Pang and Lee (2004)).
272,1,We chose a dataset that would be enjoyable to reannotate: the movie review dataset of (Pang et al. _comma_ 2002; Pang and Lee_comma_ 2004).3 The dataset consists of 1000 positive and 1000 negative movie reviews obtained from the Internet Movie Database (IMDb) review archive_comma_ all written before 2002 by a total of 312 authors_comma_ with a cap of 20 reviews per author per 2Taking Ccontrast to be constant means that all rationales are equally valuable.
273,1,In fact_comma_ it has already been established that sentence level classification can improve document level analysis (Pang and Lee_comma_ 2004).
274,1,First_comma_ even when sentiment is the desired focus_comma_ researchers in sentiment analysis have shown that a two-stage approach is often beneficial_comma_ in which subjective instances are distinguished from objective ones_comma_ and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou_comma_ 2003; Pang and Lee_comma_ 2004; Wilson et al. _comma_ 2005; Kim and Hovy_comma_ 2006).
275,1,Sentence-level subjectivity detection_comma_ where training data is easier to obtain than for positive vs. negative classification_comma_ has been successfully performed using supervised statistical methods alone (Pang and Lee_comma_ 2004) or in combination with a knowledgebased approach (Riloff et al. _comma_ 2006).
276,1,3 CLaC-NB System: Nave Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level: on movie review texts they reach an accuracy of 85-90% (Aue and Gamon_comma_ 2005; Pang and Lee_comma_ 2004) and up to 92% accuracy on classifying movie review snippets into subjective and objective using both Nave Bayes and SVM (Pang and Lee_comma_ 2004).
277,1,SVM has been shown to be useful for text classification tasks (Joachims_comma_ 1998)_comma_ and has previously given good performance in sentiment classification experiments (Kennedy and Inkpen_comma_ 2006; Mullen and Collier_comma_ 2004; Pang and Lee_comma_ 2004; Pang et al._comma_ 2002).
278,1,Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon_comma_ 2005; Pang and Lee_comma_ 2004).
279,1,These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon_comma_ 2005).
280,1,A two-tier scheme (Pang and Lee_comma_ 2004) where sentences are  rst classi ed as subjective versus objective_comma_ and then applying the sentiment classi er on only the subjective sentences further improves performance.
281,1,Moreover_comma_ Ng et al.(2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%_comma_ which is higher than the result reported by Pang and Lee (2004) with the same dataset.
282,1,And 20NG is a collection of approximately 20_comma_000 20-category documents 1 . In sentiment text classification_comma_ we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee_comma_ 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al._comma_ 2007).
