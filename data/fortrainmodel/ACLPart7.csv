id,sennum,review
E91-1051.txt,38,Thirdly_comma_ the approach cannot be extended to the case discussed here.
E91-1051.txt,39,In this paper we focus on the alternative proposal_comma_ in which head switching or alternatively_comma_ splitting is a x operation.
E91-1051.txt,40,1 Adverbs are taken to be f structure SADJs.
E91-1051.txt,41,The z annotation to ADVP states that the x of the mother fstructure is the XCOMP of the of the SADJ I Notice that a third possibility_comma_ not discussed in Kaplan el al 1989 _comma_ involving a flat f structure but treating the adverb as a semantic head at s structure_comma_ simply means that the mapping problem we describe below arises in the monolinsual mapping between f structure and s structure. slot Kaplan et al s fig. 26 2 3 S NP ADVP VP t SUB 3  t SADJ  t SADr XCOMP  t This has the effect of subordinating the translation of the f structure which contains the SADJ to the translation of the SADJ itself.
E91-1051.txt,42,The lexical entries themselves contribute further z equations following Kaplan et al s fig. 21 4 arrive V t PRED arrive SUBJ x t PRED FN arriver c t SUBJ   1 SUBJ just ADV PRED just  t PRED Ft_comma_0 venir john N PILED john x t PRED FN jean This is unproblernatic for examples such as 2a_comma_ b .
E91-1051.txt,44,The English structure_comma_ and lex2 Note that here and subsequently_comma_ following Kaplan et al_comma_ we ignore the monolingual potential for more than one ADVP with its attendant problems for translation.
E91-1051.txt,47,It does not give a correct translation of the source string.
E91-1051.txt,48,In this section we have shown that the proposal as outlined in Kaplan et al 1989 does not produce an adequate analysis of these cases_comma_ The problem_comma_ which is not at first apparent_comma_ arises from the combination of the regular and irregular equations from the emphasised lines.
E91-1051.txt,49,Note that there is no problem stating this correspondence in the French English direction see below .
E91-1051.txt,51,In this section we will briefly consider a number of alternatives.
E91-1051.txt,52,3 To facilitate discussion_comma_ it is worth noting that the proposal in Kaplan et al 1989 involves basically three elements 11a a set of regular equations constraining both source and target  t SUB   t sum llb a set of regular equations assigning target PRED values  t PRED 9 sere form llc a special equation on ADVP constraining both source and target  I SADJ XCOMP  t The problem noted above arises from the combination of an equation from a with the equation c .
E91-1051.txt,54,The first alternative we considered involves maintaining equations of type a so that T f3 is indeed  fl COMe _comma_ and then switching heads only rather than whole constructions .
E91-1051.txt,55,The basic idea is that the annotations to ADVP provide a PRED value for xD and specify that the of the PRED of t3 is the PRED in f3 XCOMP .
E91-1051.txt,56,To do this_comma_ the PRED value must be made into a complex feature_comma_ and heavy use is made of equations on the cstructure rules_comma_ so that the mapping is essentially structurally determined.
E91-1051.txt,57,3 These alternat ves have been ex ored by usin 8 at Essex an implementation of PArR due to Bob Carpenter_comma_ and at Edinburgh a ve on of MicroPATR.
E91-1051.txt,58,295 Intuitively_comma_ the approach works by building target constructions without assigning them PRED values directly_comma_ then specifying the target PRED values in such a way that it is possible to switch the heads for the eases in question..LP In fact_comma_ though this works for cases such as 2c_comma_d_comma_e _comma_ it is limited to cases in which it is correct to raise all the dependents of a predicate to the same slot in the construction headed by the translation of the adverb.
E91-1051.txt,59,It thus fails with 12a in which races must remain a dependent of the embedded construction_comma_ and 12b in which the same is true of Jean 12a Peter zwemt gruag wedstrijden.
E91-1051.txt,60,Peter likes to swim races.
E91-1051.txt,61,12b I said that John will probably come.
E91-1051.txt,62,J ai dit qu il est probable que Jean viendra.
E91-1051.txt,63,This is of course an immediate consequence of the fact that the proposal works by switching not constructions but heads..SH Section 2.2.
E91-1051.txt,64,It is clear from the above that any solution must achieve constructional integrity in translation.
E91-1051.txt,65,This idea can be achieved in a number of slightly different ways.
E91-1051.txt,66,In the following we exploit the path equation variables available in LFG_comma_ which permit one to use a value assigned elsewhere as an attribute that is_comma_ our proposal here is modelled on the use of 1  PCASE  in LFG.
E91-1051.txt,67,We alter lla and llb so that the paths that they constrain are sensitive to the value of an attribute which we call CTYPE 13  c 1 1 CTYPE PRED FN sem form  c 1 1 CTYPE SUBJ .0 1 SUEd  x 1 1 CTYPE OBJ  1 OBJ a The value of Cq YPE is given by the adverbial annotations 14a on ADVP t TD 1 CTYPE  TYPE 4 Notice that edl dependents of a head must be mado sensitive to the value of the CTYPE attribute to mainlain constructional integrity .
E91-1051.txt,68,To deal with nonsubeategorised constituents such as SADJs whose x equations are given by c structure rule we must annolate ADVP with  t SADJ  x t  CTYPE SAm 14b on the adverb just 1 TYPE XCOMP Notice that the c annotation to ADVP which states that the translation of the containing fstructure is the translation of the f structure associated with the ADVP i.e the SADJ slot simply equates the x of two f structures and avoids the problem which beset the proposal in Kaplan et al 1989 .
E91-1051.txt,69,This can be seen from the equation set for 2c in 15 .
E91-1051.txt,70,Note that when there is no adverb_comma_ the value of t CTYPE must be since paths are regular expressions   0 GD  T t G 3 .
E91-1051.txt,71,15  fl PRED FN  penser  n sum   fl sum z fl COMP   fl COMP _comma_ f2 PRED FN je z f3 XCOMP PRED FN arriver  f3 XCOMP SUm   f3 SUm  c f4 PRED FN jean t3  f5 x f5 PRED FN venir What is the cost of this proposal
E91-1051.txt,72,The translational correspondences in all lexical entries will be sensitive in this way to the value of the CTYPE feature.
E91-1051.txt,73,We must guarantee that when a value is not contributed by the type feature on the adverb_comma_ the value of 1 CTYPE is e_comma_ either by some priority union operation to initialise it to e_comma_ or by some other convention with this effect_comma_ or by assuming different versions of the c structure rules with VP contributing 1 CTYPE E as appropriate.
E91-1051.txt,74,Variants of this solution which do not exploit the path equation variable apparatus of LFG are also possible_comma_ though at the cost of massively increasing the size of the lexicon.
E91-1051.txt,75,For example_comma_ lexieal translation correspondences could be disjunctions as in 16 .
E91-1051.txt,78,Our third alternative involves giving the path equations some sort of functional uncertainty interpretation.
E91-1051.txt,79,Our starting point is the problematic pair of equations repeated in 17 and the observation that the required target structure embeds the XCOMP within the COMP.
E91-1051.txt,80,17 x COMP  x I COMP    a CoMe x 1  x SAD J XCOMP x f3  t fS XCOMP The interpretation of the r t COMP  c t COMP could be loosened on the source side_comma_ as in 18 18 c t COMa OF   t COMe .
E91-1051.txt,81,x r3 oF  x n OMP which specifies that the translation of some fstructure on a path from the source COMP e.g. the COMP SADJ fills the COMP slot in the translation.
E91-1051.txt,82,This avoids the problem in 17 .
E91-1051.txt,83,Equally_comma_ the interpretation could be loosened on the target side 19 x t COMP  t COMP OF _comma__comma_ _comma_t f3  c n COMP OF which says that the translation of the COMP fills some path from the COMP slot in the translation e.g. the COMP XCOMP .
E91-1051.txt,84,This proposal raises a number of interesting questions for further research about whether functional uncertainty can be used here while still guaranteeing some determinate set of output structures to be validated or not by the target grammar.
E91-1051.txt,85,Notice however that for the case in hand_comma_ the uncertainty equation can be quite specific all that is required is the source functional uncertainty  f COMP SAD J  t t COMP 3.
E91-1051.txt,87,Our starting point in this paper was the observation that a treatment proposed for cases such as 2 in Kaplan et al 1989 is unworkable.
E91-1051.txt,88,We have then discussed alternative approaches available within the general model assumed by Kaplan et al 1989 .
E91-1051.txt,89,We have shown that the problem is to achieve simple general statements of the correspondence mapping which cover exceptional eases without spreading the effect of exceptionality throughout the grammar.
E91-1051.txt,90,The discussion in section 2 raises intricate technical issues about the formalism itself_comma_ but also relates to wider issues concerning the modularity of the approach to translation proposed in Kaplan et al 1989 as well as the suitability and expressivity of the formalism_comma_ raising serious questions about the feasibility of a large MT system along these lines.
E91-1051.txt,91,We also noted that these eases are unproblematic in the fusing direction_comma_ for then we do not run into problems with the functionality of the x correspondence.
E91-1051.txt,92,In this direction_comma_ the special equations are within the lexieal entry for venir 20  T SADJ PRED FN just t  t XCOMP Substituting variables for clarity_comma_ combining fhese equations with the regular equation from the embedding verb penser produces no inconsistency_comma_ since the path specifications all COMP and xf5 can be equated 21  n COMP  a COMe  c 13   fl COMP  13  13 XCOMP  13 O fs   13 SADJ PRED FN just This observation raises interesting questions concerning the directionality assumed in Kaplan et al 1989 .
E91-1051.txt,93,R seems that the correct way to view all this is that we have a system of correspondences relating 4 structures Source and Target c and f structures .
E91-1051.txt,94,For a given set of correspondences and a partially determined Set of structures_comma_ three possibilities exist no solution can be found 297 a finite number of solutions can be found an indeterminate and or infinite number of solutions can be found.
E91-1051.txt,95,We might expect therefore that a solution may be found even if we state correspondences in the French English direction but supply the partial determination from the English side that is_comma_ when English is source .
E91-1051.txt,96,The system for translating in either direction would then be a pair of monolingual grammars with a set of x equations stated in the fusing direction i.e. in the French grammar .
E91-1051.txt,97,This is currently under investigation.
E91-1051.txt,98,Preliminary results suggest that this approach will in fact cleanly overcome the specific problem at hand.
E91-1051.txt,99,It has proved possible to translate sentences 2b and 2d above from Dutch to English using grammars and lexicons in which only appears in the English rules and entries.
E91-1051.txt,100,But this work has in turned raised a number of fundamental issues_comma_ some of which apply not only to LFG but to any other attempt at theory based translation Exactly what does the formal definition of the translates relation look like_comma_ in LFG or any other theory based approach to translation Can this formal definition actually be implemented
E91-1051.txt,101,Existing approaches to generation from f s structure in LFG are too restrictive Wedekind 1988 _comma_ and our current implementation over compensates. Is the functional nature of correspondences appropriate to the z family_comma_ or would a relation be more appropriate
E91-1051.txt,102,If so_comma_ what would the theoretical and practical consequences be What is the relation between strict theory based translation and translation in the ordinary sense of the word
E91-1051.txt,103,Is it not likely that its applicability will in practice be limited to closely related language pairs Is there a substantive difference between the structures and correspondences approach of LFG and the single structured sign approach of I IPSG or UCG
E91-1051.txt,104,Translation seems a strenuous test.
E91-1051.txt,105,ACKNOWLEDGEMENTS We thank an anonymous EACL reviewer for helpful comments and constructive critiCisms_comma_ and Doug Arnold and Pete Whitelock for useful discussion.
E91-1051.txt,106,All remaining errors are_comma_ of course_comma_ our own. .
E91-1004.txt,1,i al er describes a Ilatural language i ars ing algorith_comma_n for unrestricted text which uses a prol al ility I ased scoring function to select the l est i arse of a sclfl_comma_ence. The parser_comma_ T earl_comma_ is a time asynchronous I ottom ul chart parser with Earley tyl e tol down prediction which l ur sues the highest scoring theory iu the chart_comma_ where the score of a theory represents tim extent to which the context of the sentence predicts that interpretation.
E91-1004.txt,2,This parser dilrers front previous attemi ts at stochastic parsers in that it uses a richer form of conditional prol alfilities I ased on context to l rediet likelihood.
E91-1004.txt,3,T carl also provides a framework for i_comma_lcorporating the results of previous work in i art of spe ch assignrlmn ._comma_ unknown word too lois_comma_ and other probal ilistic models of lingvistic features into one parsing tool_comma_ interleaving these techniques instead of using the traditional pipeline a_comma_ chitecture_comma_ lu preliminary tests_comma_ Pearl has I ee._comma_i st_comma_ccessl ul at resolving l art of speech and word in sl eech processing ambiguity_comma_ d etermining categories for unknown words_comma_ and selecting correct parses first using a very loosely fitting cove_comma_ ing grammar_comma_ l .
E91-1004.txt,4,All natural language grammars are alnbiguous.
E91-1004.txt,5,Even tightly fitting natural language grammars are ambiguous in some ways.
E91-1004.txt,6,Loosely fitting grammars_comma_ which are necessary for handling the variability and complexity of unrestricted text and speech_comma_ are worse.
E91-1004.txt,7,Tim standard technique for dealing with this ambiguity_comma_ pruning This work was p_comma_ rtially supported by DARPA grant No. N01114 85 1 0018_comma_ ONR contract No. N00014 89C 0171 by DARPA and AFOSR jointly under grant No. AFOSR 90 0066_comma_ and by ARO grant No. DAAL 03 89C 1031 PRI. Special thanks to Carl Weir and Lynette llirschman at Unisys for their valued input_comma_ guidance and support. I Fhe grammar used for our experiments is the string ra.mmar used in Unisys PUNI IT natura.I language iindt rsl.a ndi n 4 sysl.tml. gra.nunars I y hand_comma_ is painful_comma_ time consuming_comma_ and usually arbitrary.
E91-1004.txt,8,The solution which many people have proposed is to use stochastic models to grain statistical grammars automatically from a large corpus.
E91-1004.txt,9,Attempts in applying statistical techniques to natura_comma_ I iangt_comma_ age parsi_comma_lg have exhibited varying degrees of success.
E91-1004.txt,10,These successful and unsuccessful attempts have suggested to us that .
E91-1004.txt,12,Interactive_comma_ interleaved architectvres are preferable to pipeline architectures in NLU systems_comma_ because they use more of the available information in the decision nmkiug process.
E91-1004.txt,13,Wc have constructed a stoch tic parser_comma_ earl_comma_ which is based on these ideas.
E91-1004.txt,14,The development of the 7 earl parser is an effort to combine the statistical models developed recently into a single tool which incorporates all of these models into the decisiou making component of a parser_comma_ While we have only attempted to incorporate a few simple statistical models into this parser_comma_ earl is structured in a way which allows any nt_comma_ mber of syntactic_comma_ semantic_comma_ and other knowledge sources to contribute to parsing decisions.
E91-1004.txt,15,The current implementation of Pearl uses ChurclFs part of speech assignment trigram model_comma_ a simple probabilistic unknown word model_comma_ and a conditional probability model for grammar rules based on part of speech trigrams and parent rules.
E91-1004.txt,16,By combining multiple knowledge sources and using a chart parsing framework_comma_ 7 earl attempts to handle a number of difficult problems.
E91-1004.txt,17,7 arl has the capability to parse word lattices_comma_ an ability which is useful in recognizing idioms in text processing_comma_ as well as in speech processing.
E91-1004.txt,18,The parser uses probabilistic training from a corpus to disambiguate between grammatically ac i ptal h _comma_ structures_comma_ such m determining i repo 15sitional l hrase attachment and conjunction scope.
E91-1004.txt,19,Finally_comma_ earl maintains a well formed substring I_comma_able within its chart to allow for partial parse retrieval.
E91-1004.txt,20,Partial parses are usefid botll for error message generation aud for pro cssitlg lulgrattUllal_comma_i al or illCOllll h I_comma_e . l I_comma_ llCes. ht i reliluinary tests_comma_ earl has shown protnisillg resuits in ha_comma_idling part of speech ussignnlent_comma__comma_ preposit_comma_ ional I hrase d_comma_l_comma_ achnlcnl._comma_ ait I Ilnknowlt wor I categoriza6on. Trained on a corpus of 1100 sentences from the Voyager direction linding system 2 and using the string gra_comma_ulm r from l_comma_he I UNDIT l_comma_aug_comma__comma_age IhM_comma_.rsl.atJ ling Sysl_comma_cuh carl correcl_comma_ ly i a.rse I 35 out of 10 or 88 of scIitellces sele tcd frolu Voyager sentcil . tier used in the traini_comma_lg data. We will describe the details of this exl crimelfl_comma_ lal_comma_cr.
E91-1004.txt,21,In this I al cr _comma_ wc will lirsl_comma_ explain our contribul_comma_ ion l_comma_o the sl_comma_ochastic _comma_nodels which are used in earl a context free granunar with context sensitive condil_comma_ ional probal ilities.
E91-1004.txt,22,Then_comma_ we will describe the parser s architecture and the parsing algorithtn_comma_ l ina.lly_comma_ we will give the results of some exi erinlents we performed using earl which explore its capabilities.
E91-1004.txt,23,Using Statistics to Parse Recent work involving conl_comma_ext free a_comma_.I contextsensitive probal ilistic gramnlars I rovide little hope for the success of processing unrestricted text osing I roba.bilistic teclmiques.
E91-1004.txt,26,This average likelihood is often a poor estimat e of probability. Parsing algorithnls which accumulate I rol abilities of parse theories by simply multiplying the_comma_n overpenalize infrequent constructs. earl avoids the first pitfall by t_comma_sing a contextsensitive conditional probability CFG_comma_ where cot ttext of a theory is determi_comma_ted by the theories which predicted it and the i art of sl eech sequences in the input s_comma_ml_comma_ence.
E91-1004.txt,27,To address the second issue_comma_ Pearl scores each theory by usi.g the geometric mean of Lhe contextl_comma_al conditional probalfilities of all of I.he theories which have contributed to timt theory.
E91-1004.txt,28,This is e lt_comma_ ivalent to using the sum of the logs of l.hese probal ilities.
E91-1004.txt,29,Spcclnl thanks to Victor Zue at Mlq h r the use of the Sl c h da.t r from MIT s Voyager sysl_comma_ Clll. CFG with context sensitive conditional probabilities In a very large parsed corpus of English text_comma_ one finds I_comma_ Imt_comma_ I_comma_be most freq.ently occurring noun phrase structure in I_comma_ Iw text is a nomt plu asc containing a determiner followed by a noun. Simple probabilistic CFGs dictate that_comma_ given this information_comma_ determiner noun should be the most likely interpretation of a IlOUn phrase. Now_comma_ consider only those noun phrases which occur as subjects of a senl_comma_ence. In a given corpus_comma_ you nlighl_comma_ liml that pronouns occur just as fre luently as lletermincr nou_comma__comma_ s in the subject I ositiou.
E91-1004.txt,30,This type of information can easily be cai tnred by conditional l robalfilities.
E91-1004.txt,31,Finally_comma_ tmsume that the sentence begins with a pronoun followed by a verb.
E91-1004.txt,32,In l.his case_comma_ it is quite clear that_comma_ while you can probably concoct a sentence which fit_comma_ s this description and does not have a pronoun for a subject_comma_ I_comma_he first_comma_ theory which you should pursue is one which makes this hypothesis.
E91-1004.txt,33,The context sensitive conditional probabilities which earl uses take into account the irnmediate parent of a theory 3 and the part of speech trigram centered at the beginning of the theory.
E91-1004.txt,34,For example_comma_ consider the sentence My first love was named earl. no subliminal propaganda intended A theory which tries to interpret love as a verb will be scored based ou the imrl_comma_ of speecll trigranl adjective verb verb and the parent theory_comma_ probably S  NP VP. A theory which interprets love as a noun will be scored based on the trigram adjective noun w rl . AIl_comma_llo.gll Io.xical prollabilities favor love as a verb_comma_ I_comma_ he comlitional i robabilities will heavily favor love as a noun in tiffs context.
E91-1004.txt,35,4 Using the Geometric Mean of Theory Scores According to probability theory_comma_ the likelihood of two independent events occurring at the same time is the product of their individual probabilities.
E91-1004.txt,36,Previous statistical parsing techniques apply this definition to the cooceurrence of two theories in a parse_comma_ and claim that the likelihood of the two theories being correct is the product of the probabilities of the two theories.
E91-1004.txt,37,3The parent of a theory is defined as a theory with a CF rule which co.tains the left hand side of tile theory.
E91-1004.txt,38,For instance_comma_ if S _comma_ NP VP and NP  det n are two grammar rules_comma_ the first rule can be a parent of tile second_comma_ since tl_comma_e left hand side of tile second NP occurs in the right hand side of the first rule.
E91-1004.txt,39,4In fact_comma_ tile part of speech tagging model which is Mso used in earl will heavily favor love as a noun.
E91-1004.txt,40,We ignore this behavior to demonstrate the benefits of the trigram co.ditioni.g.
E91-1004.txt,41,16 l his application of probal ility theory ignores two vital observations el out the domain of statistical parsing Two CO_comma_lstructs .occurring in the same sentence are _comma_lot n_comma_ ccssa_comma_ ily indel cndc.nt and frequ ml.ly are not .
E91-1004.txt,42,If the indel el de e_comma__comma_ msuniption is violated_comma_ then tile prodl_comma_ct of individual probabilities has no meaning with _comma_ espect to the joint probability of two events. SiilCe sl_comma_al_comma_isl_comma_i al liarshig sllil t rs froln Sl ars_comma_ data_comma_ liroliil.I ilil_comma_y esl_comma_ inlatcs of low frequency evenl.s will ilsually lie iiiaccurate estiliiaLes.
E91-1004.txt,44,The raw score of a theory_comma_ 0 is calculated by takiug I_comma_he.
E91-1004.txt,45,i rodul l_comma_ of the onditiona.I i rol ability of that theory s _comma_1 i ride giw il the conl_comma_ext whel l _comma_ COlitelt is it I iirl_comma_ of sl ech I_comma_rigraln a.n I a l areiit I_comma_heol y s rule alid I_comma_ he score of tim I_comma_ rigrani _comma_5 C r aw 0  P r tics I oPl 1 2 _comma_ ruic parent sc pol_comma_ 1 2 llere_comma_ the score of a trigram is the product of the mutual infornlation of the part of speech trigram_comma_ 5 POPII 2_comma_ and tile lexical prol ability of the word at the Ioeatioil of Pi lieing assigiled that liart of specch pi .s In the case of anlhiguil_comma_y part of speech ambiguity or inuitil le parent theories _comma_ the inaxinuim value of this lirothict is used.
E91-1004.txt,46,The score of a partial theory or a conlI lete theory is the geometric liieali of the raw scores of all of the theories which are contained in that theory.
E91-1004.txt,47,The liilltilal iliforlll ll.iOll el r part of sl eech trigram_comma_ llopipil is lelincd to lie li lillll 2 where x is lilly lillrt 7 PlizP1 7 Ill  of speech.
E91-1004.txt,49,GTlie trigrani . coring funcl.ion actually ilsed by tile parser is SOill wh il_comma_ tiler  onllili al_comma_t d I_comma_ Ilall this. Theory Length Independence This scoring function_comma_ although heuristic in derivation_comma_ provides a nlethod Ibr evaluating the value of a theory_comma_ regardless of its length. When a rule is first predicted Earleystyh _comma_ its score is just its raw score_comma_ which relireseuts how uiuch _comma_lie context predicts it. llowever_comma_ when the parse process hypothesizes interpretations of tile senteuce which reinforce this theory_comma_ the geornetric nlean of all of the raw scorn of the rule s subtree is used_comma_ rcllrescnting the ow_comma_rall likelihood or I.he i.heory given the coutcxt of the sentence.
E91-1004.txt,51,Example of Scoring Function As an example of how the conditional probability b sed scoring flinction handles anlbiguity_comma_ consider the sentence Fruit_comma_ flies like a banana.
E91-1004.txt,52,i_comma_i the dontain of insect studies.
E91-1004.txt,53,Lexical probabilities should indicate that the word flies is niore likely to be a plural noun than an active verb.
E91-1004.txt,54,This information is incorporated in the trigram scores_comma_ llowever_comma_ when the interliretation S  .
E91-1004.txt,55,NP VP is proposed_comma_ two possible NPs will be parsed_comma_ NP nolnl fruit all d NP noun nouu fruit flies .
E91-1004.txt,56,Sitlce this sentence is syntactically ambiguous_comma_ if the first hypothesis is tested first_comma_ the parser will interpret this sentence incorrectly.
E91-1004.txt,57,ll0wever_comma_ this will not happen in this donlain.
E91-1004.txt,58,Since fruit flies is a common idiom in insect studies_comma_ the score of its trigram_comma_ noun noun verb_comma_ will be much greater than the score of the trigram_comma_ noun verb verb.
E91-1004.txt,59,Titus_comma_ not only will the lexical probability of the word flies verb be lower than that of flies noun_comma_ but also tile raw score of NP  noun fruit will be lower than 7We are not deliberately avoiding using _comma_ ill probability estinlatioll techniques_comma_ o_comma__comma_ly those backillg off techaiques which use independence assunlptions that frequently provide misleading information when applied to natural liillgU age.
E91-1004.txt,60,17that of NP nolln nolln fruit flies _comma_ because of the differential between the trigram score s.
E91-1004.txt,61,So_comma_ NP noun noun will I e used first to advance the S  NI VP rid0..
E91-1004.txt,62,Further_comma_ even if the I arser a lva.llCeS I ol_comma_h NII hyliol_comma_h ses_comma_ I_comma_he S  NP . VI rule IlSilig N I j  liOllll iiOlln will have a higher s ore l_comma_ hau the S  INIP . Vl  rule using NP notul. Interleaved Architecture in Pearl The interleaved architecture implemented in Pearl provides uiany advantages over the tradil_comma_ionai pilieline ar hil_comma_ . l.ln e_comma_ liut it_comma_ also iiil.rodu _comma_s c_comma_ rl_comma_a.ili risks. I  iSiOllS abollt word alld liarl_comma_ of sl ee h alnliiguity ca.ii I e dolaye I until synl_comma_acl_comma_ ic I rocessiug can disanlbiguate l_comma_h ni.
E91-1004.txt,63,And_comma_ using I_comma_he al llroprial_comma_e score conibhia.tion flilicl_comma_iolis_comma_ the scoring of aliihigliOllS hoi es Call direct I_comma_ li parser towards I_comma_ he most likely inl_comma_erl re.tal_comma_ ioii ellicicutly.
E91-1004.txt,65,The Parsing Algorithm T earl is a time asynchronous I ottom up chart parser with Earley tyi e top down i rediction.
E91-1004.txt,66,The significant difference I etween Pearl and non I robabilistic bol_comma_tOllHI I i arsers is tha.t instead of COml letely generating all grammatical interpretations of a word striug_comma_ Tcarl pursues i.he N highest scoring incoml lete theories ill the chart al. each I mS.
E91-1004.txt,67,Ilowcw r_comma_ Pearl I a._comma_ scs wilhoul pruniny.
E91-1004.txt,69,The liarsing alg u ithill begins with the inl ut word lati_comma_ice.
E91-1004.txt,70,An 11 x It cha.rl_comma_ is allocated_comma_ where It iS the hmgl_comma_ h of the Iongesl_comma_ word sl_comma_rillg in l_comma_lie lattice_comma_ l_comma_ xical i uh s for I_comma_he inliut word lal.l_comma_ ice a_comma_ re inserted into the cha.rt.
E91-1004.txt,71,Using Earley tyl e liredicLi6u_comma_ a st ntence is pre licl.ed at_comma_ the beginuilig of tim SClitence_comma_ and all of the theories which are I re licl.c I l y l_comma_ hat initial sentence are inserted into the chart.
E91-1004.txt,72,These inconll lete theetics are scored accordiug to the context sensitive conditional probabilities and the trigram part of speech nlodel.
E91-1004.txt,74,This allproach should s_comma_ i vt lily I_comma_o iiiinilnize I_comma_ he lilliilber of UliSUllCrvised passes reqilired for l.lio freqileilcy dal_comma_ a I_comma_o converge.
E91-1004.txt,75,Preliminary Evaluation While we haw _comma_rot yet done xte_comma_miw testing of all of the Cal abilities of carl_comma_ we perforumd some simple tests to determine if its I erformance is at least consistent with the premises _comma_port which it is based.
E91-1004.txt,77,l0 In fact_comma_ h r certain grail liiars_comma_ th . fr . qllClicy I. tl les may not conw rge at all_comma_ or they may converge to zero_comma_ with the g_comma_ _comma_ tmmar gc_comma_tcrati_comma_lg no pa.rscs for the entire corpus.
E91-1004.txt,78,This is a worst case sccl_comma_ario whicl_comma_ we do oct a_comma_lticipate halq cning. training data on which the parser was trained.
E91-1004.txt,79,Using .p_comma_ arl s cont . xt free gr unmar_comma_ i_comma_h . e test sentences produced an average of 64 parses per sentence_comma_ with some sentences producing over 100 parses. Unknown Word Part of speech Assignment To determine how Pearl hamlles unknown words_comma_ we remow d live words from the lexicon_comma_ i_comma_ kuow_comma_ lee_comma_ describe_comma_ aml station_comma_ and tried to parse the 40 sample sentences I_comma_sing the simple unknown word model previe_comma_rely d_comma_ scribcd. I_comma_i this test_comma_ the pl onollll_comma_ il W L_comma_ q assigncd the correct. i art of speech 9 of 10 I.iiiies it occurred in the test _comma_s nt mces.
E91-1004.txt,80,The nouns_comma_ lee and slalion_comma_ were correctly I. tggcd 4 of 5 I.inics.
E91-1004.txt,81,And the w rbs_comma_ kltow and describe_comma_ were corl cl.ly I_comma_ aggcd l of l tiilles. pronoun 90 nou_comma_i 80 verb 100 overall 89 .... Figure 1 Performance on Unknown Words in Test SenI_comma_ ences While this accuracy is expected for unknown words in isolation_comma_ based oil the accuracy of the part ofspeech tagging model_comma_ the performance is expected to degrade for sequences of unk_comma_lown words.
E91-1004.txt,82,Prepositional Phrase Attachment Acc0rately determining prepositional phrase attachnlent in general is a difficult and well documented problem_comma_ llowever_comma_ based on experience with several different donmins_comma_ we have found prel ositional phrase attachment to be a domain specific pheuomenon for which training ca_comma_t I e very helpfld.
E91-1004.txt,83,For insta_comma_tce_comma_ in the dirccl.ion li_comma_ldi_comma__comma_g do_comma_lmin_comma_ from aml to prepositional phrases generally attach to the preceding verb and not to any noun phrase.
E91-1004.txt,84,This tende_comma_icy is captured iu the training process for .pearl and is used to guide the parscr to the more likely attach_comma_nent with respect to he domain.
E91-1004.txt,86,And_comma_ using a more sophisticated statistical model_comma_ this pcrfornla_comma_lcc can easily be improved.
E91-1004.txt,87,Pearl s performance on prepositional phrase attachmeat was very high 54 55 or 98.2 correct .
E91-1004.txt,88,The reaso_comma_i the accuracy rate was so high is that .lie directionfinding domain is very consistent in it s use of individt_comma_al prepositions.
E91-1004.txt,89,The accuracy rate is not expected to be as high in other domains_comma_ although it certainly 19should be higher than 50 and we would expect it to bc greater than 75 _comma_ although wc have nol. performed any rigorous tests on other Ionmius to verify this. i_comma_.ro_comma__comma_ositio._comma_ I to i o_comma__comma_ Accuracy R_comma_ate 92 100 100 98.2 I igure 2 Accl_comma_racy Rate for Prepositional Phr se AtI.achnlcnt_comma_ I y l reposition Overall Parsing Accuracy The 40 test sentences were parsed by 7 earl and the highest scoring parse for each sentence was compared to the correct parse produced by I UNI rr.
E91-1004.txt,91,Ill fact_comma_ on 2 of tile 3 sciitences which were iucorrectly i arsed_comma_ POal l i roduced the corl t ct i ll SC is well_comma_ but the correct i a_comma_ se did not have the highest score.
E91-1004.txt,92,Future Work The Pearl parser takes advantage of donmin depen lent information to select the most approi riate interpretation of an inpul_comma_.
E91-1004.txt,93,Ilowew .r_comma_ i_comma_he statistical measure used to disalnbiguate these interpretations is sensitive to certain attributes of the grammatical formalism used_comma_ as well as to the part of si eech categories used to laI el lexical entries.
E91-1004.txt,94,All of the exl erimcnts performed on T carl titus fa_comma_ have been using one gra.linrla.r_comma_ one pa.rl.of speech tag set_comma_ and one donlaiu hecause of availability constra.ints .
E91-1052.txt,1,grammars are an elegant formalization of the augmented context free grammars characteristic of most current natural language systems. This paper presents an extension of Earley s algorithm to Knuth s attribute grammars_comma_ considering the case of S attributed grammars.
E91-1052.txt,2,For this case_comma_ we study the conditions on the underlying base grammar under which the extended algorithm may be guaranteed to terminate.
E91-1052.txt,3,Finite partitioning of attribute domains is proposed to guarantee the termination of the algorithm_comma_ without the need for any restrictions on the context free base.
E91-1052.txt,5,Earley s 1970 algorithm is a general algorithm for context free languages_comma_ widely used in natural language processing King_comma_ 1983 Shieber_comma_ 1985 and syntactic pattern recognition Fu_comma_ 1982 _comma_ where the full generative power of context free grammar is required.
E91-1052.txt,6,The original algorithm and its common implementations_comma_ however_comma_ assume the atomic symbols of context free grammars_comma_ thus limiting its applicability to systems with attributed symbols_comma_ or attribute grammars Knuth_comma_ 1968 .
E91-1052.txt,7,Attribute grammar is an elegant formalization of the augmented context free grammars characteristic of most current NLP systems.
E91-1052.txt,8,It is more general than members of the family of unification based grammar formalisms Kay_comma_ 1985 Shieber_comma_ 1986 _comma_ mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values_comma_ and hence can lead to computationally efficient grammatical definitions_comma_ while maintaining the advantages of a well understood declarative formalism.
E91-1052.txt,9,Attribute grammar has been used in the past by the author to define computational models of Chomsky s Government binding theory_comma_ from which practical parsing programs were developed Correa_comma_ 1987a .
E91-1052.txt,10,Many systems based on Earley s algorithm have a clear division between the phases of syntactic and semantic analysis.
E91-1052.txt,11,Yellin 1988 _comma_ for instance_comma_ uses a syntactic analysis phase in which Earley s algorithm builds a factored parse tree FPT for the given input_comma_ which is then followed by up to two phases of semantic analysis_comma_ during which the FPT is attributed and evaluated.
E91-1052.txt,12,Watt 1980 and Jones and Madsen 1980 propose a close interaction between syntactic and semantic analysis in the form of attribute directed parsing.
E91-1052.txt,13,However_comma_ their particular realization of the technique is severely restricted for NLP applications_comma_ since it uses a deterministic one path LR algorithm_comma_ applicable only to semantically unambiguous grammars.
E91-1052.txt,14,Pereira and Warren 1983 and Shieber 1985 present v6rsions of Earley s algorithm for unification grammars_comma_ in which unification is the sole operation responsible for attribute evaluation.
E91-1052.txt,15,However_comma_ given the high computational cost of unification_comma_ important differences between attribute and unification grammars in their respective attribution domains and functions Correa_comma_ forthcoming _comma_ and the more general nature of attribute grammars in this regard_comma_ it is of interest to investigate the extension of Earley s algorithm directly to the main subclasses of attribute grammar.
E91-1052.txt,16,The paper is organized as follows Section 2 presents pieliminary elements_comma_ including a definition of attribute grammar and Earley s algorithm.
E91-1052.txt,17,Section 3 presents the extension of the algorithm for S attributed grammars.
E91-1052.txt,18,In Section 4_comma_ we consider the conditions on the underlying grammar under which the extended algorithm may be guaranteed to terminate for each input.
E91-1052.txt,19,For the S attributed case we show that the algorithm terminates if the grammar has no cycles or_comma_ equivalently_comma_ if it is finitely ambiguous.
E91-1052.txt,20,However_comma_ finite partitioning of attribute domains may be used to guarantee the termination of the algorithm_comma_ without the need for restrictions on the context free base.
E91-1052.txt,21,Finally_comma_ a conclusion and note on implementation are given.
E91-1052.txt,23,Notation and Preliminaries We follow the usual notation and terminology for grammars and languages.
E91-1052.txt,24,A language is a set of strings o _comma_er a finite set T of symbols.
E91-1052.txt,25,A grammar is a formal device for specifying which strings are in the set.
E91-1052.txt,26,In particular_comma_ a context free grammar is a cuadruple 299 N_comma_ T_comma_ P_comma_ S _comma_ where N is a finite set of string categories T a finite set of terminal symbols P a finite set of productions or rewriting rules of the form X t _comma_ Xe N_comma_ ae NUT and S a distinguished symbol of N.
E91-1052.txt,27,A binary relation of derivation between strings over the vocalulary NuT of the grammar is defined such that aXl ctol3 iff Xo o is a production of P now_comma_ may be defined as the reflexive and transitive closure of .
E91-1052.txt,28,The language generated by the grammar_comma_ noted L G _comma_ is the set of strings toe T _comma_ such that S to.
E91-1052.txt,29,An attribute grammar is defined upon a context free grammar G N_comma_ T_comma_ P_comma_ S _comma_ by associating each symbol Xe NuT with a finite set A X of attributes_comma_ and a type or domain dom a for each attribute a thus defined Knuth_comma_ 1968 .
E91-1052.txt,30,Each attribute a of X_comma_ noted X.a_comma_ takes values over its domain and represents a specific_comma_ possibly context sensitive property of the symbol.
E91-1052.txt,31,Attribute values are defined by attribution rules of the form Xi.a f Xj.b ..... Xk.c _comma_ associated with each production p Xo Xt...Xn in the grammar_comma_ 0 i_comma_j_comma_k n.
E91-1052.txt,32,Here_comma_ f is an applicative expression function whose value depends on the values of attribute occurrences associated with symbols in the production.
E91-1052.txt,33,Each time p applies in a derivation_comma_ the attribution rule defines the value of the attribute occurrence X.a as a function of the occurrences Xj.b ..... Xk.c_comma_ associated with other symbols in p .
E91-1052.txt,34,We let R p denote the packet of attribution rules associated with p.
E91-1052.txt,35,The grammar may also define attribute conditions of the form B Xi.a ..... Xk.b _comma_ 0 i_comma_ k n_comma_ where B is a Boolean predicate on the values of attribute ocurrences in p.
E91-1052.txt,36,This condition must be satisfied in any derivation requiring the application of p_comma_ and thus contributes to the notion of grammaticality in the language generated by the grammar.
E91-1052.txt,37,We let B p denote the packet of attribute conditions associated with p.
E91-1052.txt,38,The above remarks are summarized as follows An attribute grmmnar is a cuadruple AG G_comma_A_comma_R_comma_B _comma_ where i.
E91-1052.txt,39,G N_comma_ T_comma_ P_comma_ S is a context free grammar ii.
E91-1052.txt,40,A L3Xe NuT A X is a finite set of attributes iii.
E91-1052.txt,41,R k..Jpe pR p is a finite set of attribution rules_comma_ as above and iv.
E91-1052.txt,42,B L3pe p B p is a finite set of attribute conditions_comma_ as above.
E91-1052.txt,43,The base grammar G assigns a derivation tree x to each sentence in L G .
E91-1052.txt,44,The tree is annotated at each node labelled X with the set A X of attributes associated with X each attribute ae A X defines an attribute occurrence X.a at node X.
E91-1052.txt,45,If the grammar is well defined Knuth_comma_ 1968 _comma_ it is possible to evaluate each attribute occurrenc on the tree_comma_ and we say that is correctly attributed iff all attribute conditions yield true. The language generated by the attribute grammar_comma_ L AG _comma_ is now the subset of L G whose members have at least one correctly attributed tree.
E91-1052.txt,46,It is possible to classify the attributes in AG according to the manner in which their values are defined.
E91-1052.txt,47,We say an attribute X.a is synthesized if its value depends only on attributes of daughters of X it is inherited if its value depends on attributes associated with the parent or sisters of X.
E91-1052.txt,48,We say the grammar is S attributed if it contains only synthesized attributes.
E91-1052.txt,49,A more general and practically important class of L attributed grammars is obtained if we allow attributes of both kinds_comma_ but such that each inherited attribute depends only on inherited attributes of the parent_comma_ or attributes of the sisters to its left Bochmann_comma_ 1976 .
E91-1052.txt,50,Earley s algorithm is a recognizer for CFGs which uses top down prediction in combination with bottom up parsing actions.
E91-1052.txt,51,Given an input string Xl ..... Xn it builds a state set Si at each position i of the string_comma_ 0 i n l.
E91-1052.txt,52,Each state in Si is of the form A a.l _comma_ f_comma_ 5 _comma_ where A a. is a dotted production_comma_f an index to the position in the input string where this instance of the production began to be recognized 0 f i _comma_ and 8 a string of k symbols of Iookahead k 0 .
E91-1052.txt,53,To begin_comma_ all state sets are initialized to empty and the initial state .S _1__comma_ 0_comma_ _l_k is put into SO here _1_ is the end of input marker.
E91-1052.txt,54,States are processed in order according to the position of their dot following three actions_comma_ Predictor_comma_ Completer_comma_ and Scanner_comma_ while maintaining the following invariant State A a 13_comma_ f_comma_ 5 is in Si iff the following derivations are valid S GA a x ...xf and ot Xf l...xi.
E91-1052.txt,55,Since the number of possible states is finite the algorithm terminates.
E91-1052.txt,56,The input string is accepted if Sn I  S _1_._comma_ 0_comma_ .l_k .
E91-1052.txt,57,The correctness of this acceptance condition is a consequence of the invariant.
E91-1052.txt,59,Extension to S attributed Grammars The chief element of the extension of the algorithm is a change in the representation of the states in Earley s original algorithm to attributed representations.
E91-1052.txt,60,Now_comma_ each dotted production A a l in a state consists of symbols attributed according to the grammar.
E91-1052.txt,62,The extended algorithm_comma_ in addition to syntactically recognizing the input string_comma_ evaluates the attribution associated with each of its possible derivations.
E91-1052.txt,64,For an S attributed grammar_comma_ this is achieved by the following modification of Earley s algorithm_comma_ in its Completer step SO    SIS.al ..... S.anl _comma_ O_comma_ k  for i 1 rondo begin For each state s in Si_comma_ repeat until no more states may be added to Si or Si l begin 1.
E91-1052.txt,66,Extension of Earley s Algorithm for S attributed Grammars The states in the algorithm are attributed as indicated above.
E91-1052.txt,68,As the state enters the completer_comma_ the attribute occurrences A.ai of A are unevaluated however_comma_ since the grammar is S attributed it is easy to show that the attribute occurrences on the right hand side a of the production have already been evaluated.
E91-1052.txt,69,Hence_comma_ evaluation of the attribute occurrences of A reduces to application of the attribution associated with the production A a_comma_ according to the attribute values in a.
E91-1052.txt,70,This is done by the function eval_s A_comma_ ct_comma_ A ot _comma_ which returns the attributed symbol Ae_comma_ identical to A_comma_ except that its attribute occurrences have been evaluated_comma_ as required.
E91-1052.txt,72,Here the extended algorithm differs form Earley s whereas the original algorithm generates at most one final state_comma_ regardless of the ambiguity of the underlying grammar_comma_ the extended algorithm may generate several instances of this state_comma_ if the grammar is ambiguous.
E91-1052.txt,73,Each instance of the final state corresponds to a different derivation of the initial symbol_comma_ leading to a different evaluation of the symbors attributes.
E91-1052.txt,75,Finite Partitioning of Attribute Domains The last remark in the extension of section 3 shows a defect of the Extended Algorithm It may not terminate in the general case.
E91-1052.txt,76,For the S attributed case_comma_ however_comma_ this may happen only if the underlying grammar is nfinitely ambiguous or_comma_ equivalently_comma_ if it has cycles Or derivations of the form A A_comma_ for some A N.
E91-1052.txt,78,Cyclic grammars play an important role in most recent linguistic theories_comma_ including Government binding GB _comma_ LexicaI Functional Grammar LFG and GPSG cf. Bcrwick_comma_ 1988 Con ca_comma_ 1987b Kornai and Pullum_comma_ 1990 .
E91-1052.txt,79,These have in common that they have shifted from rule based descriptions of language_comma_ to declarative orprinciple based descriptions_comma_ in which the role of phrase structure rules or principles is relatively minor.
E91-1052.txt,80,Thus_comma_ to make the extension of the algorithm useful for natural language applications it becomes necessary to ensure its termination_comma_ in spite of cyclic bases.
E91-1052.txt,81,301 The termination of the Extended Algorithm may be guaranteed while maintaining its full generality_comma_ through a finite partition on the attribute domains associated with each cyclic symbol in the grammar.
E91-1052.txt,82,For each such domain dom a _comma_ the partition defines a finite collection of equivalence classes on attribute values.
E91-1052.txt,83,Now_comma_ before adding a new state A a l _comma_ f_comma_ i to a state set Si_comma_ we test for equivalence according to the defined partitions rather than equality to some previously added state if the new state is equivalent to some other_comma_ it is not added.
E91-1052.txt,84,It is easy to show that the number of attributed dotted items in the grammar_comma_ and hence the size of the state sets_comma_ is now finite.
E91-1052.txt,85,This number is in fact identical to that of Earley s algorithm_comma_ except for a constant multiplicative factor_comma_ dependent on the grammar and the size of the partitions selected for attribute domains.
E91-1052.txt,86,Since the size of the state sets possible with finite partitioning is now finite_comma_ the algorithm always terminates.
E91-1052.txt,87,After establishing a correspondence between attribute and unification grammar UG _comma_ we may see that the technique of restriction used by Shieber 1985 in his extended algorithm is related to finite partitioning on attribute domains_comma_ in fact a particular case which takes advantage of the more structured attribute domains of UG.
E91-1052.txt,88,For attribute grammar_comma_ given that the domains involved are more general e.g._comma_ the integers _comma_ finite partitioning is the required device.
E91-1052.txt,90,Conclusions and Implementalion Status This paper presented and extension of Earley s algorithm to S attributed grammars.
E91-1052.txt,91,Combining on line semantic evaluation with the execution of syntactic actions_comma_ the algorithm is an effective realization of attribute directed parsing_comma_ as proposed by Watt 1980 and Jones and Madsen 1980 .
E91-1052.txt,92,Although the algorithm is a recognizer_comma_ it computes the semantic values associated with each derivation of the input string_comma_ and hence need not be extended to compute tree representations.
E91-1052.txt,93,In attribute grammars with conditions on productions_comma_ the values of attributes already evaluated unay be used to guide the parsing process_comma_ reducing the number of states that may be generated by the algorithm.
E91-1052.txt,94,The extension of the algorithm has been written in C _comma_ using an efficient C implementation of Earley s original algorithm Chamorro and Correa_comma_ 1990 _comma_ and is currently being tested on small grammars.
E91-1052.txt,95,The extended algorithm will be the kernel of ANDES l_comma_ a programming environment for attribute grammars_comma_ intended for natural language applications. .
E91-1048.txt,1,lexical transfer phase is the most crucial step in MT because most of difficult problems are caused by lexical differences between two languages. In order to treat lexical issues systematically in transfer based MT systems_comma_ we introduce the concept of bilingual sings which are defined by pairs of equivalent monolingual signs.
E91-1048.txt,2,The bilingual signs not only relate the local linguistic structures of two languages but also play a central role in connecting the linguistic processes of translation with knowledge based inferences.
E91-1048.txt,3,We also show that they can be effectively used to formulate appropriate questions for disambiguating transfer ambiguities _comma_ which is crucial in interactive MT systems.
E91-1048.txt,6,Research in transfer based MT systems has focussed on discovering an appropriate level of linguistic description for translation_comma_ at which we can specify translation relations or transfer rules in a simple manner.
E91-1048.txt,7,However_comma_ lexical differences between languages have caused problems in this attempt.
E91-1048.txt,8,Besides structural changes caused by lexical Iransfer_comma_ selecting appropriate translations of source lexical items has been one of the hardest problems in MT.
E91-1048.txt,9,Because languages have their own ways of reflecting the structure of the world in their lexicons_comma_ and the process of lexicalization is more or less arbitrary_comma_ bilingual knowledge about lexical correspondences is highly dependent on language pairs and individual words.
E91-1048.txt,10,We have to .prepare a framework in which such idiosyncratic bilingual knowledge about lexical items can be systematically accumulated.
E91-1048.txt,11,Our approach in this paper follows the general trend in computational linguistics which emphasizes the role of the lexicon in linguistic theory.
E91-1048.txt,13,As with their proposal_comma_ we too specify local structural correspondences between two languages in bilingual lexicons.
E91-1048.txt,14,Unlike former approaches_comma_ however_comma_ we explicitly define bilingual signs and use them as predi. cates in logical formulae bilingual pivot expressions .
E91-1048.txt,15,Bilingual signs in our framework not only link the local linguistic structures of two languages where the corresponding two monolingual signs appear_comma_ but also_comma_ by behaving as logical predicates_comma_ they connect linguistic based processes in MT with inference processes.
E91-1048.txt,17,The framework has the following advantages over conventional methods.
E91-1048.txt,18,i Reversibility of bilingual dictionaries lexical transfer rules ii Natural interfaces between knowledge based inference processes and MT iii Ease of paraphrasing using different words see section 6 2.
E91-1048.txt,19,Bilingual signs as logical predicates and their definition The basic idea of bilingual signs is simple. instead of using predicates corresponding directly to surface words_comma_ we use bilingual pairs of lexical items as predicates.
E91-1048.txt,21,1 The teacher runs the program.
E91-1048.txt,22,2 The teacher runs the company.
E91-1048.txt,23,Corresponding to the obvious meaning difference of run in 1 and 2 _comma_ we have to use different surface verbs in Japanese_comma_ jikkoosuru for 1 and un eisuru for 2 .
E91-1048.txt,26,Our system is a conventional transfer based MT system where the monolingual analysis and transfer phases are executed separately.
E91-1048.txt,27,The analysis phase of English produces the following schema of logical formulae 3 as the description of 1 .
E91-1048.txt,31,Because a bilingual sign is defined by two languages here English and Japanese _comma_ the two relationships of logical form  English and logical form  Japanese are specified in the same place.
E91-1048.txt,32,In order to avoid further complications caused by changes of grammatical functions passive constructions_comma_ etc. _comma_ we use thematic role representations as linguistic descriptions in the definitions of bilingual signs.
E91-1048.txt,35,That is_comma_ we define a single bilingual sign which corresponds to a complex linguistic object in one language_comma_ if the other language expresses the same meaning by a single word.
E91-1048.txt,38,We can expect the set of bilingually defined predicates to have appropriate_comma_ at least necessary if not sufficient_comma_ granularity of the semantic domains for translation of the two given languages.
E91-1048.txt,39,Furthermore_comma_ we can use logical formulae to specify mutual relationships among bilingual signs_comma_ which means that we can specify explicitly logical relationships among iexical transfer rules see section 4 .
E91-1048.txt,41,Complex structural changes complex bilingual signs The following show how our framework treats structural changes caused by lexical correspondences.
E91-1048.txt,44,Some of the English verbs paint_comma_ varnish_comma_ etc. include the objects of the Japanese in their meaning.
E91-1048.txt,45,For example_comma_ the structural change between 6a and 6b is treated by the definition 7 .
E91-1048.txt,48,The definition shows that the phrase penki wo nuru in Japanese corresponds to the English paint and that this correspondence defines a predicate as a basic unit of semantic representation.
E91-1048.txt,50,The same kind of phenomena has often been observed in itranslation between English and Japanese.
E91-1048.txt,51,The event expressed by the verb manage On the usage of manage to do something is captured by an adverb nantoka somehow or other or with great effort in English in Japanese.
E91-1048.txt,52,The adverb is used to modify the event expressed as an infinitive clause in English.
E91-1048.txt,53,The correspondence between 8a and Sb is captured by the definition 9 .
E91-1048.txt,57,Our treatment in this paper is basically bilingual in the sense that the non head construction in Japanese is directly related with the English construction in which the corresponding element is expressed as the head.
E91-1048.txt,58,However if we deem the logical level of representation a separate_comma_ more .
E91-1003.txt,1,interfaces combining_comma_ e.g._comma_ natural language and graphics take advantage of both the individual strength of each communication mode and the fact that several modes can be employed in parallel_comma_ e.g._comma_ in the text picture combinations of illustrated documents. It is an important goal of this research not simply to merge the verbalization results of a natural language generator and the visualization results of a knowledge based graphics generator_comma_ but to carefully coordinate graphics and text in such a way that they complement each other.
E91-1003.txt,2,We describe the architecture of the knowledge based presentation system WIP which guarantees a design process with a large degree of freedom that can be used to tailor the presentation to suit the specific context.
E91-1003.txt,3,In WIP_comma_ decisions of the language generator may influence graphics generation and graphical constraints may sometimes force decisions in the language production process_comma_ In this paper_comma_ we focus on the influence of graphical constraints on text generation.
E91-1003.txt,4,In particular_comma_ we describe the generation of cross modal references_comma_ the revision of text due to graphical constraints and the clarification of graphics through text. particular combination of communication modes_comma_ the automatic generation of multimodal presentations is one of the tasks of such presentation systems.
E91-1003.txt,5,The task of the knowledge based presentation system WIP is the generation of a variety of multimodal documents from an input consisting of a formal description of the communicative intent of a planned presentation.
E91-1003.txt,6,The generation process is controlled by a set of generation parameters such as target audience_comma_ presentation objective_comma_ resource limitations_comma_ and target language.
E91-1003.txt,7,One of the basic principles underlying the WIP project is that the various constituents of a multimodal presentation should be generated from a common representation.
E91-1003.txt,8,This raises the question of how to divide a given communicative goal into subgoals to be realized by the various mode specific generators_comma_ so that they complement each other.
E91-1003.txt,9,To address this problem_comma_ we have to explore computational models of the cognitive decision processes coping with questions such as what should go into text_comma_ what should go into graphics_comma_ and which kinds of links between the verbal and nonverbal fragments are necessary.
E91-1003.txt,11,With increases in the amount and sophistication of information that must be communicated to the users of complex technical systems comes a corresponding need to find new ways to present that information flexibly and efficiently.
E91-1003.txt,12,Intelligent presentation systems are important building blocks of the next generation of user interfaces_comma_ as they translate from the narrow output channels provided by most of the current application systems into high bandwidth communications tailored to the individual user.
E91-1003.txt,13,Since in many situations information is only presented efficiently through a The WlP project is supported by the German Ministry of Research and Technology under grant ITW8901 8.
E91-1003.txt,14,We would like to thank Doug Appelt_comma_ Steven Feiner and Ed Hovy for stimulating discussions about multimodal information presentation. _comma__comma__comma_ _comma_ i _comma__comma__comma__comma__comma_ i.
E91-1003.txt,15,I Uft iihe ild iili  i  i i i i .
E91-1003.txt,16,i k.. To fill thewatercontalner_comma__comma_   . .  .   _comma_   . i reniove the cover_comma_  . Fig. l Example Instruction In the project WIP_comma_ we try to generate on the fly illustrated texts that are customized for the intended target audience and situation_comma_ flexibly presenting information whose content_comma_ in contrast to hypermedia systems_comma_ cannot be fully anticipated.
E91-1003.txt,17,The current testbed for WIP is the generation of instructions for the use of an espresso machine.
E91-1003.txt,18,It is a rare instruction manual that does not contain8illustrations.
E91-1003.txt,19,WIP s 2D display of 3D graphics of machine parts help the addressee of the synthesized multimodal presentation to develop a 3D mental model of the object that he can constantly match with his visual perceptions of the real machine in front of him.
E91-1003.txt,20,Fig. 1 shows a typical text picture sequence which may be used to instruct a user in filling the watercontainer of an espresso machine.
E91-1003.txt,21,Currently_comma_ the technical knowledge to be presented by WIP is encoded in a hybrid knowledge representation language of the KL ONE family including a terminological and assertional component see Nebel 90 .
E91-1003.txt,22,In addition to this propositional representation_comma_ which includes the relevant information about the structure_comma_ function_comma_ behavior_comma_ and use of the espresso machine_comma_ WIP has access to an analogical representation of the geometry of the machine in the form of a wireframe model.
E91-1003.txt,23,The automatic design of multimodal presentations has only recently received significant attention in artificial intelligence research cf. the projects SAGE Roth et al. 89 _comma_ COMET Feiner McKeown 89 _comma_ FN ANDD Marks Reiter 90 and WlP Wahlster et al. 89 .
E91-1003.txt,24,The WIP and COMET projects share a strong research interest in the coordination of text and graphics.
E91-1003.txt,25,They differ from systems such as SAGE and FN ANDD in that they deal with physical objects espresso machine_comma_ radio vs. charts_comma_ diagrams that the user can access directly.
E91-1003.txt,26,For example_comma_ in the WIP project we assume that the user is looking at a real espresso machine and uses the presentations generated by WlP to understand the operation of the machine.
E91-1003.txt,27,In spite of many similarities_comma_ there are major differences between COMET and WIP_comma_ e.g._comma_ in the systems architecture.
E91-1047.txt,1,this paper we discuss some problems arising in German Russian Machine Translation with regard to tense and aspect. Since the formal category of aspect is missing in German the information required for generating Russian aspect forms has to be extracted from different representation levels.
E91-1047.txt,2,A sentence based procedure for aspect choice in the MT system VIRTEX is presented which takes lexieal_comma_ morphological and semantic criteria into account.
E91-1047.txt,3,The limits of this approach are shown.
E91-1047.txt,4,To overcome these difficulties a human interaction component is proposed. .
E91-1047.txt,5,Aspect is considered to bca grammatico semanticai category for expressing various temporal .
E91-1011.txt,1,this document_comma_ we present a language which associates type construction principles to constraint logic programming. We show that it is very appropriate for language processing_comma_ providing more uniform_comma_ expressive and efficient tools and treatments.
E91-1011.txt,2,We introduce three kinds of constraints_comma_ that we exemplify by motivational examples.
E91-1011.txt,3,Finally_comma_ we give the procedural semantics of our language_comma_ combining type construction with SLDresolution. .
E91-1011.txt,4,With the development of highly parameterized syntactic theories like Government and Binding theory and Head Driven phrase structure grammars and with the development of theories where rewriting and unification plays a central role_comma_ like Categorial grammars and Unification Grammars_comma_ there is an increasing need for more appropriate and more efficient feature systems.
E91-1011.txt,5,Feature systems must be designed to preserve the adequacy_comma_ the expressiveness and the explanatory power of the linguistic system that one wants to model.
E91-1011.txt,6,Real parsing as well as generation systems often require the manipulation of large sets of features_comma_ these systems must therefore offer a great flexibility in the specification of features in grammar symbols and a significant modularity so that each linguistic aspect morphological_comma_ categorial .... can be dealt with independently.
E91-1011.txt,7,Features are often subject to various constraints.
E91-1011.txt,8,These constraints cannot always be evaluated at the level they are formulated e.g. a feature value is not yet known bu t have to be evaluated later and must be true throughout the whole parsing or generation process.
E91-1011.txt,9,The development of principled based approaches to language processing also require the definition of more .
E91-1008.txt,1,work is concerned with the development of instruments for GB parsing. An alternative to the well known indexation system of Chomsky_comma_ 1981 will be proposed and then used to formalize the view of Binding Theory in terms of the generation of constraints on the referential properties of the NPs of a sentence.
E91-1008.txt,2,Finally the problems of verification and satisfiability of BT will be addressed within the proposed framework.
E91-1008.txt,4,This work is concerned with the development of instruments for GB parsing see Barton_comma_ 1984 Berwick 1987 Kolb Tiersch_comma_ 1990 in particular_comma_ our attention will be focused on the Binding Theory henceforth_comma_ BT a module of the theory of Government and Binding henceforth_comma_ GB see Chomsky 1981 1986 .
E91-1008.txt,5,It has been pointed out eg. in Kolb Tiersch_comma_ 1990 that the lack of a complete and coherent formalization of a linguistic theory like GB can be a major obstacle in addressing the issue of principle based parsing this is true of BT too_comma_ in particular with respect to the indexing system of Chomsky 1981 _comma_ the shortcomings of which have often been pointed out in the literature.
E91-1008.txt,6,A formalism for the treatment of the referential relationships among the NPs of a sentence will be presented that is more expressive than indexation and more effective as a computational tool.
E91-1008.txt,7,In Section 2 the indexing system and the role it plays within BT will be discussed in Section 3_comma_ an alternative will be developed that overcomes some of the shortcomings of indexing.
E91-1008.txt,8,Such a system will_comma_ then_comma_ be used to formalize the view of BT as a device that generates syntactic constraints on reference.
E91-1008.txt,9,In Section 4_comma_ it will be shown how our proposal could be applied to some computational problems_comma_ i.e. the problems of verification and satisfiability within BT.
E91-1008.txt,10,2 Preliminaries Since Chomsky 1981 _comma_ it has become commonplace to denote the interpretative relations among the NPs of a sentence by means of indices_comma_ i.e. integers attached to NPs in such a way that elements bearing the same index are taken to denote the same object s _comma_ while different indices correspond to different denotations most of the statements of BT have been aid down in terms of this system Chomsky_comma_ 1981_comma_ 1986 .
E91-1008.txt,11,In a number of works see Chomsky 1981 _comma_ Higginbotham 1983 and Lasnik Uriagereka 1988 _comma_ however_comma_ it has been pointed out that the indexing device is not adequate to capture certain referential relations this is true for the relation between pronouns and split antecedents_comma_ i.e. antecedents composed of two or more arguments bearing different thematic roles_comma_ l Furthermore_comma_ indices blur the distinction between coindexing under c command and coindexing without c command_comma_ thereby making it difficult to capture the dependence of an element_comma_ behaving like a variable_comma_ upon its antecedent see Reinhart_comma_ 1983 .
E91-1008.txt,12,2 The replacement of indices with index sets has been proposed as a way to address the first problem see Higginbotham_comma_ 1983 an ordinary index is substituted by a singleton when there are pluralities_comma_ e.g. when an NP is coindexed with a split antecedent_comma_ it is annotated with the set union of the index sets of each component of the plurality therefore_comma_ coindexing amounts to equating index sets.
E91-1008.txt,13,In this view_comma_ the ordinary conditions on disjoint reference Principles B and C of BT must be extended to avoid not only identical reference but_comma_ more generally_comma_ reference intersection.
E91-1008.txt,14,It has also been argued Higginbotham_comma_ 1983 that indices should be abandoned and substituted by the non symmetric relation of linking when the antecedent is split_comma_ a plurality of links should be used.
E91-1008.txt,15,This way_comma_ however_comma_ two different situations are collapsed together the one in which an item is coindexed with a plurality of elements all of which share the same index_comma_ and the case of true split antecedents_comma_ where the elements composing the antecedent do not have the same index.
E91-1008.txt,16,Furthermore_comma_ the asymmetric behaviour of linking has no clear correlate at the structural level it will be suggested below that c command should continue to play a role here.
E91-1008.txt,17,Computational works about BT have been mainly concerned with providing lists of possible or impossible antecedents for the NPs of a sentence see Correa 1988 Ingria Stallard 1989 additional procedures select actual antecedents 1 R expressions can take split antecedents too_comma_ at least in certain cases epithets however_comma_ we will not explicitly address this point here.
E91-1008.txt,18,Anaphors_comma_ instead_comma_ can never take lit antecedents.
E91-1008.txt,19,There is a full range of phenomena for which such a distinction seems crucial_comma_ eg. weak crossover and sloppy reading of pronouns Reinhart_comma_ 1983 donkey sentences and the so called indirect binding Ha de_comma_ 1984 Reinhart_comma_ 1987 .
E91-1008.txt,20,However_comma_ only few of them will be addressed here.
E91-1008.txt,21,39 among the potential ones.
E91-1008.txt,22,Berwick 1989 considers only R expressions and a device actually_comma_ a Turing machine assigning the same index to multiple occurences of the same R expression names furthermore_comma_ a set of disjoint indices is associated with each item.
E91-1008.txt,23,Finally_comma_ Fong 1990 performs a combinatorial analysis of the paradigm of free indexation_comma_ as proposed in Chomsky_comma_ 1981 he shows that free indexation gives rise to an exponential number of alternatives and argues for the necessity of interleaving indexing and structure computation.
E91-1008.txt,24,In any case_comma_ indexing has been either explicitly or implicitly assumed_comma_ so that most of the computational approaches to BT suffer the same shortcomings pointed out above.
E91-1008.txt,25,In particular_comma_ given that both split antecedents and the distinction between binding and coreference cannot be easily accounted for_comma_ this results in an impoverished input being provided to the semantic intepretative routine.
E91-1008.txt,26,In the following section a formal system will be discussed that tries to address such problems by explicitly distinguishing between binding and coreference at the same time_comma_ BT will be seen as a theory that states very general constraints constraint schemata _comma_ which are then at least in part instantiated according to the structural properties of the sentence at hand.
E91-1008.txt,27,These instantiated constraints are then used to test sets of positive specifications indexations which constitute the input to further semantic processing.
E91-1008.txt,28,3 3 The formal apparatus For a given sentence w_comma_ let N n 1_comma_ n 2 ..... nm be the set of its NPs furthermore let us indicate with A_comma_ P and R the subset of N whose members are anaphors_comma_ pronouns and R expressions_comma_ respectively.
E91-1008.txt,29,Sets A_comma_ P_comma_ R_comma_ constitute a partition of set N.
E91-1008.txt,30,Finally_comma_ Q denotes the set of quantified expressions and syntactic variables.
E91-1008.txt,31,Split antecedents will be considered as members of the power set of N_comma_ P N for the sake of uniformity_comma_ single NPs will be denoted by members of P N with cardinality equal to one_comma_ i.e. by singletons.
E91-1008.txt,32,Definition 1 A relation s P N P N is defined such that 9 es iff m _comma_ ly n I ..... np _comma_ p l and me lg.
E91-1008.txt,33,For any i n _comma_ neN_comma_ sets . n _comma_ B n and G n will denote the set of elements that c command n and lie 3Disjoint reference constraints arising from Principles B and C of BT are not carried over to semantic routines but are resolved at an earlier stage.
E91-1008.txt,34,Furthermore_comma_ it is assumed that_comma_ whatever processing the semanti routines perform_comma_ their default behaviour consists of assigning non sharing semantic import to different NPs_comma_ unless otherwise stated in the input constraint set. inside its binding domain whenever_comma_ respectively_comma_ n eA_comma_ nEP or neR finally_comma_ if n is a pronoun D n will denote the set of NPs c commanding it and lying outside its binding domain.
E91-1008.txt,35,4 Definition 2 Given a sentence w_comma_ a relation b P N P N is defined_comma_ such that 9 eb iff one of the following conditions obtains i  n t _comma_ nieA _comma_ nj and nje. ni ii  ni _comma_ nieP_comma_ II nj _comma_ and njeD ni .
E91-1008.txt,36,Definition 3 Given a sentence w_comma_ a relation d P N P N is defined_comma_ such that 9 e d iff ni _comma_ II nj and either njeB ni or njeC ni _comma_ depending on whether nieP or nieR.
E91-1008.txt,37,In the following_comma_ b . and s . _comma_ the inverse relations_comma_ will be used as well.
E91-1008.txt,38,Definition 4 Given a sentence w and a phrase structure tree representation for it_comma_ Zw_comma_ the set of binding constraints for T_comma_v is the set R_comma__comma_  r I 9_comma_ veP N _comma_ r is a symbol_comma_ re d_comma_ b_comma_ b .  _comma_ such that 9 r e R_comma__comma_ iff 9 Ig er_comma_ where r is the corresponding relation.
E91-1008.txt,39,5 Given sentence w and a phrase structure representation_comma_ a binding constraint set states disjoint reference constraints essentially_comma_ the consequencies of Principle B and C of BT and the range of the binding relation see below for each NP.
E91-1027.txt,1,a grammar for a language_comma_ it is possible to create finite state mechanisms that approximate its recognition capacity. These simple automata consider only short context information drawn from local syntactic constraints which the grammar hnposes.
E91-1027.txt,2,While it is short of providing the strong generative capacity of the grammar_comma_ such an approximation is useful for removing most word tagging ambiguities_comma_ identifying many cases of iU fonncd input_comma_ and assisting efficiently in othcr natural language processing tasks.
E91-1027.txt,3,Our basic approach to the acquisition and usage of local syntactic constraints was presented clsewhcre in this papcr we present some formal and empiric _comma_d results pertaining to properties of the approximating automata.
E91-1027.txt,5,Parsing is a process by which an input sentence is not only recognized as belonging to the language_comma_ but is also assigned a structure.
E91-1027.txt,8,Still weaker than recognition procedures are methods which approximate the recognition capacity.
E91-1027.txt,9,This is the kind of methods that we discuss in this paper.
E91-1027.txt,10,More specifically_comma_ we analyze the recognition capacity of automata based on local short context considerations.
E91-1027.txt,12,After briefly reviewing this method in section 2 below_comma_ we examine in more detail various characteristics of the approximating automata_comma_ and suggest several applications.
E91-1027.txt,14,Background Local Syntactic Constraints Let S Wi_comma_..._comma_ W be a sentence of length N_comma_ Wi being the words composing the sentence.
E91-1027.txt,15,And let ti ..... t be a tag image corresponding to the sentence S_comma_ ti belonging to the tag set T the set of word class tags used as terminal symbols in a given grammar G.
E91-1027.txt,16,Typically_comma_ M N_comma_ but in a more general environment we allow M N .
E91-1027.txt,17,This is useful when dealing with languages where morphology allows cliticization_comma_ concatenation of conjunctions_comma_ prepositions_comma_ Or determiners to a verb or a noun_comma_ etc. in grammars for l lebrew_comma_ for example_comma_ it is convenient J M.
E91-1027.txt,18,Rimon s main atfiliafion is the IBM Scientific Center_comma_ i laifa_comma_ Israel_comma_ E mail rimon haifasc3.iinusl.ibm.com 2 j.
E91-1027.txt,19,I Icrz was partly supported by the I.eihniz enter for R.esearch in Computer Science_comma_ the lebrew University_comma_ and by the Rau foundation of the Open University. 155 to assume that a preliminary morphological phase separated word forms to basic sequences of tags_comma_ and then state syntactic rules in terms of standard word classes. In any case_comma_ it is reasonable to assume that the tag image it ..... IM cannot be uniquely assigned. Fven with a coarse tag set e.g. parts of speech with no features many words have more than one interpretation_comma_ thus giving rise to exponentially many tag images for a sentence.
E91-1027.txt,21,We use the term path to refer to a sequence of M tags M N which is a tagimage corresponding to the words W_comma_..._comma_ WN of a given sentence S.
E91-1027.txt,22,This is motivated by a view of lexical mnbiguity as a graph problem we try to reduce the number of tentative paths in ambiguous cases by removing arcs from the Sentence Graph SG a directed graph with vertices for all tags in all cohorts of the words in the given sentence_comma_ and arcs connecting each tag to dl tags in the cohort which follows it.
E91-1027.txt,23,The removal of arcs and the testing of paths for validity as complete sentence interpretations are done using local constraints.
E91-1027.txt,24,A local constraint of length k on a given tag t is a rule allowing or disaUowing a sequence of k tags from being in its right or left neighborhood in any tag image of a sentence.
E91-1027.txt,27,Local Constraints Automata We denote by LCA I the simple finite state automaton which uses the pre processed next t sets to check if a given tag stream path satisfies the SCr t_comma_l constraints.
E91-1027.txt,28,In a similar manner it is possible to define LCA k _comma_ relative to the short context of length k.
E91-1027.txt,29,We denote by L the language generated by the 3 Our studies of modern written lebrew suggest that about 60 of the word forms in running texts are ambiguous with respect to a basic tag set_comma_ and the average number of possible readings of such word forms is 2.4.
E91-1027.txt,30,Even when counting only natural readings _comma_ i.e. interpretations which are likely to occur in typical corpora_comma_ this number is quite large_comma_ around 1.8 it is somewhat larger for the small subset of the most common words .
E91-1027.txt,31,156 underlying grammar_comma_ and by L k the language accepted by the automaton LCA k .
E91-1027.txt,32,The following relations hold for the family of automata LCA i L I _ L 2 _ ... L llfis guarantees a security feature If for some i_comma_ I.CA i does not recognize accept a string of tags_comma_ then this string is sure to be illegM i.e. not in 1. .
E91-1027.txt,33,On the other hand_comma_ any LCA k may recognize sentences not in L or_comma_ from a dual point of view_comma_ will reject only part of the illegal tag images .
E91-1027.txt,34,The important question is how tight are the inclusion relations above i.e. how well LCA k approximates the language I.. in particular we are interestcd in LCA I .
E91-1027.txt,35,There is no simple analytic answer to tiffs question.
E91-1027.txt,36,Contradictory forces play here the nature of the language c.g a rigid word order and constituent order yield stronger constraints the grain of the tag set better refined tags different languages may require different tag sets help express refined syntactic claims_comma_ hence more specific constraints_comma_ but they also create a greater level of tagging ambiguity the size of the grammar a larger grammar offers more information_comma_ but_comma_ covering a richer set of structures_comma_ it allows more tag pairs to co occur etc. It is interesting to note that for l lebrew_comma_ short context methods are most needed because of the considerable ambiguity at the lexical level_comma_ but their cll ctiveness suffers from the rather free word constituent order.
E91-1027.txt,37,Finally_comma_ a comment about the computational efficiency of the LCA k automaton.
E91-1027.txt,38,The time complexity of checking a tag string of length n using I_comma_CA k is at most O n x k x loglTI _comma_ while a non deterministic parser for a context free grmntnar may require O n3x IGI2 .
E91-1027.txt,41,Note that for a sentence of length k_comma_ the power of LCA k is idcnticM to the weak generative capacity of the full underlying grammar.
E91-1027.txt,42,But since the size of sentences tag sequences in L is unbounded_comma_ there is no fixed k which suffices.
E91-1027.txt,44,A Sample Grammar To illustrate claims made in the sections below_comma_ we will use the following toy grammar of a small fragment of English.
E91-1027.txt,45,Statements about the correctness of sentences etc._comma_ are of course relative to this toy grammar.
E91-1027.txt,46,The tag set T includes n noun _comma_ v verb _comma_ det determiner _comma_ adj adjective and prep preposition .
E91-1027.txt,47,The context free grammar G is S  NP VP NP  det adj n NP  NP PP PP  prep NP VP  v NP VP  VP PP To extract the local constraints from this grammar_comma_ we first compute the function next t for every tag t in T_comma_ and from the resulting sets we obtain the graph below_comma_ showing valid pairs in the short context of length 1 again_comma_ validity is relative to the given toy grammar  This graph_comma_ or more conveniently the table of valid neighbors below_comma_ define the LCA I automaton.
E91-1027.txt,48,The table is actually the union of the SCr t_comma_l sets for all t in T_comma_ and it is derived directly from the graph  det adj n prep adj adj v det prep n n v adj n prep det adj v n n v det n prep det n 157 5.
E91-1027.txt,49,A Lucky Bag Experiment Consider the following sentence_comma_ which is in the language gcncratcd by grammar G of section 4 1 Thc channing princess kissed a frog.
E91-1027.txt,51,Now let us look at the 720 random inputs generated by permutations of the six words in i _comma_ and the set of corresponding tag images.
E91-1027.txt,53,These are exactly the images corresponding to the eight syntactically correct sentences relative to G _comma_ la b The a charming princess kissed a the frog. lc d The a chamfing frog kissed a the princess. lc t The a princess kissed a the charming frog. lg h The a frog kissed a the charming princess.
E91-1027.txt,54,This result is not surprising_comma_ given the simple scntence and toy grammar.
E91-1027.txt,55,In general_comma_ a grammar with a small number of rules relative to the size of the tag set cannot produce too many valid short contexts .
E91-1027.txt,56,It is therefore interesting to examine another example_comma_ where each word is associated with a cohort of several interpretations.
E91-1027.txt,58,Assuming the word tagging shown in section 6_comma_ there are 256 2 x 2 x 2 x 4 x 2 x 2 x 2 tentative tag hnages paths for this sentence and for each of its 5040 permutations.
E91-1027.txt,59,This generates a very htrge number of rather random tag images.
E91-1027.txt,60,Applying LCA I _comma_ only a small number of hnages are rccogtfizcd as potentially valid.
E91-1027.txt,62,These two examples do not suggest any kind of proof_comma_ but they well illustrate the recognition power of even the least powerful automaton in the LeA i family.
E91-1027.txt,63,To get another point of view_comma_ one may consider the simple formal language L consisting of the strings ar b m for I rn_comma_ which can be generated by a context free grammar over T a_comma_ b . I.CA I based on will recognize all strings of the form a b for 1 j_comma_k_comma_ but none of the very many other strings over T. It can be shown that_comma_ given arbitrary strings of length n over T_comma_ the probability that LeA I will not reject strings not belonging to L is proportional to n 2 _comma_ a term which tends rapidly to 0.
E91-1027.txt,64,This is the over recognition margin.
E91-1027.txt,66,Use of LeA in Conjunction with a Parser The number of potentially valid tag images paths for a given sentence can be exponential in the length of the sentence if all words are ambiguous.
E91-1027.txt,67,It is therefore desirable to filter out invalid tag images before or during parsing.
E91-1027.txt,68,To examine the power of LCAs as a pre parsing fdter_comma_ we use example 2 again_comma_ demonstrating lexical ambiguities as shown in the chart below.
E91-1027.txt,69,The chart shows the Reduced Sentence Graph RSG the original SG from which invalid arcs relative to the SCr t_comma_l table were removed.
E91-1027.txt,70,ALL OLD PEOPLE LIKE BOOKS ABOUT FISH det adj n v  n prep n n n v__prepj e v n We are left with four valid paths through the sentence_comma_ out of the 256 tentative paths in SG.
E91-1027.txt,71,Two paths represent legal syntactic interpretations of which one is the intended meaning .
E91-1027.txt,72,The other two are locally valid but globally incorrect_comma_ having either two verbs or no verb at 158 all_comma_ in contrast to the grammar.
E91-1027.txt,73,SCr t_comma_2 would have rejected one of the wrong two.
E91-1027.txt,74,Note that in this particular example the method was quite effective in reducing sentence wide interpretations leaving an easy job even for a deterministic parser _comma_ but it was not very good in individual word tagging disambiguation.
E91-1027.txt,75,These two sub goals of raging disambiguation reducing the number of paths and reducing word level possibilities are not identical.
E91-1027.txt,76,It is possible to construct sentences in which all words are two way ambiguous and only two disjoint paths out of the 2 N possible paths are legal_comma_ thus preserving all word level ambiguity.
E91-1027.txt,77,We demonstrated the potential of efficient path reduction for a pre parsing filter.
E91-1027.txt,78,But short context techniques can also be integrated into the parsing process itself.
E91-1027.txt,79,In this mode_comma_ when the parser hypothesizes the existence of a constituent_comma_ it will first check if local constraints do not rule out that hypothesis.
E91-1027.txt,80,In the example above_comma_ a more sophisticated method could have used the fact that our grammar does not allow verbs in constituents other than VP_comma_ or that it requires one and only one verb in the whole sentence.
E91-1027.txt,81,The motiwttion for this method_comma_ and its principles of operation_comma_ are similar to those behind different tecimiques combining top down and bottom up considerations.
E91-1027.txt,82,The performance gains depend on the parsing technique in general_comma_ allowing early decisions regarding inconsistent tag assignments_comma_ based on information Which may be only implicit in the grammar_comma_ offers considerable savings.
E91-1027.txt,84,Educated Guess of Unknown Words Another interesting aid Which local syntactic constraints can provide for practical parsers is an oracle which makes educated guesses about unknown words.
E91-1027.txt,85,It is typical for language analysis systems to assume a noun whenever an unknown word is encountered.
E91-1027.txt,86,There is sense in tiffs strategy_comma_ but the use of LCA_comma_ even LCA I _comma_ can do much better.
E91-1027.txt,87,To illustrate this feature_comma_ we go back to the princess and the frog.
E91-1027.txt,88,Suppose that an adjective unknown to the system_comma_ say q ransylvanian was used rather than charming in example 1 _comma_ yielding the input sentence 3 The Transylvanian princess kissed a frog.
E91-1027.txt,89,Checking out all tags in T in the second position of the tag image of this sentence_comma_ the only tag that satisfies the constraints of LCA 1 is adj.
E91-1027.txt,91,Context Sensitive Spelling Verification A related application of local syntactic constraints is spelling verification beyond the basic word level which is_comma_ in fact_comma_ SCr t_comma_0 .
E91-1027.txt,92,Suppose that while typing sentence 1 _comma_ a user made a typing error and instead of the adjective charming u wrote charm or arming _comma_ or any other legal word which is interpreted as a noun 4 The charm princess kissed a frog.
E91-1027.txt,93,This is the kind of errors that a full parser would recognize but a word based spell checker would not.
E91-1027.txt,94,But in many such cases there is no need for the full power and complexity of a parser even LCA I can detect the error.
E91-1027.txt,95,In general_comma_ an LCA which is based on a detailed grammar_comma_ offers cheap and effective means for invalidation of a large set of ill formed inputs.
E91-1027.txt,96,Here too_comma_ one may want to get another point of view by considering the simple formal language L ambm .
E91-1027.txt,97,A single typo results in a string with one a _comma_ changed for a W_comma_ or vice versa.
E91-1027.txt,98,Since LCA i recognizes strings of the form aJb for 1 _j_comma_k_comma_ given arbitrary strings of length n over T a_comma_ b _comma_ LCA I will detect all but two of the n single typos possible those on the borderline between the a s and b s.
E91-1027.txt,99,Remember that everything is relative to the toy grammar used throughout this paper.
E91-1027.txt,100,Hence_comma_ although the charm princess may be a perfect noun phrase_comma_ it is illegal relative to our grammar.
E91-1027.txt,102,Assistance to Tagging Systems Taggcd corpora are important resources for many applications.
E91-1027.txt,103,Since manual tagging is a slow and expensive process_comma_ it is a common approach to try automatic hcuristics and resort to user interaction only when there is no dccisive information.
E91-1027.txt,104,A well built tagging system can learn and improve its performance as more text is processed e.g. by using the already tagged corpus as a statistical knowledge base .
E91-1027.txt,105,Arguments such as those given in sections 7 and 8 above suggest that the use of local constraints can resolve many tagging ambiguities_comma_ thus incrcasing the specd of convergence of an automatic tagging system This seems to be true even for the rather simple and inexpensive I_comma_CA I for laaaguagcs with a relatively rigid word order.
E91-1027.txt,106,For related work cf.
E91-1027.txt,109,Final Remarks To make our presentation simpler_comma_ we have limited thc discussion to straightforward context free grammars.
E91-1027.txt,110,But the method is more gcnerzd.
E91-1027.txt,111,It can_comma_ for example_comma_ he extended to Ci Gs augmented with conditional equations on features such as agrccmcnt cither by translathag such grammars to equivalent CFGs with a more detailed tag set assuming a finite range of feature values _comma_ or by augmenting our a utomata with conditions on arcs.
E91-1027.txt,113,Perhaps more interestingly_comma_ the method can be used even without an underlying grammar_comma_ if a large corpus and a lexical analyzer which suggests prc disambiguatcd cohorts are available.
E91-1027.txt,114,This variant is based on a tcchnique of invalidation of tag pairs or longer sequences which satisfy certain conditions over the whole language L_comma_ and the fact that L can be approximatcd by a large corpus.
E91-1027.txt,115,We cannot elaborate on this extcnsion here. .
E91-1010.txt,1,relations are important tools in linguistics_comma_ especially in anaphora theory. In this paper I present an indexing technique which allows us to implement a simple and efficient check for most cases of command relations which have been presented in linguistic literature.
E91-1010.txt,2,I also show a wide perspective of applications for the indexing technique in the implementation of other linguistic phenomena in syntax as well as in semantics.
E91-1010.txt,4,Barker and Pullum 1990 have given a general definition of command relations.
E91-1010.txt,5,Their definition covers most cases of command relations that have been presented in linguistic literature.
E91-1010.txt,6,I will present here an indexing technique for syntax trees which allows us to implement a check for all command relations which fulfill the definition from Barker Pullum 1990 .
E91-1010.txt,7,The indexing technique can be implemented in a simple and efficient way without any special requierments for the formalism used.
E91-1010.txt,8,Hence_comma_ the indexing technique has a wide spectrum of applications for testing command relations in syntactic analysis.
E91-1010.txt,9,Futhermore_comma_ this method can also be used for command tests in semantics_comma_ i.e. to test for any two semantic representations whether the corresponding nodes of the syntax tree are in a command relation.
E91-1010.txt,10,The usefulness and necessity of a command test in semantics have been demonstrated in Latecki Pinkal 1990 .
E91-1010.txt,11,The general idea of the indexing technique is the following while a syntax tree is being built_comma_ special indices are assigned to the nodes of this tree.
E91-1010.txt,12,Afterwards we can check whether a command relation between two nodes of this tree holds by merely examining simple set theoretical relations of corresponding index sets.
E91-1010.txt,14,A GENERAL DEFINITION FOR COMMAND RELATIONS The general command definition from Barker Pullum 1990 can be informally stated in the following way 1.1 DEFINITION all . a P commands 13 iff every node with a property P that properly dominates a also dominates .
E91-1010.txt,15,In this chapter I will show that this definition is equivalent to the following definition minimum .
E91-1010.txt,16,1.2 DEFINITION minimum . a P commands iff the first node with a property P that properly dominates x also dominates 13.
E91-1010.txt,17,In this definition the first node that dominates a means the node most immediately dominating a_comma_ as it isusually used in linguistics.
E91-1010.txt,18,Below I will specify both of these definitions formally.
E91-1010.txt,19,The main difference between these two definitions is that in the first we must check every node with a property P that properly dominates a_comma_ while in the second it is enough to check only one node_comma_ just the first node with the property P that properly diominates a.
E91-1010.txt,20,It can be easily seen that the command tests based on definition 1.2 are an important improvement in efficiency for computational applications.
E91-1010.txt,21,4 51 Both versions all and minimum are used as command definitions in linguistic literature_comma_ so their equivalence also has linguistic consequences.
E91-1010.txt,22,For example_comma_ definition 1.3 of MAX command from Barker Pullum 1990 which I formulate following Sells definition of c command_comma_ Sells 1987 is equivalent to Definition 1.4_comma_ which has been proposed in Aoun Sportich 1982 .
E91-1010.txt,23,1.3 DEFINITION. a MAX commands iff every maximal projection properly dominating a dominates .
E91-1010.txt,24,1.4 DEFINITION. a MAX commands iff the first maximal projection properly dominating a dominates .
E91-1010.txt,25,These definitions are special cases of definitions 1.1 and 1.2 for the property of being a set of maximal projections.
E91-1010.txt,26,Before I formulate the general command definition in a formal way_comma_ I will now quote some other definitions from Barker Pullum 1990 .
E91-1010.txt,28,A relation R on a set N is reflexive iff aRa for all a in N irreflexive iff aRa symmetric iff aRb implies bRa asymmetric iff aRb implies bRa antisymmetric iff aRb and bRa implies a b transitive iff aRb and bRc implies aRc.
E91-1010.txt,29,A relation R on a set N is called a linear order if it is reflexive_comma_ antisymmetric_comma_ transitive and has the following property comparability for every a and b in N_comma_ either aRb or bRa.
E91-1010.txt,30,The following definition of a tree stems from Wall 1972 .
E91-1010.txt,32,A tree is a 5 tuple T N_comma_L_comma__ D_comma_ P_comma_LABEL _comma_ where N is a finite nonempty set_comma_ the nodes of T_comma_ L is a finite set_comma_ the labels of T_comma_ D is a reflexive_comma_ antisymmetric relation on N_comma_ the dominance relation of T_comma_ P is an irreflexive_comma_ asymmetric_comma_ transitive relation on N_comma_ the precedence relation of T_comma_ and LABEL is a total function from N into L_comma_ the labeling function of T_comma_ such that for all a_comma_ b_comma_ c and d from N and some unique r in N the root node of T _comma_ the following hold i The Single Root Condition r Da ii The Exclusivity Condition a_ Db v b_ Da  a Pb v b Pa iii The Nontangling Condition a Pb a. _ Dc b Dd c Pd I will also use D the proper dominance relation_comma_ which will be just like _D but with all pairs of the form a_comma_a removed.
E91-1010.txt,36,A property P on a set of nodes N is a subset of N.
E91-1010.txt,37,If a node ot satisfies P_comma_ I will write cte P or P o0. 1.9 DEFINITION. The set of upper bounds for a with respect to a property P_comma_ written UB a_comma_P _comma_ is given by UB a_comma_P 13e N 13 Da P .
E91-1010.txt,38,Thus 13 is an upper bound for a if and only if it properly dominates a and satisfies P.
E91-1010.txt,40,Let X be any nonempty subset of a set of nodes N of a tree T.
E91-1010.txt,41,We will call an element a the smallest element of X and denote it as minX if cte X and for every node x xe X _comma_ x Da.
E91-1010.txt,42,If X is an empty set_comma_ then minX the root node of T.
E91-1010.txt,43,A set X is said to be well ordered by D if the relation D is a linear order on X and every nonempty subset of X has a smallest element.
E91-1010.txt,44,For example_comma_ the set Z of integers with the. usual ordering relation is well ordered.
E91-1010.txt,45,Now I can formally specify the meaning of the expression the first node with a property P that properly dominates a from the definition 1.2 it denotes the smallest element of the set UB a_comma_P _comma_ minUB a_comma_P .
E91-1010.txt,46,First I will show that this element always exists. .In set theory_comma_ it is a well known fact that in any tree_comma_ a set of nodes that dominate a given node is well ordered in the dominance relation see Kuratowski Mostowski 1976 _comma_ for example .
E91-1010.txt,47,To be precise_comma_ for a given node a of a tree T_comma_ the set UB ct xe T x Dct is well ordered.
E91-1010.txt,48,Hence_comma_ the set UB a_comma_P UB a n P has a smallest element_comma_ which we denote minUB a_comma_P .
E91-1010.txt,49,At this point we are ready to formally state command definitions 1.1 and 1.2.
E91-1010.txt,50,1.U DEFINITION all . a P commands 13 iff Vx xe UB a_comma_P  x DI3 .
E91-1010.txt,51,1.12 DEFINITION minimum . ct P commands 13 iff minUB a_comma_P _ DIL 52 We say that P generates the P command relation.
E91-1010.txt,52,For example_comma_ we obtain the MAX command relation 1.3 as a special case of Definition 1.11 if we take the set ae N LABEL a e MAX as a property P_comma_ where MAX is any set of maximal projections.
E91-1010.txt,53,Def mition 1.11 is the general command definition from Barker PuHum 1990 .
E91-1010.txt,55,Definitions 1.11 all and 1_comma_12 minimum are equivalent.
E91-1010.txt,57,If a pair x_comma_l fulfills the definition all _comma_ then it also fulfills the definition minimum _comma_ because minUB c UB cx_comma_P if UB a_comma_P O.
E91-1010.txt,60,From the transitivity of the inclusion relation_comma_ indp y indp .
E91-1022.txt,1,pairs or groups of sentences appear to be semantically distinct_comma_ yet specify the same underlying state of affairs_comma_ from different perspectives. This leads to questions about what that underlying state of affairs might be_comma_ and_comma_ for generation_comma_ how and why the alternative expressions might be produced.
E91-1022.txt,2,This paper looks at how such sentences may be generated in a Natural Language interface to a database system. .
E91-1022.txt,3,The following sentences would have a different semantics if parsed_comma_ yet they seem to specify the same state of affairs at some level of representation. la.
E91-1022.txt,4,I can stay until 5. lb.
E91-1022.txt,5,I must leave by 5.
E91-1022.txt,6,For generation_comma_ we ought to be able to produce either.
E91-1022.txt,7,McDonald comments on these sentences What mutually known cognitive structure do we recognise from them that would show them to be two sides of the same coin McDonald 1988 This paper describes a language generation system which is designed as the output component of a database interface_comma_ and is capable of producing similar synonymous sentences.
E91-1022.txt,8,The architecture relies on a two level semantic representation one describes data in the system s application database_comma_ and plays the role of McDonald s mutually known cognitive structure the other describes the semantics of sentences of Natural Language_comma_ and the primitives correspond to specific entries in the lexicon.
E91-1022.txt,9,Information to be communicated is initially expressed in the application level semantics_comma_ and is be mapped to the language level semantics as part of the generation process.
E91-1022.txt,10,Alternatives similar to la and lb arise during this mapping_comma_ and represent a complexity inherent in language which did not exist in the original data they are a property of the description.
E91-1022.txt,11,Application level information is described by linking it with an event or state from now on the term event will cover both these _comma_ for which it provides some parameter.
E91-1022.txt,12,Thus_comma_ the origin of a flight could be described by saying that the plane flies from the origin.
E91-1022.txt,13,The mapping process exploits a domain model which has two parts.
E91-1022.txt,14,The first lays out how nontemporal information is related to domain events.
E91-1022.txt,15,The second describes the temporal characteristics these events using an ontology which is rich enough to capture the temporal semantics of English expressions.
E91-1022.txt,16,Temporal information 125 from the application is described by first expressing it in a way that relates it to times in the model_comma_ and by then attempting to add it to the description of the event which is currently active.
E91-1022.txt,17,The alternatives arise when more than one event can be used.
E91-1022.txt,18,The temporal ontology is based on a recent theory of temporal semantics developed by Moens and Steedman 1988 .
E91-1022.txt,19,This allows a modular representation of the semantics of temporal adverbials like until and by _comma_ and also aids in the generation of tense and aspect.
E91-1022.txt,20,This system looks at the mechanics of how the alternatives can be generated from the initial data_comma_ but we will have less to say about choosing between them.
E91-1022.txt,21,Some simple choice criteria are presented_comma_ although these do not properly address the issue of what perspective is and how it can be quantified and used.
E91-1022.txt,22,We point to proposals from McDonald 1991 which seem more promising on this front.
E91-1022.txt,23,In more general terms_comma_ this work addresses just one of the many issues involved in mapping between Natural Language descriptions of data and the more restricted representation an application database affords.
E91-1022.txt,24,Overview The generation system has been designed as the output stage of an airline information system.
E91-1022.txt,25,The application database holds timetabling data such as plane origins and destinations_comma_ departure and arrival times and so on.
E91-1022.txt,26,Input to the generator is a semantic form compiled from database relations.
E91-1022.txt,27,For example DEST BA123_comma_ROME AARR TIME BA i23_comma_2PM This is an expression of the application level semantics_comma_ and states that the destination of flight BA123 is Rome_comma_ and that the arrival time is 2 p.m.
E91-1022.txt,28,One of the possible surface level semantic descriptions of this would be is arrive BA 123_comma_E Ain E_comma_ROME Aat E_comma_2PM Once the information is in this form_comma_ it can be handed to a grammatical encoder for production of the surface form.
E91-1022.txt,29,The final sentence for this example would be BA123 arrived in Rome at 2 p.m.
E91-1022.txt,30,In this example_comma_ the input data has been described as a point event occurring at a given time.
E91-1022.txt,31,As we will see_comma_ other descriptions could view it in other ways_comma_ such as a state ending at that time_comma_ or as a state beginning at that time.
E91-1022.txt,32,The Domain Model So_comma_ database relations may be described by finding events in a model of the domain to which they correspond.
E91-1022.txt,33,This assumes_comma_ of course_comma_ that the hearer has a similar model of the domain.
E91-1022.txt,34,Figure 1 overleaf shows the model for an airplane flight_comma_ giving the various events and states.
E91-1022.txt,35,It shows an agent_comma_ A_comma_ flying from an origin O_comma_ to a destination at D.
E91-1022.txt,36,The state which can be described as A be at 0 or A not leave O leads on to an event of A leave 0 which initiates a state described as A not arrive at D _comma_ and so on.
E91-1022.txt,37,The causal relations between the events are included in the model_comma_ and used in the generation of tense and aspect_comma_ but their use is not described in this paper.
E91-1022.txt,38,The model is represented declaratively in a Prolog style database.
E91-1022.txt,39,For each event there are two sorts of entry.
E91-1022.txt,40,The first sort record how non temporal input data can be translated to event based logical forms.
E91-1022.txt,41,These entries link up the data parameters with the case roles of the event.
E91-1022.txt,42,For example trans E_comma_ Input sem_comma_ Ling sem The  is used here to denote a variable.
E91-1022.txt,43,The first argument is the event index_comma_ the second is the semantic form of the input data_comma_ and the third is the language level semantics describing the event.
E91-1022.txt,44,An example is trans e5_comma_DEST A_comma_ D _comma_arrive e5_comma_ A Aat e5_comma_ O 126 Figure I Domain Model for a Flight X leave A l Xbe aZ A I X not leave A X fly from A zoB X not arrive as B X arrive at B X be at B TTMR The language level event here is that of arriving _comma_ and is recorded using a Davidsonia.n style semantics Davidson 1967 .
E91-1022.txt,45,The second sort of entry records the temporal characteristics of the event_comma_ using a temporM calculus developed by Moens 1987 _comma_ and based on Kowalski s event logic 1986 .
E91-1022.txt,46,Each event is classified according to its temporal characteristics_comma_ and entries in the calculus are made accordingly.
E91-1022.txt,47,The arrive event is classified as a culmination type of event_comma_ for which_comma_ the entry is occur cul e5 _comma_T6 This characterises the event e5 as a punctual event represented by the single marker cul e5 which occurs at the time T6.
E91-1022.txt,48,The model is a prototypical one for the events of the domain_comma_ and actual times are unknown.
E91-1022.txt,49,Instead_comma_ tetnporal information is recorded using temporal indices_comma_ of which T6 is an example.
E91-1022.txt,50,A process such as fly is represented by two entries_comma_ one for the start point_comma_ and one for the end.
E91-1022.txt,51,The model includes a record of the relative times of the indices_comma_ and actual times may be included if they become known.
E91-1022.txt,52,The model also includes causal relations between events_comma_ which can be used in the generation of tense and aspect.
E91-1022.txt,53,This model has been identified by Moens as capable of capturing the semantics of English temporal expressions more fully thau other formalisms_comma_ such McCarthy and Hayes 1969 _comma_ or Allen 1984 .
E91-1022.txt,54,Semantics of Temporal Adverbials With this sort of model_comma_ the semantics of adverbials may be defined in modular fashion.
E91-1022.txt,55,For instance_comma_ until is defined as describing the time at the end of a process type of event.
E91-1022.txt,56,So_comma_ if a process such as Jim ran ends at the time 2 p.m. _comma_ this would be described as Jim ran until 2 p.m. .
E91-1022.txt,57,Similar interpretations may be defined for for _comma_ in _comma_ since _comma_ by _comma_ later and so on.
E91-1022.txt,58,An Example An example will show how several different descriptions of the same initial data may be pro 127 duced using this machinery.
E91-1022.txt,59,Beginning with the input data structure shown previously in the overview_comma_ the first step is to split it into temporal and non temporal data_comma_ which is done with a simple set of rewriting rules Temp Data ARR TIME BA123_comma_2PM Other Data DEST BA123_comma_ROME This is mapped onto the model by attaching the temporal data to one or more if necessary of the temporal indices_comma_ and by inserting the non temporal data into a trans predicate Temp Data T6_comma_2PM Other Data trans E_comma_DEST BA123_comma_ROME _comma_ Ling sem A duration_comma_ such as the flight time could be attached to two indices using span T5_comma_T6_comma_Flighttime .
E91-1022.txt,60,Instantiating the trans predicate in the model picks out an event that describes the data.
E91-1022.txt,61,Backtracking allows all possibilities to be produced.
E91-1022.txt,62,In the current model_comma_ this picks out four events_comma_ giving the linguistic semantics fly e3_comma_BA123 A to e3_comma_ROME not arrive e4_comma_BA123 at e4_comma_ROME arrive e5_comma_BA123 at e5_comma_ROME be e6_comma_BA123 at e6_comma_ROME Of these_comma_ e3 is characterised as a culminating process like a process_comma_ but with a definite end point ending at T6_comma_ e4 is a state ending at T6_comma_ e5 is a culmination occurring at T6_comma_ and e6 is a state beginning at T6.
E91-1022.txt,63,Next_comma_ we must describe the temporal data T6_comma_2PM .
E91-1022.txt,64,A set of rules looks at the event characteristics_comma_ and the data to be expressed_comma_ to see which adverb is appropriate.
E91-1022.txt,65,For e4_comma_ the until adverb is chosen_comma_ and added to the semantic form to give not arrive e4_comma_BA123 A at e4_comma_ROME A until e4_comma_2PM Similarly_comma_ for e5_comma_ the adverbs at or by can be used_comma_ and for e6 by or since .
E91-1022.txt,66,That since is only used if conditions for the perfect also hold.
E91-1022.txt,67,Insufficient space prevents discussion of the details here.
E91-1022.txt,68,No adverb is available to describe the end time of a culminating process_comma_ and so no phrase can be built using e3.
E91-1022.txt,69,The successful cases could eventually be realised as 2a.
E91-1022.txt,70,BA123 didn t arrive at Rome until 2 p.m.
E91-1022.txt,72,BA123 arrived at Rome at 2 p.m.
E91-1022.txt,74,BA123 arrived at Rome by 2 p.m.
E91-1022.txt,76,BA123 was at Rome by 2 p.m.
E91-1022.txt,77,If conditions for using the perfect held_comma_ the last of these could be replaced by 2e.
E91-1022.txt,78,BA123 has been at Rome since 2 p.m.
E91-1022.txt,79,Choosing Between The Alternatives For the question answering system_comma_ several criteria are being investigated for choosing between the alternatives.
E91-1022.txt,80,The first is a simple mirroring of the phrasing of the question_comma_ the syntactic and semantic analysis of the question being retained in the discourse model.
E91-1022.txt,81,For example 3a.
E91-1022.txt,82,User When will BA123 be at Rome
E91-1022.txt,84,System It will be at Rome by 2 p.m.
E91-1022.txt,85,The main verb of the question is be with a subject of BA123 .
E91-1022.txt,86,One of the possible descriptions uses the same verb and subject albeit pronominalised _comma_ and would be the chosen alternative.
E91-1022.txt,87,This criteria is used when the generated sentence is simply supplying new information which the user has requested.
E91-1022.txt,88,A second criteria seems to be useful when the answer violates a presupposition detected in the query.
E91-1022.txt,89,For example_comma_ take the question 128 4a.
E91-1022.txt,90,User Will BA123 be at Rome by noon
E91-1022.txt,91,This includes the presupposition that BA123 arrives at noon.
E91-1022.txt,92,If it doesn t_comma_ the best form for the answer seems to depend on the actual time of arrival.
E91-1022.txt,94,System No_comma_ it doesn t arrive here until 2 p.m.
E91-1022.txt,96,System No_comma_ it will be here by 11 a.m.
E91-1022.txt,97,Construction 4b would be chosen if the presupposed time lay before the arrival time_comma_ and thus within the timespan covered by the state not arrive .
E91-1022.txt,98,On the other hand_comma_ construction 4c would be chosen if the pre supposed time lay after the actual time_comma_ placing it within the timespan covered by the state be at Rome .
E91-1022.txt,99,Finally_comma_ the alternatives could be useful to promote cohesion in multi sentence explanations of the following sort 5a.
E91-1022.txt,100,BA123 won t be here until noon.
E91-1022.txt,101,It was delayed at Paris.
E91-1022.txt,103,BA123 arrives at noon.
E91-1022.txt,104,It will taxi to Terminal 3.
E91-1022.txt,105,The second sentence is an explanation or elaboration of the first.
E91-1022.txt,106,In the first example_comma_ the explanation refers to an event located in the time period before the arrival_comma_ and in the second_comma_ it is more closely associated with the arrival time.
E91-1022.txt,107,The description of the arrival time is chosen to reflect this.
E91-1022.txt,108,Related work and Discussion In a description of the process of language given by Levelt 1989 _comma_ a module called micro planning is included.
E91-1022.txt,109,This module comes after the content of the output has been decided on_comma_ and before grammatical encoding.
E91-1022.txt,110,Micro planning consists of choosing the language related semantic primitives used for describing a data structure which is not linguistically based.
E91-1022.txt,111,Levelt notes that_comma_ because of the nature of language_comma_ this process will be forced to make choices of perspective.
E91-1022.txt,112,Much work on generation has assumed that the input semantic form is already in some sort of languagese see_comma_ for example McDonald 1983_comma_ McKeown 1985 _comma_ but the processing described in this paper would be part of the micro planner.
E91-1022.txt,113,There are several precedents for the use of two level semantic descriptions for generation.
E91-1022.txt,114,The first_comma_ perhaps_comma_ was HAM ANS Wahlster 1983 _comma_in which the generator translated from the language DEEP to the language SURF.
E91-1022.txt,115,More recently there has been the TENDUM system Bunt 1987 _comma_ using the model theoretic logical languages EL F and EL R_comma_ and others Kernpen 1987_comma_ De Roeck 1986 .
E91-1022.txt,116,These systems translated between the levels_comma_ but did not address the issues of alternative mappings.
E91-1022.txt,117,However_comma_ this question has been investigated by McDonald 1991 .
E91-1022.txt,118,He has proposed a solution in which the data structures of the application program a diary manager are based on primitives such as transition at 4PM .
E91-1022.txt,120,One of these sets is selected and included in evolving text structure.
E91-1022.txt,121,This doesn t seem to take account of the nature of the the events described by leave and stay _comma_ or the temporal semantics involved in using adverbials like at and until .
E91-1022.txt,122,McDonald does_comma_ however_comma_ address the important issue of the criteria for choosing between alternatives.
E91-1022.txt,123,The choice of perspective is intimately bound up with the reasoning of the manager_comma_ which can use knowledge about intentions and surrounding events to decide which version of the description is the most appropriate.
E91-1022.txt,124,This sort of approach seems to be necessary for the development of more comprehensive choice criteria.
E91-1022.txt,125,Conclusion This paper describes a generation system which is capable of generating A range of Natural Language descriptions of the output of a database enquiry program.
E91-1022.txt,126,The system uses a two level model of semantics.
E91-1022.txt,127,The possibility of alternative descriptions arises from the mapping be .
E91-1022.txt,128,129 tween the two levels.
E91-1022.txt,129,Some simple criteria are used to choose the alternative which fits best into the dialogue context.
E91-1022.txt,130,Acknowledgements The author is supported by the Science and Engineering Research Council_comma_ and by Logica UK.
E91-1022.txt,131,I would like to thanks the many colleagues who have provided support and encouragement_comma_ especially Steve Pulman_comma_ Julia Galliers_comma_ Richard Crouch_comma_ Ann Copestake_comma_ Nick Youd_comma_ Victor Poznanski_comma_ Arnold Smith and Derek Bridge. .
E91-1026.txt,1,basic problem that must be dealt with in order to build an intelligent tutoring system ITS in the domain of foreign language teaching is that of establishing what kind of grammatical knowledge has to be included in the domain expert module. Two basic options are possible i to use a naive or pedagogical grammar_comma_ comprising knowledge derived from textbooks and school grammars or ii to use one of the formal grammars developed by theoretical and computational linguists.
E91-1026.txt,2,The paper discusses the relationships between naive and formal grammars in foreign language teaching and presents_comma_ as a case study_comma_ an attempt to integrate the two approaches within ET English Tutor _comma_ an ITS aimed at helping Italian students master English verb usage.
E91-1026.txt,3,More particularly_comma_ the paper focuses on the possibility of integrating a naive grammar into a systemic framework.
E91-1026.txt,4,The reliability of the proposed approach is currently being evaluated by means of a series of computational experiments with the Verb Generation Expert of ET. .
E91-1026.txt,5,A problem that must be dealt with in order to build an ITS in the domain of foreign language teaching is that of establishing what kind of grammatical knowledge has to be included in the Domain Expert module.
E91-1026.txt,6,At first sight_comma_ two distinct options are possible a to utilize the knowledge contained in textbooks and school grammars b to adopt one of the formal grammars developed by theoretical and computational linguists.
E91-1026.txt,7,Both these solutions have their shortcomings.
E91-1026.txt,8,Traditional grammar textbooks have serious drawbacks which concern both their content and the way it is presented to the student.
E91-1023.txt,1,Inheritance PI morphology provides uniform treatment of both concatenative and non concatenative morphological and phonological generalisations using default inheritance. Models of an extensive range of German Umlaut and Arabic intercalation facts_comma_ implemented in DATR_comma_ show that the PI approach also covers hard cases more homogeneously and more extensively than previous computational treatments.
E91-1023.txt,3,Computational models of sentence syntax are increasingly based on well defined linguistic theories and implemented using general formalisms by contrast_comma_ morphology and phonology in the lexicon tend to be handled with tailor made hybrid formalisms selected for properties such as finite state compilability_comma_ object orientation_comma_ default inheritance_comma_ or procedural efficiency.
E91-1023.txt,4,The linguistically motivated Prosodic Inheritance PI model with defaults captures morphotactic and morphophonological generalisations in a unified declarative formalism_comma_ and has broad linguistic coverage of both concatenative morphology and the notorious hard cases of non concatenative morphology.
E91-1023.txt,5,This paper integrates the PI concepts underlying previous descriptions of German Umlaut Reinhard 1990a_comma_ 1990b _comma_ Bantu tone morphology and Arabic C V intercalation Gibbon 1990 Umlaut and intercalation are treated here.
E91-1023.txt,6,PI descriptions are currently Implemented in a DATR dialect Gibbon 1989 for DATR cf. Evans Gazdar 1989_comma_ 1990_comma_ 1989a_comma_ 1989b DATR was chosen for its syntactic simplicity and its explicit formal semantics.
E91-1023.txt,8,INHERITANCE AND NON CONCATENATIVE MORPHOLOGY Morphological generalisations are of three basic kinds morphotactic_comma_ the combinatorial principles of word composition in terms of immediate dominance ID relations_comma_ morphosemantic_comma_ interpretation functions from morphotactic structures to semantic representations_comma_ and morphophonologica or morphograph ic _comma_ interpretation functions from morphotactic structures to surface phonological or orthographic representations.
E91-1023.txt,9,This paper is mainly concerned with modelling morphotactic and morphophonological generalisations.
E91-1043.txt,1,this paper I will argue for a model of grammatical processing that is based on uniform processing and knowledge sources. The main feature of this model is to view parsing and generation as two strongly interleaved tasks performed by a single parametrized deduction process.
E91-1043.txt,2,It will be shown that this view supports flexible and efficient natural language processing.
E91-1043.txt,4,The aspect of bidirectionality has been gaining importance since the growing rate of research on natural language generation over the last years offers us deeper insights into this cognitive ability of humans.
E91-1043.txt,5,There are theoretical as well as practical reasons for adopting bidirectionality.
E91-1043.txt,6,Theoretically_comma_ the assumption of common knowledge sources for both generation and analysis is essential for the view of language as an interpersonal medium and an interface to thought McDonald 1987 .
E91-1043.txt,7,From a psychological point of view_comma_ there is a certain amount of empirical evidence for shared processors or facilities Jackendoff 1987 From a system engineering view_comma_ a bidirectional system produces utterances only from that subset of language that it is capable to understand.
E91-1043.txt,8,Therefore_comma_ inconsistencies of the language behaviour of the system can be avoided Jacobs 1988 .
E91-1043.txt,9,A fundamental requirement of a bidirectional knowledge base is that it be represented declaratively Appelt 1987 .
E91-1043.txt,10,From this viewpoint one can distinguish two different types of bidirectional natural hmguage systems systems that use uniform knowledge sources_comma_ but different processes systems that use uniform knowledge sources as well as uniform processes 1 Up to now_comma_ systems that are capable of analysing and producing language fall into the first class_comma_ i.e. they use different operations for both directions cf. Hoeppner et al. 1983 Busemann and Hauenschild 1988 Allgayer et al. 1989 Currently_comma_ it is an open question what degree of bidirectionality should or could be desired cf. Appelt 1987 Mann 1987 McDonald 1987 Shieber 1988 Jacobs 1988 .
E91-1043.txt,11,One of the reasons could be that the formal specification of some tasks e.g._comma_ the determination of content in generation is currently not well understood in order to decide whether they could be bidirectional in principle.
E91-1043.txt,12,But in some research areas uniform processing models have been developed that are based on formalisms which are well suited for uniform irepresentation and processing_comma_ e.g._comma_ Koskenniemi s 1984 two level model of morphology.
E91-1043.txt,13,Recently_comma_ there are first approaches to uniform architectures for grammaticallprocessing e.g._comma_ Shieber 1988 Dymetman and Isabelle 1988 Dymetman et al. 1990 .
E91-1043.txt,14,These architectures are based on Pereira and lWarren s 1983 paradigm of parsing as deduction.
E91-1043.txt,15,In principle_comma_ parsing and generation are viewed as a single parametrizeddeduction process.
E91-1043.txt,16,PROBLEMS OF BIDIRECTIONAL GRAMMATICAL PROCESSING Thanks to Klaus Netter_comma_ Karel Oliva_comma_ Norbert Reithinger_comma_ Harald Trost and Hans Uszkoreit for fruitful discussions about the aspects of the paper s contents.
E91-1043.txt,17,1Besides these two classes there are also systems that use different knowledge sources that are compiled from the same source e.g._comma_ Horacek and Pyka 1988 and systems that use common basic representation devices e.g._comma_ Lancel et_comma_al. 1988 Neumann and Finkler 1990 .
E91-1043.txt,18,245 Currently developed approaches that consider parsing as well as generation e.g._comma_ Shieber 1988 Shieber et al. 1990 Dymetman et al. 1990 van Noord 1990 Zajac and Emele 1990 assume that both tasks take place independently from each other_comma_ i.e. an utterance is either generated or parsed and that grammatical processing can be performed without considerations of discourse.
E91-1043.txt,19,A great problem with this view is that it offers no solution of the problem of choice between paraphrases in generation The proposed approaches assume more or less explicitly modularity between the conceptual and grammatical component of a natural language system.
E91-1043.txt,20,2 A great advantage of a modular design especially for uniform architectures is that it is possible to view the grammatical component as relatively autonomous and selfcontained cf. Appelt 87 .
E91-1043.txt,21,But then the following problems emerge The conceptual component will be unable to exactly specify the logical form as input to the grammatical component that will precisely lead to the utterance that reflects the intended meaning unless the conceptual module has detailed information about the grammar and knows when to use a specific construction which renders the modular design meaningless .
E91-1043.txt,22,On the other hand_comma_ when parsing and generation are performed within the grammatical component by a single process only then the opposite view of computing all possible parses of an utterance is the computation of all possible paraphrases of a logical form.
E91-1043.txt,23,When gramm ttical processing should be modelled by means of a bidirectional grammar_comma_ the declarative structure of the grammar must not contain pragmatical or stylistical information because of the modular design.
E91-1043.txt,24,But then the process can only choose randomly between paraphrases during generation and this means that the intended meaning will possibly not be conveyed.
E91-1043.txt,25,Ideally_comma_ a logical language would be helpful which necessarily and sufficiently represents all meaning distinctions of natural 2By a conceptual component I mean either the whatto say component of a generation system or the component that performs inference_comma_ plan recognition or anaphora resolution of an understanding system. language.
E91-1043.txt,26,But as Shieber 1988 states this ... is just the central problem of knowledge representation for natural language 10 general .
E91-1043.txt,27,Currently_comma_ there exist only approximate solutions to this problem for example the use of canonical logical forms cf. Shieber 88 .
E91-1043.txt,28,3 But this still offers no solution of the problem of choice between paraphrases.
E91-1043.txt,29,In this paper it will be argued that the following two points will contribute to an approximate solution interleaved parsing and generation using the language use of interlocutors as an additional access criterion to linguistic knowledge Interleaved parsing and generation means that both tasks take place in parallel see section 2 .
E91-1043.txt,30,In principle this results in a bidirectional and incremental flow of information during natural language processing see section 4.1 .
E91-1043.txt,31,An important point during the use of language is that the Choice of linguistic material is influenced by the language use of others see section 3 .
E91-1043.txt,32,This leads to more flexibility not all necessary parameters e.g._comma_ pragmatical values need to be specified in the input of a generator because decision points can also be set dynamically during run time.
E91-1043.txt,33,A promising approach to realize these two features will be to base grammatical processing on a uniform process that is parametrized by means of a declaratively specified preference structure of knowledge sources.
E91-1043.txt,34,But_comma_ it is necessary to be aware that the grammatical component must be assumed to be an integrated part of a whole natural language system in particular in models for performing dialogs in order to realize this solution. Before the architecture of the model will be described in section 4 the two issues are explained in more detail in the next sections.
E91-1043.txt,35,2 INTERLEAVING GENERATION AND ANALYSIS The strategy of viewing natural language processing as based on a uniform deduction process has a formal elegance and results in more compact systems.
E91-1043.txt,36,There is one further advantage that is of both theoretical and practical relevance uniform architectures offer the possibility to view generation and parsing as strongly interleaved tasks.
E91-1043.txt,37,By this I mean 3It is questionable whether there exists a full solution.
E91-1043.txt,38,246 that during performing one task e.g._comma_ generation the other one e.g._comma_ analysis is used for monitoring the former.
E91-1043.txt,39,In principle this results in a bidirectional and incremental flow of information During the parse of an utterance the addressee of the utterance can simultaneously start to construct his answer.
E91-1043.txt,40,In doing so_comma_ partial results of the parsing process can be used directly during generation e.g._comma_ if a paraphrase will be generated .
E91-1043.txt,41,In such flow of control it will be possible that generation can be used for completing the resulting structure of elliptic_comma_ underspecified or ill formed input during the process of understanding or for generating paraphrases in due time. During generation interleaved parsing could help to avoid the construction of ambiguous utterances.
E91-1043.txt,42,E.g._comma_ it is necessary for a natural language help system to generate utterances that reflect exactly the intended meaning if possible at all to be sure that the dialog partner will perform the correct operations.
E91-1043.txt,43,For instance_comma_ producing the utterance Remove the folder by means of the system tools is better than Remove the folder with the system tools because for the latter utterance there exists the reading Remove the folder that contains the system tools _comma_ too.
E91-1043.txt,44,Of course_comma_ it is also possible to analyse a generated utterance if processes are performing their tasks in an isolated way.
E91-1043.txt,45,4 In such flow of control the complete istructure has to be generated again if ambiguities are detected that have to be avoided.
E91-1043.txt,46,BeCause the source of an ambiguous utterance is not used directly to guide the generation process it is possible that the newly generated structure is still ambiguous and it may happen that the same ambiguous structure is generated again .
E91-1043.txt,47,This results in inefficient systems because in general the loop between the i isolated processes must be performed several times.
E91-1036.txt,1,paper describes a classical logic for attribute value or feature description languages which ate used in urfification grammar to describe a certain kind of linguistic object commonly called attribute value structure or feature structure . Tile algorithm which is used for deciding satisfiability of a feature description is based on a restricted deductive closure construction for sets of literals atomic formulas and negated atomic formulas .
E91-1020.txt,1,new approach to structure driven generation is I resented that is based on a separate semantics as input structure. For the first time_comma_ a GPSGbased formalism is complemented with a system of pattern action rules that relate the parts of a semantics to appropriate syntactic rules.
E91-1020.txt,2,This way a front end generator can be adapted to some application system such as a machine translation system more easily than would be possible with many previous generators based on modern grammar formalisms.
E91-1020.txt,4,In the licld of unification based computational linguistics_comma_ current research on tactical natural language NL generation concentrates on the foliowing problem i Given a semantic representation which is often called logical form LF and a grammar that includes a lexicon_comma_ what are the surface strings corresponding to the semantic representation
E91-1020.txt,6,Most of this work shares a Montagovian view of semantics by assuming that LF be integrated into the grammar rules_comma_ thus assigning to each syntactic category its semantic representation.
E91-1020.txt,8,The purpose of these rulcs is to explicitly relate the semantic sub structures to possible syntactic counterparts.
E91-1020.txt,9,The rnappizJg process is driven by the semantic input structure that is traversed step by step.
E91-1020.txt,10,At each step PA rules are applied_comma_ which contribute to successively i roducing an overall syntactic structure from which the terminal string can easily be produced.
E91-1020.txt,11,This new approach allows for a carefully directed and nearly deterministic choice of grammar rules.
E91-1020.txt,12,KEEPING SEMANTICS SEPARATE FROM SYNTAX The integrated semantics approach is often illustrated in a Prolog like notation using DCG rules.
E91-1020.txt,13,The infix function symbol  is used in each category to separate tile syntactic from the semantic part.
E91-1020.txt,15,The relation between the semantics S and the semantics of Comp l. is established in tile lexical entry for tile verb 2 .
E91-1020.txt,19,It was observed that top down generation may not terminate.
E91-1020.txt,20,This is illustrated in 1 .
E91-1020.txt,21,If the vp node is used for top down expansion_comma_ there is nothing to prevent the subcategorization list from growing infinitely.
E91-1020.txt,22,If the Comp node is used_comma_ the constituent to be generated must completely be guessed due to the uninstantiated semantics.
E91-1020.txt,23,Since the grammar will contain recursive rules e.g. for relative clauses _comma_ the guessing procedure will not terminate either.
E91-1020.txt,24,In view of this problem a bottom up approach was suggested that is guided by semantic information in a top down fashion.
E91-1020.txt,25,The benefits of integrated semantics are manifold.
E91-1020.txt,27,LF is defined on purely linguistic grounds and as such_comma_ it is well suited to tile contputationai linguist s work. llowever_comma_ if a generator based on an integrated semantics is to be used for conveying the results of some application system into NL_comma_ expressions of the application system s SRL have to be adalJted to LF.
E91-1020.txt,28,Given that tile grammar should not be rewritten_comma_ this amou_comma__comma_ts to an additional step of processing.
E91-1020.txt,29,This step may turn out to be costly since the SRL will typically contain application dependent information that must be considered.
E91-1020.txt,31,The results of the transfer say_comma_ from German to English are encoded in a semantic representation that is given to the system s generation component to produce the English target sentence.
E91-1020.txt,32,In a system capable of translating between a variety of languages_comma_ representations of this kind may themselves be subject to transfer and will therefore contain information relevant for translation.
E91-1020.txt,35,The effort of introducing an additional step of processing can be saved to a large extent by adopting a separate semantics approach.
E91-1020.txt,36,The SRL of some application system may directly serve as an interface to the generator.
E91-1020.txt,37,3 In the case at hand_comma_ two additional components must be introduced into the generation scenario the definition of SRL and PA rules.
E91-1020.txt,38,Instead of mapping SRL onto LF_comma_ SRL is directly related to syntax by virtue of the PA rules.
E91-1020.txt,42,FAS is defined by context free rule schemata with complex categories consisting of a main category e.g. clause in Figure la _comma_ which is associated with a fixed list of feature specifications.
E91-1020.txt,43,5 The categories are in canonical order with the functor preceding all of its arguments.
E91-1020.txt,44,In contrast to syntactic structures where agreement relations are established by virtue of feature propagation_comma_ FAS categories contain alnmst no redundant information.
E91-1020.txt,45,For instance_comma_ number information is only located at the det category.
E91-1020.txt,46,The use of semantic relations encoded by the role feature _comma_ role configurations conf and semantic features allows us to discriminate between different readings of words that result in different translational equivalents.
E91-1020.txt,47,Moreover_comma_ part of the thematic structure of the source language sentence is preserved during transfer and encoded by virtue of the feature them with the numerical values indicating which portion should preferrably be presented first_comma_ second_comma_ third etc. The definitions of FAS for the German and English fragments mainly differ with regard to their terminal symbols.
E91-1020.txt,49,Rather the interface in question must be situated somewhere in the how to say it component because it presupposes many decisions about sentence formulation e.g. regarding pronominalization_comma_ or voice .
E91-1020.txt,51,Sln the present versions there are up to seven features in a FAS category.
E91-1020.txt,52,For sake of simplicity many details irrelevant to the present discussion are omitted in the examples.
E91-1020.txt,54,The GPSG formalism used includes the ID LP format_comma_ feature co occurrence restrictions FCRs and universal principles of feature instantiation FIPs .
E91-1020.txt,55,The ID rules are interpreted by the generator as providing the basic information for a local tree.
E91-1020.txt,56,The categories of each generated local tree are filrther instantiated by the FIPs and FCRz.
E91-1020.txt,57,Finally_comma_ the branches are ordered by virtue of the LP statelnen .s.
E91-1020.txt,58,Strategies for structure building and feature instantiation.
E91-1020.txt,59,The task of constructing an admissible GPSG syntactic structure call be divided up into the following suhta.sks that can be performed independently of each other_comma_ and each according to its own processing strategy _comma__comma_ Structure building by virtue of PA rules_comma_ which in turn use ID rules .
E91-1020.txt,60,Feature instantiaton and ordering of the branches by virtue of FIPs_comma_ FCRs and LP statemerits The question arises which strategies are best suited to ellicient generation.
E91-1020.txt,61,For each subtask both a top down and a bottom up strategy have been investigated.
E91-1020.txt,62,As a result it turned out that structure building shouhl occur top down whereas feature instantiation should be performed in a bottomup manner.
E91-1020.txt,63,Before .justifying the result let us have a closer look at the sl.ructure buiiding algorithm.
E91-1020.txt,64,Tile overall syntactic structure OSS is successively construed in a top down manner.
E91-1020.txt,65,At each level there is a set of nonterminal leaf nodes available serving as attachment points for further expansion steps initially tile empty category is the only attachment point .
E91-1020.txt,66,An expansion step consists of 1. generating a local tree t by virtue of an ID rule_comma_ 2. unifying its mother node with one of the attachment points_comma_ 3. removing the attachment point from the current set_comma_ 4. defining tile daughters of t as the new current set of attachment points.
E91-1020.txt,67,Since lexicai entries terminate a branch of the OSS_comma_ the fourth of the above points is dropped during expansion of lexical categories processing continues with the reduced set of attachment points.
E91-1020.txt,68,Feature instafftiation and the ordering of branches take place in a bottom up manner after a local tree has no fuither attachment points associated with it i.e. all of its daughters have been expanded .
E91-1020.txt,69,Then processing returns to tile next higher level of tile OSS examining the set of attachment points.
E91-1020.txt,70,Depending on whether or not it is empty_comma_ the next step is either feature instantiation or structure building.
E91-1020.txt,71,Given this interlinking of the two subtasks_comma_ all OSS is admitted by tile grammar if 115 its top most local tree has passed feature instantiation.
E91-1020.txt,72,The effects of feature instantiation with respect to the German example in Figure lb 6 can be better understood with the help of the S expansion rules used of.
E91-1020.txt,73,3 5 . t Rule 3 causes topicalization_comma_ 4 introduces a perfect auxiliary_comma_ and 5 requires a transitive verb whose object is topicalized.
E91-1020.txt,76,First of all_comma_ note that the top most part of an FAS expression is related to tile top most part of the GPSG structure_comma_ and that the leaves of a FAS expression usually correspond to GPSG lexicon entries.
E91-1020.txt,77,As a consequence_comma_ the order the FAS expression is traversed determines the order in which the structure building subtask is performed.
E91-1020.txt,78,Why should then_comma_ in the case of FAS_comma_ the traversal occur top down
E91-1020.txt,79,The answer is motivated by the distribution of information in FAS expressions.
E91-1020.txt,80,In order to apply a certain ID rule deterministically_comma_ information from distant portions of tim FAS expression may be needed.
E91-1020.txt,81,For instance_comma_ the FAS specification them 1 _comma_ which is part of one of the daughters of clause in Figure la_comma_ is interpreted as requiring topicalization of a syntactic constituent under the condition that a declarative sentence is being generated.
E91-1020.txt,83,Two possible methods for collecting this infornration present themselves.
E91-1020.txt,85,In that case_comma_ all the information needed is present_comma_ and the traversal of the FAS expression could occur bottom up as well as top down. Unfortunately the required size of the pattern is not always known in advance because the FAS syntax might allow an arbitrary number of recursively defined local trees to intervene.
E91-1020.txt,86,The second method which was eventually adopted requires the patterns to cover not more than one local FAS tree.
E91-1020.txt,87,In order to gather information that is locally missing_comma_ an auxiliary storage is needed.
E91-1020.txt,88,If_comma_ for instance_comma_ the illocution is matched_comma_ information about whether or not a declarative sentence is being generated is stored.
E91-1020.txt,89,Later on_comma_ them 1 is encountered.
E91-1020.txt,90,Now_comma_ the ID rule for to6These are not shown for the constituents of NPs.
E91-1020.txt,91,ZNote the different use of the symbol  here it denotes the category valued feature slash . e Square brackets are used here to indicate tree stnicture. picalization 3 is triggered iff declarative can be retrieved from the storage.
E91-1020.txt,92,If the necessary information is not available yet_comma_ one must accept either a delay of a mapping or backtracking.
E91-1020.txt,93,With a top down traversal of FAS expressions_comma_ however_comma_ such cases are sufficiently restricted to ensure efficiency.
E91-1020.txt,94,Note that a bottom up traversal or a mixed strategy could be more efficient if the distribution of information in the SRL were different.
E91-1020.txt,95,The problems observed with top down generatots using an integrated semantics cannot occur in the separate semantics approach.
E91-1020.txt,96,Expansion of grammar rules can be controlled by the semantic representation if each rule application is explicitly triggered.
E91-1020.txt,97,Situations causing an infinite expansion due to an uninstantiated semantics as with topdown expansion using the rule 2 cannot arise at all since the separate semantics is fully specified.
E91-1020.txt,98,Let us now discuss why feature instantiation should be a bottom up process.
E91-1020.txt,99,The FIPs apply to tim mother and or a subset of daughters in a local tree.
E91-1020.txt,100,In general_comma_ tile more these categories are instantiated the less likely the l lPs will have to choose between alternative instantiations_comma_ which would be a source for backtracking.
E91-1020.txt,101,A top down strategy would meet a more completely instantiated mother_comma_ but still underspecified daughters.
E91-1020.txt,102,With a bottom up strategy_comma_ howew r_comma_ only tile mother would be underspecified.
E91-1020.txt,103,For instance_comma_ consider the GPSG account of parasitic gaps_comma_ which are handled by the Foot Feature Principle.
E91-1020.txt,105,While this is easy to handle for a bottom up process_comma_ a top down strategy would have to guess at which daughters to instantiate a slash value.
E91-1020.txt,106,Pattern action rules.
E91-1020.txt,107,A PA rule is a production rule with a pattern for local FAS trees as its left hand side and two sets of actions as its right hand side.
E91-1020.txt,108,The information gathering aclions IGAs maintain the auxiliary storage.
E91-1020.txt,109,The structure building actions SBAs generate GPSG trees.
E91-1020.txt,110,Either one of these sets may be empty.
E91-1020.txt,111,In order to minimize tim power of PA rules_comma_ the inventory of IGAs and SBAs is restricted.
E91-1020.txt,112,There are only lthree 1GAs for storing information into and removing from the auxiliary storage.
E91-1020.txt,113,The auxiliary storage is a two dimensional array of a fixed size.
E91-1020.txt,114,It may contain atomic values for a set of features predetermined by the PA rule writer as well as a single GPSG category.
E91-1020.txt,115,There are only five SBAs for different kinds of mapping_comma_ three of which are explained below cf.
E91-1020.txt,118,To illustrate this let us return to the topicalization example.
E91-1020.txt,119,The responsible PAl rules are shown in Figure 2.
E91-1020.txt,120,The pattern of the first one naatches any local FAS tree whose mbther is a term them 1 .
E91-1020.txt,122,The SBA set is empty.
E91-1020.txt,123,The second PA rule matches any local FAS tree whose first daughter is a dcfinite determiner with plural number followed by zcro or more daughters.
E91-1020.txt,124,Note that both patterns match the same local tree of the FAS expression in Figure la.
E91-1020.txt,125,There is only one IGA_comma_ which adds the number information to the stored GPSG category.
E91-1020.txt,126,The single SBA_comma_ call_id_comma_ states that a local GPSG tree is generated by virtue of the ID rule indicated and added to the OSS.
E91-1020.txt,128,Fronting of the NP is achieved in accordance with the FIPs and LP statements.
E91-1020.txt,129,Three kinds of PA rules should be distinguished according to the effects of their SBAS.
E91-1020.txt,130,Figure 2 shows two of tl_comma_em the first one doesn t create structure at_comma_ all while the second one transduces a FAS local tree into a GPSG loi ai tree.
E91-1020.txt,131,A third type of rules generates GPSG structure out of FAS feature specifications.
E91-1020.txt,134,This latter subtree is generated by a PA rule whose pattern rnatches the same FAS tree as the previous one.
E91-1020.txt,135,We shall return to this problem in the following section.
E91-1020.txt,136,Controlling the ntapl ing procc. dure..
E91-1020.txt,137,First of all note that PA rules can comrnunicate with each other only indirectly_comma_ i.e. by modifying the content of the auxiliary storage or by successfully applying an SBA_comma_ thereby creating a situation in which another rule becomes applicable or cannot be applied anymore .
E91-1020.txt,138,PA rules do not contain any control knowledge.
E91-1020.txt,139,A local FAS tree is completely verbalized iff a maximum number rt 1 of applicable PA rules are successful.
E91-1020.txt,140,A PA rule is applicable to a local FAS tree t iff its pattern unifies with t.
E91-1020.txt,141,An applicable PA rule is successful iff all elements of IGA can be executed and an SBA if present is successful.
E91-1020.txt,142,An SBA is successful iff a syntactic subtree can be attached to the OSS as described above.
E91-1020.txt,143,Since the set of PA rules is not commutative_comma_ the order of application is crucial in order to ensure that 72 is maximal.
E91-1020.txt,144,Due to the restricted power of the PA rules possible conflicts can be detected and resolved a priori.
E91-1020.txt,145,A conflict arises if more than one pattern matches a given FAS tree.
E91-1020.txt,146,All FAS trees matched by more than one pattern can be identified with help of the FAS grammar.
E91-1020.txt,147,The respective PA rules are members of the same conflict set.
E91-1020.txt,148,The elements of a conflict set can be partially ordered by virtue of precedence rules operating on pairs of PA rules.
E91-1020.txt,149,For instance_comma_ the conflict regarding the perfect auxiliary is resolved by making a precedence rule check the ID rules that would be invoked by the respective SBAs.
E91-1020.txt,150,If the mother of the second one can be unified with a daughter of the first one and not vice versa_comma_ then the first PA rule must be applied before the second one.
E91-1020.txt,151,Thus a PA rule with an SBA invoking ID rule 4 will apply before another one wifll an SBA invoking ID rule 5 .
E91-1020.txt,152,Note that_comma_ in this example_comma_ the number of successful PA rules would not be maximal if the order of application was the other way around since the SBA invoking ID rule 4 would not succeed anymore.
E91-1020.txt,153,The control regime described above guarantees termination_comma_ completeness and coherence in the following way The traversal of a FAS expression terminates since there is only a finite number of local trees to be investigated_comma_ and for each of them a I17 finite number of PA rules is applicable.
E91-1020.txt,154,The aSS generated is complete because all local FAS trees are processed and for each a maximum rmmber of PA rules is successful.
E91-1020.txt,155,It is coherent because 1 no PA rule may be applied whose pattern is not matched by the FAS expression and 2 all attachment points nmst be expanded.
E91-1020.txt,156,CONCLUSION The adaptation of a GPSG based generator to an MT system using FAS as its SRL was described as an instance of the separate semantics approach to surface generation.
E91-1020.txt,157,In this instance_comma_ the OSS is most efficiently built top down whereas feature instmltiation is performed bottom up.
E91-1020.txt,158,The mapping based on PA rules has proved to be efficient in practice.
E91-1020.txt,159,There are only a few cases where backtracking is required most often the local FAS tree being verbalized allows together with the contents of the auxiliary storage and the current set of attachment points for a deterministic choice of grammar rules.
E91-1020.txt,160,The generator has been fully implemented and tested with middle sized fragments of English and German.
E91-1020.txt,161,It is part of the Berlin MT system and runs on both an IBM 4381 under VM SP in Waterloo Core Prolog and a PC XT AT in Arity Prolog.
E91-1020.txt,162,Compared to algorithms based on an integrated semantics the separate semantics approach pursued here is promising if the generator has to be adapted to the SRL of some application system.
E91-1020.txt,163,Adaptation then consists in modifying the set of PA rules rather than in rewriting the grammar. .
E91-1006.txt,1,this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented. The algorithm takes advantage of a peculiar characteristic of Lexicalized TAGs_comma_ i.e. that each elementary tree is associated with a lexical item_comma_ called its anchor.
E91-1006.txt,2,The algorithm employs a mixed strategy it works bottom up from the lexical anchors and then expands partial analyses making top down predictions.
E91-1006.txt,3,Even if such an algorithm does not improve tim worst case time bounds of already known TAGs parsing methods_comma_ it could be relevant from the perspective of linguistic information processing_comma_ because it employs lexical information in a more direct way.
E91-1006.txt,5,Tree Adjoining Grammars TAGs are a formalism for expressing grammatical knowledge that extends the domain of locality of context free grammars CFGs .
E91-1006.txt,6,TAGs are tree rewriting systems specified by a finite set of elementary trees for a detailed description of TAGs_comma_ see Joshi_comma_ 1985 .
E91-1006.txt,7,TAGs can cope with various kinds of unbounded dependencies in a direct way because of their extended domain of locality in fact_comma_ the elementary trees of TAGs are the appropriate domains for characterizing such dependencies.
E91-1006.txt,8,In Kroch and Joshi_comma_ 1985 a detailed discussion of the linguistic relevance of TAGs can be found.
E91-1006.txt,9,Lexicalized Tree Adjoining Grammars Schabes et al._comma_ 1988 are a refinement of TAGs such that each elementary tree is associated with a lexieal item_comma_ called the anchor of the tree.
E91-1006.txt,10,Therefore_comma_ Lexicalized TAGs conform to a common tendency in modem theories of grammar_comma_ namely the attempt to embed grammatical information within lexical items.
E91-1006.txt,11,Notably_comma_ the association between elementary trees and anchors improves also parsing performance_comma_ as will be discussed below.
E91-1006.txt,12,Various parsing algorithms for TAGs have been proposed in the literature the worst case time complexity varies from O n 4 log n Harbusch_comma_ 1990 to O n 6 Vijay Shanker and Joshi_comma_ 1985_comma_ Lang_comma_ 1990_comma_ Schabes_comma_ 1990 and O n 9 Schabes and Joshi_comma_ 1988 .
E91-1006.txt,13,Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova Italy .
E91-1006.txt,14,We would like to thank Yves Schabes for his valuable comments.
E91-1006.txt,15,We would also like to thank Anne Abeill6.
E91-1006.txt,16,All errors are of course our own.
E91-1006.txt,17,As for Lexicalized TAGs_comma_ in Schabes et al._comma_ 1988 a two step algorithm has been presented during the first step the trees corresponding to the input string are selected and in the second step the input string is parsed with respect to this set of trees.
E91-1006.txt,18,Another paper by Schabes and Joshi 1989 shows how parsing strategies can take advantage of lexicalization in order to improve parsers performance.
E91-1006.txt,19,Two major advantages have been discussed in the cited work grammar filtering the parser can use only a subset of the entire grammar and bottom up information further constraints are imposed on the way trees can be combined .
E91-1006.txt,20,Given these premises and starting from an already known method for bidirectional CF language recognition Satta and Stock_comma_ 1989 _comma_ it seems quite natural to propose an anchor driven bidirectional parser for Lexicalized TAGs that tries to make more direct use of the information contained within the anchors.
E91-1006.txt,21,The algorithm employs a mixed strategy it works bottom up from the lexical anchors and then expands partial analyses making top down predictions.
E91-1006.txt,23,Overview of the Algorithm The algorithm that will be presented is a recognizer for Tree Adjoining Languages a parser can be obtained from such a recognizer by additional processing see final section .
E91-1006.txt,24,As an .
E91-1050.txt,1,is often the appropriate method for expressing relations between representations in the form of feature structures however_comma_ there are circumstances in which a different approach is desirable. A declarative formalism is presented which permits direct mappings of one feature structure into another_comma_ and illustrative examples are given of its application to areas of current interest.
E91-1050.txt,3,Benefits arising from the adoption of unification as a tool in computational linguistics are well known a declarative_comma_ monotonic method of combining partial information expressed in data structures convenient for linguistic applications permits the writing of sensible grammars that can be made independent from processing mechanisms_comma_ and a growing familiarity_comma_ in both theoretical and computational circles_comma_ with the techniques of unification fosters fruitful interchange of ideas and experiences.
E91-1050.txt,4,There are_comma_ however_comma_ occasions when unification alone is not an appropriate tool.
E91-1050.txt,5,In essence_comma_ unification is a ternary relation in which two structures_comma_ when merged_comma_ form a third it is less attractive in circumstances where the relation to be expressed is binary when one would like to manipulate a single feature structure FS _comma_ perhaps simulating the direct transformation of one FS into another.
E91-1050.txt,6,1 The present paper introduces a declarative formalism intended for the expression of such relations_comma_ and shows how it may be applied to some areas of current interest.
E91-1050.txt,7,The formalism in question is based upon a notion of transfer rule informally_comma_ a set of such rules may be considered as characterizing We are indebted to Jacques Jayez for comments on an earlier draft of this paper.
E91-1050.txt,8,1 Clearly there is a sense in which such relations can be viewed as ternary T FI_comma_ R_comma_ F2 _comma_ where 171 and 172 are 17Ss_comma_ and R is the rule set which relates them. a binary relation over a set of feature structures_comma_ the properties of that relation depending on the content of the particular rule set in use.
E91-1050.txt,9,Transfer rules associate the analysis of one FS with the synthesis of another they may be thought of as a specialized variety of patternmatching rule.
E91-1050.txt,10,They are local in nature_comma_ and permit the recursive analysis and synthesis of complex structures according to patterns specified in a format closely related to that widely employed in unification based computational linguistics.
E91-1050.txt,11,Indeed_comma_ the interpretation of transfer rules involves unification_comma_ albeit in a context which restricts it to the role of a structure building operation.
E91-1050.txt,12,2 In the remainder of this paper we provide a brief specification of the transfer rule formalism_comma_ discuss its interpretation_comma_ outline two alternative rule application regimes_comma_ and illustrate the use of the formalism in the areas of machine translation and reduction of FSs to canonical form.
E91-1050.txt,13,We conclude with an overview of continuing strands of research.
E91-1050.txt,15,Rule Format and Interpretation 2.1.
E91-1050.txt,16,General Remarks A transfer rule consists of four parts i a role name 3 ii a set of constraint equations describing a FS iii a set of constraint equations describing a FS 4 2 The rule formalism is thus monotonic_comma_ being unable to effect changes in the input representation_comma_ and constmcting the output by means of unification.
E91-1050.txt,17,3 The rule name plays no part in the interpretation of roles_comma_ but provides a convenient reference for tracing their ordering and application.
E91-1050.txt,18,4 The equations in each of ii and iii must he uniquely rooted.
E91-1050.txt,19,The current implementation disallows disjunction in the equation sets for this reason.
E91-1050.txt,20,287 iv a possibly empty set of transfer correspondence statements equations describing transfer correspondences that must hold between variable bindings established in ii and iii .
E91-1050.txt,21,A transfer rule relates the two FSs it describes either directly or indirectly_comma_ via the rule s transfer correspondence statements in order for the relation to hold between the source and destination FS_comma_ it must hold between the FSs to which any transfer variables are bound.
E91-1033.txt,1,paper presents an approach for achieving conciseness in generating explanations_comma_ which is clone by exploiting formal reconstructions of aspects of the Gricean principle of relevance to simulate conversational implicature. By applying contextually motivated inference rules in an anticipation feed back loop_comma_ a set of propositions explicitly representing an explanation s content is reduced to a subset which_comma_ in the actual context_comma_ can still be considered to convey the message adequately.
E91-1033.txt,3,The task of providing informative natural language explanations for illustrating the results produced by decision support systems has been gtven increased attention recently.
E91-1033.txt,6,Whereas these approaches attack various issues important for the generation of natural language explanations_comma_ none of them has focussed on the conciseness of explanations in a broader context.
E91-1033.txt,7,Aiming at the production of natural and concise texts_comma_ we have concentrated our efforts on presenting different types of knowledge and their interrelations because this kind of information is typically relevant for explanations.
E91-1033.txt,9,This system is able to appropriately assign a set of employees to a set of rooms in offices_comma_ which is guided by a number of constraints expressing various kinds of the persons requirements.
E91-1033.txt,11,REPRESENTING DOMAIN AND INFERENCE KNOWLEDGE Terminological knowledge is represented in a sorted type hierarchy_comma_ which identifies classes of entities and their relevant subsorts_comma_ as well as relations that may hold between two types of entities.
E91-1033.txt,12,Moreover_comma_ assertions which refer to the referential level must be consistent with the ontology provided by these taxonomic definitions.
E91-1033.txt,13,Inferential knowledge is represented in terms of rules which express constraints to be satisfied in the problem solving process.
E91-1033.txt,15,The quantifiers used in our system are all_comma_ some_comma_ and unique.
E91-1033.txt,16,The predications contained are restricted to be one or two place predications corresponding to class and relation definitions introduced in the taxonomic hierarchy.
E91-1033.txt,17,In addition_comma_ the recta predicate implies is contained in the innermost predication of a rule_comma_ which constitutes the rule s conclusion see Figure 1 .
E91-1033.txt,19,The task is then to reduce this set of propositions as much as possible by exploiting a given context so that the subset obtained still conveys the same information in a partially implicit and more concise form_comma_ but without leading to wrong implications.
E91-1033.txt,20,The intuition behind this mechanism is as follows After having asked a certain explanation seeking question the questioner mentally attempts to build links between entities referred to in the question and facts or rules provided as explanation .
E91-1033.txt,21,Hence_comma_ if a regularity valid for a class of entities is uttered_comma_ the person attempts to find out which of the entities mentioned previously this rule is thought to apply to.
E91-1033.txt,22,i i _comma_ some r and room r in r g  implies single room r  Figure 1 Inference rule I Rule 1 1 191 3.
E91-1033.txt,23,EXPRESSING CONVERSATIONAL IMPLICATURE The reduction of the set of propositions that originally represents the explanation is performed by exploiting a set of rules which are contextually motivated and express conversational implicature.
E91-1033.txt,24,These rules represent formal reconstructions of aspects of the Gricean principle of relevance.
E91-1033.txt,25,They have the same format as the rules which constitute the system s inferential knowledge_comma_ but_comma_ in addition_comma_ they contain metapredications referring to contextual_comma_ conversational_comma_ or processing states associated with the individuals referred to see Figure 2 below .
E91-1033.txt,26,The rules expressing conversational implicature allow variables to denote propositions_comma_ though in an extremely limited sense only a variable x denoting a proposition must always be restricted by the predication newinfo x so that the evaluation process can rely on a definite set of entities when generating legal instances of x.
E91-1033.txt,27,We have defined three rules that constitute a fundamental repertoire for exploiting conversational implicature see Figure 3 .
E91-1033.txt,28,They express contextually motivated inferences of a fact from another one_comma_ of a fact from an inference rule_comma_ and the relevance of an inference rule justified by a fact.
E91-1033.txt,29,Moreover_comma_ logical substitution is applied to those domain inference rules which become bound to variables of a contextually motivated inference rule at some processing stage.
E91-1033.txt,30,The first rule_comma_ C Rule 1_comma_ refers to two sets of entities el and e2_comma_ which have been both addressed expressed by topic in the question and share the most general superclass topclass .
E91-1007.txt,1,notion of a Horn extended feature structure HoXF is introduced_comma_ which is a feature structure constrained so that its only allowable extensions are those satisfying some set of llorn clauses in featureterm logic_comma_ lloXF s greatly generalize ordinary feature structures in admitting explicit representation of negative and implicational constraints. In contradistinction to the general case in which arbitrary logical constraints are allowed for which the best known algorithms are exponential _comma_ there is a highly tractable algorithm for the unification of HoXF s.
E91-1007.txt,3,PRELIMINARY CONCEPTS 1.1 Unification based grammar formalisms Unification based grammar formalisms constitute a cornerstone of many of the most important approaches to natural language understanding Shieber_comma_ 1986 _comma_ Colban_comma_ 1988 _comma_ Fenstad etal._comma_ 1989 .
E91-1007.txt,4,The basic idea is that the parser generates a number of partial representations of the total parse_comma_ which are subsequently checked for consistency and combined by a second process known as a unifier.
E91-1007.txt,5,A common form of representation for the partial representations is that of cature structures_comma_ which are record like data structures which are allowed to grow in three distinct ways by adding missing values_comma_ by adding attributes_comma_ and by coalescing existing attributes forcing them to be the same .
E91-1007.txt,6,The last operation may lead to cyclic structures_comma_ which we do not exclude.
E91-1007.txt,7,If the feature structure Sz is an extension of 1 i.e._comma_ 1 grows into 2 by application of some sequence of the above rules _comma_ we write 1 E 2 and say that St subsumes 2.
E91-1007.txt,8,Intuitively_comma_ if Sl E 2_comma_ S contains more information than does Sl.
E91-1007.txt,9,It is easy to show that E is a partial order on the class of all feature structures.
E91-1007.txt,10,Each feature structure represents partial information generated during the parse.
E91-1007.txt,11,To obtain the total picture_comma_ these partial components must be combined tThe research reported herein was performed while the author was visiting the COSMOS Computational Linguistics Group of the Mathematics Department at the University of Oslo. lie wishes to thank Jens Erik Fenstad and the members of that group for providing a stimulating research environment.
E91-1007.txt,12,Particular thanks are due Tore Langholm for many invaluable discussions regarding the i_comma__comma_terplay of logic_comma_ feature structures_comma_ and unification. into one consistent piece of knowledge.
E91-1007.txt,13,The formal process of unification is precisely this operation of combination.
E91-1007.txt,14,The most general unifier mgu 1 LI 2 of feature structures Sj and Sa is the least feature structure under E which is larger than both Sl and 2.
E91-1007.txt,15,Such an mgu exists if and only if 1 and 2 are consistent that is_comma_ if and only if they subsume a common feature structure.
E91-1007.txt,16,1.2 Unification algorithms and this paper While the idea of a most general unifier is a pleasing theoretical notion_comma_ its real utility rest with the fact that there are efficient algorithms for its computation.
E91-1007.txt,17,The fastest known algorithm_comma_ identified by Ait Kaci 1984 _comma_ runs in time which is_comma_ for all practical purposes_comma_ linear in the size of the input i.e._comma_ the combined sizes of the structures to be unified .
E91-1007.txt,18,In proposing any extension to the basic framework_comma_ a primary consideration must be the complexity of the ensuing unification algorithm.
E91-1007.txt,19,The principal contribution of the research summarized here is to provide an extension of ordinary feature structures_comma_ admitting negation and limited disjunction_comma_ while at the same time continuing to admit a provably efficient unification algorithm.
E91-1007.txt,20,Due to space limitations_comma_ we must omit substantial background material from this paper.
E91-1007.txt,21,Specifically_comma_ we assume that the reader is familiar with the notation and definitions surrounding feature structures Shieber_comma_ 1986 Fenstad et al._comma_ 1989 _comma_ as well as the traditional unification algorithm Colban_comma_ 1990 .
E91-1007.txt,22,We also have been forced to omit much detail from the description and verification of our algorithm.
E91-1007.txt,23,A full report on this work will be available in the near future.
E91-1007.txt,25,UNIFICATION IN THE PRESENCE OF CONSTRAINTS 2.1 Constraints on feature structures Not every feature structure is a possibility as the ultimate output of the parsing mechanism.
E91-1007.txt,26,Typically_comma_ there are constraints which must be observed.
E91-1007.txt,27,One way of ensuring this sort of consistency is to build the checks right into the grammar_comma_ so that the feature structures generated are always legitimate substructures of tile final output.
E91-1007.txt,28,The CLG formalism Dumas and Vatlie_comma_ 1989 is an example of such a philosophy. n many ways_comma_ this is an attractive option because it provides a 33 unified context for expressing all aspects of the grammar. liowever_comma_ this approach has the disadvantage that it limits the use of independent parsing subalgorithms whose results are subsequently unified_comma_ since the consistency checks nmst be performed before the feature structures are presented to the unifier.
E91-1007.txt,29,Therefore_comma_ to maintain such independence_comma_ it would be a distinct advantage if some of the constraint checking could be relegated to the unification process.
E91-1007.txt,30,To establish a formal framework in which this is possible_comma_ we must start by extending our notion of a feature structure.
E91-1007.txt,31,Following the ideas of Moshier and Rounds 1987 and Langholm 1989 _comma_ we define an extended fcature structure to be a pair N_comma_ K in which C is a set of feature structures and N is the least element of C under the ordering _.
E91-1007.txt,32,Titus_comma_ by definition_comma_ K has a least element_comma_ and K determines N. Think of N a.s the current feature structure_comma_ and C as the set of all structures into which N is allowed to grow.
E91-1007.txt,33,We define N _comma_K t C N _comma_ C to mean precisely that K C_ C .
E91-1007.txt,34,In other words_comma_ the set of all structures which N can grow into is a subset of those which N can grow into.
E91-1007.txt,35,It follows necessarily that N _ N2 in this case. Note that if we identify the ordinary feature structure N with the pair N_comma_ IM I N M _comma_ we precisely recapture ordinary subsumption.
E91-1007.txt,36,Finally_comma_ the notion of unification associated with _ is given by Mr_comma_ Ct LI M _comma_ C  M_comma_ 17 C2 if C n c2 has a least element M undefined oOmrwise.
E91-1007.txt,37,2.2 Logical feature structures with constraints To operate on pairs of the form N C algorithmically_comma_ we must have in place an appropriate representation for the set g .
E91-1007.txt,38,There are many possible choices ours is to let it be the set of all structures satisfying a set of sentences iu a particular logic.
E91-1007.txt,39,The logic which we use is a simple modification of the language of Rounds and Ka.sper 1986 see also Kasper and Rounds_comma_ 1990 admitting negation but only binary path equivalences.
E91-1007.txt,40,Specifically_comma_ an atomic feature term is one of the following.
E91-1007.txt,41,FormltJa T  a _comma_ f Semantics The identically true term.
E91-1007.txt,42,The identically false term.
E91-1007.txt,43,The path nesting of attributes cz exists and terminates with label a.
E91-1007.txt,44,The paths cr and have a common end point coalesced end points .
E91-1007.txt,45,In a a _comma_ the label a may be T_comma_ denoting a missing value.
E91-1007.txt,46,The notation a  is borrowed from Langholm_comma_ 1989 _comma_ and has the same semantics as _comma__comma_B of Rounds and Kasper_comma_ 1986 .
E91-1007.txt,47,A general featur term is b.ilt up from atomic feature terms using the connectives _comma_ v_comma_ and ._comma_ with the usual semantics.
E91-1007.txt,48,In particular_comma_ the negation we use is the classical notion a structure satisfes _comma_ 0 if and only if it does not satisfy .
E91-1007.txt,49,For any set of feature terms_comma_ Mod denotes the set of all feature structures for which each E r is true.
E91-1007.txt,50,For a formal definition of satisfaction_comma_ we refer the reader to the above cited references.
E91-1007.txt,51,Intuitively_comma_ any set of terms which defines a consistent rooted_comma_ directed graph is satisfiable.
E91-1007.txt,52,Ilowever_comma_ let us specifically remark that only nodes with no outgoing edges may have labels other than T_comma_ that labels other than T may occur at at most one end point_comma_ that no two outgoing edges from the same node may have the same label_comma_ and that any term of the form a .L is equivalent to _L_comma_ and so inconsistent.
E91-1007.txt,53,Now we define a logical extended feature structure LoXF to be an extended feature structure iN_comma_ K in which K Mod for some consistent finite set of feature terms.
E91-1007.txt,54,In particular_comma_ Mod must have a least model.
E91-1007.txt,55,We also denote this pair by Y  g._comma_Mod b .
E91-1007.txt,56,Now Y b_comma_ E__comma_  2 reduces to Mod C_ Mod 4_comma_a _comma_ and  u 2 undefined if Mod a U q has a least element under E otherwise.
E91-1007.txt,57,2.3 Remark on negation A full discussign of the nature of negation in LoXF s is complex_comma_ and will be the focus of a separate paper.
E91-1007.txt,58,IIowever_comma_ because this topic has received a great deal of attention Moshier and Rounds_comma_ 1987 _comma_ Langholm_comma_ 1989 _comma_ Dawar and Vijay Shanker_comma_ 1990 _comma_ we feel it essential to remark here that  does not have the classical negation semantics which can be determined by looking solely at the least element.
E91-1007.txt,59,Indeed_comma_ the appropriate definition is that .  satisfies 7 precisely when no member of Mod satisfies in other words_comma_ the structure N. is not allowed to be extended to satisfy o.
E91-1007.txt,60,2.4 Unification algorithms for logical extended feature structures In view of the definition immediately above_comma_ it is easy to see that that any unification algorithm for LoXF s must solve the following two problems in the course of attempting to unify  i and  2 . ul It must decide whether or not i U q 2 is consistent i.e._comma_ whether or not there is a feature structure satisfying all sentences of both i and cb2.
E91-1007.txt,61,u2 In case that 4 I U 2 is satisfiable_comma_ it must also determine if there is a least model_comma_ and if so_comma_ identify it.
E91-1007.txt,62,Now it is well known that ul is an NP complete problem_comma_ even if we disallow negation and path equivalence Rounds and Kasper_comma_ 1986_comma_ Thin. 4 .
E91-1007.txt,63,Therefore_comma_ barring the eventuality that P NP_comma_ we cannot expect to allow I and 2 to be arbitrary finite sets of feature terms and still have a tractable algorithm for unification.
E91-1007.txt,64,One solution_comma_ which has been taken by a number of authors_comma_ such as Kasper 1989 and Eisele and D6rre 1988 _comma_ is to devise clever algorithms which apply to the general case and appear empirically to work well on typical inputs_comma_ but still are provably 34 exponential in the worst case.
E91-1007.txt,65,While such work is undeniably of great value_comma_ we here propose a companion strategy namely_comma_ we restrict attention to pairs N_comma_ such that the very nature of guarantees a tractable algorithm.
E91-1007.txt,67,HORN FEATURE LOGIC In the field of mathematicM logic in general_comma_ and in the computational logic relevant to computer science in particular_comma_ Horn clauses play a very special r61e Makowsky_comma_ 1987 .
E91-1007.txt,68,Indeed_comma_ they form the basis for the programming language Prolog Sterling and Shapiro_comma_ 1986 and the database language Datalog Ceri et ai._comma_ 1989 .
E91-1007.txt,69,This is due to the fact that while they possess substantial representational power_comma_ tractable inference algorithms are well known.
E91-1007.txt,70,It is perhaps the main thesis of this work that the utility of llorn clauses carries over to computational linguistics as well.
E91-1007.txt,71,3.1 Horn feature clauses A feature literal is either an atomic feature term e.g._comma_  a _comma_  . _comma_ or _L or its negation.
E91-1007.txt,72,A feature clause is a finite disjunction lvt v...vl_comma_n of feature literals.
E91-1007.txt,73,A feature clause is florn if at most one of the ti s is not negated.
E91-1007.txt,74,A Horn extended feature structure lloXF is a LoXF 4_comma_ such that is a finite set of llorn feature clauses.
E91-1007.txt,75,3.2 A taxonomy of Horn feature clauses Before moving on to a presentation of algorithms on tIoXF s_comma_ it is appropriate to provide a brief sketch of thc utility and limits of restricting our attention to collections of lIorn clauses_comma_ hnplication here is classical in the case of ordinary propositional logic_comma_ we use the notation et r ... am p to denote the clause O l v 0r2v... V O rnVp.
E91-1007.txt,76,Horn feature clauses may then be thought of as falling into one of the following four categories.
E91-1007.txt,77,lIl A clause of the form a_comma_ consisting of a single positive literal_comma_ is just a fact.
E91-1007.txt,78,lI2 A clause of the form e_comma_ consisting of a single negative literal_comma_ is a negated fact.
E91-1007.txt,79,In terms of lloXF s_comma_ if a E _comma_ this means that within  _comma_ no extension of N in which a is true is permitted.
E91-1007.txt,80,As a concrete example_comma_ a constraint stating that a subject may not have an attribute named tense would be of this form.
E91-1007.txt,81,H3 A clause of the form ai 2 ... am p is called a rule or an implication.
E91-1007.txt,82,Numerous examples of the utility of implication in linguistics are identified in Wedekind_comma_ 1990_comma_ Sec. 1.3 .
E91-1007.txt,83,Kasper s conditional descriptions Kasper_comma_ 1988 are also a form of implication.
E91-1007.txt,84,More concretely_comma_ the requirement that a transitive verb requires a direct object is easily expressed in this form.
E91-1007.txt,85,114 A clause of the form al a2 ... am 1 is called a compound negation.
E91-1007.txt,86,The formalization of the constraint that a verb cannot be both intransitive and take a direct object is an example of the use of such a clause_comma_ The type of knowledge which is not recapturable using llorn feature logic is positive disjunction i.e._comma_ formulas of tim form rlva2_comma_ with both a.l and aa feature terms.
E91-1007.txt,87,Of course_comma_ this has nothing in particular to do with feature term logic_comma_ but is well known limitation of Itorn clauses in general.
E91-1007.txt,88,However_comma_ in accepting this limitation_comma_ we also obtain many key properties_comma_ including tractable inference and the following important property of genericity.
E91-1007.txt,89,3.3 Totally generic LoXF a Let now be any finite set of feature terms.
E91-1007.txt,90,We say that is totally generic if_comma_ for any set q of facts see H1 above _comma_ if Mod O 0 is nonempty then it contains a least element under E.
E91-1007.txt,91,Intuitively_comma_ if we use to define the LoXF  _comma_ total genericity says that however we extend the base feature structure N consistently with O _comma_ we will continue to have a LoXF.
E91-1007.txt,92,Remarkably_comma_ we have the following.
E91-1007.txt,93,3.4 Theorem A set of feature terms p is totally generic if and only if it is equivalent to a set of Horn feature clauses.
E91-1007.txt,94,Proof outline This result is essentially a translation of Makowsky_comma_ 1987_comma_ Thm. 1.9 to the logic Of feature structures.
E91-1007.txt,95,In words_comma_ it says that if and only if we work with lloXF s_comma_ condition u2 on page 4 becomes superfluous except for explicitly identifying the least model. t3 4.
E91-1007.txt,96,THE EXTENDED UNIFICATION ALGORITHM It has been shown by Dowling and Gallier 1984 that satisfiability for finite sets of propositional IIorn formulas can be tested in time linear in the length of the formulas.
E91-1007.txt,97,Their algorithms can easily be modified to deliver the least model as well.
E91-1007.txt,98,Since unification of HoXF s is essentially testing for satisfiability plus identifying the least model see ul u 2 on the previous page _comma_ a natural approach would be to adapt one of their algorithms.
E91-1007.txt,99,Essentially_comma_ this is what we do.
E91-1007.txt,101,However_comma_ the adaptation is not trivial_comma_ because feature term logic is more expressive than propositional logic.
E91-1007.txt,102,In particular_comma_ feature term logic contains countably many tautologies which have no correlates in ordinary propositional logic.
E91-1007.txt,103,The main contribution of our algorithm is to implicitly recapture the full semantics of these tautologies while keeping the time complexity within reasonable bounds.
E91-1007.txt,104,Due to space limitations_comma_ we cannot present tile full formality of the rather complex data structures.
E91-1007.txt,105,Rather_comma_ to highlight tile key features_comma_ we step through an annotated example.
E91-1007.txt,106,We focus only upon the special problems inherent in the extension to feature term logic_comma_ and assume familiarity with the forward chaining algorithm in Dowling and Gallier_comma_ 1984 and the graph unification algorithm in Colban_comma_ 1990 .
E91-1007.txt,107,4.1 An example theory and extended feature graplm The set E contains the following eight llorn feature clauses. _comma_ AA a . _comma_ n a .
E91-1007.txt,108,35  AA a B a v CCDDG t .  A T C T _comma_ ABDDG T . s AA.X B ABDDG T  ABDDEF T .  A13DD T B T _comma_ CCD x ABD . _comma_ CCDD x ABDD  AC T . s ACD T v ACC t .
E91-1007.txt,109,Just as we may represent a set of atomic feature terms with a feature graph_comma_ so too may we represent_comma_ in part_comma_ a set of llorn feature clauses with an extended feature graph.
E91-1007.txt,110,Shown in Figure I below is the initial extended feature graph for the set _comma_ representing the state of inference before any deductions are made. a t a  i i i  .
E91-1007.txt,111,D _comma_ _comma__comma_ .
E91-1007.txt,112,c D D a__ i . I_comma_ i_comma_ i t Figure 1 Initial extended feature structure for .
E91-1007.txt,113,Every path and every node label which occurs in some literal of E is represented.
E91-1007.txt,114,The labels of all edges_comma_ as well as all non T node labels_comma_ are underscored_comma_ denoting that they are virtual_comma_ which means that they are only possibilities for the minimal model_comma_ and not yet actually part of it.
E91-1007.txt,115,The root node is denoted by _comma_ and nodes with value T are denoted with a ..
E91-1007.txt,116,Note that paths with common virtual end labels e.g._comma_ AA and B are not coalesced virtual nodes and edges are never unified.
E91-1007.txt,117,As a result_comma_ the predecessors along any directed path of any actual node or edge is itself actual.
E91-1007.txt,118,As inferences are made_comma_ edges and nodes become actual depicted by deleting underscores _comma_ and actual nodes with common labels are ultimately coalesced.
E91-1007.txt,119,The final extended feature graph is shown in Figure 2 below.
E91-1007.txt,120,For easier visibility_comma_ actual edges are also highlighted with heavier lines.
E91-1007.txt,121,A B D a 4    C   D G   i_comma_ t Figure 2 Final extended feature structure for . ..
E91-1007.txt,122,If we delete the remaining virtual nodes and edges_comma_ we obtain the graphical representation of the least model of .
E91-1007.txt,123,4.2 Computing the minimal model of the example Now let us consider the process of actually obtaining the structure of Figure 2 from E.
E91-1007.txt,124,In the propositional forward chaining approach_comma_ we start by pooling the facts that we know in this ease 1_comma_ 2 .
E91-1007.txt,125,We then look for rules whose left hand sides have been satisfied.
E91-1007.txt,126,In the example_comma_ the left hand side of 3 is satisfied_comma_ so we may fire that rule and add CCDDG t to our list of known facts_comma_ exactly as in the propositional case.
E91-1007.txt,127,We may also conclude that AA x B _comma_ because both are actual paths which terminate with the same label a_comma_ and non T labels are unique.
E91-1007.txt,128,The representative extended feature graph at this point is shown in Figure 3 below.
E91-1007.txt,129,A B D D O_ a ql _comma__comma_ Q  _comma_  D    D ql D N  C D D G II o  ._comma__comma__comma_ ._comma_... i_comma_  1 Figure 3 Intcrmcdlate structure for .
E91-1007.txt,130,There are other things which we may implicitly conclude_comma_ and which we must conclude to fire the other rules.
E91-1007.txt,131,For example_comma_ we may fire rule 4 at this point_comma_ because AA a  A T and IJ a  3 T are both tautologies in tile logic of feature terms_comma_ and so its left hand side is satisfied.
E91-1007.txt,132,Thus_comma_ we may add A BDDG T to our list of known facts.
E91-1007.txt,133,Similarly_comma_ since_comma_ as noted above_comma_ AA 13 holds_comma_ we may fire rule 5 to conclude ABDDEF T .
E91-1007.txt,134,Likewise_comma_ we may now fire rule s and conclude CCD x ABD .
E91-1007.txt,135,The representative extended graph structure at this pc4nt is shown in Figure 4 below.
E91-1007.txt,136,A B D D G D  il C G   t Figure 4 Intermediate structure for E. We mr_comma_ st eventually invoke a unification at the common end point of CCD and ABD. Such unification implicitly entails the tautology CCD x ABD  CCDD x A13DD and permits us to conclude that rule 7 should fire and add AC T to the set of facts of the least model.
E91-1007.txt,137,The result represented by the final extended feature graph of Figure 2.
E91-1007.txt,138,Note that rule s never fires_comma_ and that there are virtual edges and nodes left at the conclusion of the process.
E91-1007.txt,139,4.3 A taxonomy of implicit rules for sets of Horn feature clauses As we remarked in the . to this section_comma_ to correctly adapt forward chaining to the context of IIoXF s_comma_ we must implicitly iticlude the semantics of countably many tautologies.
E91-1007.txt,140,These fall into three classes. il Whenever an atomic term of the form or  a is determined to be true ap denotes the concate_comma_nation of a and fl _comma_ and another term of the form 36 c_comma_ T occurs as au antecedent of a ilorn feature clause_comma_ with either fl not the empty string or else a fl T _comma_ we must be able to automatically make the deduction of the tautology oq a   T to conclude that c T is now true.
E91-1007.txt,141,We call this node and path subsumption.
E91-1007.txt,142,In computing the least model of _comma_ the deductions AA a  CA T and B a  B T are examples of such rules.
E91-1007.txt,143,i2 Whenever we deduce two terms of the form a a and fl a to be true_comma_ with a T_comma_ we must implicitly realize the semantics of the rule a a fl a  a x fl _comma_ due to tile constraint that nonT labels are unique.
E91-1007.txt,144,We call this label matching.
E91-1007.txt,145,In computing the least model of E_comma_ tile deduction AA a A B a  .
E91-1007.txt,146,AA X B is a specific example.
E91-1007.txt,147,i3 Whenever we coalesce two paths_comma_ we must perform local unification on the subgraph rooted at the point of coalescence.
E91-1035.txt,1,of Lambek s 1958 categorial grammar for linguistic work has generally been rather limited. There appear to be two main reasons for this the notations most commonly used can sometimes obscure the structure of proofs and fail to clearly convey linguistic structure_comma_ and the cMculus as it stands is apparently not powerful enough to describe many phenomena encountered in natural language.
E91-1035.txt,2,In this paper we suggest ways of dealing with both these deficiencies.
E91-1035.txt,3,Firstly_comma_ we reformulate Lambek s system using proof figures based on the natural deduction notation commonly used for derivations in logic_comma_ and discuss some of the related proof theory.
E91-1035.txt,4,Natural deduction is generally regarded as the most economical and comprehensible system for working on proofs by hand_comma_ and we suggest that the same advantages hold for a similar presentation of categorial derivations.
E91-1035.txt,5,Secondly_comma_ we introduce devices called structural modalities_comma_ based on the structural rules found in logic_comma_ for the characterization of commutation_comma_ iteration and optionality.
E91-1035.txt,6,This permits the description of linguistic phenomena which Lambek s system does not capture with the desired sensitivity and gencrallty.
E91-1035.txt,7,LAMBEK CATEGORIAL GRAMMAR PRELIMINARIES Categorial grammar is an approach to language description in which the combination of expressions is governed not by specific linguistic rules but by general logical inference mechanisms.
E91-1035.txt,8,The point of departure can be seen as Frege s position that there are certain complete expressions which are the primary bearers of meaning_comma_ and that the meanings of incomplete expressions including words are derivative_comma_ being We would like to thank Robin Cooper_comma_ Martin Pickering and Pete Whitelock for comments and discussion relating to this work.
E91-1035.txt,9,The authors were respectively supported by SERC Research Studentship 883069 1 ESRO Research Studentshlp C00428722003 ESPRIT Project 393 and Cognitive Science HCI Research Initiative 89 CS01 and 89 CS25 SERC Postdoctoral Fellowship B ITF 206.
E91-1035.txt,11,We refer to sets of such objects as categories_comma_ which axe indexed by types_comma_ and stipulate that all complete expressions belong to categories indexed by primitive types.
E91-1035.txt,12,We then recursively classify incomplete expressions according to the meems by which they combine syntactically and semantically with other expressions.
E91-1035.txt,13,In the syntactic calculus of Lambek 1958 variously known as Lambek categoriai grammar_comma_ Lambek calculus_comma_ or L _comma_ expressions are classified by means of a set of bidirectional types as defined in 1 .
E91-1035.txt,15,If X is a primitive type then X is a type. b.
E91-1035.txt,18,Let us assume complete expressions to be sentences indexed by the primitive type S _comma_ noun phrases NP _comma_ common nouns N _comma_ and non finite verb phrases VP .
E91-1035.txt,20,For instance_comma_ the form of the word man will be represented as man and its meaning as man.
E91-1035.txt,21,PROOF FIGURES We shall present the rules of L by means of proo fi res_comma_ based on Prawitz 1965 systems of natural deduction .
E91-1035.txt,22,Natural deduction was developed by Gentzen 1936 to reflect the natural process of mathematical reasoning in which one uses a number of in erence tulsa to justify a single proposition_comma_ the conclusion_comma_ on the basis of having justifications of a number of propositions_comma_ called assumptions.
E91-1035.txt,23,During 198 a proof one may temporarily make a new assumption if one of the rules licenses the subsequent withdrawal of this assumption.
E91-1035.txt,24,The rule is said to discharge the assumption.
E91-1035.txt,25,The conclusion is said to depend on the undischarged assumptions_comma_ which are called the by. potheses of the proof.
E91-1035.txt,26,A proof is usually represented as a tree with the assumptions as leaves and the conclusion at the root.
E91-1035.txt,27,Finding a proof is then seen as the task of filling this tree in_comma_ and the inference rules as operations on the partially completed tree.
E91-1035.txt,29,The use of square brackets around an assumption indicates its discharge.
E91-1035.txt,30,R is the name of the rule_comma_ and the index i is included to disambiguate proofs_comma_ since there may be an uncertainty as to which rule has discharged which assumption.
E91-1035.txt,31,As propositions are represented by formulas in logic_comma_ so linguistic categories are represented by type formulas in L.
E91-1035.txt,32,The left to right order of types indicates tim order in which the forms of subexpressions are to be concatenated to give a composite expression derived by the proof.
E91-1035.txt,33,Thus we must take note of the order and place of occurrence of the premises of the rules in the proof figures for L.
E91-1035.txt,34,There is also a problem with the presentation of the rules in the compact notation as some of the rules will be written us if they had a number of conclusions_comma_ as follows 4 Xl ..
E91-1035.txt,35,X_comma_ _comma__comma__comma_ r q ... Y.
E91-1035.txt,36,This rule should be seen as a shorthand for _comma_  Xl ... Xm tt Y_comma_ ... Y..
E91-1035.txt,37,Z If the rules are viewed in this way it will be seen that they do not violate the single conclusion nature of the figures.
E91-1035.txt,38,As with standard natural deduction_comma_ for each connective there is an elimination rule which gates how a type containing that connective may be consumed_comma_ and an . rule which states how a type conraining that connective may be derived.
E91-1035.txt,39,The elimination rule for states that a proof of type X Y followed by a proof of type Y yields a proof of type X.
E91-1035.txt,41,Using the notation above_comma_ we may write these rules as follows  b.
E91-1035.txt,42,i . .
E91-1035.txt,46,We represent function application by juxtaposition_comma_ so that likes John means likes applied to John.
E91-1037.txt,1,this paper M grammars that are used in the Rosetta translation system will be looked at as the specification of attribute grammars. We will show that the attribute evaluation order is such that instead of the special purpose parsing and generation algorithms introduced for M grammars in Appelo et al. 1987 _comma_ also Earley like context free parsing and ordinary generation strategies can be used.
E91-1037.txt,2,Furthermore_comma_ it is illustrated that the attribute grammar approach gives an insight into the weak generative capacity of M grammars and into the computational complexity of the parsing and generation process.
E91-1037.txt,3,Finally_comma_ the attribute grammar approach will be used to reformulate the concept of isomorphic grammars.
E91-1037.txt,4,M grammars In this section we will introduce_comma_ very globally_comma_ the grammars that are used in the Rosett_comma_ machine translation system which is being developed at Philips Research Laboratories in Eindhoven.
E91-1037.txt,5,The original Rosetta grammar formalism_comma_ called M grammars_comma_ was a computational variant of Montague grammar.
E91-1037.txt,6,The formalism was introduced in Landsbergen 1981 .
E91-1037.txt,7,Whereas rules in Montague grammar operate on strings_comma_ M grammar rules M rules operate on labelled ordered trees_comma_ called S trees.
E91-1037.txt,8,The nodes of S trees are labelled with syntactic categories and attribute value pairs.
E91-1037.txt,9,Because of the reversibility of M rules_comma_ it is possible to define two algorithms M Parser and M Generator .
E91-1037.txt,10,The M Parser algorithm starts with a surface structure in the form of an S tree and breaks it down into basic expressions by recursive application of reversed M rules.
E91-1037.txt,11,The result of the M Parser algorithm is a syntactic derivation tree which reflects the history of the analysis process.
E91-1037.txt,12,The leaves of the derivation tree are names of basic expressions.
E91-1037.txt,13,The M Generator algorithm generates a set of S trees by bottom up application of M rules_comma_ the names of which are mentioned in a syntactic derivation tree.
E91-1037.txt,14,Analogous to Montague Grammar_comma_ with each M rule a rule is associated which expresses its meaning.
E91-1037.txt,15,This allows for the transformation of a syntactic derivation tree into a semantic derivation tree by replacing the name of each M rule by the name of the corresponding meaning rule.
E91-1037.txt,16,In Landsbergen 1982 it was shown that the formalism is very well fit to be used in an interlingual machine translation system in which semantic derivation trees make up the interlingua.
E91-1037.txt,17,In the analysis part of the translation system an S tree of the source language is mapped onto a set of semantic derivation trees.
E91-1037.txt,18,Next_comma_ each semantic derivation tree is mapped onto a set of S trees of the target language.
E91-1037.txt,19,In order to guarantee that for a sentence which can be analysed by means of the source language grammar a translation can always be generated using the target language grammar_comma_ source and target grammars in the Rosetta system are attuned.
E91-1037.txt,20,Grammars_comma_ attuned in the way described in Landsbergen 1982 _comma_ are called isomorphic.
E91-1037.txt,21,Appelo et al. 1987 introduces some extensions of the formalism_comma_ which make it possible to assign more structure to an M grammar.
E91-1037.txt,22,The new formalism was called controlled M grammars.
E91-1037.txt,23,In this new approach a grammar consists of set of subgrammars.
E91-1037.txt,24,Each of the subgrammars contains a set of M rules and a regular expression over the alphabet of rule names.
E91-1037.txt,25,The set of M rules is subdivided into meaningful rules and transformations.
E91-1037.txt,26,Transformations have no semantic relevance and will therefore not occur in a derivation tree.
E91-1037.txt,27,The regular expression can be looked at as a prescription of the order in which the rules of the subgrammar have to be applied.
E91-1037.txt,28,Because of these changes in the formalism_comma_ new versions of the M Parser and M Generator algorithm were introduced which were able to deal with subgrammars.
E91-1037.txt,29,These algorithms_comma_ however_comma_ are complex and result in a rather cumbersome implementation.
E91-1037.txt,30,In this paper we will show that they can be replaced by normal context free parse and generation algorithms if we interpret an M grammar as the specification of an attribute grammar Knuth 1968 _comma_ Deransart et al. 1988 .
E91-1037.txt,31,M grammars as attribute grammars The control expression which is used in the definition of a Rosetta subgrammar specifies a regular language over the alphabet of rule names.
E91-1037.txt,32,Another way to define such a language is by means of a regular grammar.
E91-1037.txt,33,Let control expression cei of subgrammar i define the regular language i .
E91-1037.txt,34,Then we can construct a minimal regular grammar rgi which defines the same language.
E91-1037.txt,35,The grammar rgi will have the following form A set of non terminals Ni  ..... I M  A set of terminals Ei.
E91-1037.txt,36,Ei is the smMlest set such that there is a terminal f EEi for e u h M rule r . Start symbol I  210 A set of production rules P containing the following type of rules I  I _comma_ where f E El _ _.
E91-1037.txt,37,We will use the regular grammar defined above as a starting point for the construction of an attributed subgrammar.
E91-1037.txt,38,An elegant view of attribute grammars can be found in Hemerik 1984 .
E91-1037.txt,39,Hemerik defines an attribute grammar as a context free grammar with parametrized non terminals and production rules.
E91-1037.txt,40,In general_comma_ non. terminals may have a number of parameters_comma_ attributes associated with them.
E91-1037.txt,41,Production rules of an attribute grammar are pairs rule form_comma_ rule condition .
E91-1037.txt,42,From a rule form_comma_ production rules can be obtained by means of substitution of values for the attribute variables that satisfy the rule condition.
E91-1037.txt,43,In the grammars presented in this paper_comma_ non terminals have only one attribute of type S tree.
E91-1037.txt,44,The attribute grammar rules that are used throughout this paper also have a very restricted form.
E91-1037.txt,45,A typical attribute grammar rule r with context free skeleton A .
E91-1037.txt,46,BC will look like A o B p C q o_comma_ p_comma_ q  Here_comma_ A o .
E91-1037.txt,47,B p C q is the rule form_comma_ o_comma_p_comma_ q are the attributes and o_comma_ p_comma_q E is the rule condition_comma_ g defines a relation between the attributes at the left hand side and the attributes at the right hand side of the rule form.
E91-1037.txt,48,For each subgrammar rgi_comma_ 1 i M we will construct an attributed subgrammar agi.
E91-1037.txt,49,Each constructed attributed subgrammar agi will have a start symbol J T .
E91-1037.txt,50,First_comma_ however_comma_ we define two new attributed subgrammars that have no direct relation with a subgrammar of a given M grammar the start subgrammar and the terminal subgrammar.
E91-1037.txt,53,Obeying the termination conditions that we adhere to in the current Rosetta system_comma_ it can be proved that the recognition and the generation problems axe NP hard_comma_ which makes them computation. ally intractable.
E91-1037.txt,54,In comparison with other formalisms_comma_ M grammaxs axe no exception with respect to the complexity of these issues.
E91-1037.txt,55,LFG recognition and FUG generation have both been proved to be NP hard in Barton et ai_comma_ 1987 and Ritchie 1986 respectively.
E91-1037.txt,57,We should keep in mind_comma_ however_comma_ that the computational complexity analysis is a worstease analysis.
E91-1037.txt,58,The average case behaviour of the parse and generation algorithm that we experience in the dally use of the Rosetta system is certainly not exponential.
E91-1037.txt,59,Isomorphic Grammars The decidability of the question whether two Mgrammars axe isomorphic is another computational aspect related to M grammars.
E91-1037.txt,60,Although this mathematical issue appears not to be very relevant from a practical point of view_comma_ it enables us to show what grammar isomorphy means in the context of stag s.
E91-1037.txt,61,According to the Rosetta Compositionality Principle Landsbergen 1987 to each meaningful M rule r a meaning rule mr corresponds which expresses the semantics of r.
E91-1037.txt,62,Furthermore_comma_ there is a set of basic meanings for each basic expression of an M grammar.
E91-1037.txt,64,Let us repeat here what it means for two M grammars to be isomorphic ...Two grammars are isomorphic iff each semantic derivation tree which is welbformed with respect to one grammar is also well formed with respect to the other grammar... Landsbergen 1987 .
E91-1037.txt,65,We can reformulate the original definition of isomorphic M grammars in . very elegant way for samg s Definition Two samg s X and X2 are isomorphic iff they are equivalent_comma_ that is iff XI  X2 This definition says that writing isomorphic grammars comes down to writing two attribute grammars which define the same language.
E91-1037.txt,66,From formal language theory e.g. Hopcroft and Ullman 1979 we know that there is no algorithm that can test an arbitrary p ir of context free grammars G1 and G2 to determine whether G  G2 .
E91-1037.txt,67,It can also be shown that samg s can define any recursive language.
E91-1037.txt,68,Consequently_comma_ checking the equivalence of two arbitrary samg s will be an un. decidable problem.
E91-1037.txt,69,Rosetta grammars that are used for translation purposes_comma_ however_comma_ are not arbitrary samg s they are not created completely independently.
E91-1037.txt,70,The strategy followed in Rosetta to accomplish the definition of equivalent grammars_comma_ that is_comma_ grammars that define identical languages_comma_ is to attune two samg s to each other.
E91-1037.txt,71,This grammar attuning strategy is extensively described in Appelo et al. 1987 _comma_ Landsbergen 1982 and Landsbergen 1987 for ordinary M grammars.
E91-1037.txt,72,Here_comma_ we will show what the attuning strategy means in the context of samg s_comma_ together with a few extensions.
E91-1037.txt,73,The attuning measures below must not be looked at as the weakest possible conditions that guarantee isomorphy.
E91-1037.txt,74,The list merely is an enumeration of conditions which together should help to establish isomorphy.
E91-1037.txt,75,If two samg s Xa and X2 have to be isomorphic_comma_ the following measures are proposed _comma_ The production rules of both samg s must be consistent. .
E91-1037.txt,77,In the context of the ordin y M grammar formalism this condition is formulated as for each basic expression in one M grammar there has to be at least one basic expression in the other M grammar with the same meaning which comes aThis condition is equivalent to the attuning measures described in Appelo et al. 1987 _comma_ Landsbergen 1982 and Landsbergen 1987 . down to the condition that the terminal set of the terminal subgrammars should be identical for each meaningful rule in one M grammar there has to be at least one meaningful rule in the other M graanmar which has the same meaning. The underlying contezt Jree grammars oJ both samg s should be equivalent.
E91-1037.txt,78,Equivalence of the underlying context free grammars can be established by putting an equivalenee condition on the underlying grammar of corresponding subgrammars of the samg s in question.
E91-1037.txt,79,Suppose that for each subgrammar of an samg X1 a subgrammar of another samg 3 2 would exist that performs the same linguistic task and vice versa. Such an ideal situation could be expressed by a relation g on the sets of subgrammars of both samg s. Let i and j be subgrammars of the samg s X1 and Xa respectively_comma_ such that i_comma_ j E g_comma_ then the underlying grammars 4 Bi and B i have to be constructed in such a way that they define the same language. Notice that Bi and B i are regular grammars. More formally v i_comma_i e g c B_comma_  oi . The three attuning conditions above guarantee that the underlying context free grammars of two attuned samg s are equivalent.
E91-1037.txt,80,However_comma_ the language defined by an samg is a subset of the language defined by its underlying grammar.
E91-1037.txt,81,The rule conditions determine which elements are in the subset and which are not.
E91-1037.txt,82,Because of the great expressive power of M rules_comma_ the attuning measures place no effective restrictions on the kind of languages an samg can define.
E91-1037.txt,83,Hence_comma_ it can be proved that Theorem The question whether two attuned samg s are isomorphic is undecidable.
E91-1037.txt,84,Because of the equivalence between samg s and Mgrammars this also applies to arbitrary attuned Mgr nmars.
E91-1037.txt,85,Future research is needed to find extensions for the attuning measures in a way that guarantees isom0tphy if grammar writers adhere to the attuning condil ions.
E91-1037.txt,86,The extensions will probably include restrictions on the form of the underlying grammar and on the expressive power of M rules.
E91-1037.txt,87,Also formal attuning measures between M rules or sets of M rules of different grammars are conceivable.
E91-1037.txt,88,4Because we are dealing with a subgrammar_comma_ the nonterminal S is discarded from the production rules of the underlying grammar.
E91-1037.txt,89,SThis attuning measure sketches an ideal sittmtion.
E91-1037.txt,90,In practice for each subgrarnmar of an samg there is not a corresponding fully isomorphic subgrammar but only a partially isomorphic subgranunar of the other suing.
E91-1037.txt,91,However_comma_ the requirement of fully isomorphic subgranunars is not the weakest attuning condition that guarantees the equivalence of the underlying context free grammars.
E91-1037.txt,92,F__comma_quivalence can also be guaranteed if XI and X satisfy the following condition which expresses partial isomorphy between subgranunars U x nd Uj x L B 214 The current Rosetts grammars obey the three previously mentioned attuning measures.
E91-1037.txt,93,In practice these measures provide a good basis to work with.
E91-1037.txt,94,Therefore_comma_ the undecidability of the isomorphy question is not an urgent topic at the moment.
E91-1037.txt,95,Conclusions In thib paper we presented the interpretation of an Mgrammar as a specification of an attribute grammar.
E91-1037.txt,96,We showed that the resulting attribute grammar is reversible and that it can be used in ordinary context free recognition and generation algorithms.
E91-1037.txt,97,The generation algorithm is to be used in the analysis phase of Rosetta_comma_ whereas the recognition algorithm should be used in the generation phase.
E91-1037.txt,98,With respect to the weak generative capacity it has been concluded that the set of languages that can be generated and recognized depends on the termination conditions that are imposed on the grammar.
E91-1037.txt,99,If the weakest termination condition is assumed_comma_ the set of languages that can be defined by an M grammar is equivalent to the set of languages that can be recognized by a deterministic Turin8 Machine in finite time.
E91-1037.txt,100,Using more realistic termination conditions_comma_ the computational complexity of the recognition and generation problem can still be classified as NPhard and_comma_ consequently_comma_ as computationally intractable.
E91-1037.txt,101,Finally_comma_ it was concluded that the question whether two attuned M grammars are isomorphic_comma_ is undecidable.
E91-1037.txt,102,Acknowledgements The author wishes to thank Jan Landsbergen_comma_ Jan Odijk_comma_ Andr Schenk and Petra de Wit for their helpful comments on earlier versions of the paper.
E91-1037.txt,103,The author is also indebted to Lisette Appelo for encouraging him to write the paper and to Ren6 Leermakers with whom he had many fruitful discussions on the subject. references Appelo_comma_ L. _comma_ C.
E91-1037.txt,104,Fellinger and J.
E91-1037.txt,105,Landsbergen 1987 _comma_ Subgrammars_comma_ Rule Classes and Control in the Rosetta Translation System _comma_ Philips Research M.S.
E91-1037.txt,106,14.131_comma_ Proceedings of 3rd ACL Conference _comma_ European Chapter_comma_ pp.
E91-1037.txt,108,Barton_comma_ G._comma_ R.
E91-1037.txt,109,Berwick and E.
E91-1037.txt,110,Ristad 1987 _comma_ Com. putational Compi ity and Natural Language_comma_ MIT Press_comma_ Cambridge_comma_ Mass.
E91-1037.txt,112,1965 _comma_ Aspects of the Theory of Syntax_comma_ MIT Press_comma_ Cambridge_comma_ Mass.
E91-1037.txt,115,1971 _comma_ Characterizations of Pushdown Machines in Terms of Time bounded Computers_comma_ Journal of the Association for Computing Machinery 18_comma_ 1_comma_ pp.
E91-1037.txt,117,Deransart_comma_ P._comma_ M.
E91-1037.txt,119,Lorho 1988 _comma_ Attribute Grammars _comma_ Lecture Notes in Computer Science 323_comma_ Springer Verlag_comma_ Berlin.
E91-1037.txt,121,1970 _comma_ An efficient context free parsing algorithm _comma_ Commun.
E91-1037.txt,122,ACM 13 1970 _comma_ pp.
E91-1037.txt,125,1986 _comma_ The Complexity of Languages Generated by Attribute Grammars _comma_ SIAM Journal on Computing 15_comma_ l_comma_ pp.
E91-1037.txt,128,1989 _comma_ A Generalization of the Offiine Parsable Grammars _comma_ Proceedings of the 7th Annual Meeting of the Association for Computational Linguistics_comma_ pp.
E91-1037.txt,131,1984 _comma_ Formal definitions of programming languages as a basis for compiler construction _comma_ Ph.D. th._comma_ University of Eindhoven.
E91-1037.txt,132,Hopcroft_comma_ J.E. and J.D.
E91-1037.txt,133,Ullman 1979 _comma_ . to Automata Theory_comma_ Languages and Computation _comma_ Addison Wesley Publishing Company_comma_ Reading_comma_ Mass.
E91-1037.txt,135,1989 _comma_ Towards Reversible M.T.
E91-1037.txt,136,Systems _comma_ MT Summit lI_comma_ pp.
E91-1037.txt,139,1968 _comma_ Semantics of Context Free Languages _comma_ Math.
E91-1037.txt,140,Systems Theory _comma_ 2_comma_ pp.
E91-1037.txt,141,127 145 June 1968 .
E91-1037.txt,143,1981 _comma_ Adaptation of Montague grammar to the requirements of parsing _comma_ in Forreal Methods in the Study of Language Part _comma_ MC Tract 136_comma_ Mathematical Centre_comma_ Amsterdam.
E91-1037.txt,145,1982 _comma_ Machine Translation based on logically isomorphic Montague grammars _comma_ Coling 8 _comma_ North H011and_comma_ Amsterdam_comma_ pp.
E91-1037.txt,148,1987 _comma_ Isomorphic grammars and their use in the Rosetta Translation system _comma_ Machine Translation_comma_ the State of the Art_comma_ M.
E91-1037.txt,149,King ed. _comma_ Edinburg University Press.
E91-1037.txt,150,Leermakers_comma_ R 1991 _comma_ Non deterministic recursive ascent parsing _comma_ Proceedings of the 5th ACL Confer. ence_comma_ European Chapter_comma_ forthcoming.
E91-1037.txt,151,Noord_comma_ van G.
E91-1037.txt,152,1990 _comma_ Reversible Unification Based Machine l anslation _comma_ in Proceedings of the 13th International Conference on Computational Linguistics_comma_ Helsinki.
E91-1037.txt,153,Pereira_comma_ F._comma_ D.
E91-1037.txt,154,Warren 1983 _comma_ Parsing as deduction _comma_ Proceedings .of the lth Annual Meeting of the Association for Computational Linguistics_comma_ pp.
E91-1037.txt,155,137144. Perrault_comma_ C.R.
E91-1037.txt,156,C1984 _comma_ On the Mathematical Properties of Linguistic Theories _comma_ Computational Linguistics 10_comma_ pp.
E91-1037.txt,159,1986 _comma_ The computational complexity of sentence derivation in functional unification grammar _comma_ Proceedings of Coling 86_comma_ pp.
E91-1037.txt,162,1989 _comma_ New directions in MT systems _comma_ MT Summit II_comma_ pp.
E91-1037.txt,165,1975 _comma_ A grammatical characterization of the exponential languages _comma_ Proceedings of the 16th Annual Symposium on Switching Theory and Automata_comma_ IEEE Computer Society_comma_ New York_comma_ pp.
E91-1037.txt,169,987 _comma_ Separating Linguistic Analyses from Linguistic Theories _comma_ in Linguistic Theory and Computer Applications_comma_ Academic Press.
E06-1020.txt,1,describe a word alignment platform which ensures text pre processing tokenization_comma_ POS tagging_comma_ lemmatization_comma_ chunking_comma_ sentence alignment as required by an accurate word alignment. The platform combines two different methods_comma_ producing distinct alignments.
E06-1020.txt,2,The basic word aligners are described in some details and are individually evaluated.
E06-1020.txt,3,The union of the individual alignments is subject to a filtering postprocessing phase.
E06-1020.txt,4,Two different filtering methods are also presented.
E06-1020.txt,5,The evaluation shows that the combined word alignment contains 10.75 less errors than the best individual aligner.
E06-1020.txt,7,It is almost a truism that more decision makers_comma_ working together_comma_ are likely to find a better solution than when working alone.
E06-1020.txt,8,Dieterich 1998 discusses conditions under which different decisions in his case classifications may be combined for obtaining a better result.
E06-1020.txt,9,Essentially_comma_ a successful automatic combination method would require comparable performance for the decision makers and_comma_ additionally_comma_ that they should not make similar errors.
E06-1020.txt,10,This idea has been exploited by various NLP researchers in language modelling_comma_ statistical POS tagging_comma_ parsing_comma_ etc. We developed two quite different word aligners_comma_ driven by two distinct objectives the first one was motivated by a project aiming at the development of an interlingually aligned set of wordnets while the other one was developed within an SMT ongoing project.
E06-1020.txt,11,The first one was used for validating_comma_ against a multilingual corpus_comma_ the interlingual synset equivalences and also for WSD experiments.
E06-1020.txt,12,Although_comma_ initially_comma_ it was concerned only with open class words recorded in a wordnet_comma_ turning it into an all words aligner was not a difficult task.
E06-1020.txt,13,This word aligner_comma_ called YAWA is described in section 3.
E06-1020.txt,14,A quite different approach from the one used by YAWA_comma_ is implemented in our second word aligner_comma_ called MEBA_comma_ described in section 4.
E06-1020.txt,15,It is a multiple parameter and multiple step algorithm using relevance thresholds specific to each parameter_comma_ but different from each step to the other.
E06-1020.txt,16,The implementation of MEBA was strongly influenced by the notorious five IBM models described in Brown et al. 1993 .
E06-1020.txt,17,We used GIZA Och and Ney 2000 Och and Ney_comma_ 2003 to estimate different parameters of the MEBA aligner.
E06-1020.txt,18,The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard GS 1 annotations used in the Word Alignment Shared Tasks Romanian English track organized at HLT NAACL2003 Mihalcea and Pedersen 2003 .
E06-1020.txt,19,Given that the two aligners are based on quite different models and that their F measures are comparable_comma_ it was quite a natural idea to combine their results and hope for an improved alignment.
E06-1020.txt,20,Moreover_comma_ by analyzing the alignment errors done by each word aligner_comma_ we found that the number of common mistakes was small_comma_ so 1 We noticed in the GS Alignment various errors both sentence and word alignment errors that were corrected.
E06-1020.txt,21,The tokenization of the bitexts used in the GS Alignment was also modified_comma_ with the appropriate modification of the reference alignment.
E06-1020.txt,22,These reference data are available at http www.racai.ro res WA GS 153 the premises for a successful combination were very good Dieterich_comma_ 1998 .
E06-1020.txt,23,The Combined Word Aligner_comma_ COWAL described in section 5_comma_ is a wrapper of the two aligners YAWA and MEBA merging the individual alignments and filtering the result.
E06-1020.txt,24,At the Shared Task on Word Alignment organized by the ACL2005 Workshop on Building and Using Parallel Corpora Data driven Machine Translation and Beyond Martin_comma_ et al. 2005 _comma_ we participated on the Romanian English track with the two aligners and the combined one COWAL .
E06-1020.txt,25,Out of 37 competing systems_comma_ COWAL was rated the first_comma_ MEBA the 20th and TREQ AL Tufi et al. 2003 _comma_ the former version of YAWA_comma_ was rated the 21st.
E06-1020.txt,26,The usefulness of the aligner combination was convincingly demonstrated.
E06-1020.txt,27,Meanwhile_comma_ both the individual aligners and their combination were significantly improved.
E06-1020.txt,28,COWAL is now embedded into a larger platform that incorporates several tools for bitexts preprocessing briefly reviewed in section 2 _comma_ a graphical interface that allows for comparing and editing different alignments_comma_ as well as a word sense disambiguation module.
E06-1020.txt,29,2 The bitext processing The two base aligners and their combination use the same format for the input data and provide the alignments in the same format.
E06-1020.txt,30,The input format is obtained from two raw texts that represent reciprocal translations.
E06-1020.txt,31,If not already sentence aligned_comma_ the two texts are aligned by our sentence aligner that builds on Moore s aligner Moore_comma_ 2002 but which unlike it_comma_ is able to recover the non one to one sentence alignments.
E06-1020.txt,32,The texts in each language are then tokenized_comma_ tagged and lemmatized by the TTL module Ion_comma_ 2006 .
E06-1020.txt,33,More often than not_comma_ the translation equivalents have the same part of speech_comma_ but relying on such a restriction would seriously affect the alignment recall.
E06-1020.txt,34,However_comma_ when the translation equivalents have different parts of speech_comma_ this difference is not arbitrary.
E06-1020.txt,35,During the training phase_comma_ we estimated POS affinities p POSmRO POSnEN and p POSnEN POSmRO and used them to filter out improbable translation equivalents candidates.
E06-1020.txt,36,The next pre processing step is represented by sentence chunking in both languages.
E06-1020.txt,37,The chunks are recognized by a set of regular expressions defined over the tagsets and they correspond to non recursive noun phrases_comma_ adjectival phrases_comma_ prepositional phrases and verb complexes analytical realization of tense_comma_ aspect mood and diathesis and phrasal verbs .
E06-1020.txt,38,Finally_comma_ the bitext is assembled as an XML document Tufi and Ion_comma_ 2005 _comma_ which is the standard input for most of our tools_comma_ including COWAL alignment platform.
E06-1020.txt,39,3 YAWA YAWA is a three stage lexical aligner that uses bilingual translation lexicons and phrase boundaries detection to align words of a given bitext.
E06-1020.txt,40,The translation lexicons are generated by a different module_comma_ TREQ Tufi _comma_ 2002 _comma_ which generates translation equivalence hypotheses for the pairs of words one for each language in the parallel corpus which have been observed occurring in aligned sentences more than expected by chance.
E06-1020.txt,41,The hypotheses are filtered by a loglikelihood score threshold.
E06-1020.txt,42,Several heuristics string similarity cognates_comma_ POS affinities and alignments locality2 are used in a competitive linking manner Melamed_comma_ 2001 to extract the most likely translation equivalents.
E06-1020.txt,43,YAWA generates a bitext alignment by incrementally adding new links to those created at the end of the previous stage.
E06-1020.txt,44,The existing links act as contextual restrictors for the new added links.
E06-1020.txt,45,From one phase to the other new links are added without deleting anything.
E06-1020.txt,46,This monotonic process requires a very high precision at the price of a modest recall for the first step.
E06-1020.txt,47,The next two steps are responsible for significantly improving the recall and ensuring an increased F measure.
E06-1020.txt,48,In the rest of this section we present the three stages of YAWA and evaluate the contribution of each of them to the final result.
E06-1020.txt,49,3.1 Phase 1 Content Words Alignment YAWA begins by taking into account only very probable links that represent the skeleton alignment used by the second phase.
E06-1020.txt,50,This alignment is done using outside resources such as translation lexicons and involves only the alignment of content words nouns_comma_ verbs_comma_ adjective and adverbs .
E06-1020.txt,51,The translation equivalence pairs are ranked according to an association score i.e. loglikelihood_comma_ DICE_comma_ point wise mutual informa2 The alignments locality heuristics exploits the observation made by several researchers that adjacent words of a text in the source language tend to align to adjacent words in the target language. A more strict alignment locality constraint requires that all alignment links starting from a chunk in the one language end in a chunk in the other language. 154 tion_comma_ etc. .
E06-1020.txt,52,We found that the best filtering of the translation equivalents was the one based on the log likelihood LL score with a threshold of 9.
E06-1020.txt,53,Each translation unit pair of aligned sentences of the target bitext is scanned for establishing the most likely links based on a competitive linking strategy that takes into account the LL association scores given by the TREQ translation lexicon.
E06-1020.txt,54,If a candidate pair of words is not found in the translation lexicon_comma_ we compute their orthographic similarity cognate score Tufi _comma_ 2002 .
E06-1020.txt,55,If this score is above a predetermined threshold for Romanian English task we used the empirically found value of 0.43 _comma_ the two words are treated as if they existed in the translation lexicon with a high association score in practice we have multiplied the cognate score by 100 to yield association scores in the range 0 .. 100 .
E06-1020.txt,56,The Figure 1 exemplifies the links created between two tokens of a parallel sentence by the end of the first phase.
E06-1020.txt,57,Figure 1 Alignment after the first step 3.2 Phase 2 Chunks Alignment The second phase requires that each part of the bitext is chunked.
E06-1020.txt,58,In our Romanian English experiments_comma_ this requirement was fulfilled by using a set of regular expressions defined over the tagsets used in the target bitext.
E06-1020.txt,59,These simple chunkers recognize noun phrases_comma_ prepositional phrases_comma_ verbal and adjectival or adverbial groupings of both languages.
E06-1020.txt,60,In this second phase YAWA produces first chunk to chunk matching and then aligns the words within the aligned chunks.
E06-1020.txt,61,Chunk alignment is done on the basis of the skeleton alignment produced in the first phase.
E06-1020.txt,62,The algorithm is simple align two chunks c i in source language and c j in the target language if c i and c j have the same type noun phrase_comma_ prepositional phrase_comma_ verb phrase_comma_ adjectival adverbial phrase and if there exist a link w s _comma_ w t so that w s  c i then w t  c j .
E06-1020.txt,63,After alignment of the chunks_comma_ a language pair dependent module takes over to align the unaligned words belonging to the chunks.
E06-1020.txt,64,Our module for the Romanian English pair of languages contains some very simple empirical rules such as if b is aligned to c and b is preceded by a_comma_ link a to c_comma_ unless there exist d in the same chunk with c and the POS category of d has a significant affinity with the category of a.
E06-1020.txt,65,The simplicity of these rules derives from the shallow structures of the chunks.
E06-1020.txt,66,In the above example b and c are content words while a is very likely a determiner or a modifier for b.
E06-1020.txt,67,The result of the second alignment phase_comma_ considering the same sentence in Figure 1_comma_ is shown in Figure 2.
E06-1020.txt,68,The new links are represented by the double lines.
E06-1020.txt,69,Figure 2 Alignment after the second step 3.3 Phase 3 Dealing with sequences of unaligned words This phase identifies contiguous sequences of words blocks in each part of the bitext which remain unaligned and attempts to heuristically match them.
E06-1020.txt,70,The main criteria used to this end are the POS affinities of the remaining unaligned words and their relative positions.
E06-1020.txt,71,Let us illustrate_comma_ using the same example and the result shown in Figure 2_comma_ how new links are added in this last phase of the alignment.
E06-1020.txt,72,At the end of phase 2 the blocks of consecutive words that remain to be aligned are English en0 you _comma_ en1 that _comma_ en2 is_comma_ not _comma_ en3 and _comma_ en4 . and 155 Romanian ro0  _comma_ ro1 c _comma_ ro2 nu_comma_ e _comma_ ro3  i _comma_ ro4 . .
E06-1020.txt,73,The mapping of source and target unaligned blocks depends on two conditions that surrounding chunks are already aligned and that pairs in candidate unaligned blocks have significant POS affinity.
E06-1020.txt,74,For instance in the figure above_comma_ blocks en1 that and ro1 c  satisfy the above conditions because they appear among already aligned chunks ll notice  ve i observa and D ncu s generosity  generozitatea lui D ncu and they contain words with the same POS.
E06-1020.txt,75,After block alignment3_comma_ given a pair of aligned blocks_comma_ the algorithm links words with the same POS and then the phase 2 is called again with these new links as the skeleton alignment.
E06-1020.txt,76,In Figure 3 is shown the result of phase 3 alignment of the sentence we used as an example throughout this section.
E06-1020.txt,77,The new links are shown as before by double lines.
E06-1020.txt,78,Figure 3 Alignment after the third step The third phase is responsible for significant improvement of the alignment recall_comma_ but it also generates several wrong links.
E06-1020.txt,79,The detection of some of them is quite straightforward_comma_ and we added an additional correction phase 3.f.
E06-1020.txt,80,By analysing the bilingual training data we noticed the translators tendency to preserve the order of the phrasal groups.
E06-1020.txt,81,We used this finding which might not be valid for any language pair as a removal heuristics for the links that cross two or more aligned phrase groups.
E06-1020.txt,82,One should notice that the first word in the English side of the example in Figure 3 you remained unaligned interpreted as not translated in the Romanian side .
E06-1020.txt,83,According to the Gold Standard used for 3 Only 1 1 links are generated between blocks. evaluation in the ACL2005 shared task_comma_ this interpretation was correct_comma_ and therefore_comma_ for the example in Figure 3_comma_ the F measure for the YAWA alignment was 100 .
E06-1020.txt,84,However_comma_ Romanian is a pro drop language and although the translation of the English pronoun is not lexicalized in Romanian_comma_ one could argue that the auxiliary ve i should be aligned also to the pronoun you as it incorporates the grammatical information carried by the pronoun.
E06-1020.txt,85,Actually_comma_ MEBA as exemplified in Figure 4 produced this multiple token alignment and was penalized for it .
E06-1020.txt,86,3.4 Performance analysis The table that follows presents the results of the YAWA aligner at the end of each alignment phase.
E06-1020.txt,87,Although the Precision decreases from one phase to the next one_comma_ the Recall gains are significantly higher_comma_ so the F measure is monotonically increasing.
E06-1020.txt,88,Precision Recall F Measure Phase 1 94.08 34.99 51.00 Phase 1 2 89.90 53.90 67.40 Phase 1 2 3 88.82 73.44 80.40 Phase 1 2 3 3.f 88.80 74.83 81.22 Table 1 YAWA evaluation 4 MEBA MEBA uses an iterative algorithm that takes advantage of all pre processing phases mentioned in section 2.
E06-1020.txt,89,Similar to YAWA aligner_comma_ MEBA generates the links step by step_comma_ beginning with the most probable anchor links .
E06-1020.txt,90,The links to be added at any later step are supported or restricted by the links created in the previous iterations.
E06-1020.txt,91,The aligner has different weights and different significance thresholds on each feature and iteration.
E06-1020.txt,92,Each of the iterations can be configured to align different categories of tokens named entities_comma_ dates and numbers_comma_ content words_comma_ functional words_comma_ punctuation in decreasing order of statistical evidence.
E06-1020.txt,93,The first iteration builds anchor links with a high level of certainty that is cognates_comma_ numbers_comma_ dates_comma_ pairs with high translation probability .
E06-1020.txt,94,The next iteration tries to align content words open class categories in the immediate vicinity of the anchor links.
E06-1020.txt,95,In all steps_comma_ the candidates are considered if and only if they meet the minimal threshold restrictions.
E06-1020.txt,96,A link between two tokens is characterized by a set of features with values in the 0_comma_1 interval .
E06-1020.txt,97,We differentiate between context independ156 ent features that refer only to the tokens of the current link translation equivalency_comma_ part ofspeech affinity_comma_ cognates_comma_ etc. and context dependent features that refer to the properties of the current link with respect to the rest of links in a bi text locality_comma_ number of traversed links_comma_ tokens indexes displacement_comma_ collocation .
E06-1020.txt,98,Also_comma_ we distinguish between bi directional features translation equivalence_comma_ part of speech affinity and non directional features cognates_comma_ locality_comma_ number of traversed links_comma_ collocation_comma_ indexes displacement .
E06-1020.txt,99,Precision Recall F measure Anchor links 98.50 26.82 42.16 Words around anchors 96.78 42.41 58.97 Funct. words and punctuation 94.74 59.48 73.08 Probable links 92.05 71.00 80.17 Table 2 MEBA evaluation The score of a candidate link LS between a source token i and a target token j is computed by a linear function of several features scores Tiedemann_comma_ 2003 .  n i ii ScoreFeatjiLS 1 _comma_ O 1 1 n i i O Each feature has defined a specific significance threshold_comma_ and if the feature s value is below this threshold_comma_ the contribution to the LS of the current link of the feature in case is nil. The thresholds of the features and lambdas are different from one iteration to the others and they are set by the user during the training and system fine tuning phases. There is also a general threshold for the link scores and only the links that have the LS above this threshold are retained in the bitext alignment. Given that this condition is not imposing unique source or target indexes_comma_ the resulting alignment is inherently many tomany. In the following subsections we briefly discuss the main features we use in characterising a link. 4.1 Translation equivalence This feature may be used for two types of preprocessed data lemmatized or non lemmatized input. Depending on the input format_comma_ MEBA invokes GIZA to build translation probability lists for either lemmas or the occurrence forms of the bitext4. Irrespective of the lemmatisation option_comma_ the considered token for the translation model build by GIZA is the respective lexical item lemma or wordform trailed by its POS tag eg. plane_N_comma_ plane_V_comma_ plane_A .
E06-1020.txt,100,In this way we avoid data sparseness and filter noisy data.
E06-1020.txt,101,For instance_comma_ in case of highly inflectional languages as Romanian is the use of lemmas significantly reduces the data sparseness.
E06-1020.txt,102,For languages with weak inflectional character as English is the POS trailing contributes especially to the filtering the search space.
E06-1020.txt,103,A further way of removing the noise created by GIZA is to filter out all the translation pairs below a LL threshold.
E06-1020.txt,104,We made various experiments and_comma_ based on the estimated ratio between the number of false negatives and false positive_comma_ empirically set the value of this threshold to 6.
E06-1020.txt,105,All the probability losses by this filtering were redistributed proportionally to their initial probabilities to the surviving translation equivalence candidates.
E06-1020.txt,106,4.2 Translation equivalence entropy score The translation equivalence relation is a semantic one and it directly addresses the notion of word sense.
E06-1020.txt,107,One of the Zipffian laws prescribes a skewed distribution of the senses of a word occurring several times in a coherent text.
E06-1020.txt,108,We used this conjecture as a highly informative information source for the validity of a candidate link.
E06-1020.txt,109,The translation equivalence entropy score is a favouring parameter for the words that have few high probability translations.
E06-1020.txt,110,Since this feature is definitely sensitive to the order of the lexical items_comma_ we compute an average value for the link DES A EES B .
E06-1020.txt,111,Currently we use D E 0.5_comma_ but it might be interesting to see_comma_ depending on different language pairs_comma_ how the performance of the aligner would be affected by a different settings of these parameters.
E06-1020.txt,112,N TRWpTRWp N i ii WES log _comma_ log _comma_ 11  4.3 Part of speech affinity In faithful translations the translated words tend to be translated by words of the same part ofspeech. When this is not the case_comma_ the different POSes_comma_ are not arbitrary. The part of speech affinity_comma_ P cat A cat B _comma_ can be easily computed from a gold standard alignment.
E06-1020.txt,113,Obviously_comma_ this 4 Actually_comma_ this is a user set parameter of the MEBA aligner if the input bitext contain lemmatization information_comma_ both translation probability tables may be requested.
E06-1020.txt,114,157 is a directional feature_comma_ so an averaging operation is necessary in order to ascribe this feature to a link PA DP cat A cat B  EP cat B cat A .
E06-1020.txt,115,Again_comma_ we used D E 0.5 but different values of these weights might be worthwhile investigating.
E06-1020.txt,116,4.4 Cognates The similarity measure_comma_ COGN TS_comma_ TT _comma_ is implemented as a Levenstein metric.
E06-1020.txt,117,Using the COGN test as a filtering device is a heuristic based on the cognate conjecture_comma_ which says that when the two tokens of a translation pair are orthographically similar_comma_ they are very likely to have similar meanings i.e. they are cognates .
E06-1020.txt,118,The threshold for the COGN TS_comma_ TT test was empirically set to 0.42.
E06-1020.txt,119,This value depends on the pair of languages in the bitext.
E06-1020.txt,120,The actual implementation of the COGN test includes a language dependent normalisation step_comma_ which strips some suffixes_comma_ discards the diacritics_comma_ reduces some consonant doubling_comma_ etc. This normalisation step was hand written_comma_ but_comma_ based on available lists of cognates_comma_ it could be automatically induced.
E06-1020.txt,121,4.5 Obliqueness Each token in both sides of a bi text is characterized by a position index_comma_ computed as the ratio between the relative position in the sentence and the length of the sentence.
E06-1020.txt,122,The absolute value of the difference between tokens position indexes_comma_ subtracted from 15_comma_ gives the link s obliqueness .  1 _comma_ TSji Sentlength j Sentlength iTWSWOBL This feature is context free as opposed to the locality feature described below. 4.6 Locality Locality is a feature that estimates the degree to which the links are sticking together. MEBA has three features to account for locality i weak locality_comma_ ii chunk based locality and iii dependency based locality.
E06-1020.txt,123,The value of the weak locality feature is derived from the already existing alignments in a window of N tokens centred on the focused token.
E06-1020.txt,124,The window size is variable_comma_ proportional to the sentence length.
E06-1020.txt,125,If in the window there exist k linked tokens and the relative positions of the 5 This is to ensure that values close to 1 are good ones and those near 0 are bad .
E06-1020.txt,126,This definition takes into account the relatively similar word order in English and Romanian. tokens in these links are i1 j1 _comma_ ik jk then the locality feature of the new link ik 1_comma_ jk 1 is defined by the equation below  1_comma_1min 1 1 1  k m mk mk jj ii kLOC If the new link starts from or ends in a token already linked_comma_ the index difference that would be null in the formula above is set to 1. This way_comma_ such candidate links would be given support by the LOC feature and avoid overflow error .
E06-1020.txt,127,In the case of chunk based locality the window span is given by the indexes of the first and last tokens of the chunk.
E06-1020.txt,128,Dependency based locality uses the set of the dependency links of the tokens in a candidate link for the computation of the feature value.
E06-1020.txt,129,In this case_comma_ the LOC feature of a candidate link ik 1_comma_ jk 1 is set to 1 or 0 according to the following rule if between ik 1 and i D there is a source language dependency and if between jk 1 and j E there is also a target language dependency then LOC is 1 if i D and j E are aligned_comma_ and 0 otherwise.
E06-1020.txt,130,Please note that in case jk 1 j E a trivial dependency identity is considered and the LOC attribute of the link ik 1_comma_ jk 1 is set to always to 1.
E06-1020.txt,131,Figure 4 Chunk and dependency based locality 4.7 Collocation Monolingual collocation is an important clue for word alignment.
E06-1020.txt,132,If a source collocation is translated by a multiword sequence_comma_ very often the lexical cohesion of source words can also be found in the corresponding translated words.
E06-1020.txt,133,In this case the aligner has strong evidence for 158 many to many linking.
E06-1020.txt,134,When a source collocation is translated as a single word_comma_ this feature is a strong indication for a many to 1 linking.
E06-1020.txt,135,Bi gram lists only content words were built from each monolingual part of the training corpus_comma_ using the log likelihood score threshold of 10 and minimal occurrence frequency 3 for candidates filtering.
E06-1020.txt,136,We used the bi grams list to annotate the chains of lexical dependencies among the contents words.
E06-1020.txt,137,Then_comma_ the value of the collocation feature is computed similar to the dependencybased locality feature.
E06-1020.txt,138,The algorithm searches for the links of the lexical dependencies around the candidate link.
E06-1020.txt,139,5 Combining the reified alignments From a given alignment one can compute a series of properties for each of its links such as the parameters used by the MEBA aligner .
E06-1020.txt,140,A link becomes this way a structured object that can be manipulated in various ways_comma_ independent of the bitext or even of the lexical tokens of the link from which it was extracted.
E06-1020.txt,141,We call this procedure alignment reification.
E06-1020.txt,142,The properties of the links of two or more alignments are used for our methods of combining the alignments.
E06-1020.txt,143,One simple_comma_ but very effective method of alignment combination is a heuristic procedure_comma_ which merges the alignments produced by two or more word aligners and filters out the links that are likely to be wrong.
E06-1020.txt,144,For the purpose of filtering_comma_ a link is characterized by its type defined by the pair of indexes i_comma_j and the POS of the tokens of the respective link.
E06-1020.txt,145,The likelihood of a link is proportional to the POS affinities of the tokens of the link and inverse proportional to the bounded relative positions BRP of the respective tokens where avg is the average displacement in a Gold Standard of the aligned tokens with the same POSes as the tokens of the current link.
E06-1020.txt,146,From the same gold standard we estimated a threshold below which a link is removed from the final alignment.
E06-1020.txt,147,1 avgjiBRP A more elaborated alignment combination with better results than the previous one is modelled as a binary statistical classification problem good bad and_comma_ as in the case of the previous method_comma_ the net result is the removal of the links which are likely to be wrong.
E06-1020.txt,148,We used an off the shelf solution for SVM training and classification LIBSVM6 Fan et al._comma_ 2005 with 6 http www.csie.ntu.edu.tw cjlin libsvm the default parameters C SVC classification and radial basis kernel function .
E06-1020.txt,149,Both context independent and context dependent features characterizing the links were used for training.
E06-1020.txt,150,The classifier was trained with both positive and negative examples of links.
E06-1020.txt,151,A set of links extracted from the Gold Standard alignment was used as positive examples set.
E06-1020.txt,152,The same number of negative examples was extracted from the alignments produced by COWAL and MEBA where they differ from the Gold Standard.
E06-1020.txt,153,It is interesting to notice that for the example discussed in Figures 1 4_comma_ the first combiner didn t eliminate the link you ve i producing the result shown in Figure 4.
E06-1020.txt,154,This is because the relative positions of the two words are the same and the POS affinity of the English personal pronouns and the Romanian auxiliaries is significant.
E06-1020.txt,155,On the other hand_comma_ the SVM based combiner deleted this link_comma_ producing the result shown in Figure 3.
E06-1020.txt,156,The explanation is that_comma_ according to the Gold Standard we used_comma_ the links between English pronouns and Romanian auxiliaries or main verbs in pro drop constructions were systematically dismissed although we claim that they shouldn t and that the alignment in Figure 4 is better than the one in Figure 3 .
E06-1020.txt,157,The evaluation according to the Gold Standard of the SVM based combination COWAL _comma_ compared with the individual aligners_comma_ is shown in Table 3.
E06-1020.txt,158,Aligner Precision Recall F measure YAWA 88.80 74.83 81.22 MEBA 92.05 71.00 80.17 COWAL 86.99 79.91 83.30 Table 3 Combined alignment 6 Conclusions and further work Neither YAWA nor MEBA needs an a priori bilingual dictionary_comma_ as this will be automatically extracted by TREQ or GIZA .
E06-1020.txt,159,We made evaluation of the individual alignments in both experimental settings without a start up bilingual lexicon and with an initial mid sized bilingual lexicon.
E06-1020.txt,160,Surprisingly enough_comma_ we found that while the performance of YAWA increases a little bit approx. 1 increase of the F measure MEBA is doing better without an additional lexicon.
E06-1020.txt,161,Therefore_comma_ in the evaluation presented in the previous section MEBA uses only the training data vocabulary.
E06-1020.txt,162,YAWA is very sensitive to the quality of the bilingual lexicons it uses.
E06-1020.txt,163,We used automatically translation lexicons with or without a seed lexi159 con _comma_ and the noise inherently present might have had a bad influence on YAWA s precision.
E06-1020.txt,164,Replacing the TREQ generated bilingual lexicons with validated reference bilingual lexicons would further improve the overall performance of this aligner.
E06-1020.txt,165,Yet_comma_ this might be a harder to meet condition for some pairs of languages than using parallel corpora.
E06-1020.txt,166,MEBA is more versatile as it does not require a priori bilingual lexicons but_comma_ on the other hand_comma_ it is very sensitive to the values of the parameters that control its behaviour.
E06-1020.txt,167,Currently they are set according to the developers intuition and after the analysis of the results from several trials.
E06-1020.txt,168,Since this activity is pretty time consuming human analysis plus re training might take a couple of hours we plan to extend MEBA with a supervised learning module_comma_ which would automatically determine the optimal parameters thresholds and weights values.
E06-1020.txt,169,It is worth noticing that with the current versions of our basic aligners_comma_ significantly improved since the ACL shared word alignment task in June 2005_comma_ YAWA is now doing better than MEBA_comma_ and the COWAL F measure increased with 9.4 .
E06-1020.txt,170,However_comma_ as mentioned before_comma_ these performances were measured on a different tokenization of the evaluation texts and on the partially corrected gold standard alignment see footnote 1 . .
E06-2023.txt,1,paper reports the present results of a research on unsupervised Persian morpheme discovery. In this paper we present a method for discovering the morphemes of Persian language through automatic analysis of corpora.
E06-2023.txt,2,We utilized a Minimum Description Length MDL based algorithm with some improvements and applied it to Persian corpus.
E06-2023.txt,3,Our improvements include enhancing the cost function using some heuristics_comma_ preventing the split of high frequency chunks_comma_ exploiting penalty for first and last letters and distinguishing pre parts and post parts.
E06-2023.txt,4,Our improved approach has raised the precision_comma_ recall and f measure of discovery by respectively 32_comma_ 17 and 23.
E06-2023.txt,6,According to linguistic theory_comma_ morphemes are considered to be the smallest meaning bearing elements of a language.
E06-2023.txt,7,However_comma_ no adequate language independent definition of the word as a unit has been agreed upon.
E06-2023.txt,8,If effective methods can be devised for the unsupervised discovery of morphemes_comma_ they could aid the formulation of a linguistic theory of morphology for a new language.
E06-2023.txt,9,The utilization of morphemes as basic representational units in a statistical language model instead of words seems a promising course Creutz_comma_ 2004 .
E06-2023.txt,10,Many natural language processing tasks_comma_ including parsing_comma_ semantic modeling_comma_ information retrieval_comma_ and machine translation_comma_ frequently require a morphological analysis of the language at hand.
E06-2023.txt,11,The task of a morphological analyzer is to identify the lexeme_comma_ citation form_comma_ or inflection class of surface word forms in a language.
E06-2023.txt,12,It seems that even approximate automated morphological analysis would be beneficial for many NL applications dealing with large vocabularies e.g. text retrieval applications .
E06-2023.txt,13,On the other hand_comma_ the construction of a comprehensive morphological analyzer for a language based on linguistic theory requires a considerable amount of work by experts.
E06-2023.txt,14,This is both slow and expensive and therefore not applicable to all languages.
E06-2023.txt,15,Consequently_comma_ it is important to develop methods that are able to discover and induce morphology for a language based on unsupervised analysis of large amounts of data.
E06-2023.txt,16,Persian is the most spoken of the modern Iranian languages_comma_ which_comma_ according to traditional classification_comma_ with the Indo Aryan language constitute the Indo Iranian group within the Satem branch of the Indo European family.
E06-2023.txt,17,Persian is written right to left in the Arabic alphabet with a few modifications.
E06-2023.txt,18,Three of 32 Persian letters do double duty in representing both consonant and vowels h _comma_ v _comma_ y _comma_ doubling_comma_ as e word finally _comma_ u _comma_ and I respectively Mahootian 97 .
E06-2023.txt,19,Persian morphology is an affixal system consisting mainly of suffixes and a few prefixes.
E06-2023.txt,20,The nominal paradigm consists of a relatively small number of affixes Megerdoomian 2000 .
E06-2023.txt,21,The verbal inflectional system is quite regular and can be obtained by the combination of prefixes_comma_ stems_comma_ inflections and auxiliaries.
E06-2023.txt,22,Persian morphologically is a powerful language and there are a lot of morphological rules in it.
E06-2023.txt,23,For example we can derive more than 200 words from the stem of the verb raftan to go .
E06-2023.txt,24,Table 1 shows some morphological rules and table 2 illustrates some inflections and derivations as examples.
E06-2023.txt,25,There is no morphological irregularity in Persian and all of the words are stems or derived words_comma_ except some imported foreign words_comma_ that are not compatible with Persian rules such as irregular Arabic plural forms imported to Persian. simple past verb past stem identifier continuous present verb Mi present stem identifier Noun present stem  y e Table 1.
E06-2023.txt,26,Some Persian morphological rules.
E06-2023.txt,27,175 POS Persian Translation Verb Infinitive Nega t n to write Present Verb Stem Negar Write Past Verb Stem Nega t wrote Continuous Present verb mi negar m I am writing Simple Past verb nega t m I wrote Noun from verb Neg re Writing Table 2.
E06-2023.txt,28,Some example words.
E06-2023.txt,29,2 Related Works There are several approaches for inducing morphemes from text.
E06-2023.txt,30,Some of them are supervised and use some information about words such as part of speech POS tags_comma_ morphological rules_comma_ suffix list_comma_ lexicon_comma_ etc. Other approaches are unsupervised and use only raw corpus to extract morphemes.
E06-2023.txt,31,In this section we concentrate on some unsupervised methods as related works.
E06-2023.txt,32,Monson 2004 presents a framework for unsupervised induction of natural language morphology_comma_ wherein candidate suffixes are grouped into candidate inflection classes_comma_ which are then placed in a lattice structure.
E06-2023.txt,33,With similar arranged inflection classes placed near one candidate in the lattice_comma_ it proposes this structure to be an ideal search space in which to isolate the true inflection classes of a language.
E06-2023.txt,34,Schone and Jurafsky 2000 presents an unsupervised model in which knowledge free distributional cues are combined orthography based with information automatically extracted from semantic word cooccurrence patterns in the input corpus.
E06-2023.txt,35,Word induction from natural language text without word boundaries is also studied in Deligne and Bimtol 1997 _comma_ where MDL based model optimization measures are used.
E06-2023.txt,36,Viterbi or the forward backward algorithm an EM algorithm is used for improving the segmentation of the corpus.
E06-2023.txt,37,Some of the approaches remove spaces from text and try to identify word boundaries utilizing e.g. entropy based measures_comma_ as in Zellig and Harris_comma_ 1967 Redlich_comma_ 1993 .
E06-2023.txt,38,Brent_comma_ 1999 presents a general_comma_ modular probabilistic model structure for word discovery.
E06-2023.txt,39,He uses a minimum representation length criterion for model optimization and applies an incremental_comma_ greedy search algorithm which is suitable for on line learning such that children might employ.
E06-2023.txt,40,Baroni_comma_ et al. 2002 proposes an algorithm that takes an unannotated corpus as its input_comma_ and a ranked list of probable returning related pairs as its output.
E06-2023.txt,41,It discovers related pairs by looking morphologically for pairs that are both orthographically and semantically similar.
E06-2023.txt,42,Goldsmith 2001 concentrates on stem suffixlanguages_comma_ in particular Indo European languages_comma_ and produces output that would match as closely as possible with the analysis given by a human morphologist.
E06-2023.txt,43,He further assumes that stems form groups that he calls signatures_comma_ and each signature shares a set of possible affixes.
E06-2023.txt,44,He applies an MDL criterion for model optimization.
E06-2023.txt,45,3 Inducing Persian Morphemes Our task is to find the correct segmentation of the source text into morphemes while we don t have any information about words or any structural rules to make them.
E06-2023.txt,46,So we use an algorithm that works based on minimization of some heuristic cost function.
E06-2023.txt,47,Our approach is based on a variation of MDL model and contains some modifications to adopt it for Persian and improve the results especially for this language.
E06-2023.txt,48,Minimum Description Length MDL analysis is based on information theory Rissanen 1989 .
E06-2023.txt,49,Given a corpus_comma_ an MDL model defines a description length of the corpus.
E06-2023.txt,50,Given a probabilistic model of the corpus_comma_ the description length is the sum of the most compact statement of the model expressible in some universal language of algorithms_comma_ plus the length of the optimal compression of the corpus_comma_ when we use the probabilistic model to compress the data.
E06-2023.txt,51,The length of the optimal compression of the corpus is the base 2 logarithm of the reciprocal of the probability assigned to the corpus by the model.
E06-2023.txt,52,Since we are concerned with morphological analysis_comma_ we will henceforth use the more specific term the morphology rather than model.
E06-2023.txt,53,1  log log _comma_ 22 MCpMp MModelCCorpusnLengthDescriptio MDL analysis proposes that the morphology M which minimizes the objective function in 1 is the best morphology of the corpus.
E06-2023.txt,54,Intuitively_comma_ the first term the length of the model_comma_ in bits expresses the conciseness of the morphology_comma_ giving us strong motivation to find the simplest possible morphology_comma_ while the second term expresses how well the model describes the corpus in question.
E06-2023.txt,55,The method proposed at Creutz 2002 2004 is a derivation of MDL algorithm which we use as the basis of our approach.
E06-2023.txt,56,In this algorithm_comma_ each time a new word token is read from the input_comma_ different ways of segmenting it into morphs are evaluated_comma_ and the one with minimum cost is selected.
E06-2023.txt,57,First_comma_ the word as a whole is considered to 176 be a morph and added to the morph list.
E06-2023.txt,58,Then_comma_ every possible splits of the word into two parts are evaluated.
E06-2023.txt,59,The algorithm selects the split or no split that yields the minimum total cost.
E06-2023.txt,60,In case of no split_comma_ the processing of the word is finished and the next word is read from input.
E06-2023.txt,61,Otherwise_comma_ the search for a split is performed recursively on the two segments.
E06-2023.txt,62,The order of splits can be represented as a binary tree for each word_comma_ where the leaves represent the morphs making up the word_comma_ and the tree structure describes the ordering of the splits.
E06-2023.txt,63,During model search_comma_ an overall hierarchical data structure is used for keeping track of the current segmentation of every word type encountered so far.
E06-2023.txt,64,There is an occurrence counter field for each morph in morph list.
E06-2023.txt,65,The occurrence counts from segments flow down through the hierarchical structure_comma_ so that the count of a child always equals the sum of the counts of its parents.
E06-2023.txt,66,The occurrence counts of the leaf nodes are used for computing the relative frequencies of the morphs.
E06-2023.txt,67,To find out the morph sequence that a word consists of_comma_ we look up the chunk that is identical to the word_comma_ and trace the split indices recursively until we reach the leaves_comma_ which are the morphs.
E06-2023.txt,68,This algorithm was applied on Persian corpus and results were not satisfiable.
E06-2023.txt,69,So we gradually_comma_ applied some heuristic functions to get better results.
E06-2023.txt,70,Our approach contains 1 Utilizing a heuristic function to compute cost more precisely_comma_ 2 Using Threshold to prevent splitting high frequency chunks_comma_ 3 Exerting Penalty for first and last letters and 4 Distinguishing Pre parts and post parts.
E06-2023.txt,71,After analyzing the results of the initial algorithm_comma_ we observed that the algorithm tries to split words into some morphemes to keep the cost minimum based on current morph list so recognized morphemes may prevent extracting new correct morphemes.
E06-2023.txt,72,Therefore we applied a new reward function to find the best splitting with respect to the next words.
E06-2023.txt,73,In fact our function equation 2 rewards to the morphemes that are used in next words frequently.
E06-2023.txt,74,2   1  WNLPlenLPfreqRF CWNRPlenRPfreq  1  In which LP is the left part of word_comma_ RP is the right part of it_comma_ Len p is the length of part P number of characters _comma_ freq p is the frequency of part P in corpus_comma_ WN is the number of words corpus size and C is a constant number.
E06-2023.txt,75,In this cost function freq LP WN can be interpreted as the probability of LP being a morph in the corpus.
E06-2023.txt,76,We use len P to increase the reward for long segments that are frequent and it is decreased by 1 to avoid mono letter splitting.
E06-2023.txt,77,We found the parameter C empirically.
E06-2023.txt,78,Figure 1 shows the results of the algorithm for various amounts of C.
E06-2023.txt,79,40 50 60 70 1 2 3 4 5 6 7 8 9 10 Recall Precision f measure Figure 1.
E06-2023.txt,80,Algorithm results for various Cs.
E06-2023.txt,81,Our experiments showed that the best value for C is 8.
E06-2023.txt,82,It means that RP is 8 times more important that LP.
E06-2023.txt,83,This may be because of the fact that Persian is written right to left and moreover most of affixes are suffixes.
E06-2023.txt,84,The final cost function in our algorithm is shown in equation 3 .
E06-2023.txt,85,3 RFEF  In which E is the description length_comma_ calculated in equation 1 and RF the cost function described in equation 2 .
E06-2023.txt,86,Since RF values are in a limited range_comma_ they are large numbers in comparison with other function values in the first iterations_comma_ but after processing some words_comma_ cost function values will become large so that the RF is not significant any more.
E06-2023.txt,87,So we used the difference of cost function in two sequential processes two iterations instead of the cost function itself.
E06-2023.txt,88,In other words in our algorithm the cost function E is re evaluated and replaced with its changes E .
E06-2023.txt,89,This improvement causes better splitting in some words such as the words shown in table 3.
E06-2023.txt,90,Each word is shown by its written form in English alphabet its pronunciation its translation . word Initial alg.
E06-2023.txt,91,Improved alg. n en sand n n nva en va that can hear n va nv hear a subjective adjective sign mi nvm mi en v m I hear mi continuous tense sign  n v m mi nv m first person pronoun Table 3.
E06-2023.txt,92,Comparing the results of the initial and improved algorithm.
E06-2023.txt,93,We also used a frequency threshold T to avoid splitting words that are observed as a substring in other words.
E06-2023.txt,94,It means that in the current algorithm_comma_ for each word we first compute its frequency and it will be splitted just when it is used 177 less than the threshold.
E06-2023.txt,95,Based on our experiments_comma_ the best value for T is 4.One of the most wrong splitting is mono letter splitting which means that we split just the first or the last letter to be a morpheme.
E06-2023.txt,96,Our experiments show that the first letter splitting occurs more than the last letter.
E06-2023.txt,97,So we apply a penalty factor on splitting in these positions to avoid creating mono letter morphemes.
E06-2023.txt,98,Another improvement is that we distinguished between pre part and post part.
E06-2023.txt,99,So splitting based on observed morphemes will become more precise.
E06-2023.txt,100,In this process each morpheme that is observed at the left corner of a word_comma_ in the first splitting phase_comma_ is post part and each of them at the right corner of a word is pre part.
E06-2023.txt,101,Other morphemes are added to both pre and post part lists.
E06-2023.txt,102,4 Experimental Results We applied improved algorithm on Persian corpus and observed significant improvements on our results.
E06-2023.txt,103,Our corpus contains about 4000 words from which 100 are selected randomly for tests.
E06-2023.txt,104,We split selected words to their morphemes both manually and automatically and computed precision and recall factors.
E06-2023.txt,105,For computing recall and precision_comma_ we numerated splitting positions and compared with the gold data.
E06-2023.txt,106,Precision is the number of correct splits divided to the total number of splits done and recall is the number of correct splits divided by total number of gold splits.
E06-2023.txt,107,Our experiments showed that our approach results in increasing the recall measure from 45.53 to 53.19_comma_ the precision from 48.24 to 63.29 and fmeasure from 46.91 to 57.80.
E06-2023.txt,108,Precision improvement is significantly more than recall.
E06-2023.txt,109,This has been predictable as we make algorithm to prevent unsure splitting.
E06-2023.txt,110,So usually done splits are correct whereas there are some necessary splitting that have not been done.
E06-2023.txt,111,5 Conclusion In this paper we proposed an improved approach for morpheme discovery from Persian texts.
E06-2023.txt,112,Our algorithm is an improvement of an existing algorithm based on MDL model.
E06-2023.txt,113,The improvements are done by adding some heuristic functions to the split procedure and also introducing new cost and reward functions.
E06-2023.txt,114,Experiments showed very good results obtained by our improvements.
E06-2023.txt,115,The main problems for our experiments were the lack of good_comma_ safe and large corpora and also handling the foreign words which do not obey the morphological rules of Persian.
E06-2023.txt,116,Our proposed improvements are rarely languagedependent such as right to left feature of Persian and could be applied to other languages with a little customization.
E06-2023.txt,117,To extend the project we suppose to work on some probabilistic distribution functions which help to split words correctly.
E06-2023.txt,118,Moreover we plan to test our algorithm on large Persian and also English corpora. .
E06-1030.txt,1,text has been successfully used as training data for many NLP applications. While most previous work accesses web text through search engine hit counts_comma_ we created a Web Corpus by downloading web pages to create a topic diverse collection of 10 billion words of English.
E06-1030.txt,2,We show that for context sensitive spelling correction the Web Corpus results are better than using a search engine.
E06-1030.txt,3,For thesaurus extraction_comma_ it achieved similar overall results to a corpus of newspaper text.
E06-1030.txt,4,With many more words available on the web_comma_ better results can be obtained by collecting much larger web corpora.
E06-1030.txt,6,Traditional written corpora for linguistics research are created primarily from printed text_comma_ such as newspaper articles and books.
E06-1030.txt,7,With the growth of the World Wide Web as an information resource_comma_ it is increasingly being used as training data in Natural Language Processing NLP tasks.
E06-1030.txt,8,There are many advantages to creating a corpus from web data rather than printed text.
E06-1030.txt,9,All web data is already in electronic form and therefore readable by computers_comma_ whereas not all printed data is available electronically.
E06-1030.txt,10,The vast amount of text available on the web is a major advantage_comma_ with Keller and Lapata 2003 estimating that over 98 billion words were indexed by Google in 2003.
E06-1030.txt,11,The performance of NLP systems tends to improve with increasing amount of training data.
E06-1030.txt,12,Banko and Brill 2001 showed that for contextsensitive spelling correction_comma_ increasing the training data size increases the accuracy_comma_ for up to 1 billion words in their experiments.
E06-1030.txt,13,To date_comma_ most NLP tasks that have utilised web data have accessed it through search engines_comma_ using only the hit counts or examining a limited number of results pages.
E06-1030.txt,14,The tasks are reduced to determining n gram probabilities which are then estimated by hit counts from search engine queries.
E06-1030.txt,15,This method only gathers information from the hit counts but does not require the computationally expensive downloading of actual text for analysis.
E06-1030.txt,16,Unfortunately search engines were not designed for NLP research and the reported hit counts are subject to uncontrolled variations and approximations Nakov and Hearst_comma_ 2005 .
E06-1030.txt,17,Volk 2002 proposed a linguistic search engine to extract word relationships more accurately.
E06-1030.txt,18,We created a 10 billion word topic diverse Web Corpus by spidering websites from a set of seed URLs.
E06-1030.txt,19,The seed set is selected from the Open Directory to ensure that a diverse range of topics is included in the corpus.
E06-1030.txt,20,A process of text cleaning transforms the HTML text into a form useable by most NLP systems tokenised words_comma_ one sentence per line.
E06-1030.txt,21,Text filtering removes unwanted text from the corpus_comma_ such as non English sentences and most lines of text that are not grammatical sentences.
E06-1030.txt,22,We compare the vocabulary of the Web Corpus with newswire.
E06-1030.txt,23,Our Web Corpus is evaluated on two NLP tasks.
E06-1030.txt,24,Context sensitive spelling correction is a disambiguation problem_comma_ where the correction word in a confusion set e.g. their_comma_ they re needs to be selected for a given context.
E06-1030.txt,25,Thesaurus extraction is a similarity task_comma_ where synonyms of a target word are extracted from a corpus of unlabelled text.
E06-1030.txt,26,Our evaluation demonstrates that web text can be used for the same tasks as search engine hit counts and newspaper text.
E06-1030.txt,27,However_comma_ there is a much larger quantity of freely available web text to exploit.
E06-1030.txt,28,233 2 Existing Web Corpora The web has become an indispensible resource withavastamountofinformationavailable.
E06-1030.txt,29,Many NLP tasks have successfully utilised web data_comma_ including machine translation Grefenstette_comma_ 1999 _comma_ prepositional phrase attachment Volk_comma_ 2001 _comma_ and other anaphora resolution Modjeska et al._comma_ 2003 .
E06-1030.txt,30,2.1 Search Engine Hit Counts Most NLP systems that have used the web access it via search engines such as Altavista and Google.
E06-1030.txt,31,N gram counts are approximated by literal queries w1 ... wn .
E06-1030.txt,32,Relations between two words are approximated in Altavista by the NEAR operator whichlocateswordpairswithin10tokensofeach other .
E06-1030.txt,33,The overall coverage of the queries can be expanded by morphological expansion of the search terms.
E06-1030.txt,34,Keller and Lapata 2003 demonstrated a high degree of correlation between n gram estimates fromsearch enginehitcountsand n gramfrequencies obtained from traditional corpora such as the British National Corpus BNC .
E06-1030.txt,35,The hit counts also had a higher correlation to human plausibility judgements than the BNC counts.
E06-1030.txt,36,The web count method contrasts with traditional methods where the frequencies are obtained from a corpus of locally available text.
E06-1030.txt,37,While the corpus is much smaller than the web_comma_ an accurate count and further text processing is possible because all of the contexts are readily accessible.
E06-1030.txt,38,The web count method obtains only an approximate number of matches on the web_comma_ with no control over which pages are indexed by the search engines and with no further analysis possible.
E06-1030.txt,39,There are a number of limitations in the search engine approximations.
E06-1030.txt,40,As many search engines discard punctuation information especially when using the NEAR operator _comma_ words considered adjacent to each other could actually lie in different sentences or paragraphs.
E06-1030.txt,41,For example in Volk 2001 _comma_ the system assumes that a preposition attaches to a noun simply when the noun appears within a fixed context window of the preposition.
E06-1030.txt,42,The preposition and noun could in fact be related differently or be in different sentences altogether.
E06-1030.txt,43,The speed of querying search engines is another concern.
E06-1030.txt,44,Keller and Lapata 2003 needed to obtain the frequency counts of 26_comma_271 test adjective pairs from the web and from the BNC for the task of prenominal adjective ordering.
E06-1030.txt,45,While extracting this information from the BNC presented no difficulty_comma_ making so many queries to the Altavista was too time consuming.
E06-1030.txt,46,They had to reduce the size of the test set to obtain a result.
E06-1030.txt,47,Lapata and Keller 2005 performed a wide range of NLP tasks using web data by querying Altavista and Google.
E06-1030.txt,48,This included variety of generation tasks e.g. machine translation candidate selection and analysis tasks e.g. prepositional phrase attachment_comma_ countability detection .
E06-1030.txt,49,They showed that while web counts usually outperformed BNC counts and consistently outperformed the baseline_comma_ the best performing system is usually a supervised method trained on annotated data.
E06-1030.txt,50,Keller and Lapata concluded that having access linguistic information accurate n gram counts_comma_ POS tags_comma_ and parses outperforms using a large amount of web data.
E06-1030.txt,51,2.2 Spidered Web Corpora Afewprojectshaveutiliseddatadownloadedfrom the web.
E06-1030.txt,52,Ravichandran et al. 2005 used a collection of 31 million web pages to produce noun similarity lists.
E06-1030.txt,53,They found that most NLP algorithms are unable to run on web scale data_comma_ especially those with quadratic running time.
E06-1030.txt,54,Halacsy et al. 2004 created a Hungarian corpus from the web by downloading text from the .hu domain.
E06-1030.txt,55,From a 18 million page crawl of the web a 1 billion word corpus is created removing duplicates and non Hungarian text .
E06-1030.txt,56,A terabyte sized corpus of the web was collected at the University of Waterloo in 2001.
E06-1030.txt,57,A breadth first search from a seed set of university home pages yielded over 53 billion words_comma_ requiring 960GB of storage.
E06-1030.txt,58,Clarke et al. 2002 and Terra and Clarke 2003 used this corpus for their question answering system.
E06-1030.txt,59,They obtained increasing performance with increasing corpus size but began reaching asymptotic behaviour at the 300 500GB range.
E06-1030.txt,60,3 Creating the Web Corpus There are many challenges in creating a web corpus_comma_ as the World Wide Web is unstructured and without a definitive directory.
E06-1030.txt,61,No simple method exists to collect a large representative sample of the web.
E06-1030.txt,62,Two main approaches exist for collecting representative web samples IP address sampling and random walks.
E06-1030.txt,63,The IP address sampling technique randomly generates IP addresses 234 and explores any websites found Lawrence and Giles_comma_ 1999 .
E06-1030.txt,64,This method requires substantial resources as many attempts are made for each website found.
E06-1030.txt,65,Lawrence and Giles reported that 1 in 269 tries found a web server.
E06-1030.txt,66,Random walk techniques attempt to simulate a regular undirected web graph Henzinger et al._comma_ 2000 .
E06-1030.txt,67,In such a graph_comma_ a random walk would produce a uniform sample of the nodes i.e. the web pages .
E06-1030.txt,68,However_comma_ only an approximation of such a graph is possible_comma_ as the web is directed i.e. you cannot easily determine all web pages linking to a particular page .
E06-1030.txt,69,Most implementations of random walks approximates the number of backward links by using information from search engines.
E06-1030.txt,70,3.1 Web Spidering We created a 10 billion word Web Corpus by spidering the web.
E06-1030.txt,71,While the corpus is not designed to be a representative sample of the web_comma_ we attempt to sample a topic diverse collection of web sites.
E06-1030.txt,72,Our web spider is seeded with links from the Open Directory1.
E06-1030.txt,73,The Open Directory has a broad coverage of many topics on the web and allows us to create a topic diverse collection of pages.
E06-1030.txt,74,Before the directory can be use_comma_ we had to address several coverage skews.
E06-1030.txt,75,Some topics have many more links in the Open Directory than others_comma_ simply due to the availability of editors for different topics.
E06-1030.txt,76,For example_comma_ we found that the topic University of Connecticut has roughly the same number of links as Ontario Universities.
E06-1030.txt,77,We would normally expect universities in a whole province of Canada to have more coverage than a single university in the United States.
E06-1030.txt,78,The directory was also constructed without keeping more general topics higher in the tree.
E06-1030.txt,79,For example_comma_ we found that Chicken Salad is higher in the hierarchy than Catholicism.
E06-1030.txt,80,The Open Directory is flattened by a rule based algorithm which is designed to take into account the coverage skews of some topics to produce a list of 358 general topics.
E06-1030.txt,81,From the seed URLs_comma_ the spider performs a breadth first search.
E06-1030.txt,82,It randomly selects a topic nodefromthelistandnextunvisitedURLfromthe node.
E06-1030.txt,83,It visits the website associated from the link and samples pages within the same section of the website until a minimum number of words have been collected or all of the pages were visited.
E06-1030.txt,84,1The Open Directory Project_comma_ http www.dmoz.org External links encountered during this process are added to the link collection of the topic node regardless of the actual topic of the link.
E06-1030.txt,85,Although websites of one topic tends to link to other websites of the same topic_comma_ this process contributes to a topic drift.
E06-1030.txt,86,As the spider traverses away from the original seed URLs_comma_ we are less certain of the topic included in the collection.
E06-1030.txt,87,3.2 Text Cleaning Text cleaning is the term we used to describe the overall process of converting raw HTML found on the web into a form useable by NLP algorithms white space delimited words_comma_ separated into one sentence per line.
E06-1030.txt,88,It consists of many low level processes which are often accomplished by simple rule based scripts.
E06-1030.txt,89,Our text cleaning process is divided into four major steps.
E06-1030.txt,90,First_comma_ different character encoding of HTML pages are transform into ISO Latin 1 and HTML named entities e.g. nbsp and amp translated into their single character equivalents.
E06-1030.txt,91,Second_comma_ sentence boundaries are marked.
E06-1030.txt,92,Such boundaries are difficult to identify on web text as it does not always consists of grammatical sentences.
E06-1030.txt,93,A section of a web page may be mathematical equations or lines of C code.
E06-1030.txt,94,Grammatical sentences need to be separated from each other and from other non sentence text.
E06-1030.txt,95,Sentence boundary detection for web text is a much harder problem than newspaper text.
E06-1030.txt,96,Weuseamachinelearningapproachtoidentifying sentence boundaries.
E06-1030.txt,97,We trained a Maximum Entropy classifier following Ratnaparkhi 1998 to disambiguate sentence boundary on web text_comma_ training on 153 manually marked web pages.
E06-1030.txt,98,Systems for newspaper text only use regular text features_comma_ such as words and punctuations.
E06-1030.txt,99,Our system for web text uses HTML tag features in addition to regular text features.
E06-1030.txt,100,HTML tag features are essential for marking sentence boundaries in web text_comma_ as many boundaries in web text are only indicated by HTML tags and not by the text.
E06-1030.txt,101,Our system using HTML tag features achieves 95.1 accuracy in disambiguating sentence boundaries in web text compared to 88.9 without using such features.
E06-1030.txt,102,Third_comma_ tokenisation is accomplished using the sed script used for the Penn Treebank project MacIntyre_comma_ 1995 _comma_ modified to correctly tokenise URLs_comma_ emails_comma_ and other web specific text.
E06-1030.txt,103,235 The final step is filtering_comma_ where unwanted text is removed from the corpus.
E06-1030.txt,104,A rule based component analyses each web page and each sentence within a page to identify sections that are unlikely to be useful text.
E06-1030.txt,105,Our rules are similar to those employed by Halacsy et al. 2004 _comma_ where the percentage of non dictionary words in a sentence or document helps identify non Hungarian text.
E06-1030.txt,106,We classify tokens into dictionary words_comma_ word like tokens_comma_ numbers_comma_ punctuation_comma_ and other tokens.
E06-1030.txt,107,Sentences or documents with too few dictionary words or too many numbers_comma_ punctuation_comma_ or other tokens are discarded.
E06-1030.txt,108,4 Corpus Statistics Comparing the vocabulary of the Web Corpus and existing corpora is revealing.
E06-1030.txt,109,We compared with the Gigaword Corpus_comma_ a 2 billion token collection 1.75 billion words before tokenisation of newspaper text Graff_comma_ 2003 .
E06-1030.txt,110,For example_comma_ what types of tokens appears more frequently on the web than in newspaper text
E06-1030.txt,111,From each corpus_comma_ we randomlyselecta1billionwordsampleandclassified the tokens into seven disjoint categories Numeric At least one digit and zero or more punctuation characters_comma_ e.g.
E06-1030.txt,112,2_comma_ 3.14_comma_ 5.50 Uppercase Only uppercase_comma_ e.g.
E06-1030.txt,113,REUTERS Title Case An uppercase letter followed by one or more lowercase letters_comma_ e.g.
E06-1030.txt,114,Dilbert Lowercase Only lowercase_comma_ e.g. violin Alphanumeric At least one alphabetic and one digit allowingforothercharacters _comma_e.g.
E06-1030.txt,115,B2B_comma_ mp3_comma_ RedHat 9 Hyphenated Word Alphabetic characters and hyphens_comma_ e.g. serb dominated_comma_ vis a vis Other Any other tokens 4.1 Token Type Analysis An analysis by token type shows big differences between the two corpora see Table 1 .
E06-1030.txt,116,The same size samples of the Gigaword and the Web Corpus have very different number of token types.
E06-1030.txt,117,Title case tokens is a significant percentage of the token types encountered in both corpora_comma_ possibly representing named entities in the text.
E06-1030.txt,118,There are also a significant number of tokens classified as others in the Web Corpus_comma_ possibly representing URLs and email addresses.
E06-1030.txt,119,While 2.2 million token types are found in the 1 billion word sample of the Gigaword_comma_ about twice as many 4.8 million are found in an equivalent sample of the Web Corpus.
E06-1030.txt,120,Gigaword Web Corpus Tokens 1 billion 1 billion Token Types 2.2 million 4.8 million Numeric 343k 15.6 374k 7.7 Uppercase 95k 4.3 241k 5.0 Title Case 645k 29.3 946k 19.6 Lowercase 263k 12.0 734k 15.2 Alpha 165k 7.6 417k 8.6 numeric Hyphenated 533k 24.3 970k 20.1 Other 150k 6.8 1_comma_146k 23.7 Table 1 Classification of corpus token by type Gigaword Web Corpus rreceive reeceive receieve recceive recesive recive receieve recieive recveive recive receivce receivve receiv receivee receve receivea receiv rceive reyceive 1.7 misspellings per 3.7 misspellings per dictionary word dictionary word 3.1m misspellings in 5.6m misspellings in 699m dict. words 669m dict. words Table 2 Misspellings of receive 4.2 Misspelling One factor contributing to the larger number of tokentypesintheWebCorpus_comma_ascomparedwiththe Gigaword_comma_ is the misspelling of words.
E06-1030.txt,121,Web documents are authored by people with a widely varying command of English and their pages are not as carefully edited as newspaper articles.
E06-1030.txt,122,Thus_comma_ we anticipate a significantly larger number of misspellings and typographical errors.
E06-1030.txt,123,We identify some of the misspellings by letter combinations that are one transformation away from a correctly spelled word.
E06-1030.txt,124,Consider a target word_comma_ correctly spelled.
E06-1030.txt,125,Misspellings can be generated by inserting_comma_ deleting_comma_ or substituting one letter_comma_ orbyreorderinganytwoadjacentletters although we keep the first letter of the original word_comma_ as very few misspellings change the first letter .
E06-1030.txt,126,Table 2 shows some of the misspellings of the word receive found in the Gigaword and the Web Corpus.
E06-1030.txt,127,While only 5 such misspellings were found in the Gigaword_comma_ 16 were found in the Web 236 Algorithm Training Testing AA WAA Unpruned Brown Brown 94.1 96.4 Winnow 80 20 Unpruned Brown WSJ 89.5 94.5 Winnow 80 40 Winnow Brown WSJ 93.1 96.6 Semi Sup.
E06-1030.txt,128,80 40 Search Altavista Brown 89.3 N A Engine 100 Table 3 Context sensitive spelling correction denotes also using 60 WSJ_comma_ 5 corrupted Corpus.
E06-1030.txt,129,For all words found in the Unix dictionary_comma_ an average of 1.7 misspellings are found per word in the Gigaword by type.
E06-1030.txt,130,The proportion of mistakes found in the Web Corpus is roughly double that of the Gigaword_comma_ at 3.7 misspellings per dictionary word.
E06-1030.txt,131,However_comma_ misspellings only represent a small portion of tokens 5.6 million out of 699 million instances of dictionary word are misspellings in the Web Corpus .
E06-1030.txt,132,5 Context Sensitive Spelling Correction A confusion set is a collection of words which are commonly misused by even native speakers of a language because of their similarity.
E06-1030.txt,133,For example_comma_ the words it s_comma_ its _comma_ affect_comma_ effect _comma_ and weather_comma_ whether are often mistakenly interchanged.
E06-1030.txt,134,Context sensitive spelling correction is the task of selecting the correct confusion word in a given context.
E06-1030.txt,135,Two different metrics have been used to evaluate the performance of contextsensitive spelling correction algorithms.
E06-1030.txt,136,The Average Accuracy AA is the performance by type whereas the Weighted Average Accuracy WAA is the performance by token.
E06-1030.txt,137,5.1 Related Work Golding and Roth 1999 used the Winnow multiplicative weight updating algorithm for contextsensitive spelling correction.
E06-1030.txt,138,They found that when a system is tested on text from a different from the training set the performance drops substantially see Table 3 .
E06-1030.txt,139,Using the same algorithm and 80 of the Brown Corpus_comma_ the WAA dropped from 96.4 to 94.5 when tested on 40 WSJ instead of 20 Brown.
E06-1030.txt,140,For cross corpus experiments_comma_ Golding and Roth devised a semi supervised algorithm that is trained on a fixed training set but also extracts information from the same corpus as the testing set.
E06-1030.txt,141,Their experiments showed that even if up to 20 of the testing set is corrupted using wrong confusion words _comma_ a system that trained on both the training and testing sets outperformed the system that only trained on the training set.
E06-1030.txt,142,The Winnow Semi Supervised method increases the WAA back up to 96.6 .
E06-1030.txt,143,Lapata and Keller 2005 utilised web counts from Altavista for confusion set disambiguation.
E06-1030.txt,144,Their unsupervised method uses collocation features one word to the left and right where co occurrence estimates are obtained from web counts of bigrams.
E06-1030.txt,145,This method achieves a stated accuracy of 89.3 AA_comma_ similar to the cross corpus experiment for Unpruned Winnow.
E06-1030.txt,146,5.2 Implementation Context sensitive spelling correction is an ideal task for unannotated web data as unmarked text is essentially labelled data for this particular task_comma_ as words in a reasonably well written text are positive examples of the correct usage of confusion words.
E06-1030.txt,147,To demonstrate the utility of a large collection of web data on a disambiguation problem_comma_ we implemented the simple memory based learner from Banko and Brill 2001 .
E06-1030.txt,148,The learner trains on simple collocation features_comma_ keeping a count of wi 1_comma_wi 1 _comma_ wi 1_comma_ and wi 1 for each confusion word wi.
E06-1030.txt,149,The classifier first chooses the confusion word which appears with the context bigram most frequently_comma_ followed by the left unigram_comma_ right unigram_comma_ and then the most frequent confusion word.
E06-1030.txt,150,Three data sets were used in the experiments the 2 billion word Gigaword Corpus_comma_ a 2 billion word sample of our 10 billion word Web Corpus_comma_ and the full 10 billion word Web Corpus.
E06-1030.txt,151,5.3 Results Our experiments compare the results when the three corpora were trained using the same algorithm.
E06-1030.txt,152,Thememory basedlearnerwastestedusing the 18 confusion word sets from Golding 1995 on the WSJ section of the Penn Treebank and the Brown Corpus.
E06-1030.txt,153,For the WSJ testing set_comma_ the 2 billion word Web Corpus does not achieve the performance of the Gigaword see Table 4 .
E06-1030.txt,154,However_comma_ the 10 billion word Web Corpus results approach that of the Gigaword.
E06-1030.txt,155,Training on the Gigaword and testing 237 Training Testing AA WAA Gigaword WSJ 93.7 96.1 2 billion 100 Web Corpus WSJ 92.7 94.1 2 billion 100 Web Corpus WSJ 93.3 95.1 10 billion 100 Gigaword Brown 90.7 94.6 2 billion 100 Web Corpus Brown 90.8 94.8 2 billion 100 Web Corpus Brown 91.8 95.4 10 billion 100 Table 4 Memory based learner results on WSJ is not considered a true cross corpus experiment_comma_ as the two corpora belong to the same genre of newspaper text.
E06-1030.txt,156,Compared to the Winnow method_comma_ the 10 billion word Web Corpus outperforms the cross corpus experiment but not the semi supervised method.
E06-1030.txt,157,For the Brown Corpus testing set_comma_ the 2 billion word Web Corpus and the 2 billion word Gigaword achieved similar results.
E06-1030.txt,158,The 10 billion word Web Corpus achieved 95.4 WAA_comma_ higher than the 94.6 from the 2 billion Gigaword.
E06-1030.txt,159,This and the above result with the WSJ suggests that the Web Corpus approach is comparable with training on a corpus of printed text such as the Gigaword.
E06-1030.txt,160,The 91.8 AA of the 10 billion word Web Corpus testing on the WSJ is better than the 89.3 AA achieved by Lapata and Keller 2005 using the Altavista search engine.
E06-1030.txt,161,This suggests that a web collected corpus may be a more accurate method of estimating n gram frequencies than through search engine hit counts.
E06-1030.txt,162,6 Thesaurus Extraction Thesaurusextractionisawordsimilaritytask.
E06-1030.txt,163,Itis a natural candidate for using web corpora as most systemsextractsynonymsofatargetwordfroman unlabelled corpus.
E06-1030.txt,164,Automatic thesaurus extraction is a good alternative to manual construction methods_comma_ as such thesauri can be updated more easily and quickly.
E06-1030.txt,165,They do not suffer from bias_comma_ low coverage_comma_ and inconsistency that human creators of thesauri introduce.
E06-1030.txt,166,Thesauri are useful in many NLP and Information Retrieval IR applications.
E06-1030.txt,167,Synonyms help expand the coverage of system but providing alternatives to the inputed search terms.
E06-1030.txt,168,For n gram estimation using search engine queries_comma_ some NLP applications can boost the hit count by offering alternative combination of terms.
E06-1030.txt,169,This is especially helpful if the initial hit counts are too low to be reliable.
E06-1030.txt,170,In IR applications_comma_ synonyms of search terms help identify more relevant documents.
E06-1030.txt,171,6.1 Method We use the thesaurus extraction system implemented in Curran 2004 .
E06-1030.txt,172,It operates on the distributional hypothesis that similar words appear in similar contexts.
E06-1030.txt,173,This system only extracts one word synonyms of nouns and not multi word expressions or synonyms of other parts of speech .
E06-1030.txt,174,The extraction process is divided into two parts.
E06-1030.txt,175,First_comma_ target nouns and their surrounding contexts are encoded in relation pairs.
E06-1030.txt,176,Six different types of relationships are considered Between a noun and a modifying adjective Between a noun and a noun modifier Between a verb and its subject Between a verb and its direct object Between a verb and its indirect object Between a noun and the head of a modifying prepositional phrase The nouns including subject and objects are the target headwords and the relationships are represented in context vectors.
E06-1030.txt,177,In the second stage of the extraction process_comma_ a comparison is made between context vectors of headwords in the corpus to determine the most similar terms.
E06-1030.txt,178,6.2 Evaluation The evaluation of a list of synonyms of a target word is subject to human judgement.
E06-1030.txt,179,We use the evaluation method of Curran 2004 _comma_ against gold standard thesauri results.
E06-1030.txt,180,The gold standard list is created by combining the terms found in four thesauri Macquarie_comma_ Moby_comma_ Oxford and Roget s.
E06-1030.txt,181,The inverse rank InvR metric allows a comparison to be made between the extracted rank list of synonyms and the unranked gold standard list.
E06-1030.txt,182,For example_comma_ if the extracted terms at ranks 3_comma_ 5_comma_ and 28 are found in the gold standard list_comma_ then InvR 13 15 128 0.569.
E06-1030.txt,183,238 Corpus INVR INVR MAX Gigaword 1.86 5.92 Web Corpus 1.81 5.92 Table 5 Average INVR for 300 headwords Word INVR Scores Diff.
E06-1030.txt,184,1 picture 3.322 to 0.568 2.754 2 star 2.380 to 0.119 2.261 3 program 3.218 to 1.184 2.034 4 aristocrat 2.056 to 0.031 2.025 5 box 3.194 to 1.265 1.929 6 cent 2.389 to 0.503 1.886 7 home 2.306 to 0.523 1.783 ... ... ... ... 296 game 1.097 to 2.799 1.702 297 bloke 0.425 to 2.445 2.020 298 point 1.477 to 3.540 2.063 299 walk 0.774 to 3.184 2.410 300 chain 0.224 to 3.139 2.915 Table 6 InvR scores ranked by difference_comma_ Gigaword to Web Corpus Gigaword 24 matches out of 200 house apartment building run office resident residence headquartersvictorynativeplacemansionroomtripmile family night hometown town win neighborhood life suburb school restaurant hotel store city street season area road homer day car shop hospital friend game farm facility center north child land weekend community loss return hour ... Web Corpus 18 matches out of 200 page loan contact house us owner search finance mortgage office map links building faq equity news center estate privacy community info business car site web improvement extention heating rate directory room apartment family service rental credit shop life city school property place location job online vacation store facility library free ... Table 7 Synonyms for home Gigaword 9 matches out of 200 store retailer supermarket restaurant outlet operator shop shelf owner grocery company hotel manufacturer retail franchise clerk maker discount business sale superstore brand clothing food giant shopping firm retailing industry drugstore distributor supplier bar insurer inc. conglomerate network unit apparel boutique mall electronics carrier division brokerage toy producer pharmacy airline inc ... Web Corpus 53 matches out of 200 necklace supply bracelet pendant rope belt ring earring gold bead silver pin wire cord reaction clasp jewelry charm frame bangle strap sterling loop timing plate metal collar turn hook arm length string retailer repair strand plug diamond wheel industry tube surface neck brooch store molecule ribbon pump choker shaft body ... Table 8 Synonyms for chain 6.3 Results We used the same 300 evaluation headwords as Curran 2004 andextractedthetop200synonyms for each headword.
E06-1030.txt,185,The evaluation headwords were extracted from two corpora for comparison a2billionwordsampleofourWebCorpusandthe 2 billion words in the Gigaword Corpus.
E06-1030.txt,186,Table 5 shows the average InvR scores over the 300 headwords for the two corpora one of web text and the other newspaper text.
E06-1030.txt,187,The InvR values differ by a negligible 0.05 out of a maximum of 5.92 .
E06-1030.txt,188,6.4 Analysis However on a per word basis one corpus can sigificantlyoutperformtheother.
E06-1030.txt,189,Table6ranksthe300 headwords by difference in the InvR score.
E06-1030.txt,190,While much better results were extracted for words like home from the Gigaword_comma_ much better results were extracted for words like chain from the Web Corpus.
E06-1030.txt,191,Table 7 shows the top 50 synoyms extracted for the headword home from the Gigaword and the WebCorpus.
E06-1030.txt,192,Whilesimilarnumberofcorrectsynonyms were extracted from both corpora_comma_ the Gigaword matches were higher in the extracted list and received a much higher InvR score.
E06-1030.txt,193,In the list extractedfromtheWebCorpus_comma_web relatedcollocations such as home page and search home appear.
E06-1030.txt,194,Table 8 shows the top 50 synoyms extracted for the headword chain from both corpora.
E06-1030.txt,195,While there are only a total of 9 matches from the Gigaword Corpus_comma_ there are 53 matches from the Web Corpus.
E06-1030.txt,196,A closer examination shows that the synonyms extracted from the Gigaword belong only to one sense of the word chain_comma_ as in chain stores.
E06-1030.txt,197,The gold standard list and the Web Corpus results both contain the necklace sense of the word chain.
E06-1030.txt,198,The Gigaword results show a skew towards the business sense of the word chain_comma_ while the Web Corpus covers both senses of the word.
E06-1030.txt,199,While individual words can achieve better results in either the Gigaword or the Web Corpus than the other_comma_ the aggregate results of synonym extractionforthe300headwordsarethesame.
E06-1030.txt,200,For this task_comma_ the Web Corpus can replace the Gigaword without affecting the overall result.
E06-1030.txt,201,However_comma_ as some words are perform better under different corpora_comma_ an aggregate of the Web Corpus and the Gigaword may produce the best result.
E06-1030.txt,202,239 7 Conclusion In this paper_comma_ the accuracy of natural language applicationtrainingona10billionwordWebCorpus is compared with other methods using search engine hit counts and corpora of printed text.
E06-1030.txt,203,In the context sensitive spelling correction task_comma_ a simple memory based learner trained on our Web Corpus achieved better results than method based on search engine queries.
E06-1030.txt,204,It also rival some of the state of the art systems_comma_ exceeding the accuracy of the Unpruned Winnow method the only other true cross corpus experiment .
E06-1030.txt,205,In the task of thesaurus extraction_comma_ the same overall results are obtained extracting from the Web Corpus as a traditional corpus of printed texts.
E06-1030.txt,206,The Web Corpus contrasts with other NLP approaches that access web data through search engine queries.
E06-1030.txt,207,Although the 10 billion words Web Corpus is smaller than the number of words indexed by search engines_comma_ better results have been achieved using the smaller collection.
E06-1030.txt,208,This is due to the more accurate n gram counts in the downloaded text.
E06-1030.txt,209,Other NLP tasks that require further analysis of the downloaded text_comma_ such a PP attachment_comma_ may benefit more from the Web Corpus.
E06-1030.txt,210,We have demonstrated that carefully collected and filtered web corpora can be as useful as newswire corpora of equivalent sizes.
E06-1030.txt,211,Using the same framework describe here_comma_ it is possible to collect a much larger corpus of freely available web text than our 10 billion word corpus.
E06-1030.txt,212,As NLP algorithms tend to perform better when more data is available_comma_ we expect state of the art results for many tasks will come from exploiting web text.
E06-1030.txt,213,Acknowledgements We like to thank our anonymous reviewers and the Language Technology Research Group at the University of Sydney for their comments.
E06-1030.txt,214,This work has been supported by the Australian Research Council under Discovery Project DP0453131. .
E06-2021.txt,1,paper deals with the problem of recognizing and extracting acronymdefinition pairs in Swedish medical texts. This project applies a rule based method to solve the acronym recognition task and compares and evaluates the results of different machine learning algorithms on the same task.
E06-2021.txt,2,The method proposed is based on the approach that acronym definition pairs follow a set of patterns and other regularities that can be usefully applied for the acronym identification task.
E06-2021.txt,3,Supervised machine learning was applied to monitor the performance of the rule based method_comma_ using Memory Based Learning MBL .
E06-2021.txt,4,The rule based algorithm was evaluated on a hand tagged acronym corpus and performance was measured using standard measures recall_comma_ precision and fscore.
E06-2021.txt,5,The results show that performance could further improve by increasing the training set and modifying the input settings for the machine learning algorithms.
E06-2021.txt,6,An analysis of the errors produced indicates that further improvement of the rulebased method requires the use of syntactic information and textual pre processing.
E06-2021.txt,8,There are many on line documents which contain important information that we want to understand_comma_ thus the need to extract glossaries of domainspecific names and terms increases_comma_ especially in technical fields such as biomedicine where the vocabulary is quickly expanding.
E06-2021.txt,9,One known phenomenon in biomedical literature is the growth of new acronyms.
E06-2021.txt,10,Acronyms are a subset of abbreviations and are generally formed with capital letters from the original word or phrase_comma_ however many acronyms are realized in different surface forms i.e. use of Arabic numbers_comma_ mixed alpha numeric forms_comma_ low case acronyms etc. Several approaches have been proposed for automatic acronym extraction_comma_ with the most common tools including pattern matching techniques and machine learning algorithms.
E06-2021.txt,11,Considering the large variety in the Swedish acronym definition pairs it is practical to use pattern matching techniques.
E06-2021.txt,12,These will enable to extract relevant information of which a suitable set of schema will give a representation valid to present the different acronym pairs.
E06-2021.txt,13,This project presents a rule based algorithm to processandautomaticallydetectdifferentformsof acronym definition pairs.
E06-2021.txt,14,Since machine learning techniques are generally more robust_comma_ can easily be retrained for a new data and successfully classify unknown examples_comma_ different algorithms were tested.
E06-2021.txt,15,The acronym pair candidates recognized by the rule based algorithm were presented as feature vectors and were used as the training data for the supervised machine learning system.
E06-2021.txt,16,This approach has the advantage of using machine learning techniques without the need for manual tagging of the training data.
E06-2021.txt,17,Several machine learning algorithms were tested and their results were compared on the task.
E06-2021.txt,18,2 Related work The task of automatically extracting acronymdefinition pairs from biomedical literature has been studied_comma_ almost exclusively for English_comma_ over the past few decades using technologies from Natural Language Processing NLP .
E06-2021.txt,19,This section 167 presents a few approaches and techniques that were applied to the acronym identification task.
E06-2021.txt,20,Taghva and Gilbreth 1999 present the Acronyms Finding Program AFP _comma_ based on pattern matching.
E06-2021.txt,21,Their program seeks for acronym candidates which appear as upper case words.
E06-2021.txt,22,They calculate a heuristic score for each competing definition by classifying words into 1 stop words the _comma_ of _comma_ and _comma_ 2 hyphenated words 3 normal words words that don t fall into any of the above categories and 4 the acronyms themselves since an acronym can sometimes be a part of the definition .
E06-2021.txt,23,The AFP utilizes the Longest Common Subsequence LCS algorithm Hunt and Szymanski_comma_ 1977 to find all possible alignments of the acronym to the text_comma_ followed by simple scoring rules which are based on matches.
E06-2021.txt,24,The performance reported from their experiment are recall of 86 at precision of 98 .
E06-2021.txt,25,An alternative approach to the AFP was presented by Yeates 1999 .
E06-2021.txt,26,In his program_comma_ Three Letters Acronyms TLA _comma_ he uses more complex methods and general heuristics to match characters of the acronym candidate with letters in the definition string_comma_ Yeates reported f scoreof 77.8 .
E06-2021.txt,27,Another approach recognizes that the alignment between an acronym and its definition often follows a set of patterns Park and Byrd_comma_ 2001 _comma_ Larkey et al._comma_ 2000 .
E06-2021.txt,28,Pattern based methods use strong constraints to limit the number of acronyms respectively definitions recognized and ensure reasonable precision.
E06-2021.txt,29,Nadeau and Turney 2005 present a machine learning approach that uses weak constraints to reduce the search space of the acronym candidates and the definition candidates_comma_ they reached recall of 89 at precision of 88 .
E06-2021.txt,30,Schwartz and Hearst 2003 present a simple algorithm for extracting abbreviations from biomedical text.
E06-2021.txt,31,The algorithm extracts acronym candidates_comma_ assuming that either the acronym or the definition occurs between parentheses and by giving some restrictions for the definition candidate such as length and capital letter initialization.
E06-2021.txt,32,When an acronym candidate is found the algorithm scans the words in the right and left side of the found acronym and tries to match the shortest definition that matches the letters in the acronym.
E06-2021.txt,33,Their approach is based on previous work Pustejovsky et al._comma_ 2001 _comma_ they achieved recall of 82 at precision of 96 .
E06-2021.txt,34,It should be emphasized that the common characteristic of previous approaches in the surveyed literature is the use of parentheses as indication for the acronym pairs_comma_ see Nadeau and Turney 2005 table 1.
E06-2021.txt,35,This limitation has many drawbacks since it excludes the acronym definition candidates which don t occur within parentheses and thereby don t provide a complete coverage for all the acronyms formation.
E06-2021.txt,36,3 Methods and implementation The method presented in this section is based on a similar algorithm described by Schwartz and Hearst 2003 .
E06-2021.txt,37,However it has the advantage of recognizing acronym definition pairs which are not indicated by parentheses.
E06-2021.txt,38,3.1 Finding Acronym Definition Candidates A valid acronym candidate is a string of alphabetic_comma_ numeric and special characters such as  and .
E06-2021.txt,39,It is found if the string satisfies the conditions i and ii and either iii or iv i The string contains at least two characters.
E06-2021.txt,40,ii The string is not in the list of rejected words1.
E06-2021.txt,41,iii The string contains at least one capital letter.
E06-2021.txt,42,iv The strings first or last character is lower case letter or numeric.
E06-2021.txt,43,When an acronym is found_comma_ the algorithm searches the words surrounding the acronym for a definition candidate string that satisfies the following conditions all are necessary in conjunction i At least one letter of the words in the string matches the letter in the acronym.
E06-2021.txt,44,ii The string doesn t contain a colon_comma_ semi colon_comma_ question mark or exclamation mark.
E06-2021.txt,45,iii The maximum length of the string is min A 5_comma_ A 2 _comma_ where A is the acronym length Park and Byrd_comma_ 2001 .
E06-2021.txt,46,iv The string doesn t contain only upper case letters.
E06-2021.txt,47,3.2 Matching Acronyms with Definitions The process of extracting acronym definition pairs from a raw text_comma_ according to the constraints described in Section 3.1 is divided into two steps 1.
E06-2021.txt,49,In practice_comma_ most of the acronym definition pairs come inside parentheses Schwartz and Hearst_comma_ 2003 and can correspond to two different patterns i definition acronym ii acronym definition .
E06-2021.txt,50,The 1The rejected word list contains frequent acronyms which appear in the corpus without their definition_comma_ e.g.
E06-2021.txt,51,USA _comma_ UK _comma_ EU .
E06-2021.txt,52,168 algorithm extracts acronym definition candidates which correspond to one of these two patterns.
E06-2021.txt,54,Non parentheses matching.
E06-2021.txt,55,The algorithm seeks for acronym candidates that follow the constraints_comma_ described in Section 3.1 and are not enclosed in parentheses.
E06-2021.txt,56,Once an acronym candidate is found it scans the previous and following context_comma_wheretheacronymwasfound_comma_foradefinition candidate.
E06-2021.txt,57,The search space for the definition candidate string is limited to four words multiplied by the number of letters in the acronym candidate.
E06-2021.txt,58,The next step is to choose the correct substring of the definition candidate for the acronym candidate.
E06-2021.txt,59,This is done by reducing the definition candidatestringasfollows thealgorithmsearches for identical characters between the acronym and the definition starting from the end of both strings and succeeds in finding a correct substring for the acronym candidate if it satisfies the following conditions i at least one character in the acronym string matches with a character in the substring of the definition ii the first character in the acronym string matches the first character of the leftmost word in the definition substring_comma_ ignoring upper lower case letters.
E06-2021.txt,60,3.3 Machine Learning Approach To test and compare different supervised learning algorithms_comma_ Tilburg Memory Based Learner TiMBL 2 was used.
E06-2021.txt,61,In memory based learning the training set is stored as examples for later evaluation.
E06-2021.txt,62,Features vectors were calculated to describe the acronym definition pairs.
E06-2021.txt,63,The ten following numeric features were chosen 1 the acronym or the definition is between parentheses 0 false_comma_ 1 true _comma_ 2 the definition appears before the acronym 0 false_comma_ 1 true _comma_ 3 the distance in words between the acronym and the definition_comma_ 4 the number of characters in the acronym_comma_ 5 the number of characters in the definition_comma_ 6 the number of lower case letters in the acronym_comma_ 7 the number of lower case letters in the definition_comma_ 8 the number of upper case letters in the acronym_comma_ 9 the number of upper case letters in the definition and 10 the number of words in the definition.
E06-2021.txt,64,The 11th feature is the class to predict true candidate _comma_ false candidate .
E06-2021.txt,65,An example of the acronym definition pair vCJD _comma_ variant CJD represented as a feature vector is 0_comma_1_comma_1_comma_4_comma_11_comma_1_comma_7_comma_3_comma_3_comma_2_comma_ .
E06-2021.txt,66,2http ilk.uvt.nl 4 Evaluation and Results 4.1 Evaluation Corpus The data set used in this experiment consists of 861 acronym definition pairs.
E06-2021.txt,67,The set was extracted from Swedish medical texts_comma_ the MEDLEX corpus Kokkinakis_comma_ 2006 and was manually annotated using XML tags.
E06-2021.txt,68,For the majority of the cases there exist one acronym definition pair per sentence_comma_ but there are cases where two or more pairs can be found.
E06-2021.txt,69,4.2 Experiment and Results The rule based algorithm was evaluated on the untagged MEDLEX corpus samples.
E06-2021.txt,70,Recall_comma_ precision and F score were used to calculate the acronym expansion matching.
E06-2021.txt,71,The algorithm recognized 671 acronym definition pairs of which 47 were incorrectly identified.
E06-2021.txt,72,The results obtained were 93 precision and 72.5 recall_comma_ yielding Fscore of 81.5 .
E06-2021.txt,73,A closer look at the 47 incorrect acronym pairs that were found showed that the algorithm failed to make a correct match when 1 words that appear in the definition string don t have a corresponding letter in the acronym string_comma_ 2 letters in the acronym string don t have a corresponding word in the definition string_comma_ such as PGA from glycol alginate l osning _comma_ 3 letters in the definition string don t match the letters in the acronym string.
E06-2021.txt,74,The error analysis showed that the reasons for missing 190 acronym definition pairs are 1 letters in the definition string don t appear in the acronym string_comma_ due to a mixture of a Swedish definition with an acronym written in English_comma_ 2 mixture of Arabic and Roman numerals_comma_ such as USH3 from Usher typ III _comma_ 3 position of numbers letters_comma_ 4 acronyms of three characters which appear in lower case letters.
E06-2021.txt,75,4.3 Machine Learning Experiment The acronym definition pairs recognized by the rule based algorithm were used as the training material in this experiment.
E06-2021.txt,76,The 671 pairs were presented as feature vectors according to the features described in Section 3.3.
E06-2021.txt,77,The material was divided into two data files 1 80 training data 2 20 test data.
E06-2021.txt,78,Four different algorithms were used to create models.
E06-2021.txt,79,These algorithms are IB1_comma_ IGTREE_comma_ TRIBL and TRIBL2.
E06-2021.txt,80,The results obtained are given in Table 1.
E06-2021.txt,81,169 Algorithm Precision Recall F score IB1 90.6 97.1 93.7 IGTREE 95.4 97.2 96.3 TRIBL 92.0 96.3 94.1 TRIBL2 92.8 96.3 94.5 Table 1 Memory Based algorithm results.
E06-2021.txt,82,5 Conclusions The approach presented in this paper relies on already existing acronym pairs which are seen in different Swedish texts.
E06-2021.txt,83,The rule based algorithm utilizes predefined strong constraints to find and extract acronym definition pairs with different patterns_comma_ it has the advantage of recognizing acronyms and definitions which are not indicated by parentheses.
E06-2021.txt,84,The recognized pairs were used to test and compare several machine learning algorithms.
E06-2021.txt,85,This approach does not requires manual tagging of the training data.
E06-2021.txt,86,The results given by the rule based algorithm are as good as reported from earlier experiments that have dealt with the same task for the English language.
E06-2021.txt,87,The algorithm uses backward search algorithm and to increase recall it is necessary to combine it with forward search algorithm.
E06-2021.txt,88,The variety of the Swedish acronym pairs is large and includes structures which are hard to detect_comma_ for example V F _comma_ kammarflimmer and CT _comma_ datortomografi _comma_ the acronym is in English while the extension is written in Swedish.
E06-2021.txt,89,These structures require a dictionary database lookup3_comma_ especially because there are also counter examples in the Swedish text where both the acronym and the definition are in English.
E06-2021.txt,90,Another problematic structure is three letter acronyms which consist of only lowercase letterssincetherearemanyprepositions_comma_verbsand determinates that correspond to this structure.
E06-2021.txt,91,To solve this problem it may be suitable to combine textual pre processing such as part of speech annotation or and parsing with the exiting code.
E06-2021.txt,92,The machine learning experiment shows that the best results were given by the IGTREE algorithm4.
E06-2021.txt,93,Performance can further improve by modifying the input settings e.g test different feature weighting schemes_comma_ such as Shared Variance and 3Due to short time available and the lack of resources this feature was not used in the experiment.
E06-2021.txt,94,4The IGTREE algorithm uses information gain in a compressed decision tree structure.
E06-2021.txt,95,Gain Ratio and combine different values of k for the k nearest neighbour classifier5.
E06-2021.txt,96,On going work aim to improve the rule based method and combine it with a supervised machine learning algorithm.
E06-2021.txt,97,The model produced will later be used for making prediction on a new data.
E06-2021.txt,98,Acknowledgements Project funded in part by the SematicMining EU FP6 NoE 507505.
E06-2021.txt,99,This research has been carriedout thanks to LarsBorin and DimitriosKokkinakis.
E06-2021.txt,100,I thank Torbj orn Lager for his guidance and encouragement.
E06-2021.txt,101,I would like to thank Walter Daelemans_comma_ Ko van der Sloot Antal van den Bosch and Robert Andersson for their help and support. .
E06-1008.txt,1,this paper_comma_ we explore statistical language modelling for a speech enabled MP3 player application by generating a corpus from the interpretation grammar written for the application with the Grammatical Framework GF Ranta_comma_ 2004 . We create a statistical language model SLM directly from our interpretation grammar and compare recognition performance of this model against a speech recognition grammar compiled from the same GF interpretation grammar.
E06-1008.txt,2,The results show a relative Word Error Rate WER reduction of 37 for the SLM derived from the interpretation grammar whilemaintainingalowin grammarWER comparable to that associated with the speech recognition grammar.
E06-1008.txt,3,From this starting point we try to improve our artificially generated model by interpolating it with different corpora achieving great reduction in perplexity and 8 relative recognition improvement.
E06-1008.txt,5,Ideally when building spoken dialogue systems_comma_ we would like to use a corpus of transcribed dialogues corresponding to the specific task of the dialogue system_comma_ in order to build a statistical language model SLM .
E06-1008.txt,6,However_comma_ it is rarely the case that such a corpus exists in the early stage of the development of a dialogue system.
E06-1008.txt,7,Collecting such a corpus and transcribing it is very timeconsuming and delays the building of the actual dialogue system.
E06-1008.txt,8,An approach taken both in dialogue systems and dictation applications is to first write an interpretation grammar and from that generate an artificial corpus which is used as training corpus for the SLM Raux et al_comma_ 2003 Pakhomov et al_comma_ 2001 Fosler Lussier Kuo_comma_ 2001 .
E06-1008.txt,9,These models obtained from grammars are not as good as the ones built from real data as the estimates are artificial_comma_ lacking a real distribution.
E06-1008.txt,10,However_comma_ it is a quick way to get a dialogue system working with an SLM.
E06-1008.txt,11,When the system is up and running it is possible to collect real data that can be used to improve the model.
E06-1008.txt,12,We will explore this idea by generating a corpus from an interpretation grammar from one of our applications.
E06-1008.txt,13,A different approach is to compile the interpretation grammar into a speech recognition grammar as the Gemini and REGULUS compilers do Rayner et al_comma_ 2000 Rayner et al_comma_ 2003 .
E06-1008.txt,14,In this way it is assured that the linguistic coverage of the speech recognition and interpretation are kept in sync.
E06-1008.txt,15,Such an approach enables us to interpret all that we can recognize and the other way round.
E06-1008.txt,16,In the European funded project TALK the Grammatical Framework Ranta_comma_ 2005 has been extended with such a facility that compiles GF grammars into speech recognition grammars in Nuance GSL format www.nuance.com .
E06-1008.txt,17,Speech recognition for commercial dialogue systems has focused on grammar based approaches despite the fact that statistical language models seem to have a better overall performance Gorrell et al_comma_ 2002 .
E06-1008.txt,18,This probably depends on the time consuming work of collecting corpora for training SLMs compared with the more rapid and straightforward development of speech recognition grammars.
E06-1008.txt,19,However_comma_ SLMs are more robust_comma_ can handle out of coverage output_comma_ perform better in difficult conditions and seem to work bet57 ter for naive users see Knight et al_comma_ 2001 while speech recognition grammars are limited in their coverage depending on how well grammar writers succeed in predicting what users may say Huang et al_comma_ 2001 .
E06-1008.txt,20,Nevertheless_comma_ as grammars only output phrases that can be interpreted their output makes the following interpretation task easier than with the unpredictable output from an SLM especially if the speech recognition grammar has been compiled fromtheinterpretationgrammarandtheseareboth in sync .
E06-1008.txt,21,In addition_comma_ the grammar based approach in the experiments reported in Knight et al_comma_ 2001 outperforms the SLM approach on semantic error rate on in coverage data.
E06-1008.txt,22,This has lead to the idea of trying to combine both approaches_comma_ as shown in Rayner Hockey_comma_ 2003 .
E06-1008.txt,23,This is also something that we are aiming for.
E06-1008.txt,24,Domain adaptation of SLMs is another issue in dialogue system recognition which involves reusing a successful language model by adapting it to a new domain i.e. a new application Janiszek et al_comma_ 1998 .
E06-1008.txt,25,If a large corpus is not available for the specific domain but there is a corpus for a collection of topics we could use this corpus and adapt the resulting SLM to the domain.
E06-1008.txt,26,One may assume that the resulting SLM based on a large corpuswithagoodmixtureoftopicsshouldbeableto capture at least a part of general language use that doesnotvaryfromonedomaintoanother.
E06-1008.txt,27,Wewill explore this idea by using the Gothenburg Spoken Language Corpus GSLC Allwood_comma_ 1999 and a newspaper corpus to adapt these to our MP3 domain.
E06-1008.txt,28,We will consider several different SLMs based on the corpus generated from the GF interpretation grammar and compare their recognition performance with the baseline a Speech Recognition Grammar in Nuance format compiled from the same interpretation grammar.
E06-1008.txt,29,Hence_comma_ what we could expect from our experiment_comma_ by looking at earlier research_comma_ is very low word error rate for our speech recognition grammar on in grammar coverage but a lot worse performance on out ofgrammar coverage.
E06-1008.txt,30,The SLMs we are considering should tackle out of grammar utterances better and it will be interesting to see how well these models built from the grammar will perform on in grammar utterances.
E06-1008.txt,31,This study is organized as follows.
E06-1008.txt,32,Section 2 introduces the domain for which we are doing language modelling and the corpora we have at our disposal.
E06-1008.txt,33,Section 3 will describe the different SLMs we have generated.
E06-1008.txt,34,Section 4 describes the evaluation of these and the results.
E06-1008.txt,35,Finally_comma_ we review the main conclusions of the work and discuss future work.
E06-1008.txt,36,2 Description of Corpora 2.1 The MP3 corpus The domain that we are considering in this paper is the domain of an MP3 player application.
E06-1008.txt,37,The talking MP3 player_comma_ DJGoDiS_comma_ is one of several applications that are under development in the TALK project.
E06-1008.txt,38,It has been built with the TrindiKit toolkit Larsson et al_comma_ 2002 and the GoDiS dialogue system Larsson_comma_ 2002 as a GoDiS application and works as a voice interface to a graphical MP3 player.
E06-1008.txt,39,The user can among other things change settings_comma_ choose stations or songs to play and create playlists.
E06-1008.txt,40,The current version of DJGoDiS works in both English and Swedish.
E06-1008.txt,41,The interpretation and generation grammars are written with the GF grammar formalism.
E06-1008.txt,42,GF is being further developed in the project to adapt it to the use in spoken dialogue systems.
E06-1008.txt,43,This adaptation includes the facility of generating Nuance recognition grammars from the interpretation grammar and the possibility of generating corpora from the grammars.
E06-1008.txt,44,The interpretation grammar for the domain_comma_ written in GF_comma_ translates user utterancesto dialoguemoves and thereby holds all possible interpretations of user utterances Ljungl of et al_comma_ 2005 .
E06-1008.txt,45,We used GF s facilities to generate a corpus in Swedish consisting of all possible meaningful utterances generated by the grammar to a certain depth of the analysis trees in GF s .
E06-1043.txt,1,investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties_comma_ and demonstrate that these statistical_comma_ corpus based measures can be successfully used for distinguishing idiomatic combinations from non idiomatic ones.
E06-1043.txt,2,We also propose a means for automatically determining which syntactic forms a particular idiom can appear in_comma_ and hence should be included in its lexical representation.
E06-1043.txt,4,The term idiom has been applied to a fuzzy category with prototypical examples such as by and large_comma_ kick the bucket_comma_ and let the cat out of the bag.
E06-1043.txt,5,Providing adefinitive answerforwhatidioms are_comma_ and determining how they are learned and understood_comma_ are still subject to debate Glucksberg_comma_ 1993 Nunberg et al._comma_ 1994 .
E06-1043.txt,6,Nonetheless_comma_ they are often defined as phrases or sentences that involve some degree of lexical_comma_ syntactic_comma_ and or semantic idiosyncrasy.
E06-1043.txt,7,Idiomatic expressions_comma_ as a part of the vast family of figurative language_comma_ are widely used both in colloquial speech and in written language.
E06-1043.txt,8,Moreover_comma_ a phrase develops its idiomaticity over time Cacciari_comma_ 1993 consequently_comma_ new idioms come into existence on a daily basis Cowie et al._comma_ 1983 Seaton and Macaulay_comma_ 2002 .
E06-1043.txt,9,Idioms thus pose a serious challenge_comma_ both for the creation of widecoverage computational lexicons_comma_ and for the development of large scale_comma_ linguistically plausible natural language processing NLP systems Sag et al._comma_ 2002 .
E06-1043.txt,10,One problem is due to the range of syntactic idiosyncrasy of idiomatic expressions.
E06-1043.txt,11,Some idioms_comma_ such as by and large_comma_ contain syntactic violations these are often completely fixed and hence can be listed in a lexicon as words with spaces Sag et al._comma_ 2002 .
E06-1043.txt,12,However_comma_ among those idioms that are syntactically well formed_comma_ some exhibit limited morphosyntactic flexibility_comma_ while others may be more syntactically flexible.
E06-1043.txt,13,For example_comma_ theidiom shoot the breezeundergoes verbalinflection shot the breeze _comma_ but not internal modification orpassivization shoot the fun breeze_comma_ the breeze was shot .
E06-1043.txt,14,In contrast_comma_ the idiom spill the beans undergoes verbal inflection_comma_ internal modification_comma_ and even passivization.
E06-1043.txt,15,Clearly_comma_ a words withspaces approach does not capture the full range of behaviour of such idiomatic expressions.
E06-1043.txt,16,Another barrier to the appropriate handling of idioms in a computational system is their semantic idiosyncrasy.
E06-1043.txt,17,This is aparticular issue for those idioms that conform to the grammar rules of the language.
E06-1043.txt,18,Such idiomatic expressions are indistinguishable on the surface from compositional nonidiomatic phrases_comma_ but a computational system must be capable of distinguishing the two.
E06-1043.txt,19,For example_comma_ a machine translation system should translate the idiom shoot the breeze as a single unit of meaning to chat _comma_ whereas this is not the case for the literal phrase shoot the bird.
E06-1043.txt,20,In this study_comma_ we focus on a particular class of English phrasal idioms_comma_ i.e._comma_ those that involve the combination of a verb plus a noun in its direct object position.
E06-1043.txt,21,Examples include shoot the breeze_comma_ pull strings_comma_ and push one s luck.
E06-1043.txt,22,We refer to these as verb noun idiomatic combinations VNICs .
E06-1043.txt,23,The class of VNICs accommodates a large number of idiomatic expressions Cowie et al._comma_ 1983 Nunberg etal._comma_ 1994 .
E06-1043.txt,24,Moreover_comma_ their peculiar be337 haviour signifies the need for a distinct treatment in a computational lexicon Fellbaum_comma_ 2005 .
E06-1043.txt,25,Despite this_comma_ VNICs have been granted relatively little attention within the computational linguistics community.
E06-1043.txt,26,We look into two closely related problems confronting the appropriate treatment of VNICs i the problem of determining their degree of flexibility and ii the problem of determining their level of idiomaticity.
E06-1043.txt,27,Section 2 elaborates on the lexicosyntactic flexibility of VNICs_comma_ and how this relates to their idiomaticity.
E06-1043.txt,28,In Section 3_comma_ we propose two linguistically motivated statistical measures for quantifying the degree of lexical and syntactic inflexibility or fixedness of verb noun combinations.
E06-1043.txt,29,Section 4 presents an evaluation of the proposed measures.
E06-1043.txt,30,In Section 5_comma_ we put forward a technique for determining the syntactic variations that a VNIC can undergo_comma_ and that should be included in its lexical representation.
E06-1043.txt,31,Section 6 summarizes our contributions.
E06-1043.txt,32,2 Flexibility and Idiomaticity of VNICs Although syntactically well formed_comma_ VNICs involve a certain degree of semantic idiosyncrasy.
E06-1043.txt,33,Unlike compositional verb noun combinations_comma_ the meaning of VNICs cannot be solely predicted from the meaning of their parts.
E06-1043.txt,34,There is much evidence in the linguistic literature that the semantic idiosyncrasy of idiomatic combinations is reflected in their lexical and or syntactic behaviour.
E06-1043.txt,35,2.1 Lexical and Syntactic Flexibility A limited number of idioms have one or more lexical variants_comma_ e.g._comma_ blow one s own trumpet and toot one s own horn examples from Cowie et al. 1983 .
E06-1043.txt,36,However_comma_ most are lexically fixed nonproductive to a large extent.
E06-1043.txt,37,Neither shoot the wind nor fling the breeze are typically recognized as variations of the idiom shoot the breeze.
E06-1043.txt,38,Similarly_comma_ spill the beans has an idiomatic meaning to reveal a secret _comma_ while spill the peas and spread the beans have only literal interpretations.
E06-1043.txt,39,Idiomatic combinations are also syntactically peculiar most VNICs cannot undergo syntactic variations and at the same time retain their idiomatic interpretations.
E06-1043.txt,40,It is important_comma_ however_comma_ tonotethatVNICsdifferwithrespecttothedegree of syntactic flexibility they exhibit.
E06-1043.txt,41,Some are syntactically inflexible for the most part_comma_ while others are more versatile as illustrated in 1 and 2 1. a Tim and Joy shot the breeze. b
E06-1043.txt,42,Tim and Joy shot a breeze.
E06-1043.txt,44,Tim and Joy shot the breezes. d
E06-1043.txt,45,Tim and Joy shot the fun breeze. e
E06-1043.txt,46,The breeze was shot by Tim and Joy. f
E06-1043.txt,47,The breeze that Tim and Joy kicked was fun.
E06-1043.txt,48,2. a Tim spilled the beans. b
E06-1043.txt,49,Tim spilled some beans.
E06-1043.txt,51,Tim spilled the bean. d Tim spilled the official beans. e The beans were spilled by Tim. f The beans that Tim spilled troubled Joe.
E06-1043.txt,52,Linguists have explained the lexical and syntactic flexibility of idiomatic combinations in terms of their semantic analyzability e.g._comma_ Glucksberg 1993 Fellbaum 1993 Nunberg et al. 1994 .
E06-1043.txt,53,Semantic analyzability is inversely related to idiomaticity.
E06-1043.txt,54,For example_comma_ the meaning of shoot the breeze_comma_ a highly idiomatic expression_comma_ has nothing todowith either shoot or breeze.
E06-1043.txt,55,Incontrast_comma_ aless idiomatic expression_comma_ such as spill the beans_comma_ can be analyzed as spill corresponding to reveal and beans referring to secret s .
E06-1043.txt,56,Generally_comma_ the constituents ofasemantically analyzable idiom can be mapped onto their corresponding referents in the idiomatic interpretation.
E06-1043.txt,57,Hence analyzable less idiomatic expressions are often more open to lexical substitution and syntactic variation.
E06-1043.txt,58,2.2 Our Proposal We use the observed connection between idiomaticity and in flexibility to devise statistical measures for automatically distinguishing idiomatic from literal verb noun combinations.
E06-1043.txt,59,While VNICs vary in their degree of flexibility cf. 1 and 2 above see also Moon 1998 _comma_ on the whole they contrast with compositional phrases_comma_ which are more lexically productive and appear in a wider range of syntactic forms.
E06-1043.txt,60,We thus propose to use the degree of lexical and syntactic flexibilityofagivenverb noun combination todetermine the level of idiomaticity of the expression.
E06-1043.txt,61,It is important to note that semantic analyzability is neither a necessary nor a sufficient condition for an idiomatic combination to be lexically or syntactically flexible.
E06-1043.txt,62,Other factors_comma_ such as the communicative intentions and pragmatic constraints_comma_ can motivate a speaker to use a variant in place of a canonical form Glucksberg_comma_ 1993 .
E06-1043.txt,63,Nevertheless_comma_ lexical and syntactic flexibility may well be used as partial indicators of semantic analyzability_comma_ and hence idiomaticity.
E06-1043.txt,64,338 3 Automatic Recognition of VNICs Here we describe our measures for idiomaticity_comma_ whichquantify thedegreeoflexical_comma_ syntactic_comma_ and overall fixedness of a given verb noun combination_comma_ represented as a verb noun pair.
E06-1043.txt,65,Note that our measures quantify fixedness_comma_ not flexibility. 3.1 Measuring Lexical Fixedness AVNICislexically fixedifthereplacement ofany of its constituents by a semantically and syntactically similar word generally does not result in another VNIC_comma_ but in an invalid or a literal expression.
E06-1043.txt,66,One way of measuring lexical fixedness of a given verb noun combination is thus to examine theidiomaticity ofitsvariants_comma_ i.e._comma_ expressions generated by replacing one of the constituents by a similar word.
E06-1043.txt,67,This approach has two main challenges i it requires prior knowledge about the idiomaticity of expressions which is what we are developing our measure to determine ii it needs information on similarity among words.
E06-1043.txt,68,Inspired by Lin 1999 _comma_ weexamine the strength of association between the verb and noun constituents of the target combination and its variants_comma_ as an indirect cue to their idiomaticity.
E06-1043.txt,69,We use the automatically built thesaurus of Lin 1998 to find similar words to the noun of the target expression_comma_ in order to automatically generate variants.
E06-1043.txt,70,Only the noun constituent is varied_comma_ since replacing the verb constituent of a VNIC with a semantically related verb is more likely to yield another VNIC_comma_ as in keep lose one s cool Nunberg et al._comma_ 1994 .
E06-1043.txt,71,Let a0a2a1a4a3a6a5a8a7a10a9a12a11a14a13a16a15a17a9a18a5a20a19a22a21a24a23a26a25a27a23a29a28a31a30 be the set of the a28 most similar nouns to the noun a9 of the target pair a32a34a33a36a35 a9a38a37 .
E06-1043.txt,72,We calculate the association strength for the target pair_comma_ and for each of its variants_comma_ a32a39a33a36a35 a9 a5 a37 _comma_ using pointwise mutual information PMI Church et al._comma_ 1991 a40a42a41a44a43 a7 a33a36a35 a9a46a45a47a11a48a13 a49a51a50a53a52a55a54 a7 a33a36a35 a9 a45 a11 a54 a7 a33 a11 a54 a7a10a9 a45 a11 a13 a49a51a50a53a52 a19a56a26a57a34a58a59a19a61a60a62a7 a33a36a35 a9a46a45a47a11 a60a62a7 a33a36a35a64a63 a11a62a60a65a7 a63a66a35 a9 a45 a11 1 where a67 a23a69a68a70a23a31a28 and a9a36a71 is the target noun a56 is the set of all transitive verbs in the corpus a58 is the set of all nouns appearing as the direct object of some verb a60a2a7 a33a72a35 a9 a45 a11 is the frequency of a33 and a9 a45 occurring as a verb object pair a60a62a7 a33a36a35a64a63 a11 is the total frequency of the target verb with any noun in a58 a60a2a7 a63a66a35 a9 a45 a11 is the total frequency of the noun a9 a45 in the direct object position of any verb in a56 .
E06-1043.txt,73,Lin 1999 assumes that a target expression is non compositional if and only if its a40a73a41a74a43 value is significantly different from that of any of the variants.
E06-1043.txt,74,Instead_comma_ we propose a novel technique thatbringstogether theassociation strengths a40a42a41a44a43 values of the target and the variant expressions into a single measure reflecting the degree of lexical fixedness for the target pair.
E06-1043.txt,75,We assume that the target pair is lexically fixed to the extent that its a40a42a41a44a43 deviates from the average a40a42a41a44a43 of its variants.
E06-1043.txt,76,Our measure calculates this deviation_comma_ normalized using the sample s standard deviation a75a62a76a51a77a79a78a81a80a18a82a18a78a81a83a84a83a86a85a88a87a90a89 a7 a33a36a35 a9a91a11a48a13 a40a42a41a44a43 a7 a33a36a35 a9a12a11a62a92 a40a42a41a44a43 a93 2 a40a73a41a74a43 is the mean and a93 the standard deviation of the sample a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a84a85a88a87a97a89 a7 a33a72a35 a9a12a11a38a98a100a99a94a92a102a101 a35a104a103 a101a106a105 .
E06-1043.txt,77,3.2 Measuring Syntactic Fixedness Compared to compositional verb noun combinations_comma_ VNICs are expected to appear in more restricted syntactic forms.
E06-1043.txt,78,To quantify the syntactic fixedness of a target verb noun pair_comma_ we thus need to i identify relevant syntactic patterns_comma_ i.e._comma_ those that help distinguish VNICs from literalverb noun combinations ii translate thefrequency distribution of the target pair in the identified patterns into a measure of syntactic fixedness.
E06-1043.txt,79,3.2.1 Identifying Relevant Patterns Determining a unique set of syntactic patterns appropriate for the recognition of all idiomatic combinations is difficult indeed exactly which formsanidiomatic combination can occur inisnot entirely predictable Sag et al._comma_ 2002 .
E06-1043.txt,80,Nonetheless_comma_ there are hypotheses about the difference in behaviour of VNICs and literal verb noun combinations with respect to particular syntactic variations Nunberg et al._comma_ 1994 .
E06-1043.txt,81,Linguists note that semantic analyzability is related to the referential status of the noun constituent_comma_ which is in turn related to participation in certain morphosyntactic forms.
E06-1043.txt,82,In what follows_comma_ we describe three types of variation that are tolerated by literal combinations_comma_ but are prohibited by many VNICs.
E06-1043.txt,83,Passivization There is much evidence in the linguistic literature that VNICs often do not undergo passivization.1 Linguists mainly attribute this to the fact that only a referential noun can appear as the surface subject of a passive construction.
E06-1043.txt,84,1There are idiomatic combinations that are used only in a passivized form we do not consider such cases in our study.
E06-1043.txt,85,339 Determiner Type A strong correlation exists between the flexibility of the determiner preceding the noun in a verb noun combination and the overall flexibility of the phrase Fellbaum_comma_ 1993 .
E06-1043.txt,86,It is however important to note that the nature of the determiner is also affected by other factors_comma_ such as the semantic properties of the noun.
E06-1043.txt,87,Pluralization While the verb constituent of a VNIC is morphologically flexible_comma_ the morphological flexibility of the noun relates to its referential status.
E06-1043.txt,88,A non referential noun constituent is expected to mainly appear in just one of the singular or plural forms.
E06-1043.txt,89,The pluralization of the noun is of course also affected by its semantic properties.
E06-1043.txt,90,Merging the three variation types results in a pattern set_comma_ a0 a0 _comma_ of a1a2a1 distinct syntactic patterns_comma_ given in Table 1.2 3.2.2 Devising a Statistical Measure Thesecond stepistodeviseastatistical measure that quantifies the degree of syntactic fixedness of a verb noun pair_comma_ with respect to the selected set of patterns_comma_ a0 a0 .
E06-1043.txt,91,We propose a measure that compares the syntactic behaviour of the target pair with that of a typical verb noun pair.
E06-1043.txt,92,Syntactic behaviour of a typical pair is defined as the prior probability distribution over the patterns in a0 a0 .
E06-1043.txt,93,The prior probability of an individual pattern a3a5a4 a98 a0 a0 is estimated as a54 a7a7a6a9a8a86a11a48a13 a10 a11a13a12a15a14a17a16 a10 a18a20a19a21a14a23a22 a60a2a7 a33 a3 a35 a9a46a45 a35 a3a5a4 a11 a10 a11a24a12a25a14a17a16 a10 a18a20a19a21a14a26a22 a10 a27a29a28a31a30a26a14a33a32a35a34 a60a2a7 a33 a3 a35 a9 a45 a35 a3a5a4a37a36 a11 The syntactic behaviour of the target verb noun pair a32a34a33a72a35 a9 a37 is defined as the posterior probability distribution over the patterns_comma_ given the particular pair.
E06-1043.txt,94,The posterior probability of an individual pattern a3a5a4 is estimated as a54 a7a7a6a9a8 a19a20a38 a35a40a39 a11a48a13 a54 a7 a33a36a35 a9 a35 a3a5a4 a11 a54 a7 a33a72a35 a9a12a11 a13 a60a62a7 a33a36a35 a9 a35 a3a5a4 a11 a10 a27a29a28a31a30a41a14a17a32a35a34 a60a62a7 a33a72a35 a9 a35 a3a5a4a37a36 a11 The degree of syntactic fixedness of the target verb noun pair is estimated as the divergence of its syntactic behaviour the posterior distribution 2We collapse some patterns since with a larger pattern set the measure may require larger corpora to perform reliably. Patterns v det NULL na42a44a43 v det NULL na45a20a46 v det a an na42a44a43 v det the na42a44a43 v det the na45a20a46 v det DEM na42a44a43 v det DEM na45a20a46 v det POSS na42a44a43 v det POSS na45a20a46 v det OTHER n a42a44a43a48a47 a45a20a46 det ANY n a42a44a43a48a47 a45a20a46 be v a45a13a49 a42a44a42 a12 a50a25a51 Table 1 Patterns for syntactic fixedness measure. over the patterns _comma_ from the typical syntactic behaviour the prior distribution .
E06-1043.txt,95,The divergence of the two probability distributions is calculated using a standard information theoretic measure_comma_ the Kullback Leibler KL divergence a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83 a83a53a52a37a54a26a55 a7 a33a36a35 a9a91a11 a13 a56a44a7 a54 a7 a3a5a4 a19 a33a72a35 a9a12a11a91a19a51a19 a54 a7 a3a5a4 a11a86a11 a13 a10 a27a29a28a31a30a23a14a33a32a35a34 a54 a7 a3a5a4a37a36 a19 a33a36a35 a9a12a11a79a49a51a50a53a52 a54 a7 a3a57a4a37a36 a19 a33a72a35 a9a12a11 a54 a7 a3a5a4a37a36 a11 3 KL divergence is always non negative and is zero if and only if the two distributions are exactly the same.
E06-1043.txt,96,Thus_comma_ a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a58a52a37a54a26a55 a7 a33a72a35 a9a91a11a73a98a100a99a67a95a35a104a103 a101a106a105 .
E06-1043.txt,97,KL divergence is argued to be problematic because it is not a symmetric measure.
E06-1043.txt,98,Nonetheless_comma_ it has proven useful in many NLP applications Resnik_comma_ 1999 Dagan et al._comma_ 1994 .
E06-1043.txt,99,Moreover_comma_ the asymmetry is not an issue here since we are concerned with the relative distance of several posterior distributions from the same prior.
E06-1043.txt,100,3.3 A Hybrid Measure of Fixedness VNICs are hypothesized to be_comma_ in most cases_comma_ both lexically and syntactically more fixed than literal verb noun combinations see Section 2 .
E06-1043.txt,101,We thus propose a new measure of idiomaticity to be a measure of the overall fixedness of a given pair.
E06-1043.txt,102,We define a75a62a76a51a77a79a78a81a80a18a82a18a78a81a83a84a83a60a59a13a61a84a87a13a62a15a63 a85a88a85 a7 a33a72a35 a9a12a11 as a75a62a76a51a77a79a78a81a80a18a82a18a78a81a83a84a83a53a59a20a61a84a87a24a62a15a63 a85a88a85 a7 a33a36a35 a9a91a11 a13 a64 a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a53a52a65a54a21a55 a7 a33a72a35 a9a12a11 a103 a7 a1 a92a66a64a62a11 a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83 a83a86a85a88a87a90a89 a7 a33a36a35 a9a91a11 4 where a64 weights the relative contribution of the measures in predicting idiomaticity.
E06-1043.txt,103,4 Evaluation of the Fixedness Measures To evaluate our proposed fixedness measures_comma_ we determine their appropriateness asindicators ofidiomaticity.
E06-1043.txt,104,We pose a classification task in which idiomatic verb noun pairs are distinguished from literal ones.
E06-1043.txt,105,We use each measure to assign scores 340 to the experimental pairs see Section 4.2 below .
E06-1043.txt,106,We then classify the pairs by setting a threshold_comma_ here the median score_comma_ where all expressions with scores higher than the threshold are labeled as idiomatic and the rest as literal.
E06-1043.txt,107,We assess the overall goodness of a measure by looking at its accuracy Acc and the relative reduction in error rate RER on the classification task described above.
E06-1043.txt,108,The RER of a measure reflects the improvement in its accuracy relative to another measure often a baseline .
E06-1043.txt,109,We consider two baselines i a random baseline_comma_ a0a2a1 a82a18a80 _comma_ that randomly assigns a label literal or idiomatic to each verb noun pair ii a more informed baseline_comma_ a40a42a41a44a43 _comma_ an information theoretic measure widely used for extracting statistically significant collocations.3 4.1 Corpus and Data Extraction We use the British National Corpus BNC http www.natcorp.ox.ac.uk to extract verb noun pairs_comma_ along with information on the syntactic patterns they appear in.
E06-1043.txt,110,We automatically parse the corpus using the Collins parser Collins_comma_ 1999 _comma_ and further process it using TGrep2 Rohde_comma_ 2004 .
E06-1043.txt,111,For each instance of a transitive verb_comma_ we use heuristics to extract the noun phrase NP in either the direct object position if the sentence is active _comma_ or the subject position if the sentence is passive .
E06-1043.txt,112,We then use NP head extraction software4 to get the head noun of the extracted NP_comma_ its number singular or plural _comma_ and the determiner introducing it.
E06-1043.txt,113,4.2 Experimental Expressions We select our development and test expressions from verb noun pairs that involve a member of a predefined list of transitive basic verbs.
E06-1043.txt,114,Basic verbs_comma_ in their literal use_comma_ refer to states or acts that are central to human experience.
E06-1043.txt,115,They are thus frequent_comma_ highly polysemous_comma_ and tend to combine with other words to form idiomatic combinations Nunberg et al._comma_ 1994 .
E06-1043.txt,116,An initial list of suchverbswasselected fromseveral linguistic and psycholinguistic studies on basic vocabulary e.g._comma_ Pauwels 2000 Newman and Rice 2004 .
E06-1043.txt,117,We further augmented this initial list with verbs that are semantically related to another verb already in the 3As in Eqn.
E06-1043.txt,118,1 _comma_ our calculation of PMI here restricts the verb noun pair to the direct object relation.
E06-1043.txt,119,4We use a modified version of the software provided by Eric Joanis based on heuristics from Collins_comma_ 1999 . list e.g._comma_ lose is added in analogy with find.
E06-1043.txt,120,The final list of 28 verbs is blow_comma_ bring_comma_ catch_comma_ cut_comma_ find_comma_ get_comma_ give_comma_ have_comma_ hear_comma_ hit_comma_ hold_comma_ keep_comma_ kick_comma_ lay_comma_ lose_comma_ make_comma_ move_comma_ place_comma_ pull_comma_ push_comma_ put_comma_ see_comma_ set_comma_ shoot_comma_ smell_comma_ take_comma_ throw_comma_ touch From the corpus_comma_ we extract all verb noun pairs withminimum frequency of a1 a67 that contain abasic verb.
E06-1043.txt,121,From these_comma_ we semi randomly select an idiomatic and a literal subset.5 A pair is considered idiomatic if it appears in a credible idiom dictionary_comma_ such as the Oxford Dictionary of Current Idiomatic English ODCIE Cowie et al._comma_ 1983 _comma_ or the Collins COBUILD Idioms Dictionary CCID Seaton and Macaulay_comma_ 2002 .
E06-1043.txt,122,Otherwise_comma_ the pair is considered literal.
E06-1043.txt,123,We then randomly pull out a1a4a3 a67 development and a5 a67a53a67 test pairs half idiomatic and half literal _comma_ ensuring both low and high frequency items are included.
E06-1043.txt,124,Sample idioms corresponding to the extracted pairs are kick the habit_comma_ move mountains_comma_ lose face_comma_ and keep one s word.
E06-1043.txt,125,4.3 Experimental Setup Development expressions are used in devising the fixedness measures_comma_ as well as in determining the values of the parameters a28 in Eqn.
E06-1043.txt,126,2 and a64 in Eqn.
E06-1043.txt,128,a28 determines the maximum number of nouns similar to the target noun_comma_ to be considered in measuring the lexical fixedness of a given pair.
E06-1043.txt,129,The value of this parameter is determined by performing experiments over the development data_comma_ in which a28 ranges from a1 a67 to a1 a67a53a67 by steps of a1 a67 a28 is set to a6 a67 based on the results.
E06-1043.txt,130,We also experimented with different values of a64 ranging from a67 to a1 by steps of a7 a1 .
E06-1043.txt,131,Based on the development results_comma_ thebest value for a64 is a7a9a8 giving moreweight to the syntactic fixedness measure .
E06-1043.txt,132,Testexpressions aresaved asunseen data forthe final evaluation.
E06-1043.txt,133,We further divide the set of all testexpressions_comma_ TESTa63 a85a88a85 _comma_intotwosetscorresponding to two frequency bands TESTa10a12a11a13a15a14 contains a6 a67 idiomatic and a6 a67 literal pairs_comma_ each with total frequency between a1 a67 and a16a66a67 a1 a67 a23a20a60a18a17a20a19a20a21a96a7 a33a36a35 a9 a35a12a63 a11a23a22 a16a66a67 TESTa10a12a24a26a25a27a28a24 consists of a6 a67 idiomatic and a6 a67 literal pairs_comma_ each with total frequency of a16a66a67 or greater a60a18a17a20a19a20a21a96a7 a33a36a35 a9 a35a91a63 a11a30a29 a16a66a67 .
E06-1043.txt,134,All frequency counts are over the entire BNC.
E06-1043.txt,135,4.4 Results We first examine the performance of the individual fixedness measures_comma_ a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a104a85a88a87a90a89 and 5In selecting literal pairs_comma_ we choose those that involve a physical act corresponding to the basic semantics of the verb.
E06-1043.txt,136,341 Data Set TESTa0 a11 a11 Acc RER a1a3a2a5a4a7a6 50 a8a10a9a12a11 64 28 a13a15a14a17a16a19a18a20a6a19a4a7a18a22a21a23a21 a11 a24a26a25 65 30 a13a15a14a17a16a19a18a20a6a19a4a7a18a22a21a23a21a28a27a30a29a32a31 70 40 Table 2 Accuracy and relative error reduction for the two fixedness and the two baseline measures over all test pairs.
E06-1043.txt,137,a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a53a52a65a54a21a55 _comma_ as well as that of the two baselines_comma_ a0a2a1 a82a72a80 and a40a73a41a74a43 see Table 2.
E06-1043.txt,138,Results for the overall measure are presented later in this section. As can be seen_comma_ the informed baseline_comma_ a40a42a41a44a43 _comma_ shows a large improvement over the random baseline a5 a8 a33 error reduction .
E06-1043.txt,139,This shows that one can get relatively good performance by treating verb noun idiomatic combinations as collocations.
E06-1043.txt,140,a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a84a85a88a87a90a89 performs as well as the informed baseline a34 a67 a33 error reduction .
E06-1043.txt,141,This result shows that_comma_ as hypothesized_comma_ lexical fixedness is areasonably good predictor of idiomaticity.
E06-1043.txt,142,Nonetheless_comma_ the performance signifies a need for improvement.
E06-1043.txt,143,Possibly the most beneficial enhancement would be a change in the way we acquire the similar nouns for a target noun.
E06-1043.txt,144,The best performance shown in boldface belongs to a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a60a52a65a54a21a55 _comma_ with a16a66a67 a33 error reduction over the random baseline_comma_ and a5 a67 a33 error reduction over the informed baseline.
E06-1043.txt,145,These results demonstrate that syntactic fixedness is a good indicator of idiomaticity_comma_ better than a simple measure of collocation a40a73a41a74a43 _comma_ or a measure of lexical fixedness.
E06-1043.txt,146,These results further suggest that looking into deep linguistic properties of VNICs is both necessary and beneficial for the appropriate treatment of these expressions.
E06-1043.txt,147,a40a73a41a74a43 is known to perform poorly on low frequency data.
E06-1043.txt,148,To examine the effect of frequency on the measures_comma_ we analyze their performance on the two divisions of the test data_comma_ corresponding to the two frequency bands_comma_ TESTa10a12a11 a13a15a14 and TESTa10a12a24a26a25 a27a28a24 .
E06-1043.txt,149,Results are given in Table 3_comma_ with the best performance shown in boldface.
E06-1043.txt,150,As expected_comma_ the performance of a40a73a41a74a43 drops substantially for low frequency items.
E06-1043.txt,151,Interestingly_comma_ although it is a PMI based measure_comma_ a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a86a85a88a87a97a89 performs slightly better when the data is separated based on frequency.
E06-1043.txt,152,The performance of a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a60a52a65a54a21a55 improves quite a bit when it is applied to high frequency items_comma_ while it improves only slightly on the low frequency items.
E06-1043.txt,153,These results show that both Fixedness measures Data Set TESTa35a37a36a38a40a39 TESTa35a37a41a28a42 a43a44a41 Acc RER Acc RER a1a45a2a46a4a47a6 50 50 a8a48a9a12a11 56 12 70 40 a13a15a14a49a16a19a18a20a6a19a4a7a18a22a21a23a21 a11 a24a26a25 68 36 66 32 a13a15a14a49a16a19a18a20a6a19a4a7a18a22a21a23a21 a27a50a29a51a31 72 44 82 64 Table 3 Accuracy and relative error reduction for all measures over test pairs divided by frequency.
E06-1043.txt,154,Data Set TESTa0 a11 a11 Acc RER a13a15a14a17a16a52a18a22a6a52a4a7a18a51a21a23a21 a11 a24a53a25 65 30 a13a15a14a17a16a52a18a22a6a52a4a7a18a51a21a23a21 a27a30a29a51a31 70 40 a13a15a14a17a16a52a18a22a6a52a4a7a18a51a21a23a21 a13a40a54a28a24a26a55 a0 a11 a11 74 48 Table 4 Performance of the hybrid measure over TESTa0 a11 a11 . perform better onhomogeneous data_comma_ whileretaining comparably good performance on heterogeneous data.
E06-1043.txt,155,These results reflect that our fixedness measures are not as sensitive tofrequency as a40a42a41a44a43 .
E06-1043.txt,156,Hence they can be used with a higher degree of confidence_comma_ especially when applied to data that is heterogeneous with regard to frequency.
E06-1043.txt,157,This is important because while some VNICs are very common_comma_ others have very low frequency.
E06-1043.txt,158,Table 4 presents the performance of the hybrid measure_comma_ a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a58a59a13a61a84a87a24a62a25a63 a85a88a85 _comma_ repeating that of a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a84a85a88a87a90a89 and a75a2a76a94a77a79a78a81a80a72a82a96a78a81a83a84a83a60a52a37a54a26a55 for comparison.
E06-1043.txt,159,a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a60a59a13a61a84a87a24a62a25a63 a85a88a85 outperforms both lexical and syntactic fixedness measures_comma_ with a substantial improvement over a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83 a85a88a87a90a89 _comma_ and a small_comma_ but notable_comma_ improvement over a75a2a76a94a77a95a78a81a80a18a82a96a78a81a83a84a83a21a52a37a54a26a55 .
E06-1043.txt,160,Each of the lexical and syntactic fixedness measures is a good indicator of idiomaticity on its own_comma_ with syntactic fixedness being a better predictor.
E06-1043.txt,161,Here we demonstrate that combining them into a single measure of fixedness_comma_ while giving more weight to the better measure_comma_ results in a more effective predictor of idiomaticity.
E06-1043.txt,162,5 Determining the Canonical Forms Our evaluation of the fixedness measures demonstrates their usefulness for the automatic recognition of idiomatic verb noun pairs.
E06-1043.txt,163,To represent such pairs in a lexicon_comma_ however_comma_ we must determine their canonical form s Cforms henceforth.
E06-1043.txt,164,For example_comma_ the lexical representation of a32 shoot_comma_ breeze a37 should include shoot the breeze as a Cform.
E06-1043.txt,165,Since VNICs are syntactically fixed_comma_ they are mostly expected to have a single Cform.
E06-1043.txt,166,Nonetheless_comma_ there are idioms with two or more accept342 able forms.
E06-1043.txt,167,For example_comma_ hold fire and hold one s fire are both listed in CCID as variations of the same idiom.
E06-1043.txt,168,Our approach should thus be capable of predicting all allowable forms for a given idiomatic verb noun pair.
E06-1043.txt,169,Weexpect aVNICtooccurinitsCform s more frequently than it occurs in anyother syntactic patterns.
E06-1043.txt,170,To discover the Cform s for a given idiomatic verb noun pair_comma_ we thus examine its frequency of occurrence in each syntactic pattern in a0 a0 .
E06-1043.txt,171,Since it is possible for an idiom to have more than one Cform_comma_ we cannot simply take the most dominant pattern as the canonical one.
E06-1043.txt,172,Instead_comma_ we calculate a a0 score for the target pair a32a102a33a72a35 a9a12a37 and each pattern a3a5a4a65a36 a98 a0 a0 a0 a36 a7 a33a72a35 a9a12a11 a13 a60a62a7 a33a72a35 a9 a35 a3a5a4a37a36 a11a62a92 a60 a93 inwhicha60 isthemeanand a93 thestandard deviation over the sample a15a84a60a62a7 a33a36a35 a9 a35 a3a57a4a65a36 a11a91a19 a3a5a4a37a36 a98 a0 a0 a30 .
E06-1043.txt,173,The statistic a0 a36 a7 a33a36a35 a9a91a11 indicates how far and in which direction the frequency of occurrence of the pair a32 a33a36a35 a9 a37 in pattern a6 a8a2a1 deviates from the sample smean_comma_ expressed inunits ofthesample s standard deviation.
E06-1043.txt,174,To decide whether a3a5a4a25a36 is a canonical pattern for the target pair_comma_ we check whether a0 a36 a7 a33a36a35 a9a91a11a4a3a6a5a8a7 _comma_ where a5a9a7 is a threshold.
E06-1043.txt,175,For evaluation_comma_ we set a5a9a7 to a1 _comma_ based on the distribution of a10 and through examining the development data.
E06-1043.txt,176,We evaluate the appropriateness of this approach in determining the Cform s of idiomatic pairs by verifying its predicted forms against ODCIE and CCID.
E06-1043.txt,177,Specifically_comma_ for each of the a1 a67a53a67 idiomatic pairs in TESTa63 a85a88a85 _comma_ we calculate the precision and recall of its predicted Cforms those whose a0 scores are above a5a11a7 _comma_ compared to the Cforms listed in the two dictionaries.
E06-1043.txt,178,The average precision across the 100 test pairs is 81.7 _comma_ and the average recall is 88.0 with 69 of the pairs having 100 precision and 100 recall .
E06-1043.txt,179,Moreover_comma_ we find that for the overwhelming majority of the pairs_comma_ a8 a3 a33 _comma_ the predicted Cform with the highest a0 score appears in the dictionary entry of the pair.
E06-1043.txt,180,Thus_comma_ our method of detecting Cforms performs quite well.
E06-1043.txt,181,6 Discussion and Conclusions The significance of the role idioms play in language has long been recognized.
E06-1043.txt,182,However_comma_ due to their peculiar behaviour_comma_ idioms have been mostly overlooked by the NLP community.
E06-1043.txt,183,Recently_comma_ there has been growing awareness of the importance of identifying non compositional multiword expressions MWEs .
E06-1043.txt,184,Nonetheless_comma_ most research on the topic has focused on compound nouns and verb particle constructions.
E06-1043.txt,185,Earlier work on idiomshaveonlytouched thesurface oftheproblem_comma_ failing to propose explicit mechanisms for appropriately handling them.
E06-1043.txt,186,Here_comma_ we provide effective mechanisms for the treatment of a broadly documented and crosslinguistically frequent class of idioms_comma_ i.e._comma_ VNICs.
E06-1043.txt,187,Earlier research on the lexical encoding of idioms mainly relied on the existence of human annotations_comma_ especially for detecting which syntactic variations e.g._comma_ passivization an idiom can undergo Villavicencio et al._comma_ 2004 .
E06-1043.txt,188,We propose techniques for the automatic acquisition and encoding of knowledge about the lexicosyntactic behaviour of idiomatic combinations.
E06-1043.txt,189,We put forwardameans for automatically discovering the set ofsyntactic variations that aretolerated byaVNIC and that should be included in its lexical representation.
E06-1043.txt,190,Moreover_comma_ weincorporate suchinformation into statistical measures that effectively predict the idiomaticity level of a given expression.
E06-1043.txt,191,In this regard_comma_ our work relates to previous studies on determining the compositionality inverse of idiomaticity of MWEs other than idioms.
E06-1043.txt,192,Most previous work on compositionality of MWEs either treat them as collocations Smadja_comma_ 1993 _comma_ or examine the distributional similarity between the expression and its constituents McCarthy et al._comma_ 2003 Baldwin et al._comma_ 2003 Bannard et al._comma_ 2003 .
E06-1043.txt,193,Lin 1999 and Wermter and Hahn 2005 go one step further and look into a linguistic property of non compositional compounds their lexical fixedness to identify them.
E06-1043.txt,194,Venkatapathy and Joshi 2005 combine aspects of the above mentioned work_comma_ by incorporatinglexical fixedness_comma_ collocation based_comma_ anddistributional similarity measures into a set of features which are used to rank verb noun combinations according to their compositionality.
E06-1043.txt,195,Our work differs from such studies in that it carefully examines several linguistic properties of VNICs that distinguish them from literal compositional combinations.
E06-1043.txt,196,Moreover_comma_ we suggest novel techniques for translating such characteristics into measures that predict the idiomaticity level of verb noun combinations.
E06-1043.txt,197,More specifically_comma_ we propose statistical measures that quantify the degree of lexical_comma_ syntactic_comma_ and overall fixedness of such combinations.
E06-1043.txt,198,We demonstrate 343 that these measures can be successfully applied to the task of automatically distinguishing idiomatic combinations from non idiomatic ones.
E06-1043.txt,199,We also show that our syntactic and overall fixedness measures substantially outperform a widely used measure of collocation_comma_ a40a42a41a44a43 _comma_ even when the latter takes syntactic relations into account.
E06-1043.txt,200,Others have also drawn on the notion of syntactic fixedness for idiom detection_comma_ though specific to a highly constrained type of idiom Widdows and Dorow_comma_ 2005 .
E06-1043.txt,201,Our syntactic fixedness measure looks into a broader set of patterns associated with a large class of idiomatic expressions.
E06-1043.txt,202,Moreover_comma_ our approach is general and can be easily extended to other idiomatic combinations.
E06-1043.txt,203,Each measure we use to identify VNICs captures a different aspect of idiomaticity a40a73a41a74a43 reflects the statistical idiosyncrasy of VNICs_comma_ while the fixedness measures draw on their lexicosyntactic peculiarities.
E06-1043.txt,204,Our ongoing work focuses on combining these measures to distinguish VNICs from other idiosyncratic verb noun combinations that are neither purely idiomatic nor completely literal_comma_ so that we can identify linguistically plausible classes of verb noun combinations on this continuum Fazly and Stevenson_comma_ 2005 . .
E06-2022.txt,1,complex terms composed from Greek or Latin elements are frequent in scientific and technical texts. Word forming units are thus relevant cues for the identification of terms in domainspecific texts.
E06-2022.txt,2,This article describes a method for the automatic extraction of terms relying on the detection of classical prefixes and word initial combining forms.
E06-2022.txt,3,Word forming units are identified using a regular expression.
E06-2022.txt,4,The system then extracts terms by selecting words which either begin or coalesce with these elements.
E06-2022.txt,5,Next_comma_ terms are grouped in families which are displayed as a weighted list in HTML format.
E06-2022.txt,7,Many methods for the automatic extraction of termsmakeuseofpatternsdescribingthestructure of terms.
E06-2022.txt,8,This approach is especially helpful for multi word terms.
E06-2022.txt,9,Depending on the method_comma_ patterns rely on morpho syntactic properties Daille_comma_ 1996 Ibekwe SanJuan_comma_ 1998 _comma_ the co occurrence of terms and connectors Enguehard_comma_ 1992 Baroni and Bernardini_comma_ 2004 or the alternation of informative and non informative words Vergne_comma_ 2005 .
E06-2022.txt,10,These patterns use words as basic units and thus apply to multi word terms.
E06-2022.txt,11,Methods for the acquisition of single word terms generally depend on frequency related information.
E06-2022.txt,12,For instance_comma_ the frequency of occurrence of a word in a domain specific corpus can be compared with its frequency of occurrence in a reference corpus Rayson and Garside_comma_ 2000 Baroni and Bernardini_comma_ 2004 .
E06-2022.txt,13,Technical words usually have a high relative frequency difference between the domainspecific corpus and the reference corpus.
E06-2022.txt,14,In this paper_comma_ we present a pattern based technique to extract single word terms.
E06-2022.txt,15,In technical and scientific domains like medicine many terms are derivatives or neoclassical compounds Cottez_comma_ 1984 .
E06-2022.txt,16,There are several types of classical word forming units prefixes extra _comma_ anti _comma_ initial combining forms hydro _comma_ pharmaco _comma_ suffixes ism and final combining forms graphy_comma_logy .
E06-2022.txt,17,Interestingly_comma_ these units are rather constant in many European languages Namer_comma_ 2005 .
E06-2022.txt,18,Consequently_comma_ insteadofrelyingonasubworddictionary to analyse compounds like Schulz et al._comma_ 2002 _comma_ our method makes use of these regularities to automatically extract prefixes and initial combining forms from corpora.
E06-2022.txt,19,The system then identifies terms by selecting words which either begin or coalesce with these units.
E06-2022.txt,20,Moreover_comma_ forming elements are used to group terms in morphological and hence semantic families.
E06-2022.txt,21,The different stages of the process are detailed in section 2.
E06-2022.txt,22,Section 3 describes the results of experiments performed on four corpora_comma_ in English and in French.
E06-2022.txt,23,2 Description of the method 2.1 Extraction of words The system takes as input a corpus of texts.
E06-2022.txt,24,Paragraphs written in another language than the target language are filtered out.
E06-2022.txt,25,Texts are then tokenised and words are converted to lowercase.
E06-2022.txt,26,Besides_comma_ words containing digits or other non word characters are eliminated.
E06-2022.txt,27,However_comma_ hyphenated words are kept since hyphens mark morpheme boundaries.
E06-2022.txt,28,This preliminary step produces a word frequency list for the corpus.
E06-2022.txt,30,This regular expression represents character strings whose length is higher or equal to 4_comma_ ending with a_comma_ i or o and immediately followed by a hyphen.
E06-2022.txt,31,The first part of the regular expression accounts for words where several prefixes or combining forms follow one another as for instance in the French word h epato gastro ent erologues .
E06-2022.txt,32,This regular expression applies to English but also to other languages like French or German see for instance chimio radioth erapie in French_comma_ chemo radiotherapy in English or Chemoradiotherapie in German.
E06-2022.txt,33,2.3 Identification of terms Terms are identified using the following pattern describing their morphological structure E W where E is a prefix or combining form and W is a wordwhoselengthishigherthan3 the character represents the possible succession of several E elements at the beginning of a term.
E06-2022.txt,34,Prefixes and combining forms may be separated by a hyphen.
E06-2022.txt,35,When this pattern applies to one of the words in the corpus_comma_ two terms are recognised_comma_ one with a E W structure and the other with a W structure.
E06-2022.txt,36,For instance_comma_ given the word ferrobasalts _comma_ the system identifies the terms ferrobasalts E W and basalts W .
E06-2022.txt,37,2.4 Conflation of terms Term variants are grouped in order to ease the analysis of results.
E06-2022.txt,38,The method for terms conflation can be decomposed in two stages 1.
E06-2022.txt,39,Terms containing the same word W belong to the same family_comma_ represented by the word W.
E06-2022.txt,40,For instance_comma_ both chemotherapy and radiotherapy contain the word therapy they belong to the same family of terms_comma_ represented by the word therapy .
E06-2022.txt,42,Two families are merged if they are represented by words sharing the same initial substring with a minimum initial substring length of 4 and if the same prefix or combining form occurs in one term of each family.
E06-2022.txt,43,Consider for instance the families F1 oncology_comma_ psycho oncology_comma_ radiooncology_comma_ neuro oncology_comma_ psychooncology_comma_ neurooncology and F2 oncologist_comma_ neurooncologist .
E06-2022.txt,44,The terms representing F1 oncology and F2 oncologist share an initial substring of length 7.
E06-2022.txt,45,Moreover the terms neuro oncology from F1 and neurooncologist from F2 contain the combining form neuro .
E06-2022.txt,46,Families F1 and F2 are therefore united.
E06-2022.txt,47,When terms have been conflated_comma_ we select the most frequent term as a family s representative.
E06-2022.txt,48,2.5 Data visualisation The results obtained are displayed as a weighted list in HTML format.
E06-2022.txt,49,Such lists_comma_ also named heat maps or tag clouds when they describe tags1 usually represent the terms and topics which appear most frequently on websites or RSS feeds Wikipedia_comma_ 2006 .
E06-2022.txt,50,They can also be used to represent any kind of word list V eronis_comma_ 2005 .
E06-2022.txt,51,Different colours and font sizes are used depending on the word s frequency of occurrence.
E06-2022.txt,52,We have adapted this method to visualise the list of extracted terms.
E06-2022.txt,53,Since several hundred terms may be extracted_comma_ only the terms representing a family are displayed on the weighted list.
E06-2022.txt,54,Weight is given by the cumulated frequency of all the terms belonging to the family see Figure 1 .
E06-2022.txt,55,Figure 1 Term cloud example Corpus BC en Further information terms and frequencies is displayed thanks to tooltips see Figure 2 _comma_ using the JavaScript overLIB libray http www. bosrup.com web overlib .
E06-2022.txt,56,1See for example TagCloud http www. tagcloud.com 172 Figure 2 Detailed term family displayed as a tooltip Corpus V fr 3 Experiments and results 3.1 Corpora The system has been experimented on 4 corpora covering the domains of volcanology V and breast cancer BC _comma_ in English en and in French fr .
E06-2022.txt,57,The corpora have been automatically built from the web_comma_ using the methodology described in Baroni and Bernardini_comma_ 2004 _comma_ via the Yahoo
E06-2022.txt,58,Search Web Services http developer. yahoo.net search .
E06-2022.txt,59,The size of the corpora obtained are given in Table 1.
E06-2022.txt,60,This table also gives the number of key words_comma_ i.e._comma_ single word terms extracted by comparing the frequency of occurrence of words in both corpora for each language Rayson and Garside_comma_ 2000 .
E06-2022.txt,61,Only terms with a log likelihood of 3.8 or higher p 0.05 have been kept in the key words list.
E06-2022.txt,62,Table 2 gives a numerical overview of the results obtained by our method.
E06-2022.txt,63,Corpus Tokens Word forms Key words BC fr 1_comma_451_comma_809 46_comma_834 13_comma_700 BC en 7_comma_044_comma_146 88_comma_726 17_comma_602 V fr 1_comma_777_comma_030 59_comma_909 13_comma_673 V en 2_comma_929_comma_591 48_comma_257 19_comma_641 Table 1 Size of the corpora 3.2 Prefixes and initial combining forms As shown by Table 2_comma_ the number of prefixes and initial combining forms identified is proportionally less for the volcanology corpora both in English and in French.
E06-2022.txt,64,Medical corpora seem to be more adapted to the method since the numCorpus Word forming elements Terms Term families BC fr 334 4_comma_248 911 BC en 382 5_comma_444 1_comma_338 V fr 182 1_comma_842 583 V en 188 1_comma_648 564 Table2 Numberofword formingelements_comma_ terms and term families identified for each corpus ber of terms extracted is higher.
E06-2022.txt,65,The prefixes and combining forms identified are also highly dependent on the corpus domain.
E06-2022.txt,66,For instance_comma_ amongst the most frequent combining forms extracted for the BC corpora_comma_ we find radio and chemo chimio in French and for the V corpora_comma_ strato and volcano .
E06-2022.txt,67,3.3 Terms The overlap percentage between the list of terms and the list of key words ranges from 38.65 V fr to 56.92 V en of the total amount of terms extracted.
E06-2022.txt,68,If we compare both the list of key words and the list of terms extracted for the BC en corpus with the Unified Medical Language System Metathesaurus http www.nlm.nih.gov research umls we notice that some highly specific terms like disease _comma_ blood or x ray are not identified by our method_comma_ while they occur in the key words list.
E06-2022.txt,69,These are usually morphologically simple terms_comma_ also used in everyday language.
E06-2022.txt,70,Conversely_comma_ terms with low frequency like adenoacanthoma _comma_ chondroma or mammotomy are correctly identified by the patternbased approach but are missing in the key words list.
E06-2022.txt,71,Both methods are therefore complementary.
E06-2022.txt,72,In some cases_comma_ stop words are extracted.
E06-2022.txt,73,This is a side effect of the pattern used to retrieve terms.
E06-2022.txt,74,Remember that terms are words which coalesce with combining forms_comma_ possibly with hyphenation.
E06-2022.txt,75,In English hyphens are sometimes mistakenly used instead of the dash to mark comment clauses.
E06-2022.txt,76,Consider for instance the following sentence As this magma which drives one of the worlds largest volcanic systems rises_comma_ it pushes up the Earths crust beneath the Yellowstone Plateau. .
E06-2022.txt,77,Here magma is identified as a combining form since it ends with a and is directly followed by a hyphen.
E06-2022.txt,78,Consequently_comma_ which is wrongly identified as a term.
E06-2022.txt,79,173 3.4 Term families Several types of term variants are grouped by the termconflationalgorithm a graphicalandorthographical variants like tumour British variant and tumor American variant b inflectional variants like tumor and tumors c derivational variants like tumor and tumoral .
E06-2022.txt,80,Two types of conflation errors may however occur over conflation_comma_ i.e._comma_ the conflation of terms which do not belong to the same morphological family and under conflation_comma_ i.e. the absence of conflation for morphologically related terms.
E06-2022.txt,81,Some cases of over conflation are obvious_comma_ such as the grouping of significant with cant .
E06-2022.txt,82,In some other cases it is more difficult to tell.
E06-2022.txt,83,This especially applies to the conflation of terms composed of word final combining forms like gram or graph .
E06-2022.txt,84,Under conflation occurs when no combining form is shared between terms belonging to families represented by graphically similar terms.
E06-2022.txt,85,For instance_comma_ the following term families are extracted from the French volcanology corpus V fr F1 basalte_comma_ m etabasalte_comma_ m eta basalte _comma_ F2 basaltes_comma_ ferro basaltes_comma_ pal eobasaltes and F3 basaltique_comma_ and esitico basaltique .
E06-2022.txt,86,These families are not conflated_comma_ even though they obviously belong to the same morphological family.
E06-2022.txt,87,4 Conclusion We have presented a method for the automatic acquisition of terms from domain specific texts using morphological structure.
E06-2022.txt,88,The method also groups terms in morphological families.
E06-2022.txt,89,Families are displayed as a weighted list_comma_ thus giving an instant overview of the main topics in the corpus under study.
E06-2022.txt,90,Results obtained from the first experiments confirm the usefulness of a morphological pattern based approach for the extraction of terms from domain specific corpora and especially medical texts.
E06-2022.txt,91,The method for the identification of compound words could be improved by an automatic approach to morphological segmentation as done by Creutz and Lagus_comma_ 2004 .
E06-2022.txt,92,Term clustering could be ameliorated as well by investigating the usefulness of stemming to avoid underconflation. .
E06-1035.txt,1,this paper_comma_ we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue. We extend prior work in two ways.
E06-1035.txt,2,We first apply approaches that have been proposed for predicting top level topic shifts to the problem of identifying subtopic boundaries.
E06-1035.txt,3,We then explore the impact on performance of using ASR output as opposed to human transcription.
E06-1035.txt,4,Examination of the effect of features shows that predicting top level and predicting subtopic boundaries are two distinct tasks 1 for predicting subtopic boundaries_comma_ the lexical cohesion based approach alone can achieve competitive results_comma_ 2 for predicting top level boundaries_comma_ the machine learning approach that combines lexical cohesion and conversational features performs best_comma_ and 3 conversational cues_comma_ such as cue phrases and overlapping speech_comma_ are better indicators for the toplevel prediction task.
E06-1035.txt,5,We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical cohesion and conversational features_comma_ but do not change the general preference of approach for the two tasks.
E06-1035.txt,7,Text segmentation_comma_ i.e._comma_ determining the points at which the topic changes in a stream of text_comma_ plays an important role in applications such as topic detection and tracking_comma_ summarization_comma_ automatic genre detection and information retrieval and extraction Pevzner and Hearst_comma_ 2002 .
E06-1035.txt,8,In recent work_comma_ researchers have applied these techniques to corpora such as newswire feeds_comma_ transcripts of radio broadcasts_comma_ and spoken dialogues_comma_ in order to facilitate browsing_comma_ information retrieval_comma_ and topic detection Allan et al._comma_ 1998 van Mulbregt et al._comma_ 1999 Shriberg et al._comma_ 2000 Dharanipragada et al._comma_ 2000 Blei and Moreno_comma_ 2001 Christensen et al._comma_ 2005 .
E06-1035.txt,9,In this paper_comma_ we focus on segmentation of multiparty dialogues_comma_ in particular recordings of small group meetings.
E06-1035.txt,10,We compare models based solely on lexical information_comma_ which are common in approaches to automatic segmentation of text_comma_ with models that combine lexical and conversational features.
E06-1035.txt,11,Because tasks as diverse as browsing_comma_ on the one hand_comma_ and summarization_comma_ on the other_comma_ require different levels of granularity of segmentation_comma_ we explore the performance of our models for two tasks hypothesizing where major topic changes occur and hypothesizing where more subtle nested topic shifts occur.
E06-1035.txt,12,In addition_comma_ because we do not wish to make the assumption that high quality transcripts of meeting records_comma_ such as those produced by human transcribers_comma_ will be commonly available_comma_ we require algorithms that operate directly on automatic speech recognition ASR output.
E06-1035.txt,13,2 Previous Work Prior research on segmentation of spoken documents uses approaches that were developed for text segmentation_comma_ and that are based solely on textual cues.
E06-1035.txt,14,These include algorithms based on lexical cohesion Galley et al._comma_ 2003 Stokes et al._comma_ 2004 _comma_ as well as models using annotated features e.g._comma_ cue phrases_comma_ part of speech tags_comma_ coreference relations that have been determined to correlate with segment boundaries Gavalda et al._comma_ 1997 Beeferman et al._comma_ 1999 .
E06-1035.txt,15,Blei et al. 2001 273 and van Mulbregt et al. 1999 use topic language models and variants of the hidden Markov model HMM to identify topic segments.
E06-1035.txt,16,Recent systems achieve good results for predicting topic boundaries when trained and tested on human transcriptions.
E06-1035.txt,17,For example_comma_ Stokes et al. 2004 report an error rate Pk of 0.25 on segmenting broadcast news stories using unsupervised lexical cohesion based approaches.
E06-1035.txt,18,However_comma_ topic segmentation of multiparty dialogue seems to be a considerably harder task.
E06-1035.txt,19,Galley et al. 2003 report an error rate Pk of 0.319 for the task of predicting major topic segments in meetings.1 Although recordings of multiparty dialogue lack the distinct segmentation cues commonly found in text e.g._comma_ headings_comma_ paragraph breaks_comma_ andothertypographic cues ornewsstory segmentation e.g._comma_ the distinction between anchor and interview segments _comma_ they contain conversationbased features that may be of use for automatic segmentation.
E06-1035.txt,20,These include silence_comma_ overlap rate_comma_ speaker activity change Galley et al._comma_ 2003 _comma_ and cross speaker linking information_comma_ such as adjacency pairs Zechner and Waibel_comma_ 2000 .
E06-1035.txt,21,Many of these features can be expected to be complimentary.
E06-1035.txt,22,For segmenting spontaneous multiparty dialogue into major topic segments_comma_ Galley et al. 2003 have shownthat amodel integrating lexical and conversation based features outperforms one based on solely lexical cohesion information.
E06-1035.txt,23,However_comma_ the automatic segmentation models in prior work were developed for predicting toplevel topic segments.
E06-1035.txt,24,In addition_comma_ compared to read speech and two party dialogue_comma_ multi party dialogues typically exhibit a considerably higher word error rate WER Morgan et al._comma_ 2003 .
E06-1035.txt,25,We expect that incorrectly recognized words will impair the robustness of lexical cohesion based approaches and extraction of conversation based discourse cues and other features.
E06-1035.txt,26,Past research on broadcast news story segmentation using ASR transcription has shown performance degradation from 5 to 38 using different evaluation metrics van Mulbregt et al._comma_ 1999 Shriberg et al._comma_ 2000 Blei and Moreno_comma_ 2001 .
E06-1035.txt,27,However_comma_ no prior study has reported directly on the extent of this degradation on the performance of a more subtle topic segmentation task and in spontaneous multiparty dialogue.
E06-1035.txt,28,In this paper_comma_ we extend prior work by 1For the definition of Pk and Wd_comma_ please refer to section 3.4.1 investigating the effect of using ASRoutput on the models that have previously been proposed.
E06-1035.txt,29,In addition_comma_ we aim to find useful features and models for the subtopic prediction task.
E06-1035.txt,30,3 Method 3.1 Data In this study_comma_ we used the ICSI meeting corpus LDC2004S02 .
E06-1035.txt,31,Seventy five natural meetings of ICSI research groups were recorded using closetalking far field head mounted microphones and four desktop PZM microphones.
E06-1035.txt,32,The corpus includes human transcriptions of all meetings.
E06-1035.txt,33,We addedASRtranscriptions ofall75meetingswhich were produced by Hain 2005 _comma_ with an average WER of roughly 30 .
E06-1035.txt,34,The ASR system used a vocabulary of 50_comma_000 words_comma_ together with a trigram language model trained on a combination of in domain meeting data_comma_ related texts found by web search_comma_ conversational telephone speech CTS transcripts and broadcast news transcripts about 109 words in total _comma_ resulting in a test set perplexity of about 80.
E06-1035.txt,35,The acoustic models comprised a set of contextdependent hidden Markov models_comma_ using gaussian mixture model output distributions.
E06-1035.txt,36,These were initially trained on CTSacoustic training data_comma_ and were adapted to the ICSI meetings domain using maximum a posteriori MAP adaptation.
E06-1035.txt,37,Further adaptation to individual speakers was achieved using vocal tract length normalization and maximum likelihood linear regression.
E06-1035.txt,38,A four fold crossvalidation technique was employed four recognizers were trained_comma_ with each employing 75 of the ICSI meetings as acoustic and language model training data_comma_ and then used to recognize the remaining 25 of the meetings.
E06-1035.txt,39,3.2 Fine grained and coarse grained topics We characterize a dialogue as a sequence of topical segments that may be further divided into subtopic segments.
E06-1035.txt,40,For example_comma_ the 60 minute meeting Bed003_comma_ whose theme is the planning of aresearch project on automatic speech recognition can be described by 4 major topics_comma_ from opening to general discourse features for higher layers to how to proceed to closing .
E06-1035.txt,41,Depending on the complexity_comma_ each topic can be further divided into a number of subtopics.
E06-1035.txt,42,For example_comma_ how to proceed can be subdivided to 4 subtopic segments_comma_ segmenting off regions of features _comma_ 274 ad hoc probabilities _comma_ data collection and experimental setup .
E06-1035.txt,43,Three human annotators at our site used a tailored tool to perform topic segmentation in which they could choose to decompose a topic into subtopics_comma_ with at most three levels in the resulting hierarchy.
E06-1035.txt,44,Topics are described to the annotators as what people in a meeting were talking about.
E06-1035.txt,45,Annotators were asked to provide a free text label for each topic segment they were encouraged to use keywords drawn from the transcription in these labels_comma_ and we provided some standard labels for non content topics_comma_ such as opening and chitchat _comma_ to impose consistency.
E06-1035.txt,46,For our initial experiments with automatic segmentation at different levels of granularity_comma_ we flattened the subtopic structure and consider only two levels ofsegmentation top level topics andallsubtopics.
E06-1035.txt,47,To establish reliability of our annotation procedure_comma_ we calculated kappa statistics between the annotations of each pair of coders.
E06-1035.txt,48,Our analysis indicates human annotators achieve  0.79 agreement on top level segment boundaries and  0.73 agreement on subtopic boundaries.
E06-1035.txt,49,The level of agreement confirms good replicability of the annotation procedure.
E06-1035.txt,50,3.3 Probabilistic models Our goal is to investigate the impact of ASR errors on the selection of features and the choice of models for segmenting topics at different levels of granularity.
E06-1035.txt,51,We compare two segmentation models 1 an unsupervised lexical cohesion based model LM using solely lexical cohesion information_comma_ and 2 feature based combined models CM that are trained on a combination of lexical cohesion and conversational features.
E06-1035.txt,52,3.3.1 Lexical cohesion based model In this study_comma_ we use Galley et al. s 2003 LCSeg algorithm_comma_ a variant of TextTiling Hearst_comma_ 1997 .
E06-1035.txt,53,LCSeg hypothesizes that a major topic shift is likely to occur where strong term repetitions start and end.
E06-1035.txt,54,The algorithm works with two adjacent analysis windows_comma_ each of a fixed size which is empirically determined.
E06-1035.txt,55,For each utterance boundary_comma_ LCSeg calculates a lexical cohesion score by computing the cosine similarity at the transition between the twowindows.
E06-1035.txt,56,Lowsimilarity indicates low lexical cohesion_comma_ and a sharp change in lexical cohesion score indicates a high probability of an actual topic boundary.
E06-1035.txt,57,The principal difference between LCSeg and TextTiling is that LCSeg measures similarity in terms of lexical chains i.e._comma_ term repetitions _comma_ whereas TextTiling computes similarity using word counts.
E06-1035.txt,58,3.3.2 Integrating lexical and conversation based features We also used machine learning approaches that integrate features into a combined model_comma_ casting topic segmentation as a binary classification task.
E06-1035.txt,59,Under this supervised learning scheme_comma_ a training set in which each potential topic boundary2 is labelled as either positive POS or negative NEG is used to train a classifier to predict whether each unseen example in the test set belongs to the class POS or NEG.
E06-1035.txt,60,Our objective here is to determine whether the advantage of integrating lexical and conversational features also improves automatic topic segmentation at the finer granularity of subtopic levels_comma_ as well as when ASR transcriptions are used.
E06-1035.txt,61,For this study_comma_ we trained decision trees c4.5 to learn the best indicators of topic boundaries.
E06-1035.txt,62,We first used features extracted with the optimal window size reported to perform best in Galley et al. 2003 for segmenting meeting transcripts into major topical units.
E06-1035.txt,63,In particular_comma_ this study uses the following features 1 lexical cohesion features the raw lexical cohesion score and probability of topic shift indicated by the sharpness of change in lexical cohesion score_comma_ and 2 conversational features the number of cue phrases in an analysis window of 5 seconds preceding and following the potential boundary_comma_ and other interactional features_comma_ including similarity of speaker activity measured as a change in probability distribution of number of words spoken by each speaker within 5 seconds preceding and following each potential boundary_comma_ the amount of overlapping speech within 30 seconds following each potential boundary_comma_ and the amount of silence between speaker turns within 30 seconds preceding each potential boundary.
E06-1035.txt,64,3.4 Evaluation To compare to prior work_comma_ we perform a 25fold leave one out cross validation on the set of 25 ICSI meetings that were used in Galley et 2In this study_comma_ the end of each speaker turn is a potential segment boundary.
E06-1035.txt,65,If there is a pause of more than 1 second within a single speaker turn_comma_ the turn is divided at the beginning of the pause creating a potential segment boundary.
E06-1035.txt,66,275 al. 2003 .
E06-1035.txt,67,We repeated the procedure to evaluate the accuracy using the lexical cohesion and combined models on both human and ASR transcriptions.
E06-1035.txt,68,In each evaluation_comma_ we trained the automatic segmentation models for two tasks predicting subtopic boundaries SUB and predicting only top level boundaries TOP .
E06-1035.txt,69,3.4.1 Evaluation metrics In order to be able to compare our results directly with previous work_comma_ we first report our results using the standard error rate metrics of Pk and Wd.
E06-1035.txt,70,Pk Beeferman et al._comma_ 1999 is the probability that two utterances drawn randomly from a document inourcase_comma_ ameeting transcript areincorrectly identified as belonging to the same topic segment.
E06-1035.txt,71,WindowDiff Wd Pevzner and Hearst_comma_ 2002 calculates the error rate by moving a sliding window across the meeting transcript counting the number of times the hypothesized and reference segment boundaries are different.
E06-1035.txt,72,3.4.2 Baseline To compute a baseline_comma_ we follow Kan 2003 and Hearst 1997 in using Monte Carlo simulated segments.
E06-1035.txt,73,For the corpus used as training data in the experiments_comma_ the probability of a potential topic boundary being an actual one is approximately 2.2 for all subtopic segments_comma_ and 0.69 fortop level topic segments.
E06-1035.txt,74,Therefore_comma_ the Monte Carlo simulation algorithm predicts that a speaker turnisasegment boundary withtheseprobabilities for the two different segmentation tasks.
E06-1035.txt,75,We executed the algorithm 10_comma_000 times on each meeting and averaged the scores to form the baseline for our experiments.
E06-1035.txt,76,3.4.3 Topline For the 24 meetings that were used in training_comma_ we have top level topic boundaries annotated by coders at Columbia University Col and in ourlab at Edinburgh Edi .
E06-1035.txt,77,We take the majority opinion on each segment boundary from the Col annotators as reference segments.
E06-1035.txt,78,For the Edi annotations of top level topic segments_comma_ where multiple annotations exist_comma_ we choose one randomly.
E06-1035.txt,79,The topline is then computed as the Pk score comparing the Col majority annotation to the Edi annotation.
E06-1035.txt,80,4 Results 4.1 Experiment 1 Predicting top level and subtopic segment boundaries The meetings in the ICSI corpus last approximately 1 hour and have an average of 8 10 toplevel topic segments.
E06-1035.txt,81,In order to facilitate meeting browsing and question answering_comma_ we believe it is useful to include subtopic boundaries in order to narrow in more accurately on the portion of the meeting that contains the information the user needs.
E06-1035.txt,82,Therefore_comma_ we performed experiments aimed at analysing how the LM and CM segmentation models behave in predicting segment boundaries at the two different levels of granularity.
E06-1035.txt,83,All of the results are reported on the test set.
E06-1035.txt,84,Table 1 shows the performance of the lexical cohesion model LM andthecombined model CM integrating the lexical cohesion and conversational features discussed in Section 3.3.2.3 For the task of predicting top level topic boundaries from human transcripts_comma_ CM outperforms LM.
E06-1035.txt,85,LM tends to over predict on the top level_comma_ resulting in a higher false alarm rate.
E06-1035.txt,86,However_comma_ for the task of predicting subtopic shifts_comma_ LM alone is considerably better than CM.
E06-1035.txt,87,Error Rate Transcript ASR Models Pk Wd Pk Wd LM SUB 32.31 38.18 32.91 37.13 LCSeg TOP 36.50 46.57 38.02 48.18 CM SUB 36.90 38.68 38.19 n a C4.5 TOP 28.35 29.52 28.38 n a Table 1 Performance comparison of probabilistic segmentation models.
E06-1035.txt,88,In order to support browsing during the meeting orshortly thereafter_comma_ automatic topic segmentation willhave to operate on the transcriptions produced by ASR.
E06-1035.txt,89,First note from Table 1 that the preference of models for segmentation at the two different levels of granularity is the same for ASR and human transcriptions.
E06-1035.txt,90,CM is better for predicting top level boundaries and LM is better for predicting subtopic boundaries.
E06-1035.txt,91,This suggests that these 3We do not report Wd scores for the combined model CM on ASR output because this model predicted 0 segment boundaries when operating on ASR output.
E06-1035.txt,92,In our experience_comma_ CM routinely underpredicted the number of segment boundaries_comma_ and due to the nature of the Wd metric_comma_ it should not be used when there are 0 hypothesized topic boundaries.
E06-1035.txt,93,276 are two distinct tasks_comma_ regardless of whether the system operates on human produced transcription or ASR output.
E06-1035.txt,94,Subtopics are better characterized by lexical cohesion_comma_ whereas top level topic shifts are signalled by conversational features as well as lexical cohesion based features.
E06-1035.txt,95,4.1.1 Effect of feature combinations predicting from human transcripts Next_comma_ we wish to determine which features in the combined model are most effective forpredicting topic segments at the two levels of granularity.
E06-1035.txt,96,Table 2 gives the average Pk for all 25 meetings in the test set_comma_ using the features described in Section 3.3.2.
E06-1035.txt,97,We group the features into four classes 1 lexicalcohesion based features LF including lexical cohesion value LCV and estimated posterior probability LCP 2 interaction features IF the amount of overlapping speech OVR _comma_ the amount of silence between speaker segments GAP _comma_ similarity of speaker activity ACT 3 cuephrase feature CUE and 4 allavailable features ALL .
E06-1035.txt,98,For comparison we also report the baseline see Section 3.4.2 generated by Monte Carlo algorithm MC B .
E06-1035.txt,99,All of the models using one or more features from these classes outperform the baseline model.
E06-1035.txt,100,A one way ANOVA revealed this reliable effect on the top level segmentation F 7_comma_192 17.46_comma_p 0.01 as well as onthe subtopic segmentation task F 7_comma_192 5.862_comma_p 0.01 .
E06-1035.txt,101,TRANSCRIPT Error Rate Pk Feature set SUB TOP MC B 46.61 48.43 LF LCV LCP 38.13 29.92 IF ACT OVR GAP 38.87 30.11 IF CUE 38.87 30.11 LF ACT 38.70 30.10 LF OVR 38.56 29.48 LF GAP 38.50 29.87 LF IF 38.11 29.61 LF CUE 37.46 29.18 ALL LF IF CUE 36.90 28.35 Table 2 Effect of different feature combinations for predicting topic boundaries from human transcripts.
E06-1035.txt,102,MC B is the randomly generated baseline.
E06-1035.txt,103,As shown in Table 2_comma_ the best performing model for predicting top level segments is the one using all of the features ALL .
E06-1035.txt,104,This is not surprising_comma_ because these were the features that Galley et al. 2003 found to be most effective for predicting top level segment boundaries in their combined model.
E06-1035.txt,105,Looking at the results in more detail_comma_ we see that when we begin with LF features alone and add other features one by one_comma_ the only model other than ALL that achieves significant4 improvement p 0.05 over LF is LF CUE_comma_ the model that combines lexical cohesion features with cue phrases.
E06-1035.txt,106,When we look at the results for predicting subtopic boundaries_comma_ we again see that the best performing model is the one using all features ALL .
E06-1035.txt,107,Models using lexical cohesion features alone LF and lexical cohesion features with cue phrases LF CUE both yield significantly better results than using interactional features IF alone p 0.01 _comma_ or using them with cue phrase features IF CUE p 0.01 .
E06-1035.txt,108,Again_comma_ none of the interactional features used in combination with LF significantly improves performance.
E06-1035.txt,109,Indeed_comma_ adding speaker activity change LF ACT degrades the performance p 0.05 .
E06-1035.txt,110,Therefore_comma_ we conclude that for predicting both top level and subtopic boundaries from human transcriptions_comma_ the most important features are the lexical cohesion based features LF _comma_ followed by cue phrases CUE _comma_ with interactional features contributing to improved performance only when used in combination with LF and CUE.
E06-1035.txt,111,However_comma_ a closer look at the Pk scores in Table 2_comma_ adds further evidence to our hypothesis that predicting subtopics may be a different task from predicting top level topics.
E06-1035.txt,112,Subtopic shifts occur more frequently_comma_ and often without clear conversational cues.
E06-1035.txt,113,This is suggested by the fact that absolute performance on subtopic prediction degrades when any of the interactional features are combined with the lexical cohesion features.
E06-1035.txt,114,In contrast_comma_ the interactional features slightly improve performance when predicting top level segments.
E06-1035.txt,115,Moreover_comma_ the fact that the feature OVR has a positive impact on the model for predicting top level topic boundaries_comma_ but does not improve the model for predicting subtopic boundaries reveals that having less overlapping speech isamore prominent phenomenon in major topic shifts than 4Because we do not wish to make assumptions about the underlying distribution of error rates_comma_ and error rates are not measured on an interval level_comma_ we use a non parametric sign test throughout these experiments to compute statistical significance.
E06-1035.txt,116,277 in subtopic shifts.
E06-1035.txt,117,4.1.2 Effect of feature combinations predicting from ASR output Features extracted from ASRtranscripts are distinct from those extracted from human transcripts in at least three ways 1 incorrectly recognized words incur erroneous lexical cohesion features LF _comma_ 2 incorrectly recognized words incur erroneous cue phrase features CUE _comma_ and 3 the ASR system recognizes less overlapping speech OVR .
E06-1035.txt,118,Incontrast tothefindingthat integrating conversational features with lexical cohesion features is useful for prediction from human transcripts_comma_ Table 3 shows that when operating on ASR output_comma_ neither adding interactional nor cue phrase features improvestheperformance ofthemodelusing only lexical cohesion features.
E06-1035.txt,119,In fact_comma_ the model using allfeatures ALL issignificantly worsethan the model using only lexical cohesion based features LF .Thissuggests thatwemustexplore new features that can lessen the perplexity introduced by ASR outputs in order to train a better model.
E06-1035.txt,120,ASR Error Rate Pk Feature set SUB TOP MC B 43.41 45.22 LF LCV LCP 36.83 25.27 IF ACT OVR GAP 36.83 25.27 IF CUE 36.83 25.27 LF GAP 36.67 24.62 LF IF 36.83 28.24 LF CUE 37.42 25.27 ALL LF IF CUE 38.19 28.38 Table 3 Effect of different feature combinations for predicting topic boundaries from ASR output.
E06-1035.txt,121,4.2 Experiment 2 Statistically learned cue phrases In prior work_comma_ Galley et al. 2003 empirically identified cue phrases that are indicators of segment boundaries_comma_ and then eliminated all cues that had not previously been identified as cue phrases in the literature.
E06-1035.txt,122,Here_comma_ we conduct an experiment to explore how different ways of identifying cue phrases can help identify useful new features for the two boundary prediction tasks.
E06-1035.txt,123,In each fold of the 25 fold leave one out cross validation_comma_ we use a modified5 Chi square test to 5In order to satisfy the mathematical assumptions undercalculate statistics for each word unigram and word pair bi gram that occurred in the 24 training meetings.
E06-1035.txt,124,We then rank unigrams and bigrams according to their Chi square scores_comma_ filtering out those with values under 6.64_comma_ the threshold for the Chi square statistic at the 0.01 significance level.
E06-1035.txt,125,The unigrams and bigrams in this ranked list are the learned cue phrases.
E06-1035.txt,126,We then use the occurrence counts of cue phrases in an analysis window around each potential topic boundary in the test meeting as a feature.
E06-1035.txt,127,Table 4 shows the performance of models that usestatistically learned cue phrases intheirfeature sets compared with models using no cue phrase features and Galley s model_comma_ which only uses cue phrases that correspond to those identified in the literature Col cue .
E06-1035.txt,128,We see that for predicting subtopics_comma_ models using the cue word features 1gram and the combination of cue words and bigrams 1 2gram yielda15 and8.24 improvement over models using no cue features NOCUE p 0.01 respectively_comma_ while models using only cue phrases found in the literature Col cue improve performance by just 3.18 .
E06-1035.txt,129,In contrast_comma_ for predicting top level topics_comma_ the model using cue phrases from the literature Col cue achieves a 4.2 improvement_comma_ and this is the only model that produces statistically significantly better results than the model using no cue phrases NOCUE .
E06-1035.txt,130,The superior performance of models using statistically learned cue phrases as features for predicting subtopic boundaries suggests there may exist a different set of cue phrases that serve as segmentation cues for subtopic boundaries.
E06-1035.txt,131,5 Discussion As observed in the corpus of meetings_comma_ the lack of macro level segment units e.g._comma_ story breaks_comma_ paragraph breaks makes the task of segmenting spontaneous multiparty dialogue_comma_ such as meetings_comma_ different from segmenting text or broadcast news.
E06-1035.txt,132,Compared to the task of segmenting expository texts reported in Hearst 1997 with a 39.1 chance of each paragraph end being a target topic boundary_comma_ the chance of each speaker turn being a top level or sub topic boundary in our ICSI corpus is just 2.2 and 0.69 .
E06-1035.txt,133,The imbalanced class distribution has a negative effect on the perlying the test_comma_ we removed cases with an expected value that is under a threshold in this study_comma_ we use 1 _comma_ and we apply Yate s correction_comma_ ObservedV alue ExpectedValue 0.5 2 ExpectedValue.
E06-1035.txt,134,278 NOCUE Col cue 1gram 2gram 1 2gram MC B Topline SUB 38.11 36.90 32.39 36.86 34.97 46.61 n a TOP 29.61 28.35 28.95 29.20 29.27 48.43 13.48 Table 4 Performance of models trained with cue phrases from the literature Col cue and cue phrases learned from statistical tests_comma_ including cue words 1gram _comma_ cue word pairs 2gram _comma_ and cue phrases composed of both words and word pairs 1 2gram .
E06-1035.txt,135,NOCUE is the model using no cue phrase features.
E06-1035.txt,136,The Topline is the agreement of human annotators on top level segments.
E06-1035.txt,137,0 10 20 30 40 50 60 70 800.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42 0.44 Training Set Size In meetings Error Rate Pk TRAN ALL TRAN TOP ASR ALL ASR TOP Figure 1 Performance of the combined model over the increase of the training set size. formance of machine learning approaches.
E06-1035.txt,138,In a pilot study_comma_ we investigated sampling techniques that rebalance the class distribution in the training set.
E06-1035.txt,139,We found that sampling techniques previously reported in Liu et al 2004 as useful for dealing with an imbalanced class distribution in the task of disfluency detection and sentence segmentation do not work for this particular data set.
E06-1035.txt,140,The implicit assumption of some classifiers such as pruned decision trees that the class distribution of the test set matches that of the training set_comma_ and that the costs of false positives and false negatives are equivalent_comma_ may account for the failure of these sampling techniques to yield improvements inperformance_comma_ when measured using Pk and Wd.
E06-1035.txt,141,Another approach that copes with the imbalanced class prediction problem but does not change the natural class distribution is to increase the size of the training set.
E06-1035.txt,142,We conducted an experiment in which we incrementally increased the training set size by randomly choosing ten meetings each time until all meetings were selected.
E06-1035.txt,143,We executed the process three times and averaged the scores to obtain the results shown in Figure 1.
E06-1035.txt,144,However_comma_ increasing training set size adds to the perplexity in the training phase.
E06-1035.txt,145,We see that increasing the size of the training set only improves the accuracy of segment boundary prediction for predicting top level topics on ASR output.
E06-1035.txt,146,The figure also indicates that training a model to predicttop level boundaries requires nomorethanfifteen meetings in the training set to reach a reasonable level of performance.
E06-1035.txt,147,6 Conclusions Discovering major topic shifts and finding nested subtopics are essential for the success of speech document browsing andretrieval.
E06-1035.txt,148,Meeting records contain rich information_comma_ in both content and conversation behavioral form_comma_ that enable automatic topic segmentation at different levels of granularity.
E06-1035.txt,149,The current study demonstrates that the two tasks predicting top level and subtopic boundaries are distinct in many ways 1 for predicting subtopic boundaries_comma_ the lexical cohesionbased approach achieves results that are competitive with the machine learning approach that combines lexical and conversational features 2 for predicting top level boundaries_comma_ the machine learning approach performs the best and 3 many conversational cues_comma_ such as overlapping speech and cue phrases discussed in the literature_comma_ are better indicators for top level topic shifts than for subtopic shifts_comma_ but new features such as cue phrases can belearned statistically forthesubtopic prediction task.
E06-1035.txt,150,Even in the presence of a relatively higher word error rate_comma_ using ASR output makes no difference to the preference of model for the two tasks.
E06-1035.txt,151,The conversational features also did not help improve the performance for predicting from ASR output.
E06-1035.txt,152,In order to further identify useful features for automatic segmentation of meetings at different levels of granularity_comma_ we will explore the use of 279 multimodal_comma_ i.e._comma_ acoustic and visual_comma_ cues.
E06-1035.txt,153,In addition_comma_ in the current study_comma_ we only extracted features from within the analysis windows immediately preceding and following each potential topic boundary we will explore models that take into account features of longer range dependencies.
E06-1035.txt,154,7 Acknowledgements Many thanks to Jean Carletta for her invaluable help in managing the data_comma_ and for advice and comments on the work reported in this paper.
E06-1035.txt,155,Thanks also to the AMI ASR group for producing the ASR transcriptions_comma_ and to the anonymous reviewers for their helpful comments.
E06-1035.txt,156,This work was supported by the European Union 6th FWP IST Integrated Project AMI Augmented Multiparty Interaction_comma_ FP6 506811 . .
E06-1031.txt,1,state of the art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences.
E06-1031.txt,2,In this paper_comma_ we will present a new evaluation measure which explicitly models block reordering as an edit operation.
E06-1031.txt,3,Our measure can be exactly calculated in quadratic time.
E06-1031.txt,4,Furthermore_comma_ we will show how some evaluation measures can be improved by the . of word dependent substitution costs.
E06-1031.txt,5,The correlation of the new measure with human judgment has been investigated systematically on two different language pairs.
E06-1031.txt,6,The experimental results will show that it significantly outperforms state of the art approaches in sentence level correlation.
E06-1031.txt,7,Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment.
E06-2029.txt,1,question answering QA and information retrieval IR systems are insensitive to different users needs and preferences_comma_ and also to the existence of multiple_comma_ complex or controversial answers. We introduce adaptivity in QA and IR by creating a hybrid system based on a dialogue interface and a user model.
E06-2029.txt,2,Keywords question answering_comma_ information retrieval_comma_ user modelling_comma_ dialogue interfaces.
E06-2029.txt,4,While standard information retrieval IR systems present the results of a query in the form of a ranked list of relevant documents_comma_ question answering QA systems attempt to return them in the form of sentences or paragraphs_comma_ or phrases _comma_ responding more precisely to the user s request.
E06-2029.txt,5,However_comma_ in most state of the art QA systems the output remains independent of the questioner s characteristics_comma_ goals and needs.
E06-2029.txt,6,In other words_comma_ thereisalackofusermodelling a10 year oldand a University History student would get the same answer to the question When did the Middle Ages begin .
E06-2029.txt,7,Secondly_comma_ most of the effort of currentQAisonfactoid questions_comma_ i.e. questionsconcerning people_comma_ dates_comma_ etc._comma_ which can generally be answered by a short sentence or phrase Kwok et al._comma_ 2001 .
E06-2029.txt,8,The main QA evaluation campaign_comma_ TREC QA 1_comma_ has long focused on this type of questions_comma_ for which the simplifying assumption is that there exists only one correct answer.
E06-2029.txt,9,Even recentTRECcampaigns Voorhees_comma_2003 Voorhees_comma_ 2004 do not move sufficiently beyond the factoid approach.
E06-2029.txt,10,They account for two types of nonfactoid questions list and definitional but not for non factoid answers.
E06-2029.txt,11,In fact_comma_ a TREC defines list questions as questions requiring multiple factoid 1http trec.nist.gov answers_comma_ b it is clear that a definition question may be answered by spotting definitional passages what is not clear is how to spot them .
E06-2029.txt,12,However_comma_ accounting for the fact that some simple questions may have complex or controversial answers e.g. What were the causes of World War II remains an unsolved problem.
E06-2029.txt,13,We argue that in such situations returning a short paragraph or text snippet is more appropriate than exact answer spotting.
E06-2029.txt,14,Finally_comma_ QA systems rarely interact with the user the typical session involves the user submitting a queryandthesystemreturningaresult thesession is then concluded.
E06-2029.txt,15,To respond to these deficiencies of existing QA systems_comma_ we propose an adaptive system where a QA module interacts with a user model and a dialogue interface see Figure 1 .
E06-2029.txt,16,The dialogue interface provides the query terms to the QA module_comma_ and the user model UM provides criteria to adapt query results to the user s needs.
E06-2029.txt,17,Given such information_comma_ the goal of the QA module is to be able to discriminate between simple factoid answersandmorecomplexanswers_comma_presentingthem in a TREC style manner in the first case and more appropriately in the second.
E06-2029.txt,18,DIALOGUE INTERFACE QUESTION PROCESSING DOCUMENT RETRIEVAL ANSWER EXTRACTION USER MODEL Question Answer QA MODULE Figure 1 High level system architecture Related work To our knowledge_comma_ our system is among the first to address the need for a different approach to non factoid complex controversial 199 answers.
E06-2029.txt,19,Although the three tiered structure of our QA module reflects that of a typical webbased QA system_comma_ e.g.
E06-2029.txt,20,MULDER Kwok et al._comma_ 2001 _comma_ a significant aspect of novelty in our architecture is that the QA component is supported by the user model.
E06-2029.txt,21,Additionally_comma_ we drastically reduce the amount of linguistic processing applied duringquestionprocessingandanswergeneration_comma_ while giving more relief to the post retrieval phase and to the role of the UM.
E06-2029.txt,22,2 User model Depending on the application of interest_comma_ the UM can be designed to suit the information needs of the QA module in different ways.
E06-2029.txt,23,As our current application_comma_ YourQA2_comma_isalearning oriented_comma_ webbased system_comma_ our UM consists of the user s 1 age range_comma_ a 7 11_comma_11 16_comma_adult 2 reading level_comma_ r poor_comma_medium_comma_good 3 webpages of interest bookmarks_comma_ w.
E06-2029.txt,24,Analogies can be found with the SeAn Ardissono et al._comma_ 2001 and SiteIF Magnini and Strapparava_comma_ 2001 news recommender systems where age and browsing history_comma_ respectively_comma_ are part of the UM.
E06-2029.txt,25,In this paper we focus on how to filter and adapt search results using the reading level parameter.
E06-2029.txt,26,3 Dialogue interface The dialogue component will interact with both the UM and the QA module.
E06-2029.txt,27,From a UM point of view_comma_ the dialogue history will store previous conversations useful to construct and update a model of the user s interests_comma_ goals and level of understanding.
E06-2029.txt,28,From a QA point of view_comma_ the main goal of the dialogue component is to provide users with a friendly interface to build their requests.
E06-2029.txt,29,A typical scenario would start this way System Hi_comma_ how can I help you User I would like to know what books Roald Dahl wrote.
E06-2029.txt,30,The query sentence what books Roald Dahl wrote _comma_ is thus extracted and handed to the QA module.
E06-2029.txt,31,In a second phase_comma_ the dialogue module is responsible for providing the answer to the user once the QA module has generated it.
E06-2029.txt,32,The dialogue manager consults the UM to decide on the most suitable formulation of the answer e.g. short sentences and produce the final answer accordingly_comma_ e.g. System RoaldDahlwrotemanybooksforkidsandadults_comma_ including The Witches _comma_ Charlie and the Chocolate Factory _comma_ and James and the Giant Peach .
E06-2029.txt,33,2http www.cs.york.ac.uk aig aqua 4 Question Answering Module The flow between the three QA phases question processing_comma_ document retrieval and answer generation is described below see Fig. 2 .
E06-2029.txt,34,4.1 Question processing We perform query expansion_comma_ which consists in creating additional queries using question word synonyms in the purpose of increasing the recall of the search engine.
E06-2029.txt,35,Synonyms are obtained via the WordNet 2.0 3 lexical database.
E06-2029.txt,36,Question QUERYEXPANSION DOCUMENT RETRIEVAL KEYPHRASE EXTRACTION ESTIMATION OF READING LEVELS CLUSTERING Language Models UM BASED FILTERING SEMANTIC SIMILARITY RANKING User Model Reading Level Ranked Answer Candidates Figure 2 Diagram of the QA module 4.2 Retrieval Document retrieval We retrieve the top 20 documents returned by Google4 for each query produced via query expansion.
E06-2029.txt,37,These are processed in the following steps_comma_ which progressively narrow the part of the text containing relevant information.
E06-2029.txt,38,Keyphrase extraction Once the documents are retrieved_comma_ we perform keyphrase extraction to determine their three most relevant topics using Kea Witten et al._comma_ 1999 _comma_ an extractor based on Na ve Bayes classification.
E06-2029.txt,39,Estimation of reading levels To adapt the readability of the results to the user_comma_ we estimate the reading difficulty of the retrieved documents using the Smoothed Unigram Model CollinsThompson and Callan_comma_ 2004 _comma_ which proceeds in 3http wordnet.princeton.edu 4http www.google.com 200 two phases.
E06-2029.txt,40,1 In the training phase_comma_ sets of representativedocumentsarecollectedforagivennumber of reading levels.
E06-2029.txt,41,Then_comma_ a unigram language model is created for each set_comma_ i.e. a list of word stem_comma_ probability entries for the words appearing in its documents.
E06-2029.txt,42,Our models account for the following reading levels poor suitable for ages 7 11 _comma_ medium ages 11 16 and good adults .
E06-2029.txt,43,2 In the test phase_comma_ given an unclassified document D_comma_ its estimated reading level is the model lmi maximizing the likelihood that D lmi5.
E06-2029.txt,44,Clustering We use the extracted topics and estimated reading levels as features to apply hierarchical clustering on the documents.
E06-2029.txt,45,We use the WEKA Witten and Frank_comma_ 2000 implementation of the Cobweb algorithm.
E06-2029.txt,46,This produces a tree where each leaf corresponds to one document_comma_ and sibling leaves denote documents with similar topics and reading difficulty.
E06-2029.txt,47,4.3 Answer extraction In this phase_comma_ the clustered documents are filtered based on the user model and answer sentences are located and formatted for presentation.
E06-2029.txt,48,UM based filtering The documents in the cluster tree are filtered according to their reading difficulty only those compatible with the UM s reading level are retained for further analysis6.
E06-2029.txt,49,Semantic similarity Within each of the retained documents_comma_ we seek the sentences which are semantically most relevant to the query by applying the metric in Alfonseca et al._comma_ 2001 we represent each document sentence p and the query q as word sets P pw1_comma_..._comma_pwm and Q qw1_comma_..._comma_qwn .
E06-2029.txt,50,The distance from p to q is then distq p summationtext1 i m minj d pwi_comma_qwj _comma_ where d pwi_comma_qwj is the word level distance between pwi and qwj based on Jiang and Conrath_comma_ 1997 .
E06-2029.txt,51,Ranking Given the query q_comma_ we thus locate in each document D the sentence p such that p argminp D distq p then_comma_ distq p becomes the document score.
E06-2029.txt,52,Moreover_comma_ each clus5The likelihood is estimated using the formula Li_comma_D summationtextw D C w_comma_D log P w lmi _comma_ where w is a word in the document_comma_ C w_comma_d is the number of occurrences of w in D and P w lmi is the probability with which w occurs in lmi 6However_comma_ if their number does not exceed a given threshold_comma_ we accept in our candidate set part of the documents havingthenextlowestreadability oramediumreadabilityifthe user s reading level is low ter is assigned a score consisting in the maximal score of the documents composing it.
E06-2029.txt,53,This allows to rank not only documents_comma_ but also clusters_comma_ and presentresultsgroupedbyclusterindecreasingorder of document score.
E06-2029.txt,54,Answer presentation We present our answers in an HTML page_comma_ where results are listed following the ranking described above.
E06-2029.txt,55,Each result consists of the title and clickable URL of the originating document_comma_ and the passage where the sentence which best answers the query is located and highlighted.
E06-2029.txt,56,Question keywords and potentially useful information such as named entities are in colour.
E06-2029.txt,57,5 Sample result We have been running our system on a range of queries_comma_ including factoid simple_comma_ complex and controversial ones.
E06-2029.txt,58,As an example of the latter_comma_ we report the query Who wrote the Iliad _comma_ which is a subject of debate.
E06-2029.txt,59,These are some top results UMgood Most Classicists would agree that_comma_ whether there was ever such a composer as Homer or not_comma_ the Homeric poems are the product of an oral tradition ... Could the Iliad and Odyssey have been oral formulaic poems_comma_ composed on the spot by the poet using a collection of memorized traditional verses and phases  UMmed No reliable ancient evidence for Homer ... General ancient assumption that same poet wrote Iliad and Odyssey and possibly other poems questioned by many modern scholars differences explained biographically in ancient world e g wrote Od. in old age but similarities could be due to imitation. UMpoor Homer wrote The Iliad and The Odyssey at least_comma_ supposedly a blind bard named Homer did . In the three results_comma_ the problem of attribution of the Iliad is made clearly visible document passages provide a context which helps to explain the controversy at different levels of difficulty.
E06-2029.txt,60,6 Evaluation Since YourQA does not single out one correct answerphrase_comma_TRECevaluationmetricsarenotsuitable for it.
E06-2029.txt,61,A user centred methodology to assess how individual information needs are met is more appropriate.
E06-2029.txt,62,Webaseourevaluationon Su_comma_2003 _comma_ which proposes a comprehensive search engine evaluation model_comma_ defining the following metrics 1.
E06-2029.txt,63,Relevance we define strict precision P1 as the ratio between the number of results rated as relevant and all the returned results_comma_ and loose pre201 cision P2 as the ratio between the number of results rated as relevant or partially relevant and all the returned results.
E06-2029.txt,65,Usersatisfaction a7 pointLikertscale7 isused to assess the user s satisfaction with loose precision of results S1 and query success S2 .
E06-2029.txt,67,Reading level accuracy given the set R of results returned for a reading level r_comma_ Ar is the ratio between the number of results R rated by the users as suitable for r and R .
E06-2029.txt,69,Overall utility U the search session as a whole is assessed via a 7 point Likert scale.
E06-2029.txt,70,We performed our evaluation by running 24 queries some of which in Tab. 2 on Google and YourQA and submitting the results i.e.
E06-2029.txt,71,Google result page snippets and YourQA passages of both to 20 evaluators_comma_ along with a questionnaire.
E06-2029.txt,72,The relevance results P1 and P2 in Tab.
E06-2029.txt,73,1 show a P1 P2 S1 S2 U Google 0_comma_39 0_comma_63 4_comma_70 4_comma_61 4_comma_59 YourQA 0_comma_51 0_comma_79 5_comma_39 5_comma_39 5_comma_57 Table 1 Evaluation results 10 15 difference in favour of YourQA for both strict and loose precision.
E06-2029.txt,74,The coarse semantic processing applied and context visualisation thus contribute to creating more relevant passages.
E06-2029.txt,75,Both user satisfaction results S1 and S2 in Tab.
E06-2029.txt,76,1 also denote a higher level of satisfaction tributed toYourQA.Tab.
E06-2029.txt,77,2showsthatevaluatorsfoundour Query Ag Am Ap When did the Middle Ages begin
E06-2029.txt,78,0_comma_91 0_comma_82 0_comma_68 Who painted the Sistine Chapel
E06-2029.txt,79,0_comma_85 0_comma_72 0_comma_79 When did the Romans invade Britain
E06-2029.txt,80,0_comma_87 0_comma_74 0_comma_82 Who was a famous cubist
E06-2029.txt,81,0_comma_90 0_comma_75 0_comma_85 Who was the first American in space
E06-2029.txt,82,0_comma_94 0_comma_80 0_comma_72 Definition of metaphor 0_comma_95 0_comma_81 0_comma_38 average 0_comma_94 0_comma_85 0_comma_72 Table 2 Sample queries and accuracy values results appropriate for the reading levels to which they were assigned.
E06-2029.txt,83,The accuracy tended to decrease from 94 to 72 with the level it is indeed more constraining to conform to a lower reading level than to a higher one.
E06-2029.txt,84,Finally_comma_ the 7This measure ranging from 1 extremely unsatisfactory to 7 extremely satisfactory is particularly suitable to assess how well a system meets user s search needs. general satisfaction values for U in Tab.
E06-2029.txt,85,1 show an improved preference for YourQA.
E06-2029.txt,86,7 Conclusion A user tailored QA system is proposed where a user model contributes to adapting answers to the user s needs and presenting them appropriately.
E06-2029.txt,87,A preliminary evaluation of our core QA module shows a positive feedback from human assessors.
E06-2029.txt,88,Our short term goals involve performing a more extensive evaluation and implementing a dialogue interface to improve the system s interactivity. .
E06-2028.txt,1,NLP systems often have low performances because they rely on unreliable and heterogeneous knowledge. We show on the task of non anaphoric it identification how to overcome these handicaps with the Bayesian Network BN formalism.
E06-2028.txt,2,The first results are very encouraging compared with the state of the art systems.
E06-2028.txt,4,When a pronoun refers to a linguistic expression previously introduced in the text_comma_ it is anaphoric.
E06-2028.txt,5,In the sentence Nonexpression of the locus even when it is present suggests that these chromosomes ... _comma_ the pronoun it refers to the referent designated as the locus .
E06-2028.txt,6,When it does not refer to any referent_comma_ as in the sentence Thus_comma_ it is not unexpected that this versatile cellular... the pronoun is semantically empty or non anaphoric.
E06-2028.txt,7,Any anaphora resolution system starts by identifying the pronoun occurrences and distinguishing theanaphoric and non anaphoric occurrences of it.
E06-2028.txt,8,The first systems that tackled this classification problem were based either on manually written rules or on the automatic learning of relevant surface clues.
E06-2028.txt,9,Whatever strategy is used_comma_ these systems see their performances limited by the quality of knowledge they exploit_comma_ which is usually only partially reliable and heterogeneous.
E06-2028.txt,10,This article describes a new approach to go beyond the limits of traditional systems.
E06-2028.txt,11,This approach stands on the formalism_comma_ still little exploited for NLP_comma_ of Bayesian Network BN .
E06-2028.txt,12,As a probabilistic formalism_comma_ it offers a great expression capacity to integrate heterogeneous knowledge in a single representation Peshkin_comma_ 2003 as well as an elegant mechanism to take into account an a priori estimation of their reliability in the classification decision Roth_comma_ 2002 .
E06-2028.txt,13,In order to validate our approach wecarried out various experiments on a corpus made up of abtsracts of genomic articles.
E06-2028.txt,14,Section 2 presents the state of the art for the automatic recognition of the non anaphoric occurences of it.
E06-2028.txt,15,Our BN based approach is exposed in section 3.
E06-2028.txt,16,The experiments are reported in section 4_comma_ and results are discussed in section 5.
E06-2028.txt,17,2 Identification of Non anaphoric it occurences The decisions made by NLP systems depend on the available knowledge.
E06-2028.txt,18,However this information isoften weakly reliable and leads toerroneous or incomplete results.
E06-2028.txt,19,One of first pronoun classifier system is presented by Paice_comma_ 1987 .
E06-2028.txt,20,It relies on a set of logical first order rules to distinguish the non anaphoric occurences of the pronoun it.
E06-2028.txt,21,Non anaphoric sequences share remarkable forms theystartwithan it and endwithadelimiter like to_comma_ that_comma_ whether... .
E06-2028.txt,22,The rules expresses some constraints which vary according to the delimiter.
E06-2028.txt,23,They concern the left context of the pronoun it should not be immediately preceded by certain words like before_comma_ from to _comma_ the distance between the pronoun and the delimiter it mustbeshorter than 25wordslong _comma_ and finallythelexical itemsoccurring betweenthepronoun and the delimiter the sequence must or must not contain certain words belonging to specific sets_comma_ such as words expressing modality over the sentence content_comma_ e.g. certain_comma_ known_comma_ unclear... .
E06-2028.txt,24,Tests performed by Paice show good results with 195 91.4 Accuracy1 on a technical corpus.
E06-2028.txt,25,However the performances are degraded if one applies them to corpora of different natures the number of false positive increases.
E06-2028.txt,26,Inorder toavoid this pitfall_comma_ Lappin_comma_ 1994 proposes some more constrained rules in the form of finite state automata.
E06-2028.txt,27,Based on linguistic knowledge the automata recognize specific sequences like It is not may be Modaladj It is Cogved that Subject where Modaladj and Cogv are modal adjective and cognitive verbs classes known to introduce non anaphoric it e.g. necessary_comma_ possible and recommend_comma_ think .
E06-2028.txt,28,This system has a good precision few false positive cases _comma_ but has a low recall many false negative cases .
E06-2028.txt,29,Any sequence with a variation is ignored by the automata and it is difficult to get exhaustive adjective and verb semantic classes2.
E06-2028.txt,30,In the next paragraphs we refer to Lappin rules as Highly Constraint rules HC rules and Paice rules as Lightly Constraint rules LC rules .
E06-2028.txt,31,Evans_comma_ 2001 gives up the constraints brought into play by these rules and proposes a machine learning approach based on surface clues.
E06-2028.txt,32,The training determines the relative weight of the variouscorpus clues.
E06-2028.txt,33,Evansconsiders 35syntactic and contextual surface clues e.g. pronoun position in the sentence_comma_ lemma of the following verb on a manually annotated sample.
E06-2028.txt,34,The system classifies the new it occurences by the k nearest neighbor method metric.
E06-2028.txt,35,The first tests achieve a satisfactory score 71.31 Acc on a general language corpus.
E06-2028.txt,36,Clement_comma_ 2004 carries out a similar test in the genomic domain.
E06-2028.txt,37,He reduces the number of Evans s surface clues to the 21 most relevant ones and classifies the new instances with a Support Vector Machine SVM .
E06-2028.txt,38,It obtains 92.71 Acc to be compared with a 90.78 Acc score for the LC rules on the same corpus.
E06-2028.txt,39,The difficulty_comma_ however_comma_ comes from the fact that the information on which 1Accuracy Acc is a classification measure Acc P NP N p n where p is the number of anaphoric pronoun occurences tagged as non anaphoric_comma_ which we call the false positive cases_comma_ n the number of non anaphoric pronoun ocurrences tagged as anaphoric_comma_ the false negative cases.
E06-2028.txt,40,P and N are the numbers of correctly tagged non anaphoric and anaphoric pronoun occurences_comma_ the true positive and negative cases respectively.
E06-2028.txt,41,2For instance in the sentences It is well documented that treatment of serum grown... and It is generally accepted that Bcl 2 exerts... the it occurences are not classified as nonanaphorics because documented does not belong to the original verb class Cogv and generally does not appear in the previous automaton. the systems are built is often diverse and heterogeneous.
E06-2028.txt,42,This system is based on atomic surface clues only and does not make use of the linguistic knowledge or the relational information that the constraints ofthe previous systems encode.
E06-2028.txt,43,Wearguethatthesethree types ofknowledge thatarethe HC rules_comma_ the LC rules_comma_ and the surfaces clues are all relevant and complementary for the task and thatthey mustbeunified inasingle representation.
E06-2028.txt,44,3 A Bayesian Network Based System Contain No Contain Contain Known Noun Anaphoric It Non anaphoric It Pronoun Star No Start Start Proposition Start No Start Start Sentence Start No Start Start .
E06-1026.txt,1,propose models for semantic orientations of phrases as well as classification methods based on the models. Although eachphraseconsistsofmultiplewords_comma_ the semantic orientation of the phrase is not a mere sum of the orientations of the component words.
E06-1026.txt,2,Some words can invert the orientation.
E06-1026.txt,3,In order to capture the property of such phrases_comma_ we introduce latent variables into the models.
E06-1026.txt,4,Through experiments_comma_ we show that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82 classification accuracy.
E06-1026.txt,6,Technologyforaffectanalysisoftextshasrecently gained attention in both academic and industrial areas.
E06-1026.txt,7,It can be applied to_comma_ for example_comma_ a survey of new products or a questionnaire analysis.
E06-1026.txt,8,Automatic sentiment analysis enables a fast and comprehensive investigation.
E06-1026.txt,9,The most fundamental step for sentiment analysis is to acquire the semantic orientations of words desirable or undesirable positive or negative .
E06-1026.txt,10,For example_comma_ the word beautiful is positive_comma_ while the word dirty is negative.
E06-1026.txt,11,Many researchers have developed several methods for this purpose and obtained good results Hatzivassiloglou and McKeown_comma_ 1997 Turney and Littman_comma_ 2003 Kamps et al._comma_ 2004 Takamura et al._comma_ 2005 Kobayashi et al._comma_ 2001 .
E06-1026.txt,12,One of the next problems to be solved is to acquire semantic orientations of phrases_comma_ or multi term expressions.
E06-1026.txt,13,No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words.
E06-1026.txt,14,The purpose of this paperistoproposecomputationalmodelsforphrases with semantic orientations as well as classification methods based on the models.
E06-1026.txt,15,Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do_comma_ but we would like to obtain the most basic orientations of phrases.
E06-1026.txt,16,We believe that we can use the obtained basic orientations of phrases for affect analysis of higher linguistic units such as sentences and documents.
E06-1026.txt,17,The semantic orientation of a phrase is not a mere sum of its component words.
E06-1026.txt,18,Semantic orientations can emerge out of combinations of non oriented words.
E06-1026.txt,19,For example_comma_ light laptopcomputer is positively oriented although neither light nor laptop computer has a positive orientation.
E06-1026.txt,20,Besides_comma_ some words can invert the orientation of a neighboring word_comma_ such as low in low risk _comma_ where the negative orientation of risk is inverted to a positive by the adjective low .
E06-1026.txt,21,This kind of non compositional operation has to be incorporated into the model.
E06-1026.txt,22,We focus on noun adjective in this paper_comma_ since this type of phrase contains most of interesting properties of phrases_comma_ such as emergence or inversion of semantic orientations.
E06-1026.txt,23,In order to capture the properties of semantic orientations of phrases_comma_ we introduce latent variables into the models_comma_ where one random variable corresponds to nouns and another random variable corresponds to adjectives.
E06-1026.txt,24,The words that are similar in terms of semantic orientations_comma_ such as risk and mortality i.e._comma_ the positive orientation emerges when they are low _comma_ make a cluster in these models.
E06-1026.txt,25,Our method is language201 independent in the sense that it uses only cooccurrence data of words and semantic orientations.
E06-1026.txt,26,2 Related Work We briefly explain related work from two viewpoints the classification of word pairs and the identification of semantic orientation.
E06-1026.txt,27,2.1 Classification of Word Pairs Torisawa 2001 used a probabilistic model to identify the appropriate case for a pair of words constituting a noun and a verb with the case of the noun verb pair unknown.
E06-1026.txt,28,Their model is the same as Probabilistic Latent Semantic Indexing PLSI Hofmann_comma_ 2001 _comma_ which is a generative probability model of two random variables.
E06-1026.txt,29,Torisawa s method is similar to ours in that a latent variable model is used for word pairs.
E06-1026.txt,30,However_comma_ Torisawa s objective is different from ours.
E06-1026.txt,31,In addition_comma_ we used not the original PLSI_comma_ but its expanded version_comma_ which is more suitable for this task of semantic orientation classification of phrases.
E06-1026.txt,32,Fujita et al. 2004 addressed the task of the detection of incorrect case assignment in automatically paraphrased sentences.
E06-1026.txt,33,They reduced the task to a problem of classifying pairs of a verb and a noun with a case into correct or incorrect.
E06-1026.txt,34,They first obtained a latent semantic space with PLSI and adopted the nearest neighbors method_comma_ in which they used latent variables as features.
E06-1026.txt,35,Fujita et al. s method is different from ours_comma_ and also from Torisawa s_comma_ in that a probabilistic model is used for feature extraction.
E06-1026.txt,36,2.2 Identification of Semantic Orientations The semantic orientation classification of words has been pursued by several researchers Hatzivassiloglou and McKeown_comma_ 1997 Turney and Littman_comma_ 2003 Kamps et al._comma_ 2004 Takamura et al._comma_ 2005 .
E06-1026.txt,37,However_comma_ no computational model for semantically oriented phrases has been proposed to date although research for a similar purpose has been proposed.
E06-1026.txt,38,Some researchers used sequences of words as features in document classification according to semantic orientation.
E06-1026.txt,39,Pang et al. 2002 used bigrams.
E06-1026.txt,40,Matsumoto et al. 2005 used sequential patterns and tree patterns.
E06-1026.txt,41,Although such patterns were proved to be effective in document classification_comma_ the semantic orientations of the patterns themselves are not considered.
E06-1026.txt,42,Suzuki et al. 2006 used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3 term evaluative expressions.
E06-1026.txt,43,They focused on the utilization of context information such as neighboring words and emoticons.
E06-1026.txt,44,Turney 2002 applied an internet based technique to the semantic orientation classification of phrases_comma_ whichhadoriginallybeendevelopedforwordsentiment classification.
E06-1026.txt,45,In their method_comma_ the number of hits returned by a search engine_comma_ with a query consisting of a phrase and a seed word e.g._comma_ phrase NEAR good is used to determine the orientation.
E06-1026.txt,46,Baron and Hirst 2004 extracted collocations with Xtract Smadja_comma_ 1993 and classified the collocations using the orientations of the words in the neighboring sentences.
E06-1026.txt,47,Their method is similar to Turney s in the sense that cooccurrence with seed words is used.
E06-1026.txt,48,The three methods above are based on context information.
E06-1026.txt,49,In contrast_comma_ our method exploits the internal structure of the semantic orientations of phrases.
E06-1026.txt,50,Inui 2004 introduced an attribute plus minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus minus attribute values and the positive negative attribute values of the component words.
E06-1026.txt,51,For example_comma_ a rule negative minus positive determines low minus risk negative to be positive.
E06-1026.txt,52,Wilson et al. 2005 worked on phrase level semantic orientations.
E06-1026.txt,53,They introduced a polarity shifter_comma_ which is almost equivalent to the plus minus attribute above.
E06-1026.txt,54,They manually created the list of polarity shifters.
E06-1026.txt,55,The method that we propose in this paper is an automatic version of Inui s or Wilson et al. s idea_comma_ in the sense that the method automatically creates word clusters and their polarity shifters.
E06-1026.txt,56,3 Latent Variable Models for Semantic Orientations of Phrases As mentioned in the .
E06-1050.txt,1,questions are implicitly associated with an expected answer type. Unlike previous approaches that require a predefined set of question types_comma_ we present a method for dynamically constructing a probability based answer type model for each different question.
E06-1050.txt,2,Our model evaluates the appropriateness of a potential answer by the probability that it fits into the question contexts.
E06-1050.txt,3,Evaluation is performed against manual and semiautomatic methods using a fixed set of answer labels.
E06-1050.txt,4,Results show our approach to be superior for those questions classified as having a miscellaneous answer type.
E06-1050.txt,6,Given a question_comma_ people are usually able to form an expectation about the type of the answer_comma_ even if they do not know the actual answer.
E06-1050.txt,7,An accurate expectation of the answer type makes it much easier to select the answer from a sentence that contains the query words.
E06-1050.txt,8,Consider the question What is the capital of Norway We would expect the answer to be a city and could filter out most of the words in the following sentence The landed aristocracy was virtually crushed by Hakon V_comma_ who reigned from 1299 to 1319_comma_ and Oslo became the capital of Norway_comma_ replacing Bergen as the principal city of the kingdom.
E06-1050.txt,9,The goal of answer typing is to determine whether a word s semantic type is appropriate as an answer for a question.
E06-1050.txt,10,Many previous approaches to answer typing_comma_ e.g._comma_ Ittycheriah et al._comma_ 2001 Li and Roth_comma_ 2002 Krishnan et al._comma_ 2005 _comma_ employ a predefined set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type.
E06-1050.txt,11,A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types.
E06-1050.txt,12,Consider the question What are tourist attractions in Reims The answer may be many things a church_comma_ a historic residence_comma_ a park_comma_ a famous intersection_comma_ a statue_comma_ etc. A common method to dealwiththisproblemistodefineacatch allclass.
E06-1050.txt,13,This class_comma_ however_comma_ tends not to be as effective as other answer types.
E06-1050.txt,14,Another disadvantage of predefined answer types is with regard to granularity.
E06-1050.txt,15,If the types are too specific_comma_ they are more difficult to tag.
E06-1050.txt,16,If they are too general_comma_ too many candidates may be identified as having the appropriate type.
E06-1050.txt,17,In contrast to previous approaches that use a supervised classifier to categorize questions into a predefined set of types_comma_ we propose an unsupervised method to dynamically construct a probabilistic answer type model for each question.
E06-1050.txt,18,Such a model can be used to evaluate whether or not a word fits into the question context.
E06-1050.txt,19,For example_comma_ given the question What are tourist attractions in Reims _comma_ we would expect the appropriate answers to fit into the context X is a tourist attraction. From a corpus_comma_ we can find the words that appeared in this context_comma_ such as A Ama Temple_comma_ Aborigine_comma_ addition_comma_ Anak Krakatau_comma_ archipelago_comma_ area_comma_ baseball_comma_ Bletchley Park_comma_ brewery_comma_ cabaret_comma_ Cairo_comma_ Cape Town_comma_ capital_comma_ center_comma_ ... Using the frequency counts of these words in the context_comma_ we construct a probabilistic model to compute P in w_comma_ w _comma_ the probability for a word w to occur in a set of contexts _comma_ given an occurrence of w.
E06-1050.txt,20,The parameters in this model are obtained from a large_comma_ automatically parsed_comma_ unlabeled corpus.
E06-1050.txt,21,By asking whether a word would occurinaparticularcontextextractedfromaques393 tion_comma_ we avoid explicitly specifying a list of possible answer types.
E06-1050.txt,22,This has the added benefit of being easily adapted to different domains and corpora in which a list of explicit possible answer types may be difficult to enumerate and or identify within the text.
E06-1050.txt,23,The remainder of this paper is organized as follows.
E06-1050.txt,24,Section 2 discusses the work related to answer typing.
E06-1050.txt,25,Section 3 discusses some of the key concepts employed by our probabilistic model_comma_ including word clusters and the contexts of a question and a word.
E06-1050.txt,26,Section 4 presents our probabilistic model for answer typing.
E06-1050.txt,27,Section 5 compares the performance of our model with that of an oracle and a semi automatic system performing the same task.
E06-1050.txt,28,Finally_comma_ the concluding remarks in are made in Section 6.
E06-1050.txt,29,2 Related Work Light et al. 2001 performed an analysis of the effect of multiple answer type occurrences in a sentence.
E06-1050.txt,30,When multiple words of the same type appear in a sentence_comma_ answer typing with fixed types must assign each the same score.
E06-1050.txt,31,Light et al. found that even with perfect answer sentence identification_comma_ question typing_comma_ and semantic tagging_comma_ a system could only achieve 59 accuracy over the TREC 9 questions when using their set of 24 non overlapping answer types.
E06-1050.txt,32,By computing the probability of an answer candidate occurring in the question contexts directly_comma_ we avoid having multiple candidates with the same level of appropriateness as answers.
E06-1050.txt,33,There have been a variety of approaches to determine the answer types_comma_ which are also known as Qtargets Echihabi et al._comma_ 2003 .
E06-1050.txt,34,Most previous approaches classify the answer type of a question as one of a set of predefined types.
E06-1050.txt,35,Many systems construct the classification rules manually Cui et al._comma_ 2004 Greenwood_comma_ 2004 Hermjakob_comma_ 2001 .
E06-1050.txt,36,The rules are usually triggered by the presence of certain words in the question.
E06-1050.txt,37,For example_comma_ if a question contains author then the expected answer type is Person.
E06-1050.txt,38,The number of answer types as well as the number of rules can vary a great deal.
E06-1050.txt,39,For example_comma_ Hermjakob_comma_ 2001 used 276 rules for 122 answer types.
E06-1050.txt,40,Greenwood 2004 _comma_ on the other hand_comma_ used 46 answer types with unspecified number of rules.
E06-1050.txt,41,The classification rules can also be acquired with supervised learning.
E06-1050.txt,42,Ittycheriah_comma_ et al. 2001 describe a maximum entropy based question classification scheme to classify each question as having one of the MUC answer types.
E06-1050.txt,43,In a similar experiment_comma_ Li Roth 2002 train a question classifier based on a modified version of SNoW using a richer set of answer types than Ittycheriah et al. The LCC system Harabagiu et al._comma_ 2003 combines fixed types with a novel loop back strategy.
E06-1050.txt,44,In the event that a question cannot be classified as one of the fixed entity types or semantic concepts derived from WordNet Fellbaum_comma_ 1998 _comma_ the answer type model backs off to a logic prover that uses axioms derived form WordNet_comma_ along with logicrules_comma_ tojustifyphrasesasanswers.
E06-1050.txt,45,Thus_comma_ the LCC system is able to avoid the use of a miscellaneous type that often exhibits poor performance.
E06-1050.txt,46,However_comma_ the logic prover must have sufficient evidence to link the question to the answer_comma_ and general knowledge must be encoded as axioms into the system.
E06-1050.txt,47,In contrast_comma_ our answer type model derives all of its information automatically from unannotated text.
E06-1050.txt,48,Answer types are often used as filters.
E06-1050.txt,49,It was noted in Radev et al._comma_ 2002 that a wrong guess about the answer type reduces the chance for the system to answer the question correctly by as much as 17 times.
E06-1050.txt,50,The approach presented here is less brittle.
E06-1050.txt,51,Even if the correct candidate does not have the highest likelihood according to the model_comma_ it may still be selected when the answer extraction module takes into account other factors such as the proximity to the matched keywords.
E06-1050.txt,52,Furthermore_comma_ a probabilistic model makes it easier to integrate the answer type scores with scores computed by other components in a question answering system in a principled fashion.
E06-1050.txt,53,3 Resources Before introducing our model_comma_ we first describe the resources used in the model.
E06-1050.txt,54,3.1 Word Clusters Natural language data is extremely sparse.
E06-1050.txt,55,Word clusters are a way of coping with data sparseness by .
E06-2012.txt,1,present a novel application of NLP and text mining to the analysis of financial documents. In particular_comma_ we describe an implemented prototype_comma_ Maytag_comma_ which combines information extraction and subject classification tools in an interactive exploratory framework.
E06-2012.txt,2,We present experimental results on their performance_comma_ as tailored to the financial domain_comma_ and some forward looking extensions to the approach that enables users to specify classifications on the fly.
E06-2012.txt,4,Our goal is to support the discovery of complex events in text.
E06-2012.txt,5,By complex events_comma_ we mean events that might be structured out of multiple occurrences of other events_comma_ or that might occur over a span of time.
E06-2012.txt,6,In financial analysis_comma_ the domain that concerns us here_comma_ an example of what we mean is the problem of understanding corporate acquisition practices.
E06-2012.txt,7,To gauge a company s modus operandi in acquiring other companies_comma_ it isn t enough to know just that an acquisition occurred_comma_ but it may also be important to understand the degree to which it was debt leveraged_comma_ or whether it was performed through reciprocal stock exchanges.
E06-2012.txt,8,In other words_comma_ complex events are often composed of multiple facets beyond the basic event itself.
E06-2012.txt,9,One of our concerns is therefore to enable end users to access complex events through a combination of their possible facets.
E06-2012.txt,10,Another key characteristic of rich domains like financial analysis_comma_ is that facts and events are subject to interpretation in context.
E06-2012.txt,11,To a financial analyst_comma_ it makes a difference whether a multi million dollar loss occurs in the context of recurring operations a potentially chronic problem _comma_ or in the context of a one time event_comma_ such as a merger or layoff.
E06-2012.txt,12,A second concern is thus to enable end users to interpret facts and events through automated context assessment.
E06-2012.txt,13,The route we have taken towards this end is to model the domain of corporate finance through an interactive suite of language processing tools.
E06-2012.txt,14,Maytag_comma_ our prototype_comma_ makes the following novel contribution.
E06-2012.txt,15,Rather than trying to model complex events monolithically_comma_ we provide a range of multi purpose information extraction and text classification methods_comma_ and allow the end user to combine these interactively.
E06-2012.txt,16,Think of it as Boolean queries where the query terms are not keywords but extracted facts_comma_ events_comma_ entities_comma_ and contextual text classifications.
E06-2012.txt,17,2 The Maytag prototype Figure 1_comma_ below_comma_ shows the Maytag prototype in action.
E06-2012.txt,18,In this instance_comma_ the user is browsing a particular document in the collection_comma_ the 2003 securities filings for 3M Corporation.
E06-2012.txt,19,The user has imposed a context of interpretation by selecting the Legal matters subject code_comma_ which causes the browser to only retrieve those portions of the document that were statistically identified as pertaining to law suits.
E06-2012.txt,20,The user has also selected retrieval based on extracted facts_comma_ in this case monetary expenses greater than 10 million.
E06-2012.txt,21,This in turn causes the browser to further restrict retrieval to those portions of the document that contain the appropriate linguistic expressions_comma_ e.g._comma_ 73 million pre tax charge. As the figure shows_comma_ the granularity of these operations in our browser is that of the paragraph_comma_ which strikes a reasonable compromise between providing enough context to interpret retrieval results_comma_ but not too much.
E06-2012.txt,22,It is also ef131 fective at enabling combination of query terms.
E06-2012.txt,23,Whereas the original document contains 5161 paragraphs_comma_ the number of these that were tagged with the Legal matters code is 27_comma_ or .5 percent of the overall document.
E06-2012.txt,24,Likewise_comma_ the query for expenses greater than 10 million restricts the return set to 26 paragraphs .5 percent .
E06-2012.txt,25,The conjunction of both queries yields a common intersection of only 4 paragraphs_comma_ thus precisely targeting .07 percent of the overall document.
E06-2012.txt,26,Under the hood_comma_ Maytag consists of both an on line component and an off line one.
E06-2012.txt,27,The online part is a web based GUI that is connected to a relational database via CGI scripts html_comma_ JavaScript_comma_ and Python .
E06-2012.txt,28,The off line part of the system hosts the bulk of the linguistic and statistical processing that creates document meta data name tagging_comma_ relationship extraction_comma_ subject identification_comma_ and the like.
E06-2012.txt,29,These processes are applied to documents entering the text collection_comma_ and the results are stored as meta data tables.
E06-2012.txt,30,The tables link the results of the off line processing to the paragraphs in which they were found_comma_ thereby supporting the kind of extraction and classification based retrieval shown in Figure 1.
E06-2012.txt,31,3 Extraction in Maytag As is common practice_comma_ Maytag approaches extraction in stages.
E06-2012.txt,32,We begin with atomic named entities_comma_ and then detect structured entities_comma_ relationships_comma_ and events.
E06-2012.txt,33,To do so_comma_ we rely on both rule based and statistical means.
E06-2012.txt,34,3.1 Named entities In Maytag_comma_ we currently extract named entities with a tried but true rule based tagger based on the legacy Alembic system Vilain_comma_ 1999 .
E06-2012.txt,35,Although we ve also developed more modern statistical methods Burger et al_comma_ 1999_comma_ Wellner Vilain_comma_ 2006 _comma_ we do not currently have adequate amounts of hand marked financial data to train these systems.
E06-2012.txt,36,We therefore found it more convenient to adapt the Alembic name tagger by manual hill climbing.
E06-2012.txt,37,Because this tagger was originally designed for a similar newswire task_comma_ we were able to make the port using relatively small amounts of training data.
E06-2012.txt,38,We relied on two 100 page long Securities filings singly annotated _comma_ one for training_comma_ and the other for test_comma_ on which we achieve an accuracy of F 94.
E06-2012.txt,39,We found several characteristics of our financial data to be especially challenging.
E06-2012.txt,40,The first is the widespread presence of company name lookalikes_comma_ by which we mean phrases like Health Care Markets or Business Services that may look like company names_comma_ but in fact denote business segments or the like.
E06-2012.txt,41,To circumvent this_comma_ we had to explicitly model non names_comma_ in effect creating a business segment tagger that captures company name look alikes and prevents them from being tagged as companies.
E06-2012.txt,42,Another challenging characteristic of these financial reports is their length_comma_ commonly reaching hundreds of pages.
E06-2012.txt,43,This poses a quandary Figure 1 The Maytag interface 132 for the way we handle discourse effects.
E06-2012.txt,44,As with most name taggers_comma_ we keep a found names list to compensate for the fact that a name may not be clearly identified throughout the entire span of the input text.
E06-2012.txt,45,This list allows the tagger to propagate a name from clear identifying contexts to non identified occurrences elsewhere in the discourse.
E06-2012.txt,46,In newswire_comma_ this strategy boosts recall at very little cost to precision_comma_ but the sheer length of financial reports creates a disproportionate opportunity for found name lists to introduce precision errors_comma_ and then propagate them.
E06-2012.txt,47,3.2 Structured entities_comma_ relations_comma_ and events Another way in which financial writing differs from general news stories is the prevalence of what we ve called structured entities_comma_ i.e._comma_ namelike entities that have key structural attributes.
E06-2012.txt,48,The most common of these relate to money.
E06-2012.txt,49,In financial writing_comma_ one doesn t simply talk of money one talks of a loss_comma_ gain or expense_comma_ of the business purpose associated therewith_comma_ and of the time period in which it is incurred.
E06-2012.txt,50,Consider Worldwide expenses for environmental compliance were 163 million in 2003.
E06-2012.txt,51,To capture such cases as this_comma_ we ve defined a repertoire of structured entities.
E06-2012.txt,52,Fine grained distinctions about money are encoded as color of money entities_comma_ with such attributes as their color in this case_comma_ an operating expense _comma_ time stamp_comma_ and so forth.
E06-2012.txt,53,We also have structured entities for expressions of stock shares_comma_ assets_comma_ and debt.
E06-2012.txt,54,Finally_comma_ we ve included a number of constructs that are more properly understood as relations job title or events acquisitions .
E06-2012.txt,55,3.3 Statistical training Because we had no existing methods to address financial events or relations_comma_ we took this opportunity to develop a trainable approach.
E06-2012.txt,56,Recent work has begun to address relation and event extraction through trainable means_comma_ chiefly SVM classification Zelenko et al_comma_ 2003_comma_ Zhou et al_comma_ 2005 .
E06-2012.txt,57,The approach we ve used here is classifier based as well_comma_ but relies on maximum entropy modeling instead.
E06-2012.txt,58,Most trainable approaches to event extraction are entity anchored given a pair of relevant entities e.g._comma_ a pair of companies _comma_ the object of the endeavor is to identify the relation that holds between them e.g._comma_ acquisition or subsidiary .
E06-2012.txt,59,We turn this around starting with the head of the relation_comma_ we try to find the entities that fill its constituent roles.
E06-2012.txt,60,This is_comma_ unavoidably_comma_ a strongly lexicalized approach.
E06-2012.txt,61,To detect an event such as a merger or acquisition_comma_ we start from indicative head words_comma_ e.g._comma_ acquire_comma_ purchases_comma_ acquisition_comma_ and the like.
E06-2012.txt,62,The process proceeds in two stages.
E06-2012.txt,63,Once we ve scanned a text to find instances of our indicator heads_comma_ we classify the heads to determine whether their embedding sentence represents a valid instance of the target concept.
E06-2012.txt,64,In the case of acquisitions_comma_ this filtering stage eliminates such non acquisitions as the use of the word purchases in the company purchases raw materials. If a head passes this filter_comma_ we find the fillers of its constituent roles through a second classification stage The role stage uses a shallow parser to chunk the sentence_comma_ and considers the nominal chunks and named entities as candidate role fillers.
E06-2012.txt,65,For acquisition events_comma_ for example_comma_ these roles include the object of the acquisition_comma_ the buying agent_comma_ the bought assets_comma_ the date of acquisition_comma_ and so forth a total of six roles .
E06-2012.txt,67,In the fourth quarter of 2000 WHEN _comma_ 3M AGENT also acquired the multi layer integrated circuit packaging line ASSETS of W.L.
E06-2012.txt,68,Gore and Associates OBJECT .
E06-2012.txt,69,The maximum entropy role classifier relies on a range of feature types the semantic type of the phrase for named entities _comma_ the phrase vocabulary_comma_ the distance to the target head_comma_ and local context words and phrases .
E06-2012.txt,70,Our initial evaluation of this approach has given us encouraging first results.
E06-2012.txt,71,Based on a hand annotated corpus of acquisition events_comma_ we ve measured filtering performance at F 79_comma_ and role assignment at F 84 for the critical case of the object role.
E06-2012.txt,72,A more recent round of experiments has produced considerably higher performance_comma_ which we will report on later this year.
E06-2012.txt,73,4 Subject Classification Financial events with similar descriptions can mean different things depending on where these events appear in a document or in what context they appear.
E06-2012.txt,74,We attempt to extract this important contextual information using text classification methods.
E06-2012.txt,75,We also use text classification methods to help users to more quickly focus on an area where interesting transactions exist in an interactive environment.
E06-2012.txt,76,Specifically_comma_ we classify each paragraph in our document collection into one of several interested financial areas.
E06-2012.txt,77,Examples include Accounting Rule Change_comma_ Acquisitions and Mergers_comma_ Debt_comma_ Derivatives_comma_ Legal_comma_ etc. 133 4.1 Experiments In our experiments_comma_ we picked 3 corporate annual reports as the training and test document set.
E06-2012.txt,78,Paragraphs from these 3 documents_comma_ which are from 50 to 150 pages long_comma_ were annotated with the types of financial transactions they are most related to.
E06-2012.txt,79,Paragraphs that did not fall into a category of interest were classified as other .
E06-2012.txt,80,The annotated paragraphs were divided into random 4x4 test training splits for this test.
E06-2012.txt,81,The other category_comma_ due to its size_comma_ was subsampled to the size of the next largest category.
E06-2012.txt,82,As in the work of Nigam et al 2002 or Lodhi et al 2002 _comma_ we performed a series of experiments using maximum entropy and support vector machines.
E06-2012.txt,83,Besides including the words that appeared in the paragraphs as features_comma_ we also experimented with adding named entity expressions money_comma_ date_comma_ location_comma_ and organization _comma_ removal of stop words_comma_ and stemming.
E06-2012.txt,84,In general_comma_ each of these variations resulted in little difference compared with the baseline features consisting of only the words in the paragraphs.
E06-2012.txt,85,Overall results ranged from F measures of 70 75 for more frequent categories down to above 3040 for categories appearing less frequently.
E06-2012.txt,86,4.2 Online Learning We have embedded our text classification method into an online learning framework that allows users to select text segments_comma_ specify categories for those segments and subsequently receive automatically classified paragraphs similar to those already identified.
E06-2012.txt,87,The highest confidence paragraphs_comma_ as determined by the classifier_comma_ are presented to the user for verification and possible re classification.
E06-2012.txt,88,Figure 1_comma_ at the start of this paper_comma_ shows the way this is implemented in the Maytag interface.
E06-2012.txt,89,Checkboxes labeled pos and neg are provided next to each displayed paragraph by selecting one or the other of these checkboxes_comma_ users indicate whether the paragraph is to be treated as a positive or a negative example of the category they are elaborating.
E06-2012.txt,90,In our preliminary studies_comma_ we were able to achieve the peak performance the highest F1 score within the first 20 training examples using 4 different categories.
E06-2012.txt,91,5 Discussion and future work The ability to combine a range of analytic processing tools_comma_ and the ability to explore their results interactively are the backbone of our approach.
E06-2012.txt,92,In this paper_comma_ we ve covered the framework of our Maytag prototype_comma_ and have looked under its hood at our extraction and classification methods_comma_ especially as they apply to financial texts.
E06-2012.txt,93,Much new work is in the offing.
E06-2012.txt,94,Many experiments are in progress now to assess performance on other text types financial news _comma_ and to pin down performance on a wider range of events_comma_ relations_comma_ and structured entities.
E06-2012.txt,95,Another question we would like to address is how best to manage the interaction between classification and extraction a mutual feedback process may well exist here.
E06-2012.txt,96,We are also concerned with supporting financial analysis across multiple documents.
E06-2012.txt,97,This has implications in the area of cross document coreference_comma_ and is also leading us to investigate visual ways to define queries that go beyond the paragraph and span many texts over many years.
E06-2012.txt,98,Finally_comma_ we are hoping to conduct user studies to validate our fundamental assumption.
E06-2012.txt,99,Indeed_comma_ this work presupposes that interactive application of multi purpose classification and extraction techniques can model complex events as well as monolithic extraction tools la MUC.
E06-2012.txt,100,Acknowledgements This research was performed under a MITRE Corporation sponsored research project. .
E06-2024.txt,1,this paper we present LX Suite_comma_ a set of tools for the shallow processing of Portuguese. This suite comprises several modules_comma_ namely a sentence chunker_comma_ a tokenizer_comma_ a POS tagger_comma_ featurizers and lemmatizers.
E06-2024.txt,3,The purpose of this paper is to present LX Suite_comma_ a set of tools for the shallow processing of Portuguese_comma_ developed under theTagShare1 project by the NLX Group.2 The tools included in this suite are a sentence chunker a tokenizer a POStagger a nominal featurizer a nominal lemmatizer and a verbal featurizer and lemmatizer.
E06-2024.txt,4,These tools were implemented as autonomous modules.
E06-2024.txt,5,This option allows to easily replace any of the modules by an updated version or even by a third party tool.
E06-2024.txt,6,It also allows to use any of these tools separately_comma_ outside the pipeline of the suite.
E06-2024.txt,7,The evaluation results mentioned in the next sections have been obtained using an accurately hand tagged 280_comma_000 token corpus composed of newspaper articles and short novels.
E06-2024.txt,8,2 Sentence chunker The sentence chunker is a finite state automaton FSA _comma_ where the state transitions are triggered by specified character sequences in the input_comma_ and the emitted symbols correspond to sentence s and paragraph p boundaries.
E06-2024.txt,9,Within this setup_comma_ a transition rule could define_comma_ for example_comma_ 1http tagshare.di.fc.ul.pt 2NLX Natural Language and Speech Group_comma_ at the Department of Informatics of the University of Lisbon_comma_ Faculty of Sciences http nlx.di.fc.ul.pt that a period_comma_ when followed by a space and a capital letter_comma_ marks a sentence boundary  .
E06-2024.txt,10,A   . s s A  Being a rule based chunker_comma_ it was tailored to handle orthographic conventions that are specific to Portuguese_comma_ in particular those governing dialog excerpts.
E06-2024.txt,11,This allowed the tool to reach a very good performance_comma_ with values of 99.95 for recall and 99.92 for precision.3 3 Tokenizer Tokenization is_comma_ for the most part_comma_ a simple task_comma_ as the whitespace character is used to mark most token boundaries.
E06-2024.txt,12,Most of other cases are also rather simple Punctuation symbols are separated fromwords_comma_ contracted formsareexpanded andclitics in enclisis or mesoclisis position are detached from verbs.
E06-2024.txt,13,It is worth noting that the first element of an expanded contraction is marked with a symbol  indicating that_comma_ originally_comma_ that token occurred as part of a contraction 4 um_comma_ dois um _comma_ dois da de a viu o viu o In what concerns Portuguese_comma_ the non trivial aspects of tokenization are found in the handling of ambiguous strings that_comma_ depending on their POS tag_comma_ may or may not be considered a contraction.
E06-2024.txt,14,For example_comma_ the word deste can be tokenized as the single token deste if it occurs as a verb Eng. you gave or as the two tokens de este if it occurs as a contraction Eng. of this .
E06-2024.txt,15,3For more details_comma_ see Branco and Silva_comma_ 2004 .
E06-2024.txt,16,4In these examples the symbol will be used to mark token boundaries more clearly.
E06-2024.txt,17,179 It is worth noting that this problem is not a minor issue_comma_ as these strings amount to 2 of the corpus that was used and any tokenization error will have a considerable negative influence on the subsequent steps of processing_comma_ such as POS tagging.
E06-2024.txt,18,To resolve the issue of ambiguous strings_comma_ a two stage tokenization strategy is used_comma_ where the ambiguous strings are not immediately tokenized.
E06-2024.txt,19,Instead_comma_ the decision counts on the contribution of the POS tagger The tagger must first be trained on a version of the corpus where the ambiguous strings are not tokenized_comma_ and are tagged with a composite tagwhenoccurring asacontraction for example P DEM for a contraction of a preposition and a demonstrative .
E06-2024.txt,20,The tagger then runs over the text and assigns a simple or a composite tag to the ambiguous strings.
E06-2024.txt,21,A second pass with the tokenizer then looks for occurrences of tokens with a composite tag and splits them deste V deste V deste P DEM de P este DEM This approach allowed us to successfully resolve 99.4 of the ambiguous strings.
E06-2024.txt,22,This is a much better value than the baseline 78.20 obtained by always considering that the ambiguous strings are a contraction.5 4 POS tagger ForthePOStagging task weused Brant s TnTtagger Brants_comma_ 2000 _comma_ a very efficient statistical tagger based on Hidden Markov Models.
E06-2024.txt,23,For training_comma_ we used 90 of a 280_comma_000 token corpus_comma_ accurately hand tagged with atagset of ca.
E06-2024.txt,24,60 tags_comma_ with inflectional feature values left aside.
E06-2024.txt,25,Evaluation showed an accuracy of 96.87 for this tool_comma_ obtained by averaging 10 test runs over different 10 contiguous portions of the corpus that were not used for training.
E06-2024.txt,26,The POS tagger we developed is currently the fastest tagger for the Portuguese language_comma_ and it is in line with state of the art taggers for other languages_comma_ as discussed in Branco and Silva_comma_ 2004 .
E06-2024.txt,27,5 Nominal featurizer This tool assigns feature value tags for inflection Gender and Number and degree Diminutive_comma_ Superlative and Comparative to words from nominal morphosyntactic categories.
E06-2024.txt,28,5For further details see Branco and Silva_comma_ 2003 .
E06-2024.txt,29,Such tagging is typically done by a POS tagger_comma_ by using a tagset where the base POS tags have been extended with feature values.
E06-2024.txt,30,However_comma_ this increase in the number of tags leads to a lower tagging accuracy due to the data sparseness problem.
E06-2024.txt,31,With our tool_comma_ we explored what could be gained by having a dedicated tool for the task of nominal featurization.
E06-2024.txt,32,We tried several approaches to nominal featurization.
E06-2024.txt,33,Herewereport onthe rule based approach which is the one that better highlights the difficulties in this task.
E06-2024.txt,34,Forthis tool_comma_ webuilt onmorphological regularities and used a set of rules that_comma_ depending on the word termination_comma_ assign default feature values to words.
E06-2024.txt,35,Naturally_comma_ these rules were supplemented by a list of exceptions_comma_ which was collected by using anmachine readable dictionary MRD that allowed us to search words by termination.
E06-2024.txt,36,Nevertheless_comma_ this procedure is still not enough to assign a feature value to every token.
E06-2024.txt,37,The most direct reason is due to the so called invariant words_comma_ which are lexically ambiguous with respect to feature values.
E06-2024.txt,38,For example_comma_ the Common Nounermita Eng. hermit can bemasculine or feminine_comma_ depending ontheoccurrence.
E06-2024.txt,39,Bysimply using termination rules supplemented with exceptions_comma_ such words will always be tagged with underspecified feature values 6 ermita S Tohandle such cases thefeaturizer makesuse of feature propagation.
E06-2024.txt,40,With this mechanism_comma_ words from closed classes_comma_ for which we know their feature values_comma_ propagate their values to the words from open classes following them.
E06-2024.txt,41,These words_comma_ in turn_comma_ propagate those features to other words o MS ermita MS humilde MS Eng. the MS humble MS hermit MS but a FS ermita FS humilde FS Eng. the FS humble FS hermit FS Special care must be taken to avoid that feature propagation reaches outside NP boundaries.
E06-2024.txt,42,For this purpose_comma_ some sequences of POS categories block feature propagation.
E06-2024.txt,43,In the example below_comma_ a PP inside an NP context_comma_ azul an invariant 6Values M masculine_comma_ F feminine_comma_ S singular_comma_ P plural and undefined. 180 adjective might agree with facaor with the preceding word_comma_ ac o.
E06-2024.txt,44,To prevent mistakes_comma_ propagation from ac o to azul should be blocked.
E06-2024.txt,45,faca FS de ac o MS azul FS Eng. blue steel knife or faca FS de ac o MS azul MS Eng. blue steel knife For the sake of comparability with other possible similar tools_comma_ we evaluated the featurizer only over Adjectives and Common Nouns It has 95.05 recall leaving ca. 5 of the tokens with underspecified tags and 99.05 precision.7 6 Nominal lemmatizer Nominal lemmatization consists in assigning to Adjectives and Common Nouns a normalized form_comma_ typically the masculine singular if available.
E06-2024.txt,46,Our approach uses a list of transformation rules that helps changing the termination of the words.
E06-2024.txt,47,For example_comma_ one states that any word ending in ta should have that ending transformed into to gata female cat gato male cat There are_comma_ however_comma_ exceptions that must be accounted for.
E06-2024.txt,48,The word porta_comma_ for example_comma_ is a feminine common noun_comma_ and its lemma is porta porta door_comma_ feminine common noun porta Relevant exceptions like the one above were collected by resorting to a MRD that allowed to search words on the basis of their termination.
E06-2024.txt,49,Being that dictionaries only list lemmas and not inflected forms _comma_ it is possible to search for words with terminations matching the termination of inflected words for example_comma_ words ending in ta .
E06-2024.txt,50,Any word found by the search can thus be considered as an exception.
E06-2024.txt,51,A major difficulty in this task lies in the listing of exceptions when non inflectional affixes are taken into account.
E06-2024.txt,52,As an example_comma_ lets consider again the word porta.
E06-2024.txt,53,This word is an exception to the rule that transforms ta into to.
E06-2024.txt,54,As expected_comma_ this word can occur prefixed_comma_ as in superporta.
E06-2024.txt,55,Therefore_comma_ this derived word 7For a much more extensive analysis_comma_ including a comparison with other approaches_comma_ see Branco and Silva_comma_ 2005a . should also appear in the list of exceptions to prevent it from being lemmatized into superporto by the rule.
E06-2024.txt,56,However_comma_ proceeding like this for every possible prefix leads to an explosion in the number of exceptions.
E06-2024.txt,57,To avoid this_comma_ a mechanism was used that progressively strips prefixes from words while checking the resulting word forms against the list of exceptions supergata gata apply rule supergato but superporta porta exception superporta A similar problem arises when tackling words with suffixes.
E06-2024.txt,58,For instance_comma_ the suffix zinho and its inflected forms zinha_comma_ zinhos andzinhas are used as diminutives.
E06-2024.txt,59,These suffixesshould be removed bythe lemmatization process.
E06-2024.txt,60,However_comma_ there are exceptions_comma_ such as the word vizinho Eng. neighbor which is not a diminutive.
E06-2024.txt,61,This word has to be listed as an exception_comma_ together with its inflected forms vizinha_comma_ vizinhos and vizinhas _comma_ which again leads to a great increase in the number of exceptions.
E06-2024.txt,62,To avoid this_comma_ only vizinhois explicitly listed as an exception and the inflected forms of the diminutive are progressively undone while looking for an exception vizinhas feminine plural vizinha feminine singular vizinho exception vizinho To ensure that exceptions will not be overlooked_comma_ when both these mechanisms work in parallel one must follow all possible paths of affix removal.
E06-2024.txt,63,An heuristic chooses the lemma as being the result found in the least number of steps.8 To illustrate this_comma_ consider the word antena Eng. antenna .
E06-2024.txt,64,Figure 1 shows the paths followed by the lemmatization algorithm when it is faced with antenazinha Eng. small antenna .
E06-2024.txt,65,Both ante and zinha are possible affixes.
E06-2024.txt,66,In a first step_comma_ two search branches are opened_comma_ the first where ante is removed and the second where zinha is transformed into 8ThiscanbeseenasfollowingarationalesimilartoKarlsson s 1990 local disambiguation procedure.
E06-2024.txt,67,181 antenazinha nazinha antenazinho nazinho nazinho antena na no na no 0 1 2 3 4 Figure 1 Lemmatization of antenazinhazinho.
E06-2024.txt,68,The search proceeds under each branch until no transformation ispossible_comma_ oran exception has been found.
E06-2024.txt,69,The end result is the leaf node with the shortest depth which_comma_ in this example_comma_ is antena an exception .
E06-2024.txt,70,This branching might seem to lead to a great performance penalty_comma_ but only a few words have affixes_comma_ and most of them have only one_comma_ in which case there is no branching at all.
E06-2024.txt,71,This tool evaluates to an accuracy of 94.75 .9 7 Verbal featurizer and lemmatizer To each verbal token_comma_ this tool assigns the corresponding lemma and tag with feature values for Mood_comma_ Tense_comma_ Person and Number.
E06-2024.txt,72,The tool uses a list of rules that_comma_ depending on the termination of the word_comma_ assign all possible lemma feature pairs.
E06-2024.txt,73,The word diria_comma_ for example_comma_ is assigned the following lemma feature pairs diria dizer_comma_Cond 1ps  dizer_comma_Cond 3ps  diriar_comma_PresInd 3ps  diriar_comma_ImpAfirm 2ps Currently_comma_ this tool does not attempt to disambiguate among the proposed lemma feature pairs.
E06-2024.txt,74,So_comma_ each verbal token will be tagged with all its possible lemma feature pairs.
E06-2024.txt,75,The tool was evaluated over a list with ca.
E06-2024.txt,76,800_comma_000 verbal forms.
E06-2024.txt,77,It achieves 100 precision_comma_ but at 50 recall_comma_ as half of those forms are ambiguous and receive more than one lemmafeature pair.
E06-2024.txt,78,9For further details_comma_ see Branco and Silva_comma_ 2005b .
E06-2024.txt,79,8 Final Remarks So far_comma_ LX Suite has mostly been used in house for projects being developed by the NLX Group.
E06-2024.txt,80,It is being used in the GramaXing project_comma_ where a computational core grammar for deep linguistic processing of Portuguese is being developed under the Delphin initiative.10 In collaboration with CLUL_comma_11 and under the TagShare project_comma_ LX Suite is being used to help in the building of a corpus of 1 million accurately hand tagged tokens_comma_ by providing an initial_comma_ highquality tagging which is then manually corrected.
E06-2024.txt,81,It is also used for the QueXting project_comma_ whose aim is to make available a question answering system on the Portuguese Web.
E06-2024.txt,82,There is an on line demo of LX Suite located at http lxsuite.di.fc.ul.pt.
E06-2024.txt,83,This on line version of the suite is a partial demo_comma_ as it currently only includes the modules up to the POS tagger.
E06-2024.txt,84,By the end of the TagShare project mid 2006 _comma_ all the other modules described in this paper are planned to have been included.
E06-2024.txt,85,Additionally_comma_ the verbal featurizer and lemmatizer can be tested as a standalone tool at http lxlemmatizer.di.fc.ul.pt.
E06-2024.txt,86,Future work will be focused on extending the suite with new tools_comma_ such as a named entity recognizer and a phrase chunker. .
E06-1041.txt,1,paper discusses two problems that arise in the Generation of Referring Expressions a numeric valued attributes_comma_ such as size or location b perspective taking in reference. Both problems_comma_ it is argued_comma_ can be resolved if some structure is imposed on the available knowledgeprior to content determination.
E06-1041.txt,2,We describe a clustering algorithm which is sufficiently general to be applied to these diverse problems_comma_ discuss its application_comma_ and evaluate its performance.
E06-1041.txt,4,The problem of Generating Referring Expressions GRE can be summed up as a search for the properties in a knowledge base KB whose combination uniquely distinguishes a set of referents from their distractors.
E06-1041.txt,5,The content determination strategy adopted in such algorithms is usually based on the assumption made explicit in Reiter 1990 that the space of possible descriptions is partially ordered with respect to some principle s which determine their adequacy.
E06-1041.txt,6,Traditionally_comma_ these principles have been defined via an interpretation of the Gricean maxims Dale_comma_ 1989 Reiter_comma_ 1990 Dale and Reiter_comma_ 1995 van Deemter_comma_ 2002 1.
E06-1041.txt,7,However_comma_ little attention has been paid to contextual or intentional influences on attribute selection but cf. Jordan and Walker 2000 Krahmer and Theune 2002 .
E06-1041.txt,8,Furthermore_comma_ it is often assumed that all relevant knowledge about domain objects is represented in the database in a format e.g. attribute value pairs that requires no further processing.
E06-1041.txt,9,This paper is concerned with two scenarios which raise problems for such an approach to GRE 1.
E06-1041.txt,10,Real valued attributes_comma_ e.g. size or spatial coordinates_comma_ which represent continuous dimensions.
E06-1041.txt,11,The utility of such attributes depends on whether a set of referents have values that are sufficiently 1For example_comma_ the Gricean Brevity maxim Grice_comma_ 1975 hasbeen interpreted asadirectivetofindtheshortest possible description for a given referent close on the given dimension_comma_ and sufficiently distant from those of their distractors.
E06-1041.txt,12,We discuss this problem in 2.
E06-1041.txt,14,Perspective taking The contextual appropriateness of a description depends on the perspective being takenin context.
E06-1041.txt,15,For instance_comma_ if it is known ofareferentthatitisateacher_comma_andasportsman_comma_it is better to talk of the teacher in a context where another referent has been introduced as the student.
E06-1041.txt,16,This is discussed further in 3.
E06-1041.txt,17,Our aim is to motivate an approach to GRE where these problems are solved by pre processing the information in the knowledge base_comma_ prior to content determination.
E06-1041.txt,18,To this end_comma_ 4 describes a clustering algorithmandshowshowitcanbeappliedtothesedifferent problems to structure the KB prior to GRE.
E06-1041.txt,19,2 Numeric values The case of location Several types of information about domain entities_comma_ such as gradable properties van Deemter_comma_ 2000 and physical location_comma_ are best captured by real valued attributes.
E06-1041.txt,20,Here_comma_ we focus on the example of location as an attribute taking a tuple of values which jointly determine the position of an entity.
E06-1041.txt,21,The ability to distinguish groups is a wellestablished feature of the human perceptual apparatus Wertheimer_comma_ 1938 Treisman_comma_ 1982 .
E06-1041.txt,22,Representing salient groups can facilitate the task of excluding distractors in the search for a referent.
E06-1041.txt,23,For instance_comma_ the set of referents marked as the intended referential target in Figure 1 is easily distinguishable as a group and warrants the use of a spatial description such as the objects in the top left corner_comma_ possibly with a collective predicate_comma_ such as clustered or gathered.
E06-1041.txt,24,In case of reference to a subset of the marked set_comma_ although location would be insufficient to distinguish the targets_comma_ it would reduce the distractor set and facilitate reference resolution2.
E06-1041.txt,25,In GRE_comma_ an approach to spatial reference based on grouping has been proposed by Funakoshi et al. 2Location has been found to significantly facilitate resolution_comma_ even when it is logically redundant Arts_comma_ 2004 321 e1 e4 e3e2 e8 e9 e13e12 e10e11 e6 e7 e5 Figure 1 Spatial Example 2004 .
E06-1041.txt,26,Given a domain and a target referent_comma_ a sequence of groups is constructed_comma_ starting from the largest group containing the referent_comma_ and recursively narrowing down the group until only the referent is identified.
E06-1041.txt,27,The entire sequence is then rendered linguistically.
E06-1041.txt,28,The algorithm used for identifying perceptual groups is the one proposed by Thorisson 1994 _comma_ the core of which is a procedure which takes as input a list of pairs of objects_comma_ ordered by the distance between the entities in the pairs.
E06-1041.txt,29,The procedure loops through the list_comma_ finding the greatest difference in distance between two adjacent pairs.
E06-1041.txt,30,This is determined as a cutoff point for group formation.
E06-1041.txt,31,Two problems are raised by this approach P1 Ambiguous clusters A domain entity can be placed in more than one group.
E06-1041.txt,32,If_comma_ say_comma_ the input list is angbracketleftbig a_comma_b _comma_ c_comma_e _comma_ a_comma_f angbracketrightbig and the greatest difference after the first iteration is between c_comma_e and a_comma_f _comma_thenthefirstgrouptobeformed will be a_comma_b_comma_c_comma_e with a_comma_f likely to be placed in a different group after further iterations.
E06-1041.txt,33,This may be confusingfroma referentialpointof view.
E06-1041.txt,34,The problem arises because grouping or clustering takes place on the basis of pairwise proximity or distance.
E06-1041.txt,35,This problem can be partially circumvented by identifying groups on several perceptual dimensions e.g. spatial distance_comma_ colour_comma_ and shape and then seeking to merge identical groups determined on the basis of these differentqualities seeThorisson 1994 .
E06-1041.txt,36,However_comma_the groupingstrategy can still return groupswhich do not conform to human perceptual principles.
E06-1041.txt,37,A better strategy is to base clustering on the Nearest Neighbour Principle_comma_ familiar from computational geometry Prepaarata and Shamos_comma_ 1985 _comma_ whereby elements are clustered with their nearest neighbours_comma_ given a distance function.
E06-1041.txt,38,The solution offered below is based on this principle.
E06-1041.txt,39,P2 Perceptual proximity Absolute distance is not sufficient for cluster identification.
E06-1041.txt,40,In Figure 1_comma_ for example_comma_ the pairs e1_comma_e2 and e5_comma_e6 could easily be consecutively ranked_comma_ since the distance between e1 and e2 is roughly equal to that between e5 and e6.
E06-1041.txt,41,However_comma_ they would not naturally be clustered together by a human observer_comma_ because grouping of objects also needs to take into account the position of the surrounding elements.
E06-1041.txt,42,Thus_comma_ while e1 is as far away from e2 as e5 is from e6_comma_ there are elements which are closer to e1_comma_e2 than to e5_comma_e6 .
E06-1041.txt,43,The proposal in 4 represents a way of getting around these problems_comma_ which are expected to arise in any kind of domain where the information given is the pairwise distance between elements.
E06-1041.txt,44,Before turning to the framework_comma_ we consider another situation in GRE where the need for clustering could arise.
E06-1041.txt,45,3 Perspectives and semantic similarity In real world discourse_comma_ entities can often be talked about from different points of view_comma_ with speakers bringing to bear world and domain specificknowledge to select information that is relevant to the current topic.
E06-1041.txt,46,In order to generate coherent discourse_comma_ a generatorshouldideallykeeptrackofhowentitieshavebeen referredto_comma_ and maintainconsistencyas far as possible. type profession nationality e1 man student englishman e2 woman teacher italian e3 man chef greek Table 1 Semantic Example Suppose e1 in Table 1 has been introduced into the discourse via the description the student and the next utterance requires a reference to e2.
E06-1041.txt,47,Any one of the three available attributes would suffice to distinguish the latter.
E06-1041.txt,48,However_comma_ a description such as the woman ortheitalianwoulddescribethisentityfromadifferent point of view relative to e1.
E06-1041.txt,49,By hypothesis_comma_ the teacher is more appropriate_comma_ because the property ascribed to e2 is more similar to that ascribed to e1.
E06-1041.txt,50,A similar case arises with plural disjunctive descriptionsof the form x p x q x _comma_ which are usuallyrealised as coordinate constructions of the form the N 1 and the N 2.
E06-1041.txt,51,For instance a reference to e1_comma_e2 such as the woman and the student_comma_ or the englishman and the teacher_comma_ would be odd_comma_ compared to the alternative the student and the teacher.
E06-1041.txt,52,The latter describes these entities under the same perspective.
E06-1041.txt,53,Note that consistency or similarity is not guaranteed simply by attempting to use values of the same attribute s for a given set of referents.
E06-1041.txt,54,The description the student 322 and the chef for e1_comma_e3 is relatively odd compared to the alternative the englishman and the greek.
E06-1041.txt,55,In both kinds of scenarios_comma_ a GRE algorithm that relied on a rigid preferenceordercouldnotguaranteethata coherent description would be generated every time it was available.
E06-1041.txt,56,The issues raised here have never been systematically addressedin the GRE literature_comma_ althoughsupport for the underlying intuitions can be found in various quarters.
E06-1041.txt,57,Kronfeld 1989 distinguishes between functionally and conversationally relevant descriptions.
E06-1041.txt,58,A descriptionis functionallyrelevantif itsucceedsin distinguishing the intended referent s _comma_ but conversational relevance arises in part from implicatures carried by the use of attributes in context.
E06-1041.txt,59,For example_comma_ describing e1 as the student carries the Gricean implicature that the entity s academic role or profession is somehow relevant to the current discourse.
E06-1041.txt,60,When two entities are described using contrasting properties_comma_ say the student and the italian_comma_ the listener may find it harder to work out the relevance of the contrast.
E06-1041.txt,61,In a related vein_comma_Aloni 2002 formalisesthe appropriatenessofan answer to a question of the form Wh x with reference to the conceptual covers or perspectives under which x can be conceptualised_comma_ not all of which are equally relevant given the hearer s information state and the discourse context.
E06-1041.txt,62,With respect to plurals_comma_ Eschenbach et al. 1989 argue that the generation of a plural anaphor with a split antecedent is more felicitous when the antecedents have something in common_comma_ such as their ontological category.
E06-1041.txt,63,This constraint has been shown to hold psycholinguistically Kaup et al._comma_ 2002 Koh and Clifton_comma_ 2002 Moxey et al._comma_ 2004 .
E06-1041.txt,64,Gatt and van Deemter 2005a have shown that people s perception of the adequacy of plural descriptions of the form_comma_ the N1 and the N2 is significantly correlated with the semantic similarity of N1 and N2_comma_ while singular descriptions are more likely to be aggregated into a plural if semantically similar attributes are available Gatt and Van Deemter_comma_ 2005b .
E06-1041.txt,65,The two kinds of problems discussed here could be resolved by pre processing the KB in order to identify available perspectives.
E06-1041.txt,66,One way of doing this is to group available properties into clusters of semantically similar ones.
E06-1041.txt,67,This requires a well defined notion of similarity which determines the distance between properties in semantic space.
E06-1041.txt,68,As with spatial clustering_comma_ the problem is then of how to get from pairwise distance to well formed clusters or groups_comma_ while respecting the principles underlying human perceptual conceptual organisation.
E06-1041.txt,69,The next section describes an algorithm that aims to achieve this.
E06-1041.txt,70,4 A framework for clustering In what follows_comma_ we assume the existence of a set of clustersCin a domain S of objects entities or properties _comma_ to be discovered by the algorithm.
E06-1041.txt,71,We further assume the existence of a dimension_comma_ which is characterised by a function that returns the pairwise distance a_comma_b _comma_ where a_comma_b S S.
E06-1041.txt,72,In case an attribute ischaracterisedbymorethanonedimension_comma_say x_comma_y coordinates in a 2D plane_comma_ as in Figure 1_comma_ then is defined as the Euclidean distance between pairs  radicalBigg summationdisplay x_comma_y D xab yab 2 1 where D is a tuple of dimensions_comma_ xab  a_comma_b on dimension x. satisfies the axioms of minimality 2a _comma_ symmetry 2b _comma_ and the triangle inequality 2c _comma_ by which it determines a metric space on S a_comma_b 0 parenleftbig a_comma_b 0 a bparenrightbig 2a a_comma_b  b_comma_a 2b a_comma_b  b_comma_c a_comma_c 2c We now turn to the problems raised in 2.
E06-1041.txt,73,P1 would be avoided by a clustering algorithm that satisfies 3 . intersectiondisplay Ci C  3 It was also suggested above that a potential solution to P1 is to cluster using the Nearest Neighbour Principle.
E06-1041.txt,74,Before considering a solution to P2_comma_ i.e. the problem of discovering clusters that approximate human intuitions_comma_ it is useful to recapitulate the classic principles of perceptual grouping proposed by Wertheimer 1938 _comma_ of which the following two are the most relevant 1.
E06-1041.txt,75,Proximity The smaller the distance between objects in the cluster_comma_ the more easily perceived it is.
E06-1041.txt,77,Similarity Similar entities will tend to be more easily perceived as a coherent group.
E06-1041.txt,78,Arguably_comma_ once a numeric definition of semantic similarity is available_comma_ the Similarity Principle boils down to the Proximity principle_comma_ where proximity is defined via a semantic distance function.
E06-1041.txt,79,This view is adopted here.
E06-1041.txt,80,How well our interpretation of these principles can be ported to the semantic clustering problem of 3 will be seen in the following subsections.
E06-1041.txt,81,To resolve P2_comma_ we will propose an algorithm that uses a context sensitive definition of nearest neighbour .
E06-1041.txt,82,Recall that P2 arises because_comma_ while is a measure of objective distance on some scale_comma_ perceived 323 proximity resp. distance of a pair a_comma_b is contingent not only on a_comma_b _comma_ but also on the distance of a and b from all other elements in S.
E06-1041.txt,83,A first step towards meeting this requirement is to consider_comma_ for a given pair of objects_comma_ not only the absolute distance proximity between them_comma_ but also the extent to which they are equidistant from other objects in S.
E06-1041.txt,84,Formally_comma_ a measure of perceived proximity prox a_comma_b can be approximated by the following function.
E06-1041.txt,85,Let the two sets Pab_comma_Dab be defined as follows Pab braceleftbigx x S x_comma_a x_comma_b bracerightbig Dab braceleftbigy y S y_comma_a negationslash y_comma_b bracerightbig Then prox a_comma_b F a_comma_b _comma_ Pab _comma_ Dab 4 that is_comma_ prox a_comma_b is a function of the absolute distance a_comma_b _comma_ the number of elements in S a_comma_b which are roughly equidistant from a and b_comma_ and the number of elements which are not equidistant.
E06-1041.txt,86,One way of conceptualising this is to consider_comma_ for a given object a_comma_ the list of all other elements of S_comma_ ranked by their distance proximity to a.
E06-1041.txt,87,Suppose there exists an object b whose ranked list is similar to that of a_comma_ while another object c s list is very different.
E06-1041.txt,88,Then_comma_ all other things being equal in particular_comma_ the pairwise absolute distance _comma_ a clusters closer to b than does c.
E06-1041.txt,89,This takes us from a metric_comma_ distance based conception_comma_toabroadernotionofthe similarity betweentwo objects in a metric space.
E06-1041.txt,90,Our definition is inspired by Tversky s feature based Contrast Model 1977 _comma_ in which the similarity of a_comma_b with feature sets A_comma_B is a linear function of the features they have in common and the features that pertain only to A or B_comma_ i.e. sim a_comma_b f A B f A B .
E06-1041.txt,91,In 4 _comma_ the distance of a and b from every other object is the relevant feature.
E06-1041.txt,92,4.1 Computing perceived proximity The computation of pairwise perceived proximity prox a_comma_b _comma_ shown in Algorithm 1_comma_ is the first step towards finding clusters in the domain.
E06-1041.txt,93,Following Thorisson 1994 _comma_ the procedure uses the absolute distance to calculate absolute proximity 1.7 _comma_ a value in 0_comma_1 _comma_ with 1 corresponding to a_comma_b 0_comma_ i.e. identity cf. axiom 2a .
E06-1041.txt,94,The procedure then visits each element of the domain_comma_ and compares its rank with respect to a and b 1.9 1.13 3_comma_ incrementing a proximity score s 1.10 if the ranks are 3We simplify the presentation by assuming the function rank x_comma_a that returns the rank of x with respect to a.
E06-1041.txt,95,In practice_comma_ this is achieved by creating_comma_ for each element of the input pair_comma_ a totally ordered list La such that La r holds the set of elements ranked at r with respect to x_comma_a Algorithm 1 prox a_comma_b Require a_comma_b Require k a constant 1 maxD max x_comma_y S S x_comma_y 2 if a b then 3 return 1 4 end if 5 s 0 6 d 0 7 p a_comma_b 1 a_comma_b maxD 8 for all x S a_comma_b do 9 if rank x_comma_a rank x_comma_b k then 10 s s 1 11 else 12 d d 1 13 end if 14 end for 15 return p a_comma_b sd approximately equal_comma_ or a distance score d otherwise 1.12 .
E06-1041.txt,96,Approximate equality is determined via a constant k 1.1 _comma_ which_comma_ based on our experiments is set to a tenth the size of S.
E06-1041.txt,97,The procedurereturns the ratio of proximity and distance scores_comma_ weighted by the absolute proximity p a_comma_b 1.15 .
E06-1041.txt,98,Algorithm 1 is called for all pairs in S S yielding_comma_ for each element a S_comma_ a list of elementsorderedby their perceivedproximityto a.
E06-1041.txt,99,The entity with the highest proximity to a is called its anchor.
E06-1041.txt,100,Note that any domain object has one_comma_ and only one anchor.
E06-1041.txt,101,4.2 Creating clusters The procedure makeClusters S_comma_Anchors _comma_ shown in its basic form in Algorithm 2_comma_ uses the notion of an anchor introduced above.
E06-1041.txt,102,The rationale behind the algorithm is captured by the following declarative principle_comma_ where C Cis any cluster_comma_ and anchor a_comma_b means b is the anchor of a a C anchor a_comma_b b C 5 A cluster is defined as the transitive closure of the anchor relation_comma_ that is_comma_ if it holds that anchor a_comma_b and anchor b_comma_c _comma_ then a_comma_b_comma_c will be clustered together.
E06-1041.txt,103,Apartfromsatisfying 5 _comma_theprocedurealsoinduces a partition on S_comma_ satisfying 3 .
E06-1041.txt,104,Given these primary aims_comma_ no attempt is made_comma_ once clusters are generated_comma_ to further sub divide them_comma_ although we briefly return to this issue in 5.
E06-1041.txt,105,The algorithm initialises a set Clusters to empty 2.1 _comma_ and iterates through the list of objects S 2.5 .
E06-1041.txt,106,For each object a and its anchor b 2.6 _comma_ it first checks whether they have already been clustered e.g. if either of them was the anchor of an object visited earlier 2.7_comma_ 2.12 .
E06-1041.txt,107,If this is not the case_comma_ then a provisionalcluster is initialised foreach element 324 Algorithm 2 makeClusters S_comma_ Anchors Ensure Snegationslash 1 Clusters 2 if S 1 then 3 return S 4 end if 5 for all a S do 6 b Anchors a 7 if C Clusters a C then 8 Ca C 9 else 10 Ca a 11 end if 12 if C Clusters b C then 13 Cb C 14 Clusters Clusters Cb 15 else 16 Cb b 17 end if 18 Ca Ca Cb 19 Clusters Clusters Ca 20 end for 21 return Clusters 2.10_comma_ 2.16 .
E06-1041.txt,108,The procedure simply merges the cluster containing a with that of its b 2.18 _comma_ having removed the latter from the cluster set 2.14 .
E06-1041.txt,109,This algorithm is guaranteed to induce a partition_comma_ since no element will end up in more than one group.
E06-1041.txt,110,It does not depend on an ordering of pairs a la Thorisson.
E06-1041.txt,111,However_comma_ problems arise when elements and anchors are clustered n aively.
E06-1041.txt,112,For instance_comma_ if an element is very distant from every other element in the domain_comma_ prox a_comma_b will still find an anchor for it_comma_ and makeClusters S_comma_Anchors will place it in the same cluster as its anchor_comma_ although it is an outlier.
E06-1041.txt,113,Before describing how this problem is rectified_comma_ we introduce the notion of a family F of elements.
E06-1041.txt,114,Informally_comma_this is a set of elementsofS that have the same anchor_comma_that is a_comma_b F anchor a_comma_x anchor b_comma_y x y 6 The solution to the outlier problem is to calculate a centroid value for each family found after prox a_comma_b .
E06-1041.txt,115,This is the averageproximitybetween the commonanchor and all members of its family_comma_ minus one standard deviation.
E06-1041.txt,116,Prior to merging_comma_ at line 2.18 _comma_ the algorithm now checks whether the proximity value between an element and its anchor falls below the centroid value.
E06-1041.txt,117,If it does_comma_ the the cluster containing an object and that containing its anchor are not merged.
E06-1041.txt,118,4.3 Two applications The algorithm was applied to the two scenarios described in 2 and 3.
E06-1041.txt,119,In the spatial domain_comma_ the algorithm returns groups or clusters of entities_comma_ based on theirspatialproximity.
E06-1041.txt,120,Thiswastested ondomainslike Figure 1 in which the input is a set of entities whose position is defined as a pair of x y coordinates.
E06-1041.txt,121,Figure 1 illustrates a potential problem with the procedure.
E06-1041.txt,122,In that figure_comma_ it holds that anchor e8_comma_e9 and anchor e9_comma_e8 _comma_ making e8 and e9 a reciprocal pair.
E06-1041.txt,123,In such cases_comma_ the algorithm inevitably groups the two elements_comma_ whatever their proximity distance.
E06-1041.txt,124,This may be problematic when elements of a reciprocal pair are very distant from eachother_comma_ in which case they are unlikely to be perceived as a group.
E06-1041.txt,125,We return to this problem briefly in 5.
E06-1041.txt,126,The second domain of application is the clustering of properties into perspectives .
E06-1041.txt,127,Here_comma_ we use the information theoretic definition of similarity developed by Lin 1998 and applied to corpus data by Kilgarriff and Tugwell Kilgarriff and Tugwell_comma_ 2001 .
E06-1041.txt,128,This measure defines the similarity of two words as a functionofthe likelihoodoftheiroccurringinthesame grammatical environments in a corpus.
E06-1041.txt,129,This measure was shown experimentally to correlate highly with human acceptability judgments of disjunctive plural descriptions Gatt and van Deemter_comma_ 2005a _comma_ when compared with a number of measures that calculate the similarity of word senses in WordNet.
E06-1041.txt,130,Using this as the measure of semantic distance between words_comma_ the algorithm returns clusters such as those in Figure 2. input waiter_comma_ essay_comma_ footballer_comma_ article_comma_ servant_comma_ cricketer_comma_ novel_comma_ cook_comma_ book_comma_ maid_comma_ player_comma_ striker_comma_ goalkeeper output 1 essay_comma_ article_comma_ novel_comma_ book 2 footballer_comma_ cricketer 3 waiter_comma_ cook_comma_ servant_comma_ maid 4 player_comma_ goalkeeper_comma_ striker Figure 2 Output on a Semantic Domain If the words in Figure 2 represented properties of different entities in the domain of discourse_comma_ then the clusters would represent perspectives or covers _comma_ whose extension is a set of entities that can be talked about from the same point of view.
E06-1041.txt,131,For example_comma_ if some entity were specified as having the property footballer_comma_ and the property striker_comma_ while another entity had the propertycricketer_comma_ then accordingto the output of the algorithm_comma_ the description the footballer and the cricketer is the most conceptually coherent one available.
E06-1041.txt,132,It could be argued that the units of representation 325 spatial semantic 1 0.94 0.58 2 0.86 0.36 3 0.62 0.76 4 0.93 0.52 mean 0.84 0.64 Table 2 Proportion of agreement among participants in GRE are not words but properties e.g. values of attributes which can be realised in a number of different ways if_comma_ for instance_comma_ there are a number of synonyms corresponding roughly to the same intension .
E06-1041.txt,133,This could be remedied by defining similarity as distance in an ontology conversely_comma_ properties could be viewed as a set of potential word realisations.
E06-1041.txt,134,5 Evaluation The evaluation of the algorithm was based on a comparisonofitsoutputagainsttheoutputofhumanbeings in a similar task.
E06-1041.txt,135,Thirteen native or fluent speakers of English volunteered to participate in the study.
E06-1041.txt,136,The materials consisted of 8 domains_comma_ 4 of which were graphical representations of a 2D spatial layout containing 13 points.
E06-1041.txt,137,The pictures were generated by plotting numerical x y coordinates the same values are used as input to the algorithm .
E06-1041.txt,138,The other four domains consisted of a set of 13 arbitrarily chosen nouns.
E06-1041.txt,139,Participants were presented with an eight page booklet with spatial and semantic domains on alternate pages.
E06-1041.txt,140,They were instructed to draw circles around the best clusters in the pictures_comma_ or write down the words in groups that were related according to their intuitions.
E06-1041.txt,141,Clusters could be of arbitrary size_comma_ but each element had to be placed in exactly one cluster.
E06-1041.txt,142,5.1 Participant agreement Participant agreement on each domain was measured using kappa.
E06-1041.txt,143,Since the task did not involve predefined clusters_comma_ the set of unique groups denoted G generated by participants in every domain was identified_comma_ representing the set of categories available post hoc.
E06-1041.txt,144,For each domain element_comma_ the number of times it occurred in each group served as the basis to calculate the proportionof agreementamongparticipantsfor the element.
E06-1041.txt,145,The total agreement P A and the agreement expected by chance_comma_ P E were then used in the standard formula k P A P E 1 P E Table 2 shows a remarkable difference between the two domain types_comma_ with very high agreement on spatial domains and lower values on the semantic task.
E06-1041.txt,146,The difference was significant t 2.54_comma_ p 0.05 .
E06-1041.txt,147,Disagreement on spatial domains was mostly due to the problemof reciprocalpairs_comma_ where participantsdisagreedonwhetherentitiessuchase8 ande9 inFigure1 gave rise to a well formed cluster or not.
E06-1041.txt,148,However_comma_ all the participants were consistent with the version of the NearestNeighbourPrinciplegivenin 5 .
E06-1041.txt,149,Ifanelement was grouped_comma_ it was always grouped with its anchor.
E06-1041.txt,150,The disagreement in the semantic domains seemed to turn on two cases4 1.
E06-1041.txt,151,Sub clusters Whereas some proposals included clusters such as man_comma_ woman_comma_ boy_comma_ girl_comma_ infant_comma_ toddler_comma_ baby_comma_ child _comma_ others chose to group infant_comma_ toddler_comma_ baby_comma_child separately.
E06-1041.txt,153,Polysemy For example_comma_ liver was in some cases clustered with steak_comma_ pizza _comma_ while others grouped it with items like heart_comma_ lung .
E06-1041.txt,154,Insofar as an algorithm should capture the whole range of phenomena observed_comma_ 1 above could be accounted for by making repeated calls to the Algorithm to subdivide clusters.
E06-1041.txt,155,One problem is that_comma_ in case only one clusterisfoundintheoriginaldomain_comma_thesamecluster willbereturnedafterfurtherattemptsatsub clustering.
E06-1041.txt,156,A possible solution to this is to redefine the parameter k in Algorithm 1 _comma_ makingthe conditionfor proximity more strict.
E06-1041.txt,157,As for the second observation_comma_ the desideratumexpressedin 3 maybetoostronginthesemantic domain_comma_ since words can be polysemous.
E06-1041.txt,158,As suggested above_comma_ one way to resolve this would be to measure distance between word senses_comma_ as opposed to words.
E06-1041.txt,159,5.2 Algorithm performance Theperformanceofthe algorithm hereafterthetarget against the human output was compared to two baseline algorithms.
E06-1041.txt,160,In the spatial domains_comma_ we used an implementationoftheThorissonalgorithm Thorisson_comma_ 1994 described in 2.
E06-1041.txt,161,In our implementation_comma_ the procedure was called iteratively until all domain objects had been clustered in at least one group.
E06-1041.txt,162,For the semantic domains_comma_ the baseline was a simple procedure which calculated the powerset of each domain S.
E06-1041.txt,163,For each subset in pow S _comma_S _comma_ the procedurecalculatesthe meanpairwisesimilarity between words_comma_ returning an ordered list of subsets.
E06-1041.txt,164,This partial order is then traversed_comma_ choosing subsets until all elements had been grouped.
E06-1041.txt,165,This seemed to be a reasonable baseline_comma_ because it corresponds to the intuition that the best cluster from a semantic point of view is the one with the highest pairwise similarity among its elements.
E06-1041.txt,166,4The conservative strategy used here probably amplifies disagreements disregarding clusters which are subsumed by other clusters would control at least for case 1 326 The outputof the targetand baseline algorithmswas compared to human output in the following ways 1.
E06-1041.txt,167,By item In each of the eight test domains_comma_ an agreement score was calculated for each domain element e i.e. 13 scores in each domain .
E06-1041.txt,168,Let Us be the set of distinct groups containing e proposed bythe experimentalparticipants_comma_and let Ua be the set of unique groups containinge proposed by the algorithm Ua 1 in case of the target algorithm_comma_ but not necessarily for the baselines_comma_ since they do not impose a partition .
E06-1041.txt,169,For each pair Uai_comma_Usj of algorithm human clusters_comma_ the agreement score was defined as Uai Usj Uai Usj Uai Usi _comma_ i.e. the ratio of the number of elements on which the human algorithmagree_comma_ and the number of elements on which they do not agree.
E06-1041.txt,170,This returns a number in 0_comma_1 with 1 indicating perfect agreement.
E06-1041.txt,172,This controlled for the possible advantage that the target algorithm might have_comma_ given that it_comma_ like the human participants_comma_ partitions the domain.
E06-1041.txt,174,By participant An overall mean agreement score was computed for each participant using the above formula for the target and baseline algorithms in each domain.
E06-1041.txt,175,Results by item Table 3 shows the mean and modal agreement scores obtained for both target and baseline in each domain type.
E06-1041.txt,176,At a glance_comma_ the target algorithm performed better than the baseline on the spatial domains_comma_ with a modal score of 1_comma_ indicating perfect agreement on 60 of the objects.
E06-1041.txt,177,The situation is different in the semantic domains_comma_ where target and baseline performedroughlyequally well in fact_comma_ the modal score of 1 accounts for 75 baseline scores. target baseline spatial mean 0.84 0.72 mode 1 60 0.67 40 semantic mean 0.86 0.86 mode 1 65 1 75 Table 3 Mean and modal agreement scores Unsurprisingly_comma_ the difference between target and baseline algorithmswas reliableon the spatialdomains t 2.865_comma_ p .01 _comma_ but not on the semantic domains t 1_comma_ ns .
E06-1041.txt,178,Thiswas confirmedby a one wayAnalysis of Variance ANOVA _comma_ testing the effect of algorithm target baseline anddomaintype spatial semantic on agreement results.
E06-1041.txt,179,There was a significant main effect of domain type F 6.399_comma_ p .01 _comma_ while the main effect of algorithm was marginallysignificant F 3.542_comma_ p .06 .
E06-1041.txt,180,However_comma_ there was a reliable type algorithm interaction F 3.624_comma_ p .05 _comma_ confirming the finding that the agreement between target and human output differed between domain types.
E06-1041.txt,181,Given the relative lack of agreement between participants in the semantic clustering task_comma_ this is unsurprising.
E06-1041.txt,182,Although the analysis focused on maximal scores obtained per entity_comma_ if participants do not agree on groupings_comma_ then the means which are statistically compared are likely to mask a significant amount of variance.
E06-1041.txt,183,We now turn to the analysis by participants.
E06-1041.txt,184,Results by participant The difference between target and baselines in agreement across participants was significant both for spatial t 16.6_comma_ p .01 and semantic t 5.759_comma_ t .01 domain types.
E06-1041.txt,185,This corroborates the earlier conclusion once participant variation is controlled for by including it in the statistical model_comma_ the differences between target and baseline show up as reliable across the board.
E06-1041.txt,186,A univariate ANOVA corroborates the results_comma_ showing no significant main effect of domain type F 1_comma_ ns _comma_ but a highly significant main effect of algorithm F 233.5_comma_ p .01 and a significant interaction F 44.3_comma_ p .01 .
E06-1041.txt,187,Summary The results of the evaluation are encouraging_comma_ showing high agreement between the output of the algorithm and the output that was judged by humans as most appropriate.
E06-1041.txt,188,They also suggest framework of 4 correspondsto human intuitions better than thebaselinestestedhere.
E06-1041.txt,189,However_comma_theseresultsshould beinterpretedwithcautioninthecaseofsemanticclustering_comma_ wheretherewas significantvariabilityin human agreement.
E06-1041.txt,190,With respect to spatial clustering_comma_ one outstanding problem is that of reciprocal pairs which are too distant from eachother to form a perceptually wellformed cluster.
E06-1041.txt,191,We are extending the empirical study to new domains involving such cases_comma_ in order to infer from the human data a threshold on pairwise distance between entities_comma_ beyond which they are not clustered.
E06-1041.txt,192,6 Conclusions and future work This paper attempted to achieve a dual goal.
E06-1041.txt,193,First_comma_ we highlighted a number of scenarios in which the performance of a GRE algorithm can be enhanced by an initial step which identifies clusters of entities or properties.
E06-1041.txt,194,Second_comma_ we describedan algorithmwhich takesas input a set of objects and returns a set of clusters based on a calculation of their perceived proximity.
E06-1041.txt,195,The definition of perceived proximity seeks to take into account some of the principles of human perceptual and conceptual organisation.
E06-1041.txt,196,In current work_comma_ the algorithm is being applied to 327 twoproblemsinGRE_comma_namely_comma_thegenerationofspatial .
E06-1014.txt,1,Latent Semantic Analysis PLSA models have been shown to provide a better model for capturing polysemy and synonymy than Latent Semantic Analysis LSA . However_comma_ the parameters of a PLSA model are trained using the Expectation Maximization EM algorithm_comma_ and as a result_comma_ the trained model is dependent on the initialization values so that performance can be highly variable.
E06-1014.txt,2,Inthispaper wepresent amethodforusing LSA analysis to initialize a PLSA model.
E06-1014.txt,3,We also investigated the performance of our method for the tasks of text segmentation and retrieval onpersonal size corpora_comma_ and present results demonstrating the efficacy of our proposed approach.
E06-1014.txt,5,In modeling a collection of documents for information access applications_comma_ the documents are often represented as a bag of words _comma_ i.e._comma_ as term vectors composed of the terms and corresponding counts for each document.
E06-1014.txt,6,The term vectors for a document collection can be organized into a term by document co occurrence matrix.
E06-1014.txt,7,When directly using these representations_comma_ synonyms and polysemous terms_comma_ that is_comma_ terms with multiple senses or meanings_comma_ are not handled well.
E06-1014.txt,8,Methods for smoothing the term distributions through the use of latent classes have been shown to improve the performance of a number of information access tasks_comma_ including retrieval over smaller collections Deerwester et al._comma_ 1990 _comma_ text segmentation Brants et al._comma_ 2002 _comma_ and text classification Wu and Gunopulos_comma_ 2002 .
E06-1014.txt,9,The Probabilistic Latent Semantic Analysis model PLSA Hofmann_comma_ 1999 provides a probabilistic framework that attempts to capture polysemy and synonymy in text for applications such as retrieval and segmentation.
E06-1014.txt,10,It uses a mixture decomposition to model the co occurrence data_comma_ and the probabilities of words and documents are obtained by a convex combination of the aspects.
E06-1014.txt,11,The mixture approximation has a well defined probability distribution andthe factors have aclear probabilistic meaningintermsofthemixture component distributions.
E06-1014.txt,12,The PLSA model computes the relevant probability distributions by selecting the model parameter values that maximize the probability of the observed data_comma_ i.e._comma_ the likelihood function.
E06-1014.txt,13,The standard method for maximum likelihood estimation is the Expectation Maximization EM algorithm.
E06-1014.txt,14,For a given initialization_comma_ the likelihood function increases with EM iterations until a local maximum is reached_comma_ rather than a global maximum_comma_ so that the quality of the solution depends on the initialization ofthe model.
E06-1014.txt,15,Additionally_comma_ thelikelihood values across different initializations are not comparable_comma_ as we will show.
E06-1014.txt,16,Thus_comma_ the likelihood function computed overthetraining datacannot be used as a predictor of model performance across different models.
E06-1014.txt,17,Rather than trying to predict the best performing model from a set of models_comma_ in this paper we focus on finding a good way to initialize the PLSA model.
E06-1014.txt,18,We will present a framework for using Latent Semantic Analysis LSA Deerwester et al._comma_ 1990 to better initialize the parameters of a corresponding PLSA model.
E06-1014.txt,19,The EM algorithm is then used to further refine the initial estimate.
E06-1014.txt,20,This combination of LSA and PLSA leverages the advantages of both.
E06-1014.txt,21,105 This paper is organized as follows in section 2_comma_ we review related work in the area.
E06-1014.txt,22,In section 3_comma_ we summarize related work on LSA and its probabilistic interpretation.
E06-1014.txt,23,In section 4 we review the PLSA model and in section 5 we present our method for initializing a PLSA model using LSA model parameters.
E06-1014.txt,24,In section 6_comma_ we evaluate the performance of our framework on a text segmentation task and several smaller information retrieval tasks.
E06-1014.txt,25,And in section 7_comma_ we summarize our results and give directions for future work.
E06-1014.txt,26,2 Background A number of different methods have been proposed for handling the non globally optimal solution when using EM.
E06-1014.txt,27,These include the use of Tempered EM Hofmann_comma_ 1999 _comma_ combining models from different initializations in postprocessing Hofmann_comma_ 1999 Brants et al._comma_ 2002 _comma_ and trying to find good initial values.
E06-1014.txt,28,For their segmentation task_comma_ Brants et al. 2002 found overfitting_comma_ which Tempered EM helps address_comma_ was not a problem and that early stopping of EM provided good performance and faster learning.
E06-1014.txt,29,Computing and combining different models is computationally expensive_comma_ so a method that reduces this cost is desirable.
E06-1014.txt,30,Different methods for initializing EM include the use of random initialization e.g._comma_ Hofmann_comma_ 1999 _comma_ k means clustering_comma_ and an initial cluster refinement algorithm Fayyad et al._comma_ 1998 .
E06-1014.txt,31,K means clustering is not a good fit to the PLSA model in several ways it is sensitive to outliers_comma_ it is a hard clustering_comma_ and the relation of the identified clusters to the PLSA parameters is not well defined.
E06-1014.txt,32,In contrast to these other initialization methods_comma_ weknow that the LSAreduces noise in the data and handles synonymy_comma_ and so should be a good initialization.
E06-1014.txt,33,The trick is in trying to relate the LSA parameters to the PLSA parameters.
E06-1014.txt,34,LSA is based on singular value decomposition SVD of a term by document matrix and retaining the top K singular values_comma_ mapping documents and terms to a new representation in a latent semantic space.
E06-1014.txt,35,It has been successfully applied in different domains including automatic indexing.
E06-1014.txt,36,Text similarity is better estimated in this low dimension space because synonyms are mapped to nearby locations and noise is reduced_comma_ although handling of polysemy is weak.
E06-1014.txt,37,In contrast_comma_ the PLSA model distributes the probability mass of a term over the different latent classes corresponding to different senses of a word_comma_ and thus better handles polysemy Hofmann_comma_ 1999 .
E06-1014.txt,38,The LSA model has two additional desirable features.
E06-1014.txt,39,First_comma_ the word document co occurrence matrix can be weighted by any weight function that reflects the relative importance of individual words e.g._comma_ tfidf .
E06-1014.txt,40,The weighting can therefore incorporate external knowledge into the model.
E06-1014.txt,41,Second_comma_ the SVD algorithm is guaranteed to produce the matrix of rank a0 that minimizes the distance to the original word document co occurrence matrix.
E06-1014.txt,42,As noted in Hofmann 1999 _comma_ an important difference between PLSA and LSA is the type of objective function utilized.
E06-1014.txt,43,In LSA_comma_ this is the L2 or Frobenius norm on the word document counts.
E06-1014.txt,44,In contrast_comma_ PLSA relies on maximizing the likelihood function_comma_ which is equivalent to minimizing the cross entropy or Kullback Leibler divergence between the empirical distribution and the predicted model distribution of terms in documents.
E06-1014.txt,45,A number of methods for deriving probabilities from LSA have been suggested.
E06-1014.txt,46,For example_comma_ Coccaro and Jurafsky 1998 proposed a method based on the cosine distance_comma_ and Tipping and Bishop 1999 give a probabilistic interpretation of principal component analysis that is formulated within a maximum likelihood framework based on a specific form of Gaussian latent variable model.
E06-1014.txt,47,In contrast_comma_ we relate the LSA parameters to the PLSA model using a probabilistic interpretation of dimensionality reduction proposed by Ding 1999 that uses an exponential distribution to model the term and document distribution conditioned on the latent class.
E06-1014.txt,48,3 LSA We briefly review the LSA model_comma_ as presented in Deerwester et al. 1990 _comma_ and then outline the LSA based probability model presented in Ding 1999 .
E06-1014.txt,49,The term to document association is presented as a term document matrix a1a3a2 a4a5 a6a8a7 a9 a9 a10a11a10a11a10 a7 a9 a12 ... ... ... a7 a13 a9 a10a11a10a11a10 a7 a13 a12 a14a16a15 a17 a2a19a18a21a20a23a22 a10a11a10a11a10 a20a25a24a27a26a28a2 a4a5 a6 a29 a22 ... a29a31a30 a14a16a15 a17 1 containing the frequency of the a32 index terms occurring in a33 documents.
E06-1014.txt,50,The frequency counts can also be weighted to reflect the relative importance of individual terms e.g._comma_ Guo et al._comma_ 2003 .
E06-1014.txt,51,a20a35a34 is an a32 dimensional column vector representing 106 document a0 and a29a2a1 is an a33 dimensional row vector representing terma3 .
E06-1014.txt,52,LSA represents terms and documents in a new vector space with smaller dimensions that minimize the distance between the projected terms and the original terms.
E06-1014.txt,53,This is done through the truncated to rank a0 singular value decomposition a1 a4 a1a6a5 a2a8a7a9a5a11a10a12a5a14a13a16a15a17 or explicitly a1a18a5 a2 a18a20a19 a22 a10a11a10a11a10 a19a21a5 a26 a4a5 a6 a22 a9 ... a22 a17 a14a16a15 a17 a4a5 a6 a23 a22 ... a23 a5 a14a16a15 a17a25a24 2 Among all a32a27a26 a33 matrices of rank a0 _comma_ a1a6a5 is the one that minimizes the Frobenius norm a28a29a28a1a31a30 a1a32a5 a28a29a28a33a34 a24 3.1 LSA based Probability Model The LSA model based on SVD is a dimensionality reduction algorithm and as such does not have a probabilistic interpretation.
E06-1014.txt,54,However_comma_ under certain assumptions on the distribution of the input data_comma_ the SVD can be used to define a probability model.
E06-1014.txt,55,In this section_comma_ we summarize the results presented in Ding 1999 of a dual probability representation of LSA.
E06-1014.txt,56,Assuming the probability distribution of a document a20a25a34 is governed by a0 characteristic normalized document vectors_comma_ a35 a22 a10a11a10a11a10 a35 a5 _comma_ and that the a35 a22 a10a11a10a11a10 a35 a5 are statistically independent factors_comma_ Ding 1999 shows that using maximum likelihood estimation_comma_ the optimal solution for a35 a22a37a36 a10a11a10a11a10 a35 a5 are the left eigenvectorsa19 a22 a10a11a10a11a10 a19a38a5 in the SVD of a1 used in LSA a39 a18a21a20 a1 a28 a19 a22 a10a11a10a11a10 a19a21a5 a26a28a2a31a40a14a41a43a42a45a44a2a46a47a37a48a45a49a51a50a53a52a21a46a54a46a54a46a52a21a41a43a42a45a44a55a46a47a57a56a58a49a59a50 a60 a18a20a19a28a22 a24a61a24a61a24 a19a38a5 a26 3 where a60 a18a20a19 a22 a10a11a10a11a10 a19a21a5 a26 is a normalization constant.
E06-1014.txt,57,The dual formulation for the probability of term a29 in terms of the tight eigenvectors i.e._comma_ the document representations a23 a22 a10a11a10a11a10 a23 a5 a26 of the matrix a1a9a5 is a39 a18 a29 a34 a28 a23 a22 a10a11a10a11a10 a23 a5 a26a28a2 a40a14a41a63a62a65a64a66a46a67a37a48a45a49 a50 a52a21a46a54a46a54a46a52a21a41a63a62a65a64a59a46a67 a56 a49 a50 a60 a18 a23 a22 a10a11a10a11a10 a23 a5 a26 4 where a60 a18a23 a22 a10a11a10a11a10 a23 a5 a26 is a normalization constant.
E06-1014.txt,58,Ding also shows thata19 a1 is related toa23 a1 by a19 a1 a2 a68 a22a70a69 a1 a18 a23 a1 a26 a15 a3 a2 a68 a36 a10a11a10a11a10 a36 a0 5 We will use Equations 3 5 in relating LSA to PLSA in section 5.
E06-1014.txt,59,4 PLSA The PLSA model Hofmann_comma_ 1999 is a generative statistical latent classmodel 1 select adocument a71 with probability a39 a18 a71 a26 2 pick a latent class a72 with probabilitya39 a18a72a73a28a71 a26 and 3 generate a worda74 with probabilitya39 a18a74a75a28a72 a26 _comma_ where a39 a18 a74a76a28 a71 a26a35a2a27a77a79a78 a39 a18 a74a75a28a72 a26 a39 a18 a72a73a28 a71 a26 a24 6 The joint probability between a word and document_comma_a39 a18a71 a36a74 a26 _comma_ is given by a39 a18 a71 a36 a74 a26 a2 a39 a18 a71 a26 a39 a18 a74a76a28 a71 a26 a2 a39 a18 a71 a26 a77 a78 a39 a18 a74a76a28a72 a26 a39 a18 a72a73a28 a71 a26 and using Bayes rule can be written as a39 a18 a71 a36 a74 a26 a2 a77a61a78 a39 a18 a74a76a28a72 a26 a39 a18 a71 a28a72 a26 a39 a18 a72 a26 a24 7 The likelihood function is given by a80 a2 a77a61a81a82a77a84a83 a33 a18 a71 a36 a74 a26a70a85a29a86a14a87 a39 a18 a71 a36 a74 a26 a24 8 Hofmann 1999 uses the EM algorithm to compute optimal parameters.
E06-1014.txt,60,The E step is given by a39 a18 a72a73a28 a71 a36 a74 a26 a2 a39 a18 a72 a26 a39 a18 a71 a28a72 a26 a39 a18 a74a75a28a72 a26 a88 a78a53a89 a39 a18 a72a14a90 a26 a39 a18 a71 a28a72a37a90 a26 a39 a18 a74a75a28a72a14a90 a26 9 and the M step is given by a39 a18 a74a76a28a72 a26 a2 a88 a81 a33 a18 a71 a36 a74 a26 a39 a18 a72a73a28 a71 a36 a74 a26 a88 a81 a91 a83 a89 a33 a18 a71 a36 a74a92a90 a26 a39 a18 a72a73a28 a71 a36 a74a92a90 a26 10 a39 a18 a71 a28a72 a26 a2 a88 a83 a33 a18 a71 a36 a74 a26 a39 a18 a72a73a28 a71 a36 a74 a26 a88 a81 a89 a91 a83 a33 a18 a71 a90 a36 a74 a26 a39 a18 a72a73a28 a71 a90 a36 a74 a26 11 a39 a18 a72 a26 a2 a88 a81 a91 a83 a33 a18 a71 a36 a74 a26 a39 a18 a72a93a28 a71 a36 a74 a26 a88 a81 a91 a83 a33 a18 a71 a36 a74 a26 a24 12 4.1 Model Initialization and Performance An important consideration in PLSA modeling is that the performance of the model is strongly affected by the initialization of the model prior to training.
E06-1014.txt,61,Thus a method for identifying a good initialization_comma_ or alternatively a good trained model_comma_ is needed.
E06-1014.txt,62,If the final likelihood value obtained after training was well correlated with accuracy_comma_ then one could train several PLSA models_comma_ each with a different initialization_comma_ and select the model with the largest likelihood as the best model.
E06-1014.txt,63,Although_comma_ for a given initialization_comma_ the likelihood 107 Table 1 Correlation between the negative loglikelihood and Average or BreakEven Precision Data Factors Average BreakEven Precision Precision Med 64 0.47 0.41 Med 256 0.15 0.25 CISI 64 0.20 0.20 CISI 256 0.12 0.16 CRAN 64 0.03 0.16 CRAN 256 0.15 0.14 CACM 64 0.64 0.08 CACM 256 0.22 0.12 increases to a locally optimal value with each iteration of EM_comma_ the final likelihoods obtained from different initializations after training do not correlate well with the accuracy of the corresponding models.
E06-1014.txt,64,This is shown in Table 1_comma_ which presents correlation coefficients between likelihood values and either average or breakeven precision for several datasets with 64 or 256 latent classes_comma_ i.e._comma_ factors.
E06-1014.txt,65,Twenty random initializations were used per evaluation.
E06-1014.txt,66,Fifty iterations of EM per initialization were run_comma_ which empirically is more than enough to approach the optimal likelihood.
E06-1014.txt,67,The coefficients range from 0.64 to 0.25.
E06-1014.txt,68,The poor correlation indicates the need for a method to handle the variation in performance due to the influence of different initialization values_comma_ for example through better initialization methods.
E06-1014.txt,69,Hofmann 1999 andBrants 2002 averaged results from five and four random initializations_comma_ respectively_comma_ and empirically found this to improve performance.
E06-1014.txt,70,The combination of models enables redundancies in the models to minimize the expression of errors.
E06-1014.txt,71,We extend this approach by replacing one random initialization with one reasonably good initialization in the averaged models.
E06-1014.txt,72,We will empirically show that having at least one reasonably goodinitialization improvestheperformance oversimply using anumber ofdifferent initializations.
E06-1014.txt,73,5 LSA based Initialization of PLSA The EM algorithm for estimating the parameters of the PLSA model is initialized with estimates of the model parameters a39 a18 a72 a26 a36 a39 a18 a74a76a28a72 a26 a36 a39 a18 a71 a28a72 a26 .
E06-1014.txt,74,Hofmann 1999 relates the parameters of the PLSA model to an LSA model as follows a0 a2 a7a2a1a4a3a6a5a8a7a10a9a11a1a12a3a6a5a8a7 a13 a15 a1a12a3a6a5a8a7 13 a7a2a1a4a3a13a5a14a7 a2 a18a16a15 a18 a71a18a17 a28a72 a17 a26 a26 a17 a91 a17 14 a13a2a1a4a3a13a5a14a7 a2 a18a16a15 a18 a74 a69 a28a72 a17 a26 a26 a69 a91 a17 15 a9 a2 a71 a0a14a19a21a20 a18a16a15 a18 a72 a17 a26 a26 a17 a24 16 Comparing with Equation 2_comma_ the a0 LSA factors_comma_ a19 a34 and a23 a1 correspond to the factors a39 a18 a74a75a28a72 a26 and a39 a18 a71 a28a72 a26 of the PLSA model and the mixing proportions of the latent classes in PLSA_comma_ a39 a18 a72 a26 _comma_ correspond to the singular values of the SVD in LSA.
E06-1014.txt,75,Note that we can not directly identify the matrix a7 a17 with a7a22a1a4a3a6a5a8a7 and a13 a17 with a13a22a1a4a3a6a5a8a7 since both a7a6a5 and a13a9a5 contain negative values and are not probability distributions.
E06-1014.txt,76,However_comma_ using equations 3 and 4_comma_ we can attach a probabilistic interpretation to LSA_comma_ and then relate a7a23a1a12a3a6a5a8a7 and a13a2a1a12a3a6a5a8a7 with the corresponding LSA matrices.
E06-1014.txt,77,We now outline this relation.
E06-1014.txt,78,Equation 4 represents the probability of occurrence of term a29a2a1 in the different documents conditioned on the SVD right eigenvectors.
E06-1014.txt,79,The a3 a36 a0a25a24a27a26 element in equation 15 represent the probability of term a74 a69 conditioned on the latent class a72 a17 .
E06-1014.txt,80,As in the analysis above_comma_ we assume that the latent classes in the LSA model correspond to the latent classes of the PLSA model.
E06-1014.txt,81,Making the simplifying assumption that the latent classes of the LSA model are conditionally independent on term a29 a1 _comma_ we can express the a39 a18 a29a2a1 a28a23 a22 a24a61a24a61a24 a23 a5 a26 as a39 a18 a29a53a1 a28 a23 a22 a24a61a24a61a24 a23 a5 a26 a2 a39 a18 a23 a22 a24a61a24a61a24 a23 a5 a28 a29a53a1 a26 a39 a18 a29a53a1 a26 a39 a18 a23 a22 a24a61a24a61a24 a23 a5 a26 a2 a39 a18 a29a53a1 a26 a39 a18 a23 a22 a28 a29a55a1 a26 a10a11a10a11a10 a39 a18 a23 a5 a28 a29a55a1 a26 a39 a18 a23 a22a31a26 a10a11a10a11a10 a39 a18 a23 a5 a26 a2 a39 a9a29a28 a17 a18 a29a53a1 a26 a17 a30 a1a12a31 a9 a39 a18 a29a53a1 a28 a23a33a32 a26 a24 17 And using Equation 4 we get a39 a9a29a28 a17 a18 a29a53a1 a26 a17 a30 a1a10a31 a9 a39 a18 a29a53a1 a28 a23 a32 a26a35a2a35a34 a17 a1a12a31 a9 a40 a41a63a62 a44 a46a67a37a36 a49 a50 a60 a18 a23 a22 a24a61a24a61a24 a23 a5 a26 18 Thus_comma_ other than a constant that is based on a39 a18 a29 a1 a26 and a60 a18 a23 a26 _comma_ we can relate each a39 a18 a29a2a1 a28a23a33a32 a26 to a corresponding a40a14a41a43a62a65a64a59a46a67 a44 a49 a50 .
E06-1014.txt,82,We make the simplifying assumption that a39 a18 a29a2a1 a26 is constant across terms and normalize the exponential term to a probability a39 a18 a29 a34 a28 a23 a1 a26a28a2 a40 a41a63a62 a64 a46a67a2a44 a49 a50 a88 a81 a17 a31 a9 a40 a41a63a62 a56 a46a67 a44 a49 a50 Relating the term a74 a17 in the PLSA model to the distribution of the LSA term over documents_comma_ a38 a17 _comma_ and relating the latent class a72 a69 in the PLSA model 108 to the LSA right eigenvector a0 a69 _comma_ we then estimate a39 a18 a74 a17 a28 a72 a69 a26 from a39 a18 a29 a34 a28 a23 a1 a26 _comma_ so that a39 a18 a74 a17 a28 a72 a69 a26 a4 a40 a41a63a62 a64a51a46a67 a44 a49 a50 a88 a81 a17 a31 a9 a40 a41a63a62 a56 a46a67 a44 a49 a50 19 Similarly_comma_ relating the document a71 a69 in the PLSA model to the distribution of LSA document over terms_comma_ a7 a69 _comma_ and using Equation 5 to show that a23 a1 is related to a19 a1 we get a39 a18 a71a18a17 a28 a72 a69 a26 a4 a40 a41a43a42 a64a59a46 a47 a44 a49 a50 a88 a81 a17 a31 a9 a40 a41a43a42 a56 a46 a47 a44 a49 a50 20 The singular values_comma_ a22 a17 in Equation 2_comma_ are by definition positive.
E06-1014.txt,83,Relating these values to the mixing proportions_comma_ a39 a18 a72 a17 a26 _comma_ we generalize the relation using a function a1 a18 a26 _comma_ where a1 a18 a26 is any nonnegative function over the range of all a22 a17 _comma_ and normalize so that the estimated a39 a18 a72 a17 a26 is a probability a39 a18 a72 a17 a26 a4 a1 a18 a22 a17 a26 a88 a17 a17 a31 a9 a1 a18 a22 a17 a26 21 We have experimented with different forms of a1 a18 a26 including the identity function and the logarithmic function.
E06-1014.txt,84,For our experiments_comma_ we used a1 a18 a22 a26 a2 a85a29a86a14a87 a18 a22 a26 .
E06-1014.txt,85,In our LSA initialized PLSA model_comma_ we initialize the PLSA model parameters using Equations 19 21.
E06-1014.txt,86,The EM algorithm is then used beginning with the E step as outlined in Equations 9 12.
E06-1014.txt,87,6 Results In this section we evaluate the performance of LSA initialized PLSA LSA PLSA .
E06-1014.txt,88,We compare the performance of LSA PLSA to LSA only and PLSA only_comma_ and also compare its use in combination with other models.
E06-1014.txt,89,We give results for a smaller information retrieval application and atext segmentation application_comma_ tasks where the reduced dimensional representation has been successfully used to improve performance over simpler word count models such as tf idf.
E06-1014.txt,90,6.1 System Description To test our approach for PLSA initialization we developed an LSA implementation based on the SVDLIBC package http tedlab.mit.edu a2 dr SVDLIBC for computing the singular values of sparse matrices.
E06-1014.txt,91,The PLSA implementation was based on an earlier implementation by Brants et al. 2002 .
E06-1014.txt,92,For each of the corpora_comma_ we tokenized the documents and used the LinguistX morphological analyzer to stem the terms.
E06-1014.txt,93,We used entropy weights Guo et al._comma_ 2003 to weight the terms in the document matrix.
E06-1014.txt,94,6.2 Information Retrieval We compared the performance of the LSA PLSA model against randomly initialized PLSA and against LSA for four different retrieval tasks.
E06-1014.txt,95,In these tasks_comma_ the retrieval is over a smaller corpus_comma_ on the order of a personal document collection.
E06-1014.txt,96,We used the following four standard document collections i MED 1033 document .
E06-2007.txt,1,paper describes an unsupervised knowledge lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus. It is based on the useofglobalcriterionfunctionsthatassess the quality of a clustering solution.
E06-2007.txt,3,The goal of word sense discrimination is to cluster the occurrences of a word in context based on its underlying meaning.
E06-2007.txt,4,This is often approached as a problem in unsupervised learning_comma_ where the only informationavailableisalargecorpusoftext e.g._comma_ Pedersen and Bruce_comma_ 1997 _comma_ Sch utze_comma_ 1998 _comma_ PurandareandPedersen_comma_ 2004 .
E06-2007.txt,5,Thesemethodsusually require that the number of clusters to be discovered k be specified ahead of time.
E06-2007.txt,6,However_comma_ inmostrealisticsettings_comma_ thevalueofkisunknown to the user.
E06-2007.txt,7,Word sense discrimination seeks to cluster N contexts_comma_ each of which contain a particular target word_comma_ into k clusters_comma_ where we would like the value of k to be automatically selected.
E06-2007.txt,8,Each context consists of approximately a paragraph of surrounding text_comma_ where the word to be discriminated the target word is found approximately in the middle of the context.
E06-2007.txt,9,We present a methodology that automatically selects an appropriate value for k.
E06-2007.txt,10,Our strategy is to perform clustering for successive values of k_comma_ and evaluate the resulting solutions with a criterion function.
E06-2007.txt,11,We select the value of k that is immediately prior to the point at which clustering does not improve significantly.
E06-2007.txt,12,Clustering methods are typically either partitional or agglomerative.
E06-2007.txt,13,The main difference is that agglomerative methods start with 1 or N clusters and then iteratively arrive at a pre specified number k of clusters_comma_ while partitional methods start by randomly dividing the contexts into k clusters and then iteratively rearranging the members of the k clusters until the selected criterion function is maximized.
E06-2007.txt,14,In this work we have used Kmeans clustering_comma_ which is a partitional method_comma_ and the H2 criterion function_comma_ which is the ratio of within cluster similarity to between cluster similarity.
E06-2007.txt,15,However_comma_ our approach can be used with any clustering algorithm and global criterion function_comma_ meaning that the criterion function should arriveatasinglevaluethatassessesthequalityofthe clustering for each value of k under consideration.
E06-2007.txt,16,2 Methodology In word sense discrimination_comma_ the number of contexts N to cluster is usually very large_comma_ and considering all possible values of k from 1...N would be inefficient.
E06-2007.txt,17,As the value of k increases_comma_ the criterion function will reach a plateau_comma_ indicating that dividing the contexts into more and more clusters does not improve the quality of the solution.
E06-2007.txt,18,Thus_comma_ we identify an upper bound to k that we refer to as deltaK by finding the point at which the criterion function only changes to a small degree as k increases.
E06-2007.txt,19,According to the H2 criterion function_comma_ the higher its ratio of within cluster similarity to between cluster similarity_comma_ the better the clustering.
E06-2007.txt,20,A large value indicates that the clusters have high internal similarity_comma_ and are clearly separated from each other.
E06-2007.txt,21,Intuitively then_comma_ one solution to selecting k might be to examine the trend of H2 scores_comma_ and look for the smallest k that results in a nearly maximum H2 value.
E06-2007.txt,22,However_comma_ a graph of H2 values for a clustering 111 ofthe4senseverbserveasshowninFigure1 top reveals the difficulties of such an approach.
E06-2007.txt,23,There is a gradual curve in this graph and the maximum value plateau is not reached until k values greater than 100.
E06-2007.txt,24,We have developed three methods that take as input the H2 values generated from 1...deltaK and automatically determine the right value of k_comma_ based on finding when the changes in H2 as k increases are no longer significant.
E06-2007.txt,25,2.1 PK1 The PK1 measure is based on Mojena_comma_ 1977 _comma_ which finds clustering solutions for all values of k from 1..N_comma_ and then determines the mean and standard deviation of the criterion function.
E06-2007.txt,26,Then_comma_ a score is computed for each value of k by subtracting the mean from the criterion function_comma_ and dividing by the standard deviation.
E06-2007.txt,27,We adapt this technique by using the H2 criterion function_comma_ and limit k from 1...deltaK PK1 k H2 k mean H2 1...deltaK std H2 1...deltaK 1 To select a value of k_comma_ a threshold must be set.
E06-2007.txt,28,Then_comma_ as soon as PK1 k exceeds this threshold_comma_ k 1 is selected as the appropriate number of clusters.
E06-2007.txt,29,We have considered setting this threshold using the normal distribution based on interpreting PK1 as a z score_comma_ although Mojena makes it clear that he views this method as an operational rule thatisnotbasedonanydistributionalassumptions.
E06-2007.txt,30,He suggests values of 2.75 to 3.50_comma_ but also states they would need to be adjusted for different data sets.
E06-2007.txt,31,We have arrived at an empirically determined value of 0.70_comma_ which coincides with the point in the standard normal distribution where 75 of the probability mass is associated with values greater than this.
E06-2007.txt,32,We observe that the distribution of PK1 scores tends to change with different data sets_comma_ making it hard to apply a single threshold.
E06-2007.txt,33,The graph of the PK1 scores shown in Figure 1 illustrates the difficulty the slope of these scores is nearly linear_comma_ and as such the threshold as shown by the horizontal line is a somewhat arbitrary cutoff.
E06-2007.txt,34,2.2 PK2 PK2 is similar to Hartigan_comma_ 1975 _comma_ in that both take the ratio of a criterion function at k and k 1_comma_ 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0 50 100 150 200 H2 vs k a115 trianglea1142.0001.5001.0000.500 0.000 0.500 1.000 1.500 2 3 4 5 6 7 8 9 1011121314151617 PK1 vs k a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a50triangle 0.900 1.000 1.100 1.200 1.300 1.400 1.500 1.600 1.700 1.800 1.900 2 3 4 5 6 7 8 9 1011121314151617 PK2 vs ka114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a50 triangle 0.990 0.995 1.000 1.005 1.010 1.015 1.020 1.025 1.030 1.035 1.040 2 3 4 5 6 7 8 9 1011121314151617 PK3 vs k a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a114 a50 triangle Figure 1 Graphs of H2 top and PK 1 3 for serve Actual number of senses 4 shown as triangle all _comma_ predicted number as square PK1 3 _comma_ and deltaK 17 shown as dot H2 and upper limit of k PK1 3 .
E06-2007.txt,35,112 in order to assess the relative improvement when increasing the number of clusters.
E06-2007.txt,36,PK2 k H2 k H2 k 1 2 When this ratio approaches 1_comma_ the clustering has reached a plateau_comma_ and increasing k will have no benefit.
E06-2007.txt,37,If PK2 is greater than 1_comma_ then an additional cluster improves the solution and we should increase k.
E06-2007.txt,38,We compute the standard deviation of PK2 and use that to establish a boundary as to whatitmeanstobe closeenough to1toconsider that we have reached a plateau.
E06-2007.txt,39,Thus_comma_ PK2 will select k where PK2 k is the closest to but not less than 1 standarddeviation PK2 1...deltaK .
E06-2007.txt,40,The graph of PK2 in Figure 1 shows an elbow that is near the actual number of senses.
E06-2007.txt,41,The critical region defined by the standard deviation is shaded_comma_ and note that PK2 selected the value of k that was outside of but closest to that region.
E06-2007.txt,42,This is interpreted as being the last value of k that resulted in a significant improvement in clustering quality.
E06-2007.txt,43,Note that here PK2 predicts 3 senses square while in fact there are 4 actual senses triangle .
E06-2007.txt,44,It is significant that the graph of PK2 provides a clearer representation of the plateau than does that of H2.
E06-2007.txt,45,2.3 PK3 PK3 utilizes three k values_comma_ in an attempt to find a point at which the criterion function increases and then suddenly decreases.
E06-2007.txt,46,Thus_comma_ for a given value of k we compare its criterion function to the preceding and following value of k PK3 k 2 H2 k H2 k 1 H2 k 1 3 PK3 is close to 1 if the three H2 values form a line_comma_ meaning that they are either ascending_comma_ or they are on the plateau.
E06-2007.txt,47,However_comma_ our use of deltaKeliminatestheplateau_comma_ soinourcasevalues of 1 show that k is resulting in consistent improvements to clustering quality_comma_ and that we should continue.
E06-2007.txt,48,When PK3 rises significantly above 1_comma_ we know that k 1 is not climbing as quickly_comma_ and we have reached a point where additional clustering may not be helpful.
E06-2007.txt,49,To select k we chose the largest value of PK3 k that is closest to but still greater than the critical region defined by the standard deviation of PK3.
E06-2007.txt,50,This is the last point where a significant increase in H2 was observed.
E06-2007.txt,51,Note that the graph of PK3 in Figure 1 shows the value of PK3 rising and falling dramatically in thecritical region_comma_ suggestinga need foradditional points to make it less localized.
E06-2007.txt,52,PK3 is similar in spirit to Salvador and Chan_comma_ 2004 _comma_ which introduces the L measure.
E06-2007.txt,53,This tries to find the point of maximum curvature in the criterion function graph_comma_ by fitting a pair of lines to thecurve wheretheintersectionoftheselinesrepresents the selected k .
E06-2007.txt,54,3 Experimental Results Weconductedexperimentswithwordsthathave2_comma_ 3_comma_ 4_comma_ and 6 actual senses.
E06-2007.txt,55,We used three words that had been manually sense tagged_comma_ including the 3 sense adjective hard_comma_ the 4 sense verb serve_comma_ and the 6 sense noun line.
E06-2007.txt,56,We also created 19 name conflations where sets of 2_comma_ 3_comma_ 4_comma_ and 6 names of persons_comma_ places_comma_ or organizations that are included in the English GigaWord corpus and that are typically unambiguous are replaced with a single name to create pseudo or false ambiguities.
E06-2007.txt,57,For example_comma_ we replaced all mentions of Bill Clinton and Tony Blair with a single name that can refer to either of them.
E06-2007.txt,58,In general the names we used in these sets are fairly well known and occur hundreds or even thousands of times.
E06-2007.txt,59,We clustered each word or name using four different configurations of our clustering approach_comma_ in order to determine how consistent the selected value of k is in the face of changing feature sets and context representations.
E06-2007.txt,60,The four configurations are first order feature vectors made up of unigrams that occurred 5 or more times_comma_ with and without singular value decomposition_comma_ and then second order feature vectors based on bigrams that occurred 5 or more times and had a log likelihood score of 3.841 or greater_comma_ with and without singular value decomposition.
E06-2007.txt,61,Details on these approaches can be found in Purandare and Pedersen_comma_ 2004 .
E06-2007.txt,62,Thus_comma_ in total there are 22 words to be discriminated_comma_ 7 with 2 senses_comma_ 6 words with 3 senses_comma_ 6 with 4 senses_comma_ and 3 words with 6 senses.
E06-2007.txt,63,Four different configurations of clustering are run for each word_comma_ leading to a total of 88 experiments.
E06-2007.txt,64,The results are shown in Tables 1_comma_ 2_comma_ and 3.
E06-2007.txt,65,In these tables_comma_ the actual numbers of senses are in the columns_comma_ and the predicted number of senses are in the rows.
E06-2007.txt,66,We see that the predicted value of PK1 agreed 113 Table 1 k Predicted by PK1 vs Actual k 2 3 4 6 1 6 6 3 3 18 2 5 5 1 3 14 3 4 1 7 2 14 4 6 5 7 1 19 5 4 2 1 7 6 2 3 3 2 10 7 1 1 2 8 1 1 9 1 1 2 11 1 1 28 24 24 12 88 Table 2 k Predicted by PK2 vs Actual k 2 3 4 6 1 3 1 4 2 8 5 7 6 26 3 8 10 8 2 30 4 4 2 3 9 5 1 3 2 6 6 1 2 1 4 7 2 2 9 1 1 2 10 1 2 3 11 1 1 12 1 1 17 2 2 28 24 24 12 88 with the actual value in 15 cases_comma_ whereas PK3 agreed in 17 cases_comma_ and PK2 agreed in 22 cases.
E06-2007.txt,67,We observe that PK1 and PK3 also experienced considerable confusion_comma_ in that their predictions were in many cases several clusters off of the correct value.
E06-2007.txt,68,While PK2 made various mistakes_comma_ it was generally closer to the correct values_comma_ and had fewer spurious responses very large or very small predictions .
E06-2007.txt,69,We note that the distribution of PK2 s predictions were most like those of the actual senses.
E06-2007.txt,70,4 Conclusions This paper shows how to use clustering criterion functionsasameansofautomaticallyselectingthe number of senses k in an ambiguous word.
E06-2007.txt,71,We have found that PK2_comma_ a ratio of the criterion functions for the current and previous value of k_comma_ is Table 3 k Predicted by PK3 vs Actual k 2 3 4 6 1 3 4 1 1 9 2 13 9 12 4 38 3 4 3 4 4 15 4 2 2 1 1 6 5 2 1 1 1 5 6 1 2 3 6 7 1 1 1 3 9 1 1 10 1 1 11 2 2 12 1 1 13 1 1 28 24 24 12 88 most effective_comma_ although there are many opportunities for future improvements to these techniques.
E06-2007.txt,72,5 Acknowledgments This research is supported by a National Science Foundation Faculty Early CAREER Development Award 0092784 .
E06-2007.txt,73,All of the experiments in this paper were carried out with the SenseClusters package_comma_ which is freely available from the URL on the title page. .
E06-3005.txt,1,the current project_comma_ we aim at developing an approach for automatically answering why questions. We created a data collection for research_comma_ development and evaluation of a method for automatically answering why questions why QA The resulting collection comprises 395 why questions.
E06-3005.txt,2,For each question_comma_ the source document and one or two user formulated answers are available in the data set.
E06-3005.txt,3,The resulting data set is of importance for our research and it will contribute to and stimulate other research in the field of why QA.
E06-3005.txt,4,We developed a question analysis method for why questions_comma_ based on syntactic categorization and answer type determination.
E06-3005.txt,5,The quality of the output of this module is promising for future development of our method for why QA.
E06-3005.txt,7,Until now_comma_ research in the field of automatic question answering QA has focused on factoid closed class questions like who_comma_ what_comma_ where and when questions.
E06-3005.txt,8,Results reported for the QA track of the Text Retrieval Conference TREC show that these types of wh questions can be handled rather successfully Voorhees 2003 .
E06-3005.txt,9,In the current project_comma_ we aim at developing an approach for automatically answering whyquestions.
E06-3005.txt,10,So far_comma_ why questions have largely been ignored by researchers in the QA field.
E06-3005.txt,11,One reason for this is that the frequency of whyquestions in a QA context is lower than that of other questions like who and what questions Hovy et al._comma_ 2002a .
E06-3005.txt,12,However_comma_ although whyquestions are less frequent than some types of factoids who_comma_ what and where _comma_ their frequency is not negligible in a QA context_comma_ they comprise about 5 percent of all wh questions Hovy_comma_ 2001 Jijkoun_comma_ 2005 and they do have relevance in QA applications Maybury_comma_ 2002 .
E06-3005.txt,13,A second reason for ignoring why questions until now_comma_ is that it has been suggested that the techniques that have proven to be successful in QA for closed class questions are not suitable for questions that expect a procedural answer rather than a noun phrase Kupiec_comma_ 1999 .
E06-3005.txt,14,The current paper aims to find out whether the suggestion is true that factoid QA techniques are not suitable for whyQA.
E06-3005.txt,15,We want to investigate whether principled syntactic parsing can make QA for whyquestions feasible.
E06-3005.txt,16,In the present paper_comma_ we report on the work that has been carried out until now.
E06-3005.txt,17,More specifically_comma_ sections 2 and 3 describe the approach taken to data collection and question analysis and the results that were obtained.
E06-3005.txt,18,Then_comma_ in section 4_comma_ we discuss the plans and goals for the work that will be carried out in the remainder of the project.
E06-3005.txt,19,2 Data for why QA In research in the field of QA_comma_ data sources of questions and answers play an important role.
E06-3005.txt,20,Appropriate data collections are necessary for the development and evaluation of QA systems Voorhees_comma_ 2000 .
E06-3005.txt,21,While in the context of the QA track of TREC data collections in support of factoid questions have been created_comma_ so far_comma_ no resources have been created for why QA.
E06-3005.txt,22,For the purpose of the present research therefore_comma_ we have developed a data collection comprising a set of questions and corresponding answers.
E06-3005.txt,23,In doing so_comma_ we have extended the time tested procedures previously developed in the TREC context.
E06-3005.txt,24,In this section_comma_ we describe the requirements that a data set must meet to be appropriate for development and we discuss a number of existing sources of why questions.
E06-3005.txt,25,Then we describe the method employed for data collection 39 and the main characteristics of the resulting data set.
E06-3005.txt,26,The first requirement for an appropriate data set concerns the nature of the questions.
E06-3005.txt,27,In the context of the current research_comma_ a why question is defined as an interrogative sentence in which the interrogative adverb why or one of its synonyms occurs in near initial position.
E06-3005.txt,28,We consider the subset of why questions that could be posed in a QA context and for which the answer is known to be present in the related document set.
E06-3005.txt,29,This means that the data set should only comprise why questions for which the answer can be found in a fixed collection of documents.
E06-3005.txt,30,Secondly_comma_ the data set should not only contain questions_comma_ but also the corresponding answers and source documents.
E06-3005.txt,31,The answer to a why question is a clause or sentence or a small number of coherent sentences that answers the question without giving supplementary context.
E06-3005.txt,32,The answer is not literally present in the source document_comma_ but can be deduced from it.
E06-3005.txt,33,For example_comma_ a possible answer to the question Why are 4300 additional teachers required _comma_ based on the source snippet The school population is due to rise by 74_comma_000_comma_ which would require recruitment of an additional 4_comma_300 teachers_comma_ is Because the school population is due to rise by a further 74_comma_000.
E06-3005.txt,34,Finally_comma_ the size of the data set should be large enough to cover all relevant variation that occur in why questions in a QA context.
E06-3005.txt,35,There are a number of existing sources of why questions that we may consider for use in our research.
E06-3005.txt,36,However_comma_ for various reasons_comma_ none of these appear suitable.
E06-3005.txt,37,Why questions from corpora like the British National Corpus BNC_comma_ 2002 _comma_ in which questions typically occur in spoken dialogues_comma_ are not suitable because the answers are not structurally available with the questions_comma_ or they are not extractable from a document that has been linked to the question.
E06-3005.txt,38,The same holds for the data collected for the Webclopedia project Hovy et al._comma_ 2002a _comma_ in which neither the answers nor the source documents were included.
E06-3005.txt,39,One could also consider questions and answers from frequently asked questions FAQ pages_comma_ like the large data set collected by Valentin Jijkoun Jijkoun_comma_ 2005 .
E06-3005.txt,40,However_comma_ in FAQ lists_comma_ there is no clear distinction between the answer itself a clause that answers the question and the source document that contains the answer.
E06-3005.txt,41,The questions in the test collections from the TREC QA track do contain links to the possible answers and the corresponding source documents.
E06-3005.txt,42,However_comma_ these collections contain too few why questions to qualify as a data set that is appropriate for developing why QA.
E06-3005.txt,43,Given the lack of available data that match our requirements_comma_ a new data set for QA research into why questions had to be compiled.
E06-3005.txt,44,In order to meet the given requirements_comma_ it would be best to collect questions posed in an operational QA environment_comma_ like the compilers of the TRECQA test collections did they extracted factoid and definition questions from search logs donated by Microsoft and AOL TREC_comma_ 2003 .
E06-3005.txt,45,Since we do not have access to comparable sources_comma_ it was decided to revert to the procedure used in earlier TRECs_comma_ and imitate a QA environment in an elicitation experiment.
E06-3005.txt,46,We extended the conventional procedure by collecting user formulated answers in order to investigate the range of possible answers to each question.
E06-3005.txt,47,We also added paraphrases of collected questions in order to extend the syntactic and lexical variation in the data collection.
E06-3005.txt,48,In the elicitation experiment_comma_ ten native speakers of English were asked to read five texts from Reuters Textline Global News 1989 and five texts from The Guardian on CD ROM 1992 .
E06-3005.txt,49,The texts were around 500 words each.
E06-3005.txt,50,The experiment was conducted over the Internet_comma_ using a web form and some CGI scripts.
E06-3005.txt,51,In order to have good control over the experiment_comma_ we registered all participants and gave them a code for logging in on the web site.
E06-3005.txt,52,Every time a participant logged in_comma_ the first upcoming text that he or she did not yet finish was presented.
E06-3005.txt,53,The participant was asked to formulate one to six why questions for this text_comma_ and to formulate an answer to each of these questions.
E06-3005.txt,54,The participants were explicitly told that it was essential that the answers to their questions could be found in the text.
E06-3005.txt,55,After submitting the form_comma_ the participant was presented the questions posed by one of the other participants and he or she was asked to formulate an answer to these questions too.
E06-3005.txt,56,The collected data was saved in text format_comma_ grouped per participant and per source document_comma_ so that the source information is available for each question.
E06-3005.txt,57,The answers have been linked to the questions.
E06-3005.txt,58,In this experiment_comma_ 395 questions and 769 corresponding answers were collected.
E06-3005.txt,59,The number of answers would have been twice the 40 number of questions if all participants would have been able to answer all questions that were posed by another participant.
E06-3005.txt,60,However_comma_ for 21 questions 5.3 _comma_ the second participant was not able to answer the first participant s question.
E06-3005.txt,61,Note that not every question in the elicitation data set has a unique topic1 on average_comma_ 38 questions were formulated per text_comma_ covering around twenty topics per text.
E06-3005.txt,62,The collected questions have been formulated by people who had constant access to the source text.
E06-3005.txt,63,As a result of that_comma_ the chosen formulations often resemble the original text_comma_ both in the use of vocabulary and sentence structure.
E06-3005.txt,64,In order to expand the dataset_comma_ a second elicitation experiment was set up_comma_ in which five participants from the first experiment were asked to paraphrase some of the original why questions.
E06-3005.txt,65,The 166 unique questions were randomly selected from the original data set.
E06-3005.txt,66,The participants formulated 211 paraphrases in total for these questions.
E06-3005.txt,67,This means that some questions have more than one paraphrase.
E06-3005.txt,68,The paraphrases were saved in a text file that includes the corresponding original questions and the corresponding source documents.
E06-3005.txt,69,We studied the types of variation that occur among questions covering the same topic.
E06-3005.txt,70,First_comma_ we collected the types of variation that occur in the original data set and then we compared these to the variation types that occur in the set of paraphrases.
E06-3005.txt,71,In the original data set_comma_ the following types of variation occur between different questions on the same topic Lexical variation_comma_ e.g. for the second year running vs. again Verb tense variation_comma_ e.g. have risen vs. have been rising Optional constituents variation_comma_ e.g. class sizes vs. class sizes in England and Wales Sentence structure variation_comma_ e.g. would require recruitment vs. need to be recruited In the set of paraphrases_comma_ the same types of variation occur_comma_ but as expected the differences between the paraphrases and the source 1 The topic of a why question is the proposition that is questioned.
E06-3005.txt,72,A why question has the form WHY P _comma_ in which P is the topic. sentences are slightly bigger than the differences between the original questions and the source sentences.
E06-3005.txt,73,We measured the lexical overlap between the questions and the source texts as the number of content words that are in both the question and the source text.
E06-3005.txt,74,The average relative lexical overlap the number of overlapping words divided by the total number of words in the question between original questions and source text is 0.35 the average relative lexical overlap between paraphrases and source text is 0.31.
E06-3005.txt,75,The size of the resulting collection 395 original questions_comma_ 769 answers_comma_ and 211 paraphrases of questions is large enough to initiate serious research into the development of why QA.
E06-3005.txt,76,Our collection meets the requirements that were formulated with regard to the nature of the questions and the presence of the answers and source documents for every question.
E06-3005.txt,77,3 Question analysis for why QA The goal of question analysis is to create a representation of the user s information need.
E06-3005.txt,78,The result of question analysis is a query that contains all information about the answer that can be extracted from the question.
E06-3005.txt,79,So far_comma_ no question analysis procedures have been created for why QA specifically.
E06-3005.txt,80,Therefore_comma_ we have developed an approach for proper analysis of why questions.
E06-3005.txt,81,Our approach is based on existing methods of analysis of factoid questions.
E06-3005.txt,82,This will allow us to verify whether methods used in handling factoid questions are suitable for use with procedural questions.
E06-3005.txt,83,In this section_comma_ we describe the components of successful methods for the analysis of factoid questions.
E06-3005.txt,84,Then we present the method that we used for the analysis of why questions and indicate the quality of our method.
E06-3005.txt,85,The first and most simple component in current methods for question analysis is keyword extraction.
E06-3005.txt,86,Lexical items in the question give information on the topic of the user s information need.
E06-3005.txt,87,In keyword selection_comma_ several different approaches may be followed.
E06-3005.txt,88,Moldovan et al. 2000 _comma_ for instance_comma_ select as keywords all named entities that were recognized as proper nouns.
E06-3005.txt,89,In almost all approaches to keyword extraction_comma_ syntax plays a role.
E06-3005.txt,90,Shallow parsing is used for extracting noun phrases_comma_ which are considered to be relevant key phrases in the retrieval step.
E06-3005.txt,91,Based on the query s keywords_comma_ 41 one or more documents or paragraphs can be retrieved that may possibly contain the answer.
E06-3005.txt,92,A second_comma_ very important_comma_ component in question analysis is determination of the question s semantic answer type.
E06-3005.txt,93,The answer type of a question defines the type of answer that the system should look for.
E06-3005.txt,94,Often cited work on question analysis has been done by Moldovan et al. 1999_comma_ 2000 _comma_ Hovy et al. 2001 _comma_ and Ferret et al. 2002 .
E06-3005.txt,95,They all describe question analysis methods that classify questions with respect to their answer type.
E06-3005.txt,96,In their systems for factoidQA_comma_ the answer type is generally deduced directly from the question word who_comma_ when_comma_ where_comma_ etc. who leads to the answer type person where leads to the answer type place_comma_ etc. This information helps the system in the search for candidate answers to the question.
E06-3005.txt,97,Hovy et al. find that_comma_ of the question analysis components used by their system_comma_ the determination of the semantic answer type makes by far the largest contribution to the performance of the entire QA system.
E06-3005.txt,98,For determining the answer type_comma_ syntactic analysis may play a role.
E06-3005.txt,99,When implementing a syntactic analysis module in a working QA system_comma_ the analysis has to be performed fully automatically.
E06-3005.txt,100,This may lead to concessions with regard to either the degree of detail or the quality of the analysis.
E06-3005.txt,101,Ferret et al. implement a syntactic analysis component based on shallow parsing.
E06-3005.txt,102,Their syntactic analysis module yields a syntactic category for each input question.
E06-3005.txt,103,In their system_comma_ a syntactic category is a specific syntactic pattern_comma_ such as WhatDoNP e.g. What does a defibrillator do or WhenBePNborn e.g. When was Rosa Park born .
E06-3005.txt,104,They define 80 syntactic categories like these.
E06-3005.txt,105,Each input question is parsed by a shallow parser and hand written rules are applied for determining the syntactic category.
E06-3005.txt,106,Ferret et al. find that the syntactic pattern helps in determining the semantic answer type e.g. company_comma_ person_comma_ date .
E06-3005.txt,107,They unfortunately do not describe how they created the mapping between syntactic categories and answer types.
E06-3005.txt,108,As explained above_comma_ determination of the semantic answer type is the most important task of existing question analysis methods.
E06-3005.txt,109,Therefore_comma_ the goal of our question analysis method is to predict the answer type of why questions.
E06-3005.txt,110,In the work of Moldovan et al. 2000 _comma_ all why questions share the single answer type reason.
E06-3005.txt,111,However_comma_ we believe that it is necessary to split this answer type into sub types_comma_ because a more specific answer type helps the system select potential answer sentences or paragraphs.
E06-3005.txt,112,The idea behind this is that every sub type has its own lexical and syntactic cues in a source text.
E06-3005.txt,113,Based on the classification of adverbial clauses by Quirk 1985 15.45 _comma_ we distinguish the following sub types of reason cause_comma_ motivation_comma_ circumstance which combines reason with conditionality _comma_ and purpose.
E06-3005.txt,114,Below_comma_ an example of each of these answer types is given.
E06-3005.txt,115,Cause The flowers got dry because it hadn t rained in a month.
E06-3005.txt,116,Motivation I water the flowers because I don t like to see them dry.
E06-3005.txt,117,Circumstance Seeing that it is only three_comma_ we should be able to finish this today.
E06-3005.txt,118,Purpose People have eyebrows to prevent sweat running into their eyes.
E06-3005.txt,119,The why questions that correspond to the reason clauses above are respectively Why did the flowers get dry _comma_ Why do you water the flowers _comma_ Why should we be able to finish this today _comma_ and Why do people have eyebrows .
E06-3005.txt,120,It is not always possible to assign one of the four answer subtypes to a why question.
E06-3005.txt,121,We will come back to this later.
E06-3005.txt,122,Often_comma_ the question gives information on the expected answer type.
E06-3005.txt,123,For example_comma_ compare the two questions below Why did McDonald s write Mr.
E06-3005.txt,124,Bocuse a letter
E06-3005.txt,125,Why have class sizes risen
E06-3005.txt,126,Someone asking the former question expects as an answer McDonald s motivation for writing a letter_comma_ whereas someone asking the latter question expects the cause for rising class sizes as answer.
E06-3005.txt,127,The corresponding answer paragraphs do indeed contain the equivalent answer sub types McDonald s has acknowledged that a serious mistake was made.
E06-3005.txt,128,We have written to apologise and we hope to reach 42 a settlement with Mr.
E06-3005.txt,129,Bocuse this week_comma_ said Marie Pierre Lahaye_comma_ a spokeswoman for McDonald s France_comma_ which operates 193 restaurants.
E06-3005.txt,130,Class sizes in schools in England and Wales have risen for the second year running_comma_ according to figures released today by the Council of Local Education Authorities.
E06-3005.txt,131,The figures indicate that although the number of pupils in schools has risen in the last year by more than 46_comma_000_comma_ the number of teachers fell by 3_comma_600.
E06-3005.txt,132,We aim at creating a question analysis module that is able to predict the expected answer type of an input question.
E06-3005.txt,133,In the analysis of factoid questions_comma_ the question word often gives the needed information on the expected answer type.
E06-3005.txt,134,In case of why_comma_ the question word does not give information on the answer type since all whyquestions have why as question word.
E06-3005.txt,135,This means that other information from the question is needed for determining the answer sub type.
E06-3005.txt,136,We decided to use Ferret s approach_comma_ in which syntactic categorization helps in determining the expected answer type.
E06-3005.txt,137,In our question analysis module_comma_ the TOSCA TOols for Syntactic Corpus Analysis system Oostdijk_comma_ 1996 is explored for syntactic analysis.
E06-3005.txt,138,TOSCA s syntactic parser takes a sequence of unambiguously tagged words and assigns function and category information to all constituents in the sentence.
E06-3005.txt,139,The parser yields one or more possible output trees for almost all input sentences.
E06-3005.txt,140,For the purpose of evaluating the maximum contribution to a classification method that can be obtained from a principled syntactic analysis_comma_ the most plausible parse tree from the parser s output is selected manually.
E06-3005.txt,141,For the next step of question analysis_comma_ we created a set of hand written rules_comma_ which are applied to the parse tree in order to choose the question s syntactic category.
E06-3005.txt,142,We defined six syntactic categories for this purpose Action questions_comma_ e.g.
E06-3005.txt,143,Why did McDonald s write Mr.
E06-3005.txt,144,Bocuse a letter
E06-3005.txt,145,Process questions_comma_ e.g.
E06-3005.txt,146,Why has Dixville grown famous since 1964
E06-3005.txt,147,Intensive complementation questions_comma_ e.g.
E06-3005.txt,148,Why is Microsoft Windows a success
E06-3005.txt,149,Monotransitive have questions_comma_ e.g.
E06-3005.txt,150,Why did compilers of the OED have an easier time
E06-3005.txt,151,Existential there questions_comma_ e.g.
E06-3005.txt,152,Why is there a debate about class sizes
E06-3005.txt,153,Declarative layer questions_comma_ e.g.
E06-3005.txt,154,Why does McDonald s spokeswoman think the mistake was made
E06-3005.txt,155,The choice for these categories is based the information that is available from the parser_comma_ and the information that is needed for determining the answer type.
E06-3005.txt,156,For some categories_comma_ the question analysis module only needs fairly simple cues for choosing a category.
E06-3005.txt,157,For example_comma_ a main verb with the feature intens leads to the category intensive complementation question and the presence of the word there with the syntactic category EXT leads to the category existential there question .
E06-3005.txt,158,For deciding on declarative layer questions_comma_ action questions and process questions_comma_ complementary lexical semantic information is needed.
E06-3005.txt,159,In order to decide whether the question contains a declarative layer_comma_ the module checks whether the main verb is in a list that corresponds to the union of the verb classes say and declare from Verbnet Kipper et al._comma_ 2000 _comma_ and whether it has a clausal object.
E06-3005.txt,160,The distinction between action and process questions is made by looking up the main verb in a list of process verbs.
E06-3005.txt,161,This list contains the 529 verbs from the causative inchoative alternation class verbs like melt and grow from the Levin verb index Levin_comma_ 1993 in an intransitive context_comma_ these verbs are process verbs.
E06-3005.txt,162,We have not yet developed an approach for passive questions.
E06-3005.txt,163,Based on the syntactic category_comma_ the question analysis module tries to determine the answer type.
E06-3005.txt,164,Some of the syntactic categories lead directly to an answer type.
E06-3005.txt,165,All process questions with non agentive subjects get the expected answer type cause.
E06-3005.txt,166,All action questions with agentive subjects get the answer type motivation.
E06-3005.txt,167,We extracted information on agentive and nonagentive nouns from WordNet all nouns that are in the lexicographer file noun.person were selected as agentive.
E06-3005.txt,168,Other syntactic categories need further analysis.
E06-3005.txt,169,Questions with a declarative layer_comma_ for example_comma_ 43 are ambiguous.
E06-3005.txt,170,The question Why did they say that migration occurs can be interpreted in two ways Why did they say it or Why does migration occur .
E06-3005.txt,171,Before deciding on the answer type_comma_ our question analysis module tries to find out which of these two questions is supposed to be answered.
E06-3005.txt,172,In other words the module decides which of the clauses has the question focus.
E06-3005.txt,173,This decision is made on the basis of the semantics of the declarative verb.
E06-3005.txt,174,If the declarative is a factive verb a verb that presupposes the truth of its complements like know_comma_ the module decides that the main clause has the focus.
E06-3005.txt,175,The question consequently gets the answer type motivation.
E06-3005.txt,176,In case of a non factive verb like think_comma_ the focus is expected to be on the subordinate clause.
E06-3005.txt,177,In order to predict the answer type of the question_comma_ the subordinate clause is then treated the same way as the complete question was.
E06-3005.txt,178,For example_comma_ consider the question Why do the school councils believe that class sizes will grow even more .
E06-3005.txt,179,Since the declarative believe is non factive_comma_ the question analysis module determines the answer type for the subordinate clause class sizes will grow even more _comma_ which is cause_comma_ and assigns it to the question as a whole.
E06-3005.txt,180,Special attention is also paid to questions with a modal auxiliary.
E06-3005.txt,181,Modal auxiliaries like can and should_comma_ have an influence on the answer type.
E06-3005.txt,182,For example_comma_ consider the questions below_comma_ in which the only difference is the presence or absence of the modal auxiliary can Why did McDonalds not use actors to portray chefs in amusing situations
E06-3005.txt,183,Why can McDonalds not use actors to portray chefs in amusing situations
E06-3005.txt,184,The former question expects a motivation as answer_comma_ whereas the latter question expects a cause.
E06-3005.txt,185,We implemented this difference in our question analysis module CAN can_comma_ could and HAVE TO have to_comma_ has to_comma_ had to lead to the answer type cause.
E06-3005.txt,186,Furthermore_comma_ the modal auxiliary SHALL shall_comma_ should changes the expected answer type to motivation.
E06-3005.txt,187,When choosing an answer type_comma_ our question analysis module follows a conservative policy in case of doubt_comma_ no answer type is assigned.
E06-3005.txt,188,We did not yet perform a complete evaluation of our question analysis module.
E06-3005.txt,189,For proper evaluation of the module_comma_ we need a reference set of questions and answers that is different from the data set that we collected for development of our system.
E06-3005.txt,190,Moreover_comma_ for evaluating the relevance of our question analysis module for answer retrieval_comma_ further development of our approach is needed.
E06-3005.txt,191,However_comma_ to have a general idea of the performance of our method for answer type determination_comma_ we compared the output of the module to manual classifications.
E06-3005.txt,192,We performed these reference classifications ourselves.
E06-3005.txt,193,First_comma_ we manually classified 130 whyquestions from our development set with respect to their syntactic category.
E06-3005.txt,194,Evaluation of the syntactic categorization is straightforward 95 percent of why questions got assigned the correct syntactic category using perfect parse trees.
E06-3005.txt,195,The erroneous classifications were due to differences in the definitions of the specific verb types.
E06-3005.txt,196,For example_comma_ argue is not in the list of declarative verbs_comma_ as a result of which a question with argue as main verb is classified as action question instead of declarative layer question.
E06-3005.txt,197,Also_comma_ die and cause are not in the list of process verbs_comma_ so questions with either of these verbs as main verb are labeled as action questions instead of process questions.
E06-3005.txt,198,Secondly_comma_ we performed a manual classification into the four answer sub types cause_comma_ motivation_comma_ circumstance and purpose .
E06-3005.txt,199,For this classification_comma_ we used the same set of 130 questions as we did for the syntactic categorization_comma_ combined with the corresponding answers.
E06-3005.txt,200,Again_comma_ we performed this classification ourselves.
E06-3005.txt,201,During the manual classification_comma_ we assigned the answer type cause to 23.3 percent of the questions and motivation to 40.3 percent.
E06-3005.txt,202,We were not able to assign an answer sub type to the remaining pairs 36.4 percent .
E06-3005.txt,203,These questions are in the broader class reason and not in one of the specific sub classes None of the questionanswer pairs was classified as circumstance or purpose.
E06-3005.txt,204,Descriptions of purpose are very rare in news texts because of their generic character e.g. People have eyebrows to prevent sweat running into their eyes .
E06-3005.txt,205,The answer type circumstance_comma_ defined by Quirk cf. section 15.45 as a combination of reason with conditionality_comma_ is also rare as well as difficult to recognize.
E06-3005.txt,206,For evaluation of the question analysis module_comma_ we mainly considered the questions that 44 did get assigned a sub type motivation or cause in the manual classification.
E06-3005.txt,207,Our question analysis module succeeded in assigning the correct answer sub type to 62.2 percent of these questions_comma_ the wrong sub type to 2.4 percent_comma_ and no sub type to the other 35.4 percent.
E06-3005.txt,208,The set of questions that did not get a sub type from our question analysis module can be divided in four groups a Action questions for which the subject was incorrectly not marked as agentive mostly because it was an agentive organization like McDonald s_comma_ or a proper noun that was not in WordNet s list of nouns denoting persons_comma_ like Henk Draijen b questions with an action verb as main verb but a non agentive subject e.g. Why will restrictions on abortion damage women s health  c passive questions_comma_ for which we have not yet developed an approach e.g. Why was the Supreme Court reopened  d Monotransitive have questions.
E06-3005.txt,209,This category contains too few questions to formulate a general rule.
E06-3005.txt,210,Group a _comma_ which is by far the largest of these four covering half of the questions without subtype _comma_ can be reduced by expanding the list of agentive nouns_comma_ especially with names of organizations.
E06-3005.txt,211,For groups c and d _comma_ general rules may possibly be created in a later stage.
E06-3005.txt,212,With this knowledge_comma_ we are confident that we can reduce the number of questions without subtype in the output of our question analysis module.
E06-3005.txt,213,These first results predict that it is possible to reach a relatively high precision in answer type determination.
E06-3005.txt,214,Only 2 percent of questions got assigned a wrong sub type. A high precision makes the question analysis output useful and reliable in the next steps of the question answering process.
E06-3005.txt,215,On the other hand_comma_ it seems difficult to get a high recall.
E06-3005.txt,216,In this test_comma_ only 62.2 percent of the questions that were assigned an answer type in the reference set_comma_ was assigned an answer type by the system this is 39.6 percent of the total.
E06-3005.txt,217,4 Conclusions and further research We created a data collection for research into why questions and for development of a method for why QA.
E06-3005.txt,218,The collection comprises a sufficient amount of why questions.
E06-3005.txt,219,For each question_comma_ the source document and one or two user formulated answers are available in the data set.
E06-3005.txt,220,The resulting data set is of importance for our research as well as other research in the field of why QA.
E06-3005.txt,221,We developed a question analysis method for why questions_comma_ based on syntactic categorization and answer type determination.
E06-3005.txt,222,In depth evaluation of this module will be performed in a later stage_comma_ when the other parts of our QA approach have been developed_comma_ and a test set has been collected.
E06-3005.txt,223,We believe that the first test results_comma_ which show a high precision and low recall_comma_ are promising for future development of our method for why QA.
E06-3005.txt,224,We think that_comma_ just as for factoid QA_comma_ answer type determination can play an important role in question analysis for why questions.
E06-3005.txt,225,Therefore_comma_ Kupiec suggestion that conventional question analysis techniques are not suitable for why QA can be made more precise by saying that these methods may be useful for a potentially small subset of why questions.
E06-3005.txt,226,The issue of recall_comma_ both for human and machine processing_comma_ needs further analysis.
E06-3005.txt,227,In the near future_comma_ our work will focus on development of the next part of our approach for why QA.
E06-3005.txt,228,Until now we have focused on the first of four sub tasks in QA_comma_ viz.
E06-3005.txt,229,1 question analysis 2 retrieval of candidate paragraphs 3 paragraph analysis and selection and 4 answer generation.
E06-3005.txt,230,Of the remaining three sub tasks_comma_ we will focus on paragraph analysis 3 .
E06-3005.txt,231,In order to clarify the relevance of the paragraph analysis step_comma_ let us briefly discuss the QA processes that follows question analysis.
E06-3005.txt,232,The retrieval module_comma_ which comes directly after the question analysis module_comma_ uses the output of the question analysis module for finding candidate answer paragraphs or documents .
E06-3005.txt,233,Paragraph retrieval can be straightforward in existing approaches for factoid QA_comma_ candidate paragraphs are selected based on keyword matching only.
E06-3005.txt,234,For the current research_comma_ we do not aim at creating our own paragraph selection technique.
E06-3005.txt,235,More interesting than paragraph retrieval is the next step of QA paragraph analysis.
E06-3005.txt,236,The paragraph analysis module tries to determine whether the candidate paragraphs contain potential answers.
E06-3005.txt,237,In case of who questions_comma_ noun phrases denoting persons are potential 45 answers in case of why questions_comma_ reasons are potential answers.
E06-3005.txt,238,In the paragraph analysis stage_comma_ our answer sub types come into play.
E06-3005.txt,239,The question analysis module determines the answer type for the input question_comma_ which is motivation_comma_ cause_comma_ purpose_comma_ or circumstance.
E06-3005.txt,240,The paragraph analysis module uses this information for searching candidate answers in a paragraph.
E06-3005.txt,241,As has been said before_comma_ the procedure for assigning the correct sub type needs further investigation in order to increase the coverage and the contribution that answer sub type classification can make to the performance of why question answering.
E06-3005.txt,242,Once the system has extracted potential answers from one or more paragraphs with the same topic as the question_comma_ the eventual answer has to be delimited and reformulated if necessary. .
E06-1039.txt,1,present and compare two approaches to the task of summarizing evaluative arguments. Thefirstisasentence extractionbased approach while the second is a language generation based approach.
E06-1039.txt,2,We evaluate these approaches in a user study and find that they quantitatively perform equally well.
E06-1039.txt,3,Qualitatively_comma_ however_comma_ we findthattheyperform wellfor different but complementary reasons.
E06-1039.txt,4,We conclude that an effective method for summarizing evaluative arguments must effectively synthesize the two approaches.
E06-1039.txt,6,Many organizations are faced with the challenge ofsummarizing large corpora oftextdata.
E06-1039.txt,7,Oneimportant application is evaluative text_comma_ i.e. any document expressing an evaluation of an entity as either positive or negative.
E06-1039.txt,8,For example_comma_ many websites collect large quantities of online customer reviews of consumer electronics.
E06-1039.txt,9,Summaries of this literature could be of great strategic value to product designers_comma_ planners and manufacturers.
E06-1039.txt,10,There are other equally important commercial applications_comma_ suchasthesummarization oftravel logs_comma_ and non commercial applications_comma_ such as the summarization of candidate reviews.
E06-1039.txt,11,The general problem we consider in this paper is how to effectively summarize a large corpora of evaluative text about a single entity e.g._comma_ a product .
E06-1039.txt,12,In contrast_comma_ most previous work on multidocument summarization has focused on factual text e.g._comma_ news McKeown et al._comma_ 2002 _comma_ biographies Zhou et al._comma_ 2004 .
E06-1039.txt,13,For factual documents_comma_ the goal of a summarizer is to select the most important facts and present them in a sensible ordering while avoiding repetition.
E06-1039.txt,14,Previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in a domain independent way.
E06-1039.txt,15,Notice however that when the source documents are assumed to contain inconsistent information e.g._comma_ conflicting reports of a natural disaster White et al._comma_ 2002 _comma_ a different approach is needed.
E06-1039.txt,16,The summarizer needs first to extract the information from the documents_comma_ then process such information to identify overlaps and inconsistencies between the different sources and finally produce a summary that points out and explain those inconsistencies.
E06-1039.txt,17,A corpus of evaluative text typically contains a large number of possibly inconsistent facts i.e. opinions _comma_ as opinions on the same entity feature may be uniform or varied.
E06-1039.txt,18,Thus_comma_ summarizing a corpus of evaluative text is much more similar to summarizing conflicting reports than a consistent set of factual documents.
E06-1039.txt,19,When there are diverse opinions on the same issue_comma_ the different perspectives need to be included in the summary.
E06-1039.txt,20,Based on this observation_comma_ we argue that any strategy to effectively summarize evaluative text about a single entity should rely on a preliminary phase of information extraction from the target corpus.
E06-1039.txt,21,In particular_comma_ the summarizer should at least know for each document what features of the entity were evaluated_comma_ the polarity of the evaluations and their strengths.
E06-1039.txt,22,Inthis paper_comma_ weexplore this hypothesis by considering two alternative approaches.
E06-1039.txt,23,First_comma_ we developed a sentence extraction based summarizer that uses the information extracted from the corpus to select and rank sentences from the corpus.
E06-1039.txt,24,We implemented this system_comma_ called MEAD _comma_ by 305 adapting MEAD Radev et al._comma_ 2003 _comma_ an opensource frameworkformulti document summarization.
E06-1039.txt,25,Second_comma_ we developed a summarizer that produces summaries primarily by generating language from the information extracted from the corpus.
E06-1039.txt,26,We implemented this system_comma_ called the Summarizer of Evaluative Arguments SEA _comma_ by adapting the Generator of Evaluative Arguments GEA Carenini and Moore_comma_ expected 2006 a framework for generating user tailored evaluative arguments.
E06-1039.txt,27,Wehave performed anempirical formative evaluation of MEAD and SEA in a user study.
E06-1039.txt,28,In this evaluation_comma_ we also tested the effectiveness of human generated summaries HGS as a topline and of summaries generated by MEAD without access to the extracted information as a baseline.
E06-1039.txt,29,The results indicate that SEA and MEAD quantitatively perform equally well above MEAD and below HGS.
E06-1039.txt,30,Qualitatively_comma_ we find that they perform well for different but complementary reasons.
E06-1039.txt,31,While SEA appears to provide a more general overview of the source text_comma_ MEAD seems to provide a more varied language and detail about customer opinions.
E06-1039.txt,32,2 Information Extraction from Evaluative Text 2.1 Feature Extraction Knowledge extraction from evaluative text about a single entity is typically decomposed into three distinct phases the determination of features of the entity evaluated in the text_comma_ the strength of each evaluation_comma_ and the polarity of each evaluation.
E06-1039.txt,33,For instance_comma_ the information extracted from the sentence The menus are very easy to navigate but the user preference dialog is somewhat difficult to locate. should be that the menus and the user preference dialog features are evaluated_comma_ and that the menus receive a very positive evaluation while the user preference dialog is evaluated rather negatively.
E06-1039.txt,34,For these tasks_comma_ we adopt the approach described indetail in Carenini et al._comma_ 2005 .
E06-1039.txt,35,This approach relies on the work of Hu and Liu_comma_ 2004a for the tasks of strength and polarity determination.
E06-1039.txt,36,For the task of feature extraction_comma_ it enhances earlier work Hu and Liu_comma_ 2004c by mapping the extracted features into a hierarchy of features which describes the entity of interest.
E06-1039.txt,37,Theresulting mapping reduces redundancy and provides conceptual organization of the extracted features.
E06-1039.txt,38,Camera Lens Digital Zoom Optical Zoom ... Editing Viewing Viewfinder ... Flash ... Image Image Type TIFF JPEG ... Resolution Effective Pixels Aspect Ratio ... Figure 1 Partial view of UDF taxonomies for a digital camera.
E06-1039.txt,39,Before continuing_comma_ we shall describe the terminology we use when discussing the extracted knowledge.
E06-1039.txt,40,The features evaluated in a corpus of reviews and extracted by following Hu and Liu s approach are called Crude Features.
E06-1039.txt,41,CF a0 a1 cfj a2 j a0 1a3a4a3a4a3 n For example_comma_ crude features for a digital camera might include picture quality _comma_ viewfinder _comma_ and lens .
E06-1039.txt,42,Each sentence sk in the corpus contains a set of evaluations of crude features called evala5 sk a6 .
E06-1039.txt,43,Each evaluation contains both a polarity and a strength represented as an integer in the range a7a9a8 3a10a11a8 2a10a11a8 1a10a13a12 1a10a13a12 2a10a13a12 3a14 where a12 3 is the most positive possible evaluation and a8 3 is the most negative possible evaluation.
E06-1039.txt,44,There is also a hierarchical set of possibly more .
E06-2009.txt,1,using reinforcement learning for in car scenarios_comma_ developed at EdinburghUniversity and Cambridge University for the TALK project1. This prototype is the first Information State Update ISU dialoguesystemto exhibitreinforcement learning of dialogue strategies_comma_ and also has a fragmentary clarification feature.
E06-2009.txt,2,Thispaperdescribesthemaincomponentsand functionality of the system_comma_ as well as the purposesandfutureuseofthesystem_comma_andsurveys theresearchissuesinvolvedin itsconstruction.
E06-2009.txt,3,Evaluation of this system i.e. comparing the baselinesystem with handcodedvs. learntdialogue policies is ongoing_comma_ and the demonstration will show both.
E06-2009.txt,5,The in car system described below has been constructed primarily in order to be able to collect data for Reinforcement Learning RL approaches to multimodal dialogue management_comma_ and also to test and further develop learnt dialogue strategies in a realistic application scenario.
E06-2009.txt,6,For these reasons we have built a system which a0 containsan interfaceto a dialoguestrategy learner module_comma_ a0 covers a realistic domain of useful in car conversation and a wide range of dialogue phenomena e.g. confirmation_comma_ initiative_comma_ clarification_comma_ information presentation _comma_ a0 can be used to complete measurable tasks i.e. there is a measure of successful and unsuccessful dialogues usable as a reward signal for Reinforcement Learning _comma_ a0 logs all interactions in the TALK data collection format Georgila et al._comma_ 2005 .
E06-2009.txt,7,1This research is supported by the TALK project European Community IST project no. 507802 _comma_ http www.talkproject.org In this demonstration we will exhibit the software system that we have developed to meet these requirements.
E06-2009.txt,8,First we describe the domain in which the dialogue system operates an in car information system .
E06-2009.txt,9,Then we describe the major components of the system and giveexamplesof their use.
E06-2009.txt,10,We then discuss the important features of the system in respect to the dialogue phenomena that they support.
E06-2009.txt,11,1.1 A System Exhibiting Reinforcement Learning The central motivation for building this dialogue system is as a platform for Reinforcement Learning RL experiments.
E06-2009.txt,12,The system exhibits RL in 2 ways a0 It can be run in online learning mode with real users.
E06-2009.txt,13,Herethe RL agentis able to learn fromsuccessfulandunsuccessfuldialogueswithrealusers.
E06-2009.txt,14,Learningwill be much slowerthan with simulated users_comma_ but can start from an already learnt policy_comma_ and slowly improve upon that.
E06-2009.txt,15,a0 It can be run using an already learnt policy e.g. the one reported in Henderson et al._comma_ 2005 Lemon et al._comma_ 2005 _comma_ learnt from COMMUNICATOR data Georgila et al._comma_ 2005 .
E06-2009.txt,16,This mode can be used to test the learnt policies in interactions with real users.
E06-2009.txt,17,Please see Henderson et al._comma_ 2005 for an explanation of the techniques developed for Reinforcement Learning with ISU dialogue systems.
E06-2009.txt,18,2 System Overview The baseline dialogue system is built around the DIPPER dialogue manager Bos et al._comma_ 2003 .
E06-2009.txt,19,This system is initially used to conduct information seekingdialogues with a user e.g. find a particular hotel and restaurant _comma_ using hand coded dialogue strategies e.g. always use implicit confirmation_comma_ except when ASR confidence is below 50 _comma_ then use explicit confirmation .
E06-2009.txt,20,We have then modified the DIPPER dialogue manager so that it can consult learnt strategies for example strategies learnt from the 2000 and 2001 COMMUNICATOR data Lemon et al._comma_ 2005 _comma_ based on its 119 currentinformationstate_comma_ andthenexecutedialogueactions from those strategies.
E06-2009.txt,21,This allows us to compare hand coded against learnt strategies within the same system i.e. the other components such as the speechsynthesiser_comma_ recogniser_comma_ GUI_comma_ etc. all remain fixed .
E06-2009.txt,22,2.1 Overview of System Features The following features are currently implemented a0 use of Reinforcement Learning policies or dialogue plans_comma_ a0 multiple tasks information seeking for hotels_comma_ bars_comma_ and restaurants_comma_ a0 overanswering question accommodation userinitiative_comma_ a0 open speech recognition using n grams_comma_ a0 confirmations explicit and implicit based on ASR confidence_comma_ a0 fragmentary clarifications based on word confidence scores_comma_ a0 multimodal output highlighting and naming entities on GUI_comma_ a0 simple user commands e.g. Show me all the indian restaurants _comma_ a0 dialogue context logging in ISU format Georgila et al._comma_ 2005 .
E06-2009.txt,23,3 Research Issues Thework presentedhereexploresa numberof research themes_comma_ in particular using learnt dialogue policies_comma_ learning dialogue policies in online interaction with users_comma_ fragmentary clarification_comma_ and reconfigurability.
E06-2009.txt,24,3.1 Moving between Domains COMMUNICATOR and In car Dialogues Thelearntpolicies in Hendersonet al._comma_ 2005 focussed on the COMMUNICATOR system for flight booking dialogues.
E06-2009.txt,25,There we reportedlearning a promisinginitial policy for COMMUNICATOR dialogues_comma_ but the issue arises of how we could transfer this policy to new domains for example the in car domain.
E06-2009.txt,26,In the in car scenarios the genre of information seeking is central.
E06-2009.txt,27,For example the SACTI corpora Stuttle et al._comma_ 2004 have driver information requests e.g. searching for hotels as a major component.
E06-2009.txt,28,One question we address here is to what extent dialogue policies learnt from data gathered for one system_comma_ or family of systems_comma_ can be re used or adapted for use in other systems.
E06-2009.txt,29,We conjecture that the slotfilling policies learnt from our experiments with COMMUNICATOR will also be good policies for other slotfilling tasks that is_comma_ that we are learning generic slot filling or informationseeking dialoguepolicies.
E06-2009.txt,30,In section 5 we describe how the dialogue policies learnt for slot filling on the COMMUNICATOR data set can be .
E06-2002.txt,1,paper describes a multi lingual phrase based Statistical Machine Translation system accessible by means of a Web page. The user can issue translation requests from Arabic_comma_ Chinese or Spanish into English.
E06-2002.txt,2,The same phrase based statistical technology is employed to realize the three supported language pairs.
E06-2002.txt,3,New language pairs can be easily added to the demonstrator.
E06-2002.txt,4,The Web based interface allows the use of the translation system to any computer connected to the Internet.
E06-2002.txt,6,At this time_comma_ Statistical Machine Translation SMT has empirically proven to be the most competitive approach in international competitions like the NIST Evaluation Campaigns1 and the International Workshops on Spoken Language Translation IWSLT 20042 and IWSLT 20053 .
E06-2002.txt,7,In this paper we describe our multi lingual phrase based Statistical Machine Translation system which can be accessed by means of a Web page.
E06-2002.txt,8,Section 2 presents the general log linear framework to SMT and gives an overview of our phrase based SMT system.
E06-2002.txt,9,In section 3 the software architecture of the demo is outlined.
E06-2002.txt,10,Section 4 focuses on the currently supported language pairs Arabic to English_comma_ Chinese toEnglish and Spanish to English.
E06-2002.txt,11,In section 5 the Web based interface of the demo is described.
E06-2002.txt,12,1http www.nist.gov speech tests mt 2http www.slt.atr.jp IWSLT2004 3http www.is.cs.cmu.edu iwslt2005 2 SMT System Description 2.1 Log Linear Model Given a string f in the source language_comma_ the goal of the statistical machine translation is to select the string e in the target language which maximizes the posterior distribution Pr e f .
E06-2002.txt,13,By introducing the hidden word alignment variable a_comma_ the following approximate optimization criterion can be applied for that purpose e argmaxe Pr e f argmaxe summationdisplay a Pr e_comma_a f argmaxe_comma_a Pr e_comma_a f Exploiting the maximum entropy Berger et al._comma_ 1996 framework_comma_ the conditional distribution Pr e_comma_a f can be determined through suitable real valued functions called features hr e_comma_f_comma_a _comma_r 1...R_comma_ and takes the parametric form p e_comma_a f exp Rsummationdisplay r 1 rhr e_comma_f_comma_a The ITC irst system Chen et al._comma_ 2005 is based on a log linear model which extends the original IBM Model 4 Brown et al._comma_ 1993 to phrases Koehn et al._comma_ 2003 Federico and Bertoldi_comma_ 2005 .
E06-2002.txt,14,In particular_comma_ target strings e are built from sequences of phrases e1 ... el.
E06-2002.txt,15,For each target phrase e the corresponding source phrase within the source string is identified through three random quantities the fertility _comma_ which establishes its length the permutation pii_comma_ which sets its first position the tablet f_comma_ which tells its word string.
E06-2002.txt,16,Notice that target phrases might have fertility equal to zero_comma_ hence they do not translate any 91 source word.
E06-2002.txt,17,Moreover_comma_ uncovered source positions are associated to a special target word null according to specific fertility and permutation random variables.
E06-2002.txt,18,The resulting log linear model applies eight feature functions whose parameters are either estimated from data e.g. target language models_comma_ phrase based lexicon models or empirically fixed e.g. permutation models .
E06-2002.txt,19,While feature functions exploit statistics extracted from monolingual or word aligned texts from the training data_comma_ the scaling factors of the log linear model are estimated on the development data by applying a minimum error training procedure Och_comma_ 2004 .
E06-2002.txt,20,2.2 Decoding Strategy The translation of an input string is performed by the SMT system in two steps.
E06-2002.txt,21,In the first pass a beam search algorithm decoder computes a word graph of translation hypotheses.
E06-2002.txt,22,Hence_comma_ either the best translation hypothesis is directly extracted from the word graph and output_comma_ or an N best list of translations is computed Tran et al._comma_ 1996 .
E06-2002.txt,23,The N best translations are then re ranked by applying additional features and the top ranking translation is finally output.
E06-2002.txt,24,The decoder exploits dynamic programming_comma_ that is the optimal solution is computed by expanding and recombining previously computed partial theories.
E06-2002.txt,25,A theory is described by its state which is the only information needed for its expansion.
E06-2002.txt,26,Expanded theories sharing the same state are recombined_comma_ that is only the best scoring one is stored for further expansions.
E06-2002.txt,27,In order to output a word graph of translations_comma_ backpointers to all expanded theories are mantained_comma_ too.
E06-2002.txt,28,To cope with the large number of generated theories some approximations are introduced during the search less promising theories are pruned off beam search and a new source position is selected by limiting the number of vacant positions on the left hand and the distance from the left most vacant position re ordering constraints .
E06-2002.txt,29,2.3 Phrase extraction and model training Training of the phrase based translation model requires a parallel corpus provided with wordalignments in both directions_comma_ i.e. from source to target positions_comma_ and viceversa.
E06-2002.txt,30,This preprocessing step can be accomplished by applying the GIZA toolkit Och and Ney_comma_ 2003 that provides Viterbi alignments based on IBM Model 4.
E06-2002.txt,31,Starting from the parallel training corpus_comma_ provided with direct and inverted alignments_comma_ the socalled union alignment Och and Ney_comma_ 2003 is computed.
E06-2002.txt,32,Phrase pairs are extracted from each sentence pair which correspond to sub intervals of the source and target positions_comma_ J and I_comma_ such that the union alignment links all positions of J into I and all positions of I into J.
E06-2002.txt,33,In general_comma_ phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax.
E06-2002.txt,34,All such phrase pairs are efficiently computed by an algorithm with complexity O lImaxJ2max Cettolo et al._comma_ 2005 .
E06-2002.txt,35,Given all phrase pairs extracted from the training corpus_comma_ lexicon probabilities and fertility probabilities are estimated.
E06-2002.txt,36,Target language models LMs used by the decoder and rescoring modules are_comma_ respectively_comma_ estimated from 3 gram and 4 gram statistics by applying the modified Kneser Ney smoothing method Goodman and Chen_comma_ 1998 .
E06-2002.txt,37,LMs are estimated with an in house software toolkit which also provides a compact binary representation of the LM which is used by the decoder.
E06-2002.txt,38,3 Demo Architecture Figure 1 shows the two layer architecture of the demo.
E06-2002.txt,39,At the bottom lie the programs that provide the actual translation services for each languagepair a wrapper coordinates the activity of a specialized pre processing tool and a MT decoder.
E06-2002.txt,40,The translation programs run on a grid based cluster of high end PCs to optimize the processing speed.
E06-2002.txt,41,All the wrappers communicate with the MT frontend whose main task is to forward translation requests to the appropriate language pair wrapper and to report an error in case of wrong requests e.g. unsupported language pair .
E06-2002.txt,42,It is worth noticing here that a new language pair can be easily added to the system with a minimal intervention on the code of the MT front end.
E06-2002.txt,43,At the top of the architecture are the programs that provide the interface with the user.
E06-2002.txt,44,This layer is separated from the translation layer hosted by internal machines only by means of a firewall.
E06-2002.txt,45,The user interface is implemented as a Web page in which a translation request a source sentence and a language pair is input by means of an HTML form.
E06-2002.txt,46,The cgi script invocated by the form manages the interaction with the MT front end.
E06-2002.txt,47,92 Web Page form scriptCGI lang 1wrapper prepro cessing MT decoder prepro cessing MT decoder wrapperlang 2 prepro cessing MT decoder wrapperlang N... MTfront end firewall external host internal hosts fast machines Figure 1 Architecture of the demo.
E06-2002.txt,48,For each language pair a set of programs in particular the MT decoder provides the translation service.
E06-2002.txt,49,The request issued by the user on the Web page is sent by the cgi script to the MT front end.
E06-2002.txt,50,The translation is then performed on the appropriate language pair service and the output sent back to the Web browser.
E06-2002.txt,51,When a user issues a translation request after filling the form fields_comma_ the cgi script sends the request to the MT front end and waits for its reply.
E06-2002.txt,52,The input sentence is then forwarded to the wrapper of the appropriate language pair.
E06-2002.txt,53,After a preprocessing step_comma_ the actual translation is performed by the specific MT decoder.
E06-2002.txt,54,The output in the target language is then sent back to the user s Web browser through the chain in the reverse order.
E06-2002.txt,55,From a technical point of view_comma_ the inter process communication is realized by means of standard TCP IP sockets.
E06-2002.txt,56,As far as the encoding of texts is concerned_comma_ all the languages are encoded in UTF8 this allows to manage the processing phase in an uniform way and to render graphically different character sets.
E06-2002.txt,57,4 The supported language pairs Although there is no theoretical limit to the number of supported language pairs_comma_ the current version of the demo provides translations to English from three source languages Arabic_comma_ Chinese and Spanish.
E06-2002.txt,58,For demonstration purpose_comma_ three different application domains are covered too.
E06-2002.txt,59,Arabic to English Tourism The Arabic to English system has been trained with the data provided by the International Workshop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus BTEC task Takezawa et al._comma_ 2002 .
E06-2002.txt,60,BTEC is a multilingual speech corpus which contains sentences coming from phrase books for tourists.
E06-2002.txt,61,Training set includes 20k sentences containing 159K Arabic and 182K English running words vocabulary size is 18K for Arabic_comma_ 7K for English.
E06-2002.txt,62,Chinese to English Newswire The Chinese to English system has been trained with the data provided by the NIST MT Evaluation Campaign 2005 _comma_ large data condition.
E06-2002.txt,63,In this case parallel data are mainly news wires provided by news agencies.
E06-2002.txt,64,Training set includes 71M Chinese and 77M English running words vocabulary size is 157K for Chinese_comma_ 214K for English.
E06-2002.txt,65,Spanish to English European Parliament The Spanish to English system has been trained with the data provided by the Evaluation Campaign 2005 of the European integrated project TCSTAR4.
E06-2002.txt,66,The context is that of the speeches of the European Parliament Plenary sessions EPPS from April 1996 to October 2004.
E06-2002.txt,67,Training set for the Final Text Edition transcriptions includes 31M Spanish and 30M English running words vocabulary size is 140K for Spanish_comma_ 94K for English.
E06-2002.txt,68,5 The Web based Interface Figure 2 shows a snapshot of the Web based interface of the demo the URL has been removed to make this submission anonymous.
E06-2002.txt,69,In the upper part of the page the user provides the two information required for the translation the source sentence can be input in a 80x5 textarea html structure_comma_ while the language pair can be selected by means of a set a radio buttons.
E06-2002.txt,70,The user can reset the input area or send the translation request by means of standard reset and submit buttons.
E06-2002.txt,71,Some examples of bilingual sentences are provided in the lower part of the page.
E06-2002.txt,72,4http www.tc star.org 93 Figure 2 A snapshot of the Web based interface.
E06-2002.txt,73,The user provides the sentence to be translated in the desired language pair.
E06-2002.txt,74,Some examples of bilingual sentences are also available to the user.
E06-2002.txt,75,The output of a translation request is simple the requested source sentence_comma_ the translation in the target language and the selected language pair are presented to the user.
E06-2002.txt,76,Figure 3 shows an example of an Arabic sentence translated into English.
E06-2002.txt,77,We plan to extend the interface with the possibility for the user to ask additional information about the translation e.g. the number of explored theories or the score of the first best translation.
E06-2002.txt,78,6 Acknowledgements This work has been funded by the European Union under the integrated project TC STAR Technology and Corpora for Speech to Speech Translation IST 2002 FP6 506738_comma_ http www.tc star.org . .
E06-2003.txt,1,presenting the LinguaStream platform_comma_ we introduce different methodological principles and analysis models_comma_ which make it possible to build hybrid experimental NLP systems by articulating corpus processing tasks. 1 .
E06-2003.txt,2,Several important tendencies have been emerging recently in the NLP community.
E06-2003.txt,3,First of all_comma_ work on corpora tends to become the norm_comma_ which constitutes a fruitful convergence area between taskdriven_comma_ computational approaches and descriptive linguistic ones.
E06-2003.txt,4,On corpora validation becomes more and more important for theoretical models_comma_ and the accuracy of these models can be evaluated either with regard to their ability to account for the reality of a given corpus pursuing descriptive aims _comma_ either with regard to their ability to analyse it accurately pursuing operational aims .
E06-2003.txt,5,From this point of view_comma_ important questions have to be considered regarding which methods should be used in order to project efficiently and accurately linguistic models on corpora.
E06-2003.txt,6,It is indeed less and less appropriate to consider corpora as raw materials to which models and processes could be immediately applicable.
E06-2003.txt,7,On the contrary_comma_ the multiplicity of approaches_comma_ would they be lexical_comma_ syntactical_comma_ semantic_comma_ rhetorical or pragmatical_comma_ would they focus on one of these dimensions or cross them_comma_ raises questions about how these different levels can be articulated within operational models_comma_ and how the related processing systems can be assembled_comma_ applied on a corpus_comma_ andevaluatedwithinanexperimentalprocess.
E06-2003.txt,8,New NLP concerns confirm these needs recent works on automatic discourse structure analysis_comma_ for example regarding thematic structures or rhetorical ones Bilhaut_comma_ 2005 Widl ocher_comma_ 2004 _comma_ show that the results obtained from lower grained analysers such as part of speech taggers or local semantics analysers can be successfully exploited to perform higher grained analyses.
E06-2003.txt,9,Indeed_comma_ such works rely on non trivial processing streams_comma_ where several modules collaborate basing on the principles of incremental enrichment of documents and progressive .
E06-1024.txt,1,this paper_comma_ we address the problem of reducing the unpredictability of userinitiated dialogue contributions in humancomputer interaction without explicitly restricting the user s interactive possibilities. We demonstrate that it is possible to identify conditions under which particular classes of user initiated contributions will occur and discuss consequences for dialogue system design.
E06-1024.txt,3,It is increasingly recognised that human computer dialogue situations can benefit considerably from mixed initiativeinteraction Allen_comma_ 1999 .
E06-1024.txt,4,Interaction where there is_comma_ or appears to be_comma_ little restriction on just when and how the user may make a dialogue contribution increases the perceived naturalness of an interaction_comma_ itself a valuable goal_comma_ and also opens up the application of human computer interaction HCI to tasks where both system and user are contributing more equally to the task being addressed.
E06-1024.txt,5,Problematic with the acceptance of mixedinitiative dialogue_comma_ however_comma_ is the radically increased interpretation load placed on the dialogue system.
E06-1024.txt,6,This flexibility impacts negatively on performance at all levels of system design_comma_ from speech recognition to intention interpretation.
E06-1024.txt,7,In particular_comma_ clarification questions initiated by the user are difficult to process because they may appear off topic and can occur at any point.
E06-1024.txt,8,But preventing users from posing such questions leads to stilted interaction and a reduced sense of control over how things are proceeding.
E06-1024.txt,9,In this paper we pursue a partial solution to the problem of user initiated contributions that takes its lead from detailed empirical studies of how such situations are handled in human human interaction.
E06-1024.txt,10,Most proposed computational treatments of this situation up until now rely on formalised notions of relevance a system attempts to interpret a user contribution by relating it to shared goals of the system and user.
E06-1024.txt,11,When a connection can be found_comma_ then even an apparently off topic clarificationcanbeaccomodated.
E06-1024.txt,12,Inourapproach_comma_ weshowhowthesearchspaceforrelevantconnections can be constrained considerably by incorporating the generic conversation analytic principle of recipient design Sacks et al._comma_ 1974_comma_ p727 .
E06-1024.txt,13,This treats user utterances as explicit instructions for how they are to be incorporated into the unfolding discourse an approach that can itself be accomodated within much current discourse semantic work whereby potential discourse interpretation is facilitated by drawing tighter structural and semantic constraints from each discourse contribution Webber et al._comma_ 1999 Asher and Lascarides_comma_ 2003 .
E06-1024.txt,14,We extend this here to include constraints and conditions for the use of clarification subdialogues.
E06-1024.txt,15,Our approach is empirically driven throughout.
E06-1024.txt,16,In Section 2_comma_ we establish to what extent the principles of recipient design uncovered for natural human interaction can be adopted for the still artificial situation of human computer interaction.
E06-1024.txt,17,Although it is commonly assumed that results concerning human human interaction can be applied to human computer interaction Horvitz_comma_ 1999 _comma_ there are also revealing differences Amalberti et al._comma_ 1993 .
E06-1024.txt,18,We report on a targetted comparison of adopted dialogic strategies in natural human interaction termed below HHC humanhuman communication and human computer interaction HCI .
E06-1024.txt,19,The study shows significant and reliable differences in how dialogue is being managed.
E06-1024.txt,20,In Section 3_comma_ we interpret these results with respect to their implications for recipient design.
E06-1024.txt,21,The results demonstrate not only that recipient design is relevant for HCI_comma_ but also that it leads to specific and predictable kinds of clarification dialogues being taken up by users confronted with an artificialdialoguesystem.
E06-1024.txt,22,Finally_comma_ inSection4_comma_ we discuss the implications of the results for dialogic 185 system design in general and briefly indicate how the required mechanisms are being incorporated in our own dialogue system.
E06-1024.txt,23,2 A targetted comparison of HHC and HCI dialogues In order to ascertain the extent to which techniques of recipient design established on the basis of human human natural interaction can be transferred to HCI_comma_ we investigated comparable task oriented dialogues that varied according to whether the users believed that that they were interacting with another human or with an artificial agent.
E06-1024.txt,24,The data for our investigation were taken from three German corpora collected in the mid1990s within a toy plane building scenario used for a range of experiments in the German Collaborative Research Centre Situated Artificial Communicators SFB 360 at the University of Bielefeld Sagerer et al._comma_ 1994 .
E06-1024.txt,25,In these experiments_comma_ one participant is the constructor who actually builds the model plane_comma_ the other participant is the instructor _comma_ who provides instructions for the constructor.
E06-1024.txt,26,The corpora differ in that the constructor in the HHC setting was another human interlocutor in the other scenario_comma_ the participants were seated in front of a computer but were informed that they were actually talking to an automatic speech processing system HCI .1 In all cases_comma_ there was no visual contact between constructor and instructor.
E06-1024.txt,27,Previous work on human human taskoriented dialogues going back to_comma_ for example_comma_ Grosz 1982 _comma_ has shown that dialogue structure commonly follows task structure.
E06-1024.txt,28,Moreover_comma_ it is well known that human human interaction employs a variety of dialogue structuring mechanisms_comma_ ranging from meta talk to discourse markers_comma_ and that some of these can usefully be employed for automatic analysis Marcu_comma_ 2000 .
E06-1024.txt,29,If dialogue with artificial agents were then to be structured as it is with human interlocutors_comma_ there would be many useful linguistic surface cues available for guiding interpretation.
E06-1024.txt,30,And_comma_ indeed_comma_ a common way of designing dialogue structure in HCI is to have it follow the structure of the task_comma_ since this defines the types of actions necessary and their sequencing.
E06-1024.txt,31,1In fact_comma_ the interlocutors were always humans_comma_ as the artificial agent in the HCI conditions was simulated employing standard Wizard of Oz methods allowing tighter control of the linguistic responses received by the user.
E06-1024.txt,32,Figure 1 Contrasting dialogue structures for HHC and HCI conditions Previous studies have not_comma_ however_comma_ addressed the issue of dialogue structure in HCI systematically_comma_ although a decrease in framing signals has been noted by Hitzenberger and WomserHacker 1995 indicating either that the discourse structure is marked less often or that there is less structure to be marked.
E06-1024.txt,33,A more precise characterisation of how task structure is used or expressed in HCI situations is then critical for further design.
E06-1024.txt,34,For our analysis here_comma_ we focused on properties of the overall dialogue structure and how this is signalled via linguistic cues.
E06-1024.txt,35,Our results show that there are in fact significant differences in HCI and HHC and that it is not possible simply to take the human human interaction results and transpose results for one situation to the other.
E06-1024.txt,36,The structuring devices of the human to human construction dialogues can be described as follows.
E06-1024.txt,37,The instructors first inform their communication partners about the general goal of the construction.
E06-1024.txt,38,Subsequently_comma_ and as would be expected for a task oriented dialogue from previous studies_comma_ the discourse structure is hierarchical.
E06-1024.txt,39,At the top level_comma_ there is discussion of the assembly of the whole toy airplane_comma_ which is divided into individual functional parts_comma_ such as the wings or the wheels.
E06-1024.txt,40,The individual constructional steps then usually comprise a request to identify one or more parts and a request to combine them.
E06-1024.txt,41,Each step is generally acknowledged by the communication partner_comma_ and the successful combination of the parts as a larger structure is signalled as well.
E06-1024.txt,42,All the human to human dialogues were similar in these respects.
E06-1024.txt,43,This discourse structure is shown graphically in the outer box of Figure 1.
E06-1024.txt,44,Instructors mark changes between phases with signals of attention_comma_ often the constructor s first name_comma_ and discourse particles or speech routines that mark the beginning of a new phase such as 186 goal discourse marker explicit marking usage HHC HCI HHC HCI HHC HCI none 27.3 100 0 52.5 13.6 52.5 single 40.9 0 9.1 25.0 54.5 27.5 frequent 31.8 0 90.9 22.5 31.8 20.0 Percentage of speakers making no_comma_ single or frequent use of a particular structuring strategy.
E06-1024.txt,45,HCI N 40 HHC N 22.
E06-1024.txt,46,All differences are highly significant ANOVA p 0.005 .
E06-1024.txt,47,Table 1 Distribution of dialogue structuring devices across experimental conditions also so or jetzt geht s los now .
E06-1024.txt,48,This structuring function of discourse markers has been shown in several studies and so can be assumed to be quite usual for human human interaction Swerts_comma_ 1998 .
E06-1024.txt,49,Furthermore_comma_ individual constructional steps are explicitly marked by means of als erstes_comma_ dann first of all_comma_ then or der erste Schritt the first step .
E06-1024.txt,50,In addition to the marking of the construction phases_comma_ we also find marking of the different activities_comma_ such as description of the main goal versus description of the main architecture_comma_ or different phases that arise through the addressing of different addressees_comma_ such as asides to the experimenters.
E06-1024.txt,51,Speakers in dialogues directed at human interlocutors are therefore attending to the following three aspects of discourse structure marking the beginning of the task oriented phase of the dialogue marking the individual constructional steps providing orientations for the hearer as to the goals and subgoals of the communication.
E06-1024.txt,52,When we turn to the HCI condition_comma_ however_comma_ we find a very different picture indicating that a straightforward tuning of dialogue structure for an artificial agent on the basis of the HHC condition will not produce an effective system.
E06-1024.txt,53,These dialogues generally start as the HHC dialogues do_comma_ i.e._comma_ with a signal for getting the communication partner s attention_comma_ but then diverge by giving very low level instructions_comma_ such as to find a particular kind of component_comma_ often even before the system has itself given any feedback.
E06-1024.txt,54,Since this behaviour is divorced from any possible feedback or input produced by the artificial system_comma_ it can only be adopted because of the speaker s initial assumptions about the computer.
E06-1024.txt,55,When this strategy is successful_comma_ the speaker continues to use it in following turns.
E06-1024.txt,56,Instructors in the HCI condition do not then attempt to give a general orientation to their hearer.
E06-1024.txt,57,This is true of all the humancomputer dialogues in the corpus.
E06-1024.txt,58,Moreover_comma_ the dialogue phases of the HCI dialogues do not correspond to the assembly of an identifiable part of the airplane_comma_ such as a wing_comma_ the wheels_comma_ or the propeller_comma_ but to much smaller units that consist of successfully identifying and combining some parts.
E06-1024.txt,59,The divergent dialogue structure of the HCI condition is shown graphically in the inner dashed box of Figure 1.
E06-1024.txt,60,These differences between the experimental conditions are quantified in Table 1_comma_ which shows for each condition the frequencies of occurrence for the use of general orienting goal instructions_comma_ describing what task the constructor instructor is about to address_comma_ the use of discourse markers_comma_ and the use of explicit signals of changes in task phase.
E06-1024.txt,61,These differences prove a that users are engaging in recipient design with respect to their partner in these comparable situations and b that the linguistic cues available for structuring an interpretation of the dialogue in the HCI case are considerably impoverished.
E06-1024.txt,62,This can itself obviously lead to problems given the difficulty of the interpretation task.
E06-1024.txt,63,3 Interpretation of the observed differences in terms of recipient design Examiningthe resultsof theprevious sectionmore closely_comma_ we find signs that the concept of the communication partner to which participants were orienting was not the same for all participants.
E06-1024.txt,64,Some speakers believed structural marking also to be useful in the HCI situation_comma_ for example.
E06-1024.txt,65,In this section_comma_ we turn to a more exact consideration of the reasons for these differences and show that directly employing the mechanisms of recipient design developed by Schegloff 1972 is a beneficial strategy.
E06-1024.txt,66,The full range of variation observed_comma_ includingintra corpusvariationthatspaceprecluded us describing in detail above_comma_ is seen to arise from a single common mechanism.
E06-1024.txt,67,Furthermore_comma_ we show that precisely the same mechanism leads to a predictive account of user initiated clarificatory dialogues.
E06-1024.txt,68,187 The starting point for the discussion is the conversation analytic notion of the insertion sequence.
E06-1024.txt,69,An insertion sequence is a subdialogue inserted between the first and second parts of an adjacency pair.
E06-1024.txt,70,They are problematic for artificial agents precisely because they are places where the user takes the initiative and demands information from the system.
E06-1024.txt,71,Clarificatory subdialogues are regularly of this kind.
E06-1024.txt,72,Schegloff 1972 analyses the kinds of discourse contents that may constitute insertion sequences in human to human conversations involving spatial reference.
E06-1024.txt,73,His results imply a strong connection between recipient design and discourse structure.
E06-1024.txt,74,This means that we can describe the kind of local sequential organisation problematic for mixed initiative dialogue interpretation on the basis of more general principles.
E06-1024.txt,75,Insertion sequences have been found to address the following kinds of dialogue work Location Analysis Speakers check upon spatial information regarding the communication partners_comma_ such as where they are when on a mobile phone_comma_ which may lead to an insertion sequence and is also responsible for one ofthemostcommontypesofutteranceswhen beginning a conversation by mobile phone i.e._comma_ I m just on the bus train tram .
E06-1024.txt,76,Membership Analysis Speakers check upon information about the recipient because the communication partner s knowledge may render some formulations more relevant than others.
E06-1024.txt,77,As a member of a particular class of people_comma_ such as the class of locals_comma_ or of the class of those who have visited the place before_comma_ the addressee may be expected to know some landmarks that the speaker may use for spatial description.
E06-1024.txt,78,Membership groups may also include differentiation according to capabilities e.g._comma_ perceptual of the interlocutors.
E06-1024.txt,79,Topic or Activity Analysis Speakers attend to which aspects of the location addressed are relevant for the given topic and activity.
E06-1024.txt,80,They have a number of choices at their disposal among which they can select geographical descriptions_comma_ e.g.
E06-1024.txt,81,2903 Main Street_comma_ descriptions with relation to members_comma_ e.g.
E06-1024.txt,82,John s place_comma_ descriptions by means of landmarks_comma_ or place names.
E06-1024.txt,83,These three kinds of interactional activity each give rise to potential insertion sequences that is_comma_ they serve as the functional motivation for particular clarificatory subdialogues being explored rather than others.
E06-1024.txt,84,In the HCI situation_comma_ however_comma_ one of them stands out.
E06-1024.txt,85,The task of membership analysis is extremely challenging for a user faced with an unknown artificial agent.
E06-1024.txt,86,There is little basis for assigning group membership indeed_comma_ there are not even grounds for knowing which kind of groups would be applicable_comma_ due to lack of experience with artificial communication partners.
E06-1024.txt,87,Since membership analysis constitutes a prerequisite for the formulation of instructions_comma_ recipient design can be expected to be an essential force both for the discourse structure and for the motivation of particular types of clarification questions in HCI.
E06-1024.txt,88,We tested this prediction by means of a further empirical study involving a scenario in which the users task was to instruct a robot to measure the distance between two objects out of a set of seven.
E06-1024.txt,89,These objects differed only in their spatial position.
E06-1024.txt,90,The users had an overview of the robot and the objects to be referred to and typed their instructions into a notebook.
E06-1024.txt,91,The relevant objects were pointed at by the instructor of the experiments.
E06-1024.txt,92,The users were not given any information about the system and so were explicitly faced with a considerable problem of membership analysis_comma_ making the need for clarification dialogues particularly obvious.
E06-1024.txt,93,The results of the study confirmed the predicted effect and_comma_ moreover_comma_ provide a classification of clarification question types.
E06-1024.txt,94,Thus_comma_ the particular kinds of analysis found to initiate insertionsequencesinHHCsituationsareclearlyactive in HCI clarification questions as well.
E06-1024.txt,95,21 subjects from varied professions and with different experience with artificial systems participated in the study.
E06-1024.txt,96,The robot s output was generated by a simple script that displayed answers in a fixed order after a particular processing time.
E06-1024.txt,97,The dialogues were all_comma_ therefore_comma_ absolutely comparable regarding the robot s linguistic material moreover_comma_ the users instructions had no impact on the robot s linguistic behaviour.
E06-1024.txt,98,The robot_comma_ a Pioneer 2_comma_ did not move_comma_ but the participants were told that it could measure distances and that they were connected to the robot s dialogue processing system by means of a wireless LAN connection.
E06-1024.txt,99,The robot s output was either error or later in the dialogues a natural language variant or a distance 188 usr11 1 hallo hello sys ERROR usr11 2 siehst du was do you see anything sys ERROR usr11 3 was siehst du what do you see sys ERROR 652 a input is invalid.
E06-1024.txt,100,usr11 4 mi denabstandzwischendervorderstentasseund der linken tasse measure the distance between the frontmost cup and the left cup Figure 2 Example dialogue extract showing membership analysis clarification questions in centimeters.
E06-1024.txt,101,This forced users to reformulate theirdialoguecontributions aneffectivemethodology for obtaining users hypotheses about the functioning and capabilities of a system Fischer_comma_ 2003 .
E06-1024.txt,102,In our terms_comma_ this leads directly to an explicit exploration of a user s membership analysis.
E06-1024.txt,103,As expected in a joint attention scenario_comma_ very limited location analysis occurred.
E06-1024.txt,104,Topic analysis is also restricted spatial formulations were chosen on the basis of what users believed to be most understandable for the robot_comma_ which also leads back to the task of membership analysis.
E06-1024.txt,105,In contrast_comma_ there were many cases of membership analysis.
E06-1024.txt,106,There was clearly great uncertainty about the robot s prerequisites for carrying out the spatial task and this was explicitly specified in the users varied formulations.
E06-1024.txt,107,A simple example is given in Figure 2.
E06-1024.txt,108,The complete list of types of questions related to membership analysis and which digress from the task instructions in our corpus is given in Table2.
E06-1024.txt,109,Eachoftheseinstancesofmembershipanalysis constitutes a clarification question that would initiate an off topic subdialogue if the robot had reacted to it.
E06-1024.txt,110,4 Consequences for system design So far our empirical studies have shown that there are particular kinds of interactional problems that will regularly trigger user initiated clarification subdialogues.
E06-1024.txt,111,These might appear off topic or out of place but when understood in terms of the membership and topic activity analysis_comma_ it becomes clear that all such contributions are_comma_ in a very strong sense_comma_ predictable .
E06-1024.txt,112,These results can_comma_ and arguably should_comma_2 be exploited in the following ways.
E06-1024.txt,113,One is to extend dialogue system design to be able to meet these contingently rele2Doran et al. 2001 demonstrate a negative relationship between number of initiative attempts and their success rate. vant contributions whenever they occur.
E06-1024.txt,114,That is_comma_ we adapt dialogue manager_comma_ lexical database etc. so that precisely these apparently out of domain topics are covered.
E06-1024.txt,115,A second strategy is to determine discourse conditions that can be used to alert the dialogue system to the likely occurrence or absence of these kinds of clarificatory subdialogues see below .
E06-1024.txt,116,Third_comma_ we can design explicit strategies for interaction that will reduce the likelihood that a user will employ them for example_comma_ by providing information about the agent s capabilities_comma_ etc. as listed in Table 2 in advance by means of system initiated assertions.
E06-1024.txt,117,That is_comma_ we can guide_comma_ or shape_comma_ to use the terminology introduced by Zoltan Ford 1991 _comma_ the users linguistic behaviour.
E06-1024.txt,118,A combination of these three capabilities promises to improve the overall quality of a dialogue system and forms the basis for a significant part of our current research.
E06-1024.txt,119,We have already ascertained empirically discourse conditions that support the second strategy above_comma_ and these follow again directly from the basic notions of recipient design and membership analysis.
E06-1024.txt,120,If a user already has a strong membership analysis in place for example_comma_ due to preconceptions concerning the abilities or_comma_ more commonly_comma_ lack of abilities of the artificial agent then this influences the design of that user s utterances throughout the dialogue.
E06-1024.txt,121,As a consequence_comma_ we have been able to define distinctive linguistic profiles that lead to the identification of distinct user groups that differ reliably in their dialogue strategies_comma_ particularly in their initiation of subdialogues.
E06-1024.txt,122,In the human robot dialogues just considered_comma_ for example_comma_ we found that eight out of 21 users did not employ any clarification questions at all and an additional four users asked only a single clarification question.
E06-1024.txt,123,Providing these users with additional information about the robot s capabilities is of limited utility because these users found ways to deal with the situation without asking clarification questions.
E06-1024.txt,124,The second group of participants consisted of nine users this group used many questions that would have led into potentially problematic clarification dialogues if the system had been real.
E06-1024.txt,125,For these users_comma_ the presentation of additional information on the robot s capabilities would be very useful.
E06-1024.txt,126,It proved possible to distinguish the members of these two groups reliably simply by attending to their initial dialogue contributions.
E06-1024.txt,127,This is 189 domain example translation perception VP7 3 do you see the cups readiness VP4 25 Are you ready for another task functional capabilities VP19 11 what can you do linguistic capabilities VP18 7 Or do you only know mugs cognitive capabilities VP20 15 do you know where is left and right of you Table 2 Membership analysis related clarification questions use of task oriented greetings clarification beginnings none 58.3 11.1 single 25.0 11.1 frequent 16.7 77.8 N 21 average number of clarification questions for task oriented group 1.17 clarification questions per dialogue average number for greeting group 3.2 significance by t test p 0.01 Table 3 Percentage of speakers using no_comma_ a single_comma_ or frequent clarification questions depending on first utterance where their pre interaction membership analysis was most clearly expressed.
E06-1024.txt,128,In the human robot dialogues investigated_comma_ there is no initial utterance from the robot_comma_ the user has to initiate the interaction.
E06-1024.txt,129,Two principally different types of first utterance were apparent whereas one group of users beginstheinteractionwithtask instructions_comma_ asecondgroupbeginsthedialoguebymeansofagreeting_comma_ an appeal for help_comma_ or a question with regard to the capabilities of the system.
E06-1024.txt,130,These two different ways of approaching the system had systematic consequences for the dialogue structure.
E06-1024.txt,131,The dependent variable investigated is the number of utterances that initiate clarification subdialogues.
E06-1024.txt,132,The results of the analysis show that those who greet the robot or interact with it other than by issuing commands initiate clarificatory subdialogues significantly more often than those who start with an instruction cf. Table 3 .
E06-1024.txt,133,Thus_comma_ user modelling on the basis of the first utterance in these dialogues can be used to predict much of users linguistic behaviour with respect to the initiation of clarification dialogues.
E06-1024.txt,134,Note that for this type of user modelling no previous information about the user is necessary and group assignment can be carried out unobtrusively by means of simple key word spotting on the first utterance.
E06-1024.txt,135,Whereas the avoidance of clarificatory userinitiated subdialogues is clearly a benefit_comma_ we can also use the results of our empirical investigations to motivate improvements in the other areas of interactive work undertaken by speakers.
E06-1024.txt,136,In particular topic and activity analysis can become problematic when the decompositions adopted by a userareeitherinsufficienttostructuredialogueappropriately for interpretation or_comma_ worse_comma_ are incompatible with the domain models maintained by the artificial agent.
E06-1024.txt,137,In the latter case_comma_ communication will either fail or invoke rechecking of membership categories to find a basis for understanding e.g._comma_ do you know what cups are .
E06-1024.txt,138,Thus_comma_ what can be seen on the part of a user as reducing the complexity of a task can in fact be removing information vital for the artificial agent to effect successful interpretation.
E06-1024.txt,139,The results of a user s topic and activity analysis make themselves felt in the divergent dialogue structures observed.
E06-1024.txt,140,As shown above in Figure 1_comma_ the structure of the dialogues is thus much flatter than the one found in the corresponding HHC dialogues_comma_ such that goal description and marking of subtasks is missing_comma_ and the only structure results from the division into selection and combination of parts.
E06-1024.txt,141,In our second study_comma_ precisely the same effects are observed.
E06-1024.txt,142,The task of measuring distances between objects is often decomposed into simpler subtasks for example_comma_ the complexity of the task is reduced by achieving reference to each of the objects first before the robot is requested to measure the distance between them.
E06-1024.txt,143,This potential mismatch between user and system can also be identified on the basis of the interaction.
E06-1024.txt,144,Proceeding directly to issuing low level instructions rather than providing background general goal information is a clear linguistically recognisable cue that a nonaligned topic activity analysis has been adopted.
E06-1024.txt,145,A successful dialogue system can therefore rely on this dialogue transition as providing an indication of problems to come_comma_ which can again be avoided in advance by explicit system initiated assertions of information.
E06-1024.txt,146,190 Our main focus in this paper has been on setting out and motivating some generic principles for dialogue system design.
E06-1024.txt,147,These principles could find diverse computational instantiations and it has not been our aim to argue for any one instantation rather than another.
E06-1024.txt,148,However_comma_ to conclude_comma_ we summarise briefly the approach that we are adopting to incorporating these mechanisms within our own dialogue system Ross et al._comma_ 2005 .
E06-1024.txt,149,Our system augments an information state based approach with a distinguished vocabulary of discourse transitions between states.
E06-1024.txt,150,We attach conceptualisation conditions to these transitions which serve to post discourse goals whose particular function is to head off user initiated clarification.
E06-1024.txt,151,The presence of a greeting is one such condition the immediate transition to basic level instructions is another.
E06-1024.txt,152,Recognition and production of instructions is aided by treating the semantic types that occur cups _comma_ measure _comma_ move _comma_ etc. as elements of a domain ontology.
E06-1024.txt,153,The diverse topic activity analyses then correspond to the specification of the granularity and decomposition of activated domain ontologies.
E06-1024.txt,154,Similarly_comma_ location analyses correspond to common sense geographies_comma_ which we model in terms similar to those of ontologies now being developed for Geographic Information Systems Fonseca et al._comma_ 2002 .
E06-1024.txt,155,The specification of conceptualisationconditions triggered by discourse transitions and classifications of the topic activity analysis given by the semantic types provided in user utterances represents a direct transfer of the implicit strategies found in conversation analyses to the design of our dialogue system.
E06-1024.txt,156,For example_comma_ in our case many simple clarifications like do you see the cups _comma_ how many cups do you see as well as what can you do are prevented by providing information in advance on what the robot can perceive to those users that use greetings.
E06-1024.txt,157,Similarly_comma_ during a scene description where the system has the initiative_comma_ the opportunity is taken to introduce terms for the objects it perceives as well as appropriate ways of describing the scene_comma_ e.g._comma_ by means of There are two groups of cups.
E06-1024.txt,158,What do you want me to do a range of otherwise necessary clarificatory questions is avoided.
E06-1024.txt,159,Even in the case of failure_comma_ users will not doubt those capabilities of the system that it has displayed itself_comma_ due to alignment processes also observable in human to human dialogical interaction Pickering and Garrod_comma_ 2004 .
E06-1024.txt,160,After a successful interaction_comma_ users expect the system to be able to process parallel instructions because they reliably expect the system to behave consistently Fischer and Batliner_comma_ 2000 .
E06-1024.txt,161,5 Conclusions In this paper_comma_ the discourse structure initiated by users in HCI situations has been investigated and the results have been three fold.
E06-1024.txt,162,The structures initiated in HCI are much flatter than in HHC no general orientation with respect to the aims of a sub task are presented to the artificial communication partner_comma_ and marking is usually reduced.
E06-1024.txt,163,This needs to be accounted for in the mapping of the task structure onto the discourse model_comma_ irrespective of the kind of representation chosen.
E06-1024.txt,164,Secondly_comma_ the contents of clarification subdialogues have also been identified as particularly dependent on recipient design.
E06-1024.txt,165,That is_comma_ they concern the preconditions for formulating utterances particularly for the respective hearer.
E06-1024.txt,166,Here_comma_ the less that is known about the communication partner_comma_ the more needs to be elicited in clarification dialogues however_comma_ crucially_comma_ we can now state precisely which kinds of elicitations will be found cf. Table 2 .
E06-1024.txt,167,Thirdly_comma_ users have been shown to differ in the strategies that they take to solve the uncertainty about the speech situation and we can predict which strategies they in fact will follow in their employment of clarification dialogues on the basisoftheirinitialinteractionwiththesystem cf. Table 3 .
E06-1024.txt,168,Since the likelihood for users to initiate such clarificatory subdialogues has been found to be predictable_comma_ we have a basis for a range of implicit strategies for addressing the users subsequent linguistic behaviour.
E06-1024.txt,169,Recipient design has therefore been shown to be a powerful mechanism that_comma_ with the appropriate methods_comma_ can be incorporated in user adapted dialogue management design.
E06-1024.txt,170,Information of the kind that we have uncovered empirically in the work reported in this paper can be used to react appropriately to the different types of users in two ways either one can adapt the system or one can try to adapt the user Ogden and Bernick_comma_ 1996 .
E06-1024.txt,171,Although techniques for both strategies are supported by our results_comma_ in general we favour attempting to influence the user s behaviour without restricting it a priori by means 191 of computer initiated dialogue structure.
E06-1024.txt,172,Since the reasons for the users behaviour have been shown tobelocatedontheleveloftheirconceptualisation of the communication partner_comma_ explicit instruction may in any case not be useful explicit guidance of users is not only often impractical but also is not received well by users.
E06-1024.txt,173,The preferred choice is then to influence the users concepts of their communication partner and thus their linguistic behaviour by shaping Zoltan Ford_comma_ 1991 .
E06-1024.txt,174,In particular_comma_ Schegloff s analysis shows in detail the human interlocutors preference for those location terms that express group membership.
E06-1024.txt,175,Therefore_comma_ in natural dialogues the speakers constantly signal to each other who they are_comma_ what the other person can expect them to know.
E06-1024.txt,176,Effective system design should therefore provide users with precisely those kinds of information that constitute their most frequent clarification questions initially and in the manner that we have discussed.
E06-1024.txt,177,Acknowledgement The authors gratefully acknowledge the support of the Deutsche Forschungsgemeinschaft DFG for the work reported in this paper. .
E06-1048.txt,1,place synchronous tree adjoining grammars and tree transducers in the single overarching framework of bimorphisms_comma_ continuing the unification of synchronous grammars and tree transducers initiated by Shieber 2004 . Along the way_comma_ we present a new definition of the tree adjoininggrammarderivationrelation based on a novel direct inter reduction of TAG and monadic macro tree transducers.
E06-1048.txt,2,Tree transformation systems such as tree transducers and synchronous grammars have seen renewed interest_comma_ based on a perceived relevance to new applications_comma_ such as importing syntactic structure into statistical machine translation models or founding a formalism for speech command and control.
E06-1048.txt,3,The exact relationship among a variety of formalisms has been unclear_comma_ with a large number of seemingly unrelated formalisms being independently proposed or characterized.
E06-1048.txt,4,An initial step toward unifying the formalisms was taken Shieber_comma_ 2004 in making use of the formallanguage theoretic device of bimorphisms_comma_ previously used to characterize the tree relations definable by tree transducers.
E06-1048.txt,5,In particular_comma_ the tree relations definable by synchronous tree substitution grammars STSG were shown to be just those definable by linear complete bimorphisms_comma_ thereby providing for the first time a clear relationship between synchronous grammars and tree transducers.
E06-1048.txt,6,In this work_comma_ we show how the bimorphism framework can be used to capture a more powerful formalism_comma_ synchronous tree adjoining grammars_comma_ providing a further uniting of the various and disparate formalisms.
E06-1048.txt,7,After some preliminaries Section 1 _comma_ we begin by recalling the definition of tree adjoining grammars and synchronous tree adjoining grammars Section 2 .
E06-1048.txt,8,We turn then to a set of known results relating context free languages_comma_ tree homomorphisms_comma_ tree automata_comma_ and tree transducers to extend them for the tree adjoining languages Section 3 _comma_ presenting these in terms of restricted kinds of functional programs over trees_comma_ using a simple grammatical notation for describing the programs.
E06-1048.txt,9,This allows us to easily express generalizations of the notions monadic macro tree homomorphisms_comma_ automata_comma_ and transducers_comma_ which bear at least some of the same interrelationships that their traditional simpler counterparts do Section 4 .
E06-1048.txt,10,Finally_comma_ we use this characterization to place the synchronous TAG formalism in the bimorphism framework Section 5 _comma_ further unifying tree transducers and other synchronous grammar formalisms.
E06-1048.txt,11,We also_comma_ in passing_comma_ provide a new characterization of the relation between TAG derivation and derived trees_comma_ and a new simpler and more direct proof of the equivalence of TALs and the output languages of monadic macro tree transducers.
E06-1048.txt,12,1 Preliminaries Wewillnotatesequenceswithanglebrackets_comma_ e.g._comma_ a_comma_b_comma_c _comma_ or where no confusion results_comma_ simply as abc_comma_ with the empty string written .
E06-1048.txt,13,Trees will have nodes labeled with elements of a RANKED ALPHABET_comma_ a set of symbols F_comma_ each with a non negative integer RANK or ARITY assigned to it_comma_ determining the number of children for nodes so labeled.
E06-1048.txt,14,To emphasize the arity of a symbol_comma_ we will write it as a parenthesized superscript_comma_ for instance f n for a symbol f of arity n.
E06-1048.txt,15,Analogously_comma_ we write F n for the set of symbols in F with arity n.
E06-1048.txt,16,Symbols with arity zero F 0 are called NULLARY symbols or CON377 STANTS.
E06-1048.txt,17,The set of nonconstants is written F 1 .
E06-1048.txt,18,To express incomplete trees_comma_ trees with holes waiting to be filled_comma_ we will allow leaves to be labeled with variables_comma_ in addition to nullary symbols.
E06-1048.txt,19,The set of TREES OVER A RANKED ALPHABET F AND VARIABLES X_comma_ notated T F_comma_X _comma_ is the smallest set such that i f T F_comma_X for all f F 0 ii x T F_comma_X for all x X and iii f t1_comma_..._comma_tn T F_comma_X for all f F 1 _comma_ and t1_comma_..._comma_tn T F_comma_X .
E06-1048.txt,20,We abbreviate T F_comma_ 0 _comma_ where the set of variables is empty_comma_ as T F _comma_ the set of GROUND TREES over F.
E06-1048.txt,21,We will also make use of the set of n numerically ordered variables Xn x1_comma_..._comma_xn _comma_ and write x_comma_ y_comma_ z as synonyms for x1_comma_ x2_comma_ x3_comma_ respectively.
E06-1048.txt,22,Trees can also be viewed as mappings from TREE ADDRESSES_comma_ sequences of integers_comma_ to the labels of nodes at those addresses.
E06-1048.txt,23,The address is the address of the root_comma_ 1 the address of the first child_comma_ 12 the address of the second child of the first child_comma_ and so forth.
E06-1048.txt,24,We will use the notationt p to pick out the subtree of the node at address p in the tree t.
E06-1048.txt,25,Replacing the subtree of t at address p by a tree tprime_comma_ written t p mapsto tprime is defined as using for the insertion of an element on a list t mapsto tprime tprime f t1_comma_..._comma_tn i p mapsto tprime f t1_comma_..._comma_ti p mapsto tprime _comma_..._comma_tn for 1 i n .
E06-1048.txt,26,The HEIGHT of a tree t_comma_ notated height t _comma_ is defined as follows height x 0 for all x X and height f t1_comma_..._comma_tn 1 maxni 1height ti for all f F.
E06-1048.txt,27,We can use trees with variables as CONTEXTS in which to place other trees.
E06-1048.txt,28,A tree in T F_comma_Xn will be called a context_comma_ typically denoted with the symbol C.
E06-1048.txt,29,For a context C T F_comma_Xn and a sequence of n trees t1_comma_..._comma_tn T F _comma_ the SUBSTITUTION OF t1_comma_..._comma_tn INTO C_comma_ notated C t1_comma_..._comma_tn _comma_ is defined inductively as follows f u1_comma_..._comma_um t1_comma_..._comma_tn f u1 t1_comma_..._comma_tn _comma_..._comma_um t1_comma_..._comma_tn xi t1_comma_..._comma_tn ti .
E06-1048.txt,30,A tree t T F_comma_X is LINEAR if and only if no variable in X occurs more than once in t.
E06-1048.txt,31,We will use a notation akin to BNF to specify equations defining functional programs of various sorts.
E06-1048.txt,32,As an . to the notation we will use_comma_ here is a grammar defining trees over a ranked alphabet and variables essentially identically to the definition given above f n F n x X  x0 x1 x2  t T F_comma_X  f m t1_comma_..._comma_tm x The notation allows definition of classes of expressions e.g._comma_ F n and specifies metavariables over them f n .
E06-1048.txt,33,These classes can be primitive F n or defined X _comma_ even inductively in terms of other classes or themselves T F_comma_X .
E06-1048.txt,34,We use the metavariables and subscripted variants on the right hand side to represent an arbitrary element of the corresponding class.
E06-1048.txt,35,Thus_comma_ the elements t1_comma_..._comma_tm stand for arbitrary trees in T F_comma_X _comma_ and x an arbitrary variable in X.
E06-1048.txt,36,Because numerically subscripted versions of x appear explicitly on the right hand side of the rule defining variables_comma_ numerically subscripted variables e.g._comma_ x1 on the right hand side of all rules are taken to refer to the specific elements of x_comma_ whereas otherwise subscripted elements e.g._comma_ xi are taken generically.
E06-1048.txt,37,2 Tree Adjoining Grammars Tree adjoining grammar TAG is a tree grammar formalism distinguished by its use of a tree adjunction operation.
E06-1048.txt,38,Traditional presentations of TAG_comma_ which we will assume familiarity with_comma_ take the symbols in elementary and derived trees to be unranked nodes labeled with a given nonterminal symbol may have differing numbers of children.
E06-1048.txt,39,Joshi and Schabes 1997 present a good overview. For example_comma_ foot nodes of auxiliary trees and substitution nodes have no children_comma_ whereas the similarly labeled root nodes must have at least one.
E06-1048.txt,40,Similarly_comma_ two nodes with the same label but differing numbers of children may match for the purpose of allowing an adjunction as the root nodes of 1 and 1 in Figure 1 .
E06-1048.txt,41,In order to integrate TAG with tree transducers_comma_ however_comma_ we move to a ranked alphabet_comma_ which presents some problems and opportunities.
E06-1048.txt,42,In some ways_comma_ the ranked alphabet definition of TAGs is slightly more elegant than the traditional one. Although the bulk of the later discussion integrating TAGs and transducers assumes without loss of expressivity Joshi and Schabes_comma_ 1997_comma_ fn. 6 a limited form of TAG that includes adjunction but not substitution_comma_ we define the more complete form here.
E06-1048.txt,43,We will thus take the nodes of TAG trees to be labeled with symbols from a ranked alphabet F a given symbol then has a fixed arity and a fixed 378 T S T c S a S a S b S b 1 2 2 1 S 0 S 0 Figure 1 Sample TAG for the copy language wcw w a_comma_b . number of children.
E06-1048.txt,44,However_comma_ in order to maintain information about which symbols may match for the purpose of adjunction and substitution_comma_ we take the elements of F to be explicitly formed as pairs of an unranked label e and an arity n.
E06-1048.txt,45,For notational consistency_comma_ we will use e for unranked and f for ranked symbols. We will notate these elements_comma_ abusing notation_comma_ as e n _comma_ and make use of a function  to unrank symbols in F_comma_ so that e n  e.
E06-1048.txt,46,Tohandlefootnodes_comma_ foreachnon nullarysymbol e i F 1 _comma_ we will associate a new nullary symbol e _comma_ which one can take to be the pair of e and the set of such symbols will be notated F .
E06-1048.txt,47,Similarly_comma_ for substitution nodes_comma_F will be the set of nullary symbols e for all e i F 1 .
E06-1048.txt,48,These additional symbols_comma_ since they are nullary_comma_ will necessarily appear only at the frontier of trees.
E06-1048.txt,49,Finally_comma_ to allow null adjoining constraints_comma_ for each f F i _comma_ we introduce a symbol f 0 also of arity i_comma_ and take F 0 to be the set of all such symbols.
E06-1048.txt,50,We will extend the function to provide the unranked symbol associated with these symbols as well_comma_ so e  e  e i 0 e.
E06-1048.txt,51,A TAG is then a quadruple F_comma_S_comma_I_comma_A _comma_ where F isarankedalphabet S Fisadistinguishedinitial symbol I isthesetofinitialtrees_comma_ afinitesubsetof T F F 0 F and A is the set of auxiliary trees_comma_ afinitesubsetofT F F 0 F F .
E06-1048.txt,52,Anauxiliary tree whose root is labeled f must have exactly onenodelabeledwith f F andnoothernodes labeled inF this node is its foot node_comma_ its address notated foot .
E06-1048.txt,53,In Figure 1_comma_ 1 and 2 are initial trees 1 and 2 are auxiliary trees.
E06-1048.txt,54,In order to allow reference to a particular tree in the set P_comma_ we associate with each tree in P a unique index_comma_ conventionally notated with a subscripted or for initial and auxiliary trees respectively.
E06-1048.txt,55,This further allows us to have multiple instances of a tree in I or A_comma_ distinguished by their index.
E06-1048.txt,56,We will abuse notation by using the index and the tree that it names interchangably. The trees are combined by two operations_comma_ substitution and adjunction.
E06-1048.txt,57,Under substitution_comma_ a S S S T c 1 1 S a S a 1 S b S b 2 1 1 S 0 S 0 Figure 2 Sample core restricted TAG for the copy language wcw w a_comma_b . node labeled e at address p in a tree can be replaced by an initial tree prime with the corresponding label f at the root when f e.
E06-1048.txt,58,The resulting tree_comma_ the substitution of prime at p in _comma_ is p mapsto prime .
E06-1048.txt,59,Under adjunction_comma_ an internal node of at p labeled f F is split apart_comma_ replaced by an auxiliary tree rooted in fprime when f  fprime .
E06-1048.txt,60,The resulting tree_comma_ the adjunction of at p in _comma_ is p mapsto foot mapsto p .
E06-1048.txt,61,This definition by requiring f to be in F_comma_ not F or F maintains the standard convention_comma_ without loss of expressivity_comma_ that adjunction is disallowed at foot nodes and substitution nodes.
E06-1048.txt,62,The TAG in Figure 1 generates a tree set whose yield is the non context free copy language wcw w a_comma_b .
E06-1048.txt,63,The arities of the nodes are suppressed_comma_ as they are clear from context.
E06-1048.txt,64,A derivation tree D records the operations over the elementary trees used to derive a given derived tree.
E06-1048.txt,65,Each node in the derivation tree specifies an elementary tree _comma_ the node s child subtrees Di recordingthederivationsfortreesthatareadjoined or substituted into that tree.
E06-1048.txt,66,A method is required to record at which node in the tree specified by child subtree Di operates.
E06-1048.txt,67,For trees recording derivations in context free grammars_comma_ there are exactly as many substitution operations as nonterminals on the right hand side of the rule used.
E06-1048.txt,68,Thus_comma_ child order in the derivation tree can be used to recordtheidentityofthesubstitutionnode.
E06-1048.txt,69,Butfor TAG trees_comma_ operations occur throughout the tree_comma_ and some_comma_ namely adjunctions_comma_ can be optional_comma_ so a simple convention using child order is not possible.
E06-1048.txt,70,Traditionally_comma_ the branches in the derivation treehavebeennotatedwiththeaddressofthenode in the parent tree at which the child node operates.
E06-1048.txt,71,Figure 4 presents a derivation tree a using this notation_comma_ along with the corresponding derived tree b for the string abcab.
E06-1048.txt,72,For simplicity below_comma_ we use a stripped down TAG formalism_comma_ one that loses no expressivity in weak generative capacity but is easier for analysis purposes.
E06-1048.txt,73,First_comma_ we make all adjunction obligatory_comma_ in the 379 A B A B a b 2 3 1 B 0 Figure 3 Sample TAG tree marked with diacritics to show the permutation of operable nodes. sense that if a node in a tree allows adjunction_comma_ an adjunction must occur there.
E06-1048.txt,74,To get the effect of optional adjunction_comma_ for instance at a node labeled B_comma_ we add a vestigial tree of a single node B B _comma_ which has no adjunction sites and does not itself modify any tree that it adjoins into.
E06-1048.txt,75,It thus founds the recursive structure of derivations.
E06-1048.txt,76,Second_comma_ now that it is determinate whether an operation must occur at a node_comma_ the number of children of a node in a derivation tree is determined by the elementary tree at that node it is just the number of adjunction or substitution nodes in the tree_comma_ the OPERABLE NODES.
E06-1048.txt,77,All that is left to determine is the mapping between child order in the derivation tree and node in the elementary tree labeling the parent_comma_ that is_comma_ a permutation pi on the operable nodes or equivalently_comma_ their addresses _comma_ so that the i th child of a node labeled in a derivation tree is taken to specify the tree that operates at the node pii in .
E06-1048.txt,78,This permutation can be thought of as specified as part of the elementary tree itself.
E06-1048.txt,79,For example_comma_ the tree in Figure 3_comma_ whichrequiresoperationsatthenodesataddresses _comma_ 12_comma_ and 2_comma_ may be associated with the permutation 12_comma_2_comma_ .
E06-1048.txt,80,This permutation can be marked on the tree itself with numeric diacritics i_comma_ as shown in the figure.
E06-1048.txt,81,Finally_comma_ as mentioned before_comma_ we eliminate substitution Joshi and Schabes_comma_ 1997_comma_ fn. 6 .
E06-1048.txt,82,With these changes_comma_ the sample TAG grammar and derivation tree of Figures 1 and 4 a might be expressed with the core TAG grammar and derivation tree of Figures 2 and 4 c .
E06-1048.txt,83,3 Tree Transducers_comma_ Homomorphisms_comma_ and Automata 3.1 Tree Transducers Informally_comma_ a TREE TRANSDUCER is a function from T F to T G defined such that the symbol at the root ofthe input tree and a current state determines an output context in which the recursive images of the subtrees are placed.
E06-1048.txt,84,Formally_comma_ we can define a transducer as a kind of functional program_comma_ that is_comma_ a set of equations characterized by the following grammar for equations Eqn.
E06-1048.txt,85,The set of states is conventionally notated Q_comma_ with members notated q. One of the states is distinguished as the INITIAL STATE of the transducer. 1 q Q f n F n g n G n xi X  x0 x1 x2  Eqn  q f n x1_comma_..._comma_xn  n n R n  g m n 1 _comma_..._comma_ n m  qj xi where 1 i n Intuitively speaking_comma_ the expressions in R n are right hand side terms using variables limited to the first n.
E06-1048.txt,86,For example_comma_ the grammar allows definition of thefollowingsetofequationsdefiningatreetransducer 2 q f x  g qprime x _comma_q x q a a qprime f x  f qprime x qprime a a This transducer allows for the following derivation q f f a  g qprime f a _comma_q f a  g f qprime a _comma_g qprime a _comma_q a  g f a _comma_g a_comma_a The relation defined by a tree transducer with initial state q is t_comma_u q t u .
E06-1048.txt,87,By virtue of nondeterminism in the equations_comma_ multiple equations for a given state q and symbol f_comma_ tree transducers define true relations rather than merely functions.
E06-1048.txt,88,TREE HOMOMORPHISMS are a subtype of tree transducers_comma_ those with only a single state_comma_ hence essentially stateless.
E06-1048.txt,89,Other subtypes of tree transducers can be defined by restricting the trees 1Strictly speaking_comma_ what we define here are nondeterministic top down tree transducers.
E06-1048.txt,90,2Full definitions of tree transducers typically describe a transducer in terms of a set of states_comma_ an input and output ranked alphabet_comma_ and an initial state_comma_ in addition to the set of transitions_comma_ that is_comma_ defining equations.
E06-1048.txt,91,We will leave off these details_comma_ in the expectation that the sets of states and symbols can be inferred from the equations_comma_ and the initial state determined under a convention that it is the state defined in the textually first equation.
E06-1048.txt,92,Note also that we avail ourselves of consistent renaming of the variables x1_comma_ x2_comma_ and so forth_comma_ where convenient for readability.
E06-1048.txt,93,380 that form the right hand sides of equations_comma_ the elements of R n used.
E06-1048.txt,94,A transducer is LINEAR if all such are linear is COMPLETE if contains every variable in Xn is FREE if negationslash Xn is SYMBOL TO SYMBOL if height  1 and is a DELABELING if is complete_comma_ linear_comma_ and symbolto symbol.
E06-1048.txt,95,Another subcase is TREE AUTOMATA_comma_ tree transducers that compute a partial identity function these are delabeling tree transducers that preserve the label and the order of arguments.
E06-1048.txt,96,Because they compute only the identity function_comma_ tree automata are of interest for their domains_comma_ not the mappings they compute.
E06-1048.txt,97,Their domains define tree languages_comma_ in particular_comma_ the so called REGULAR TREE LANGUAGES.
E06-1048.txt,98,3.2 The Bimorphism Characterization of Tree Transducers Tree transducers can be characterized directly in terms of equations defining a simple kind of functionalprogram_comma_ asabove.
E06-1048.txt,99,Thereisanelegantalternative characterization of tree transducers in terms of a constellation of elements of the various subtypes of transducers homomorphisms and automata we have introduced_comma_ called a bimorphism.
E06-1048.txt,100,A bimorphism is a triple L_comma_hi_comma_ho _comma_ consisting of a regular tree language L or_comma_ equivalently_comma_ a tree automaton and two tree homomorphisms hi and ho.
E06-1048.txt,101,The tree relation defined by a bimorphism is the set of tree pairs that are generable from elements of the tree language by the homomorphisms_comma_ that is_comma_ L L_comma_hi_comma_ho  hi t _comma_ho t t L .
E06-1048.txt,102,We can limit attention to bimorphisms in which the input or output homomorphisms are restricted to a certain type_comma_ linear L _comma_ complete C _comma_ epsilonfree F _comma_ symbol to symbol S _comma_ delabeling D _comma_ or unrestricted M .
E06-1048.txt,103,We will write B I_comma_O where I and O characterize a subclass of homomorphisms for the set of bimorphisms for which the input homomorphism is in the subclass indicated by I and the output homomorphism is in the subclass indicated by O.
E06-1048.txt,104,Thus_comma_ B D_comma_M is the set of bimorphisms for which the input homomorphism is a delabeling but the output homomorphism can be arbitrary.
E06-1048.txt,105,The tree relations definable by tree transducers turn out to be exactly this class B D_comma_M Comon et al._comma_ 1997 .
E06-1048.txt,106,The bimorphism notion thus allows us to characterize the tree transductions purely in terms of tree automata and tree homomorphisms.
E06-1048.txt,107,We have shown Shieber_comma_ 2004 that the tree relations defined by synchronous tree substitution grammars were exactly the relations B LC_comma_LC .
E06-1048.txt,108,Intuitively speaking_comma_ the tree language in such a bimorphism represents the set of derivation trees for the synchronous grammar_comma_ and each homomorphism represents the relation between the derivation tree and the derived tree for one of the projected tree substitution grammars.
E06-1048.txt,109,The homomorphismsarelinearandcompletebecausethetreerelation between a tree substitution grammar derivation tree and its associated derived tree is exactly a linear complete tree homomorphism.
E06-1048.txt,110,To characterize the tree relations defined by a synchronous tree adjoining grammar_comma_ it similary suffices to find a simple homomorphism like characterization of the tree relation between TAG derivation trees and derived trees.
E06-1048.txt,111,In Section 5 below_comma_ we show that linear complete embedded tree homomorphisms_comma_ which we introduce next_comma_ serve this purpose.
E06-1048.txt,112,4 Embedded Tree Transducers Embedded tree transducers are a generalization of tree transducers in which states are allowed to take a single additional argument in a restricted manner.
E06-1048.txt,113,They correspond to a restrictive subcase of macro tree transducers with one recursion variable.
E06-1048.txt,114,We use the term embedded tree transducer rather than the more cumbersome monadic macro tree transducer for brevity and by analogy with embedded pushdown automata Schabes and Vijay Shanker_comma_ 1990 _comma_ another automata theoretic characterization of the tree adjoining languages.
E06-1048.txt,115,Wemodifythegrammaroftransducerequations to add an extra argument to each occurrence of a state q.
E06-1048.txt,116,To highlight the special nature of the extra argument_comma_ it is written in angle brackets before the input tree argument.
E06-1048.txt,117,We uniformly use the otherwise unused variable x0 for this argument in the left hand side_comma_ and add x0 as a possible right hand side itself.
E06-1048.txt,118,Finally_comma_ right hand side occurrences of states may be passed an arbitrary further righthand side tree in this argument. q Q f n F n xi X  x0 x1 x2  Eqn  q x0 f n x1_comma_..._comma_xn  n n R n  f m n 1 _comma_..._comma_ n m  x0 qj n j xi where 1 i n 381 Embedded transducers are strictly more expressive than traditional transducers_comma_ because the extra argument allows unbounded communication between positions unboundedly distant in depth in the output tree.
E06-1048.txt,119,For example_comma_ a simple embedded transducer can compute the reversal of a string_comma_ e.g._comma_ 1 2 2 nil reverses to 2 2 1 nil .
E06-1048.txt,120,This is not computable by a traditional tree transducer. It is given by the following equations r x rprime nil x rprime x0 nil x0 rprime x0 1 x  rprime 1 x0 x rprime x0 2 x  rprime 2 x0 x 1 This is_comma_ of course_comma_ just the normal accumulating reverse functional program_comma_ expressed as an embedded transducer.
E06-1048.txt,121,The additional power of embedded transducers is_comma_ we will show in this section_comma_ exactly what is needed to characterize the additional power that TAGs represent over CFGs in describing tree languages.
E06-1048.txt,122,In particular_comma_ we show that the relation between a TAG derivation tree and derived tree is characterized by a deterministic linear complete embedded tree transducer DLCETT .
E06-1048.txt,123,The relation between tree adjoining languages and embedded tree transducers may be implicit in a series of previous results in the formal language theory literature.3 For instance_comma_ Fujiyoshi and Kasai 2000 show that linear_comma_ complete monadic context free tree grammars generate exactly the tree adjoining languages via a normal form for spine grammars.
E06-1048.txt,124,Separately_comma_ the relation between context free tree grammars and macro tree transducers has been described_comma_ where the relationship between the monadic variants of each is implicit.
E06-1048.txt,125,Thus_comma_ taken together_comma_ an equivalence between the tree adjoining languages and the image languages of monadic macro tree transducers might be pieced together.
E06-1048.txt,126,In the present work_comma_ we define the relation between tree adjoining languages and linear complete monadic tree transducers directly_comma_ simply_comma_ and transparently_comma_ by giving explicit constructions in both directions_comma_ carefully handling the distinction between the unranked trees of tree adjoining grammars and the ranked trees of macro tree transducers and other important issues of detail in the constructions.
E06-1048.txt,128,First_comma_ we show that for any TAG we can construct a DLCETT that specifies the tree relation between the derivation trees for the TAG and the derived 3We are indebted to Uwe M onnich for this observation. trees.
E06-1048.txt,129,Then_comma_ we show that for any DLCETT we can construct a TAG such that the tree relation between the derivation trees and derived trees is related through a simple homomorphism to the DLCETT tree relation.
E06-1048.txt,130,4.1 From TAG to Transducer Given an elementary tree with the label A at its root_comma_ let the sequence pi pi1_comma_..._comma_pin be a permutation on the nodes in at which adjunction occurs.
E06-1048.txt,131,We use this ordering by means of the diacritic representation below. Then_comma_ if is an auxiliary tree_comma_ construct the equation qA x0 x1_comma_..._comma_xn  floorleft floorright and if is an initial tree_comma_ construct the equation qA  x1_comma_..._comma_xn  floorleft floorright wheretheright hand sidetransformationfloorleft floorrightisdefined by4 floorleftA 0 t1_comma_..._comma_tn floorright A floorleftt1floorright_comma_..._comma_floorlefttnfloorright floorleftk A t1_comma_..._comma_tn floorright qA floorleftA 0 t1_comma_..._comma_tn floorright xk floorleftA floorright x0 floorleftafloorright a 2 Note that the equations are linear and complete_comma_ because each variable xi is generated once as the tree is traversed_comma_ namely at position pii in the traversal marked with i _comma_ and the variable x0 is generated at the foot node only.
E06-1048.txt,132,Thus_comma_ the generated embedded tree transducer is linear and complete.
E06-1048.txt,133,Because only one equation is generated per tree_comma_ the transducer is trivially deterministic.
E06-1048.txt,134,By way of example_comma_ we consider the core TAG grammar given by the following trees  1 A e A A 0 1 B a _comma_ 2 C 3 D A  B 1 B b_comma_B B B C C D D 4It may seem like trickery to use the diacritics in this way_comma_ as they are not really components of the tree being traversed_comma_ but merely reflexes of an extrinsic ordering.
E06-1048.txt,135,But their use is benign.
E06-1048.txt,136,The same transformation can be defined_comma_ a bit more cumbersomely_comma_ keeping the permutation pi separate_comma_ by tracking the permutation and the current address p in a revised transformation floorleft floorrightpi_comma_p defined as follows floorleftA 0 t1_comma_..._comma_tn floorrightpi_comma_p A floorleftt1floorrightpi_comma_p 1_comma_..._comma_floorlefttnfloorrightpi_comma_p n floorleftA t1_comma_..._comma_tn floorrightpi_comma_p qA floorleftA 0 t1_comma_..._comma_tn floorrightpi_comma_p xpi 1 p floorleftA floorrightpi_comma_p x0 floorleftafloorrightpi_comma_p a We then use floorleft floorrightpi_comma_ for the transformation of the tree .
E06-1048.txt,137,382 1 2 1 2 1 2 a b S aS T c b S S S S 1 2 1 a b c Figure 4 Derivation and derived trees for the sample grammars a derivation tree for the grammar of Figure 1 b corresponding derived tree c corresponding derivation tree for the core TAG version of the grammar in Figure 2.
E06-1048.txt,138,Starting with the auxiliary tree A A 0 1 B a _comma_ 2 C 3 D A _comma_ the adjunction sites_comma_ corresponding to the nodes labeled B_comma_ C_comma_ and D at addresses 1_comma_ 2_comma_ and 21_comma_ have been arbitrarily given a preorder permutation.
E06-1048.txt,139,We therefore construct the equation as follows qA x0 A x1_comma_x2_comma_x3  floorleftA 0 1 B a _comma_ 2 C 3 D A floorright A floorleft1 B a floorright_comma_floorleft2 C 3 D A floorright A qB floorleftB 0 a floorright x1 _comma_floorleft2 C 3 D A floorright A qB B floorleftafloorright x1 _comma_floorleft2 C 3 D A floorright   A qB B a x1 _comma_qC C qD D x0 x3 x2 Similar derivations for the remaining trees yield the deterministic linear complete embedded tree transducer defined by the following set of equations qA  x1  qA A e x1 qA x0 A x1_comma_x2_comma_x3  A qB B a x1 _comma_qC C qD D x0 x3 x2 qB x0 B x1  qB B b_comma_x0 x1 qB x0 B  x0 qC x0 C  x0 qD x0 D  x0 We can use this transducer to compute the derived tree for the derivation tree A B B _comma_ C_comma_ D .
E06-1048.txt,140,qA  A B B _comma_ C_comma_ D  qA A e A B B _comma_ C_comma_ D  A qB B a B B _comma_ qC C qD D A e  D  C  A qB B b_comma_B a  B _comma_C qD D A e  D  A B b_comma_B a _comma_C D A e  As a final step_comma_ useful later for the bimorphism characterization of synchronous TAG_comma_ it is straightforwardtoshowthatthetransducersoconstructed is the composition of a regular tree language and a linear complete embedded tree homomorphism.
E06-1048.txt,141,4.2 From Transducer to TAG Givenalinearcompleteembeddedtreetransducer_comma_ we construct a corresponding TAG as follows For each rule of the form qi x0 f m x1_comma_..._comma_xm  we build a tree named qi_comma_ f_comma_ .
E06-1048.txt,142,Where this tree appears is determined solely by the state qi_comma_ so we take the root node of the tree to be the state.
E06-1048.txt,143,Any foot node in the tree will also need to be marked with the same label_comma_ so we pass this information down as the tree is built inductively.
E06-1048.txt,144,The tree is therefore of the form qi 0 ceilingleft ceilingrighti where the right hand side transformation ceilingleft ceilingrighti constructs the remainder of the tree by the inductive walk of _comma_ with the subscript noting that the root is labeled qi.
E06-1048.txt,145,ceilingleftf t1_comma_..._comma_tm ceilingrighti f 0 ceilingleftt1ceilingrighti_comma_..._comma_ceilinglefttmceilingrighti ceilingleftqj xk ceilingrighti k qj ceilingleft ceilingrighti ceilingleftx0ceilingrighti qi ceilingleftaceilingrighti a Note that at x0_comma_ a foot node is generated of the proper label.
E06-1048.txt,146,Because the equation is linear_comma_ only one foot node is generated_comma_ and it is labeled appropriately by construction. Where recursive processing of the input tree occurs qj xl _comma_ we generate a tree that admits adjunctions at qj.
E06-1048.txt,147,The role of the diacritic k is merely to specify the permutationofoperablenodesforinterpretingderivation trees it says that the k th child in a derivation tree rooted in the current elementary tree is taken to specify adjunctions at this node.
E06-1048.txt,148,The trees generated by this TAG are intended to correspond to the outputs of the corresponding tree transducer.
E06-1048.txt,149,Because of the more severe constraints on TAG_comma_ in particular that all combinatorial limitations on putting subtrees together must be manifest in the labels in the trees themselves_comma_ the outputs actually contain more structure than the corresponding transducer output.
E06-1048.txt,150,In particular_comma_ the state labeled nodes are merely for bookkeeping.
E06-1048.txt,151,A homomorphism removing these nodes gives the desired transducer output.
E06-1048.txt,152,Most importantly_comma_ then_comma_ the weak generative capacity of TAGs and LCETTs are identical.
E06-1048.txt,153,383 Some examples may clarify the construction.
E06-1048.txt,154,Recall the reversal embedded transducer in 1 above.
E06-1048.txt,155,The construction above generates a TAG containing the following trees.
E06-1048.txt,156,We have given themindicativenamesratherthanthecumbersome ones of the form qi_comma_ f_comma_ .  r 0 1 rprime nil nil rprime 0 rprime 1 rprime 0 1 rprime 1 0 rprime  2 rprime 0 1 rprime 2 0 rprime  It is simple to verify that the derivation tree 1 2 2 nil  derives the tree r rprime6 2 rprime 2 rprime 1 rprime nil   Simple homomorphisms that extract the input function symbols on the input and drop the bookkeeping states on the output reduce these trees to 1 2 2 nil and 2 2 1 nil respectively_comma_ just as for the corresponding tree transducer.
E06-1048.txt,157,5 Synchronous TAGs as Bimorphisms The major advantage of characterizing TAG derivation in terms of tree transducers via the compilation 2 is the integration of synchronous TAGs into the bimorphism framework.
E06-1048.txt,158,A synchronous TAG Shieber_comma_ 1994 is composed of a set of triples tL_comma_tR_comma_slurabove where the two trees tL and tR areelementarytreesandsluraboveisasetoflinksspecifying pairs of linked operable nodes from tL and tR.
E06-1048.txt,159,Without loss ofgenerality_comma_ wecan stipulate that each operable node in each tree is impinged upon by exactly one link in slurabove.
E06-1048.txt,160,If a node is unlinked_comma_ the triple can never be used if overlinked_comma_ a set of replacement triples can be multiplied out . In this case_comma_ a projection of the triples on first or second component_comma_ with a permutation defined by the correspondingprojectionsonthelinks_comma_ isexactlya TAG as defined above.
E06-1048.txt,161,Thus_comma_ derivations proceed just as in a single TAG except that nodes linked by some link in slurabove are simultaneously operated on by paired trees derived by the grammar.
E06-1048.txt,162,In order to model a synchronous grammar formalism as a bimorphism_comma_ the well formed derivations of the synchronous formalism must be characterizable as a regular tree language and the relation between such derivation trees and each of the paired derived trees as a homomorphism of some sort.
E06-1048.txt,163,For synchronous tree substitution grammars_comma_ derivation trees are regular tree languages_comma_ and the map from derivation to each of the paired derived trees is a linear complete tree homomorphism.
E06-1048.txt,164,Thus_comma_ synchronous tree substitution grammars fall in the class of bimorphisms B LC_comma_LC .
E06-1048.txt,165,The other direction can be shown as well all bimorphisms in B LC_comma_LC define tree relations expressible by an STSG.
E06-1048.txt,166,A similar result follows immediately for STAG.
E06-1048.txt,167,Crucially relying on the result above that the derivation relation is a DLCETT_comma_ we can use the method of Shieber 2004 directly to characterize the synchronous TAG tree relations as just B ELC_comma_ELC .
E06-1048.txt,168,We have thus integrated synchronous TAG with the other transducer and synchronous grammar formalisms falling under the bimorphism umbrella.
E06-1048.txt,169,Acknowledgements We wish to thank Mark Dras_comma_ Uwe M onnich_comma_ Rebecca Nesson_comma_ James Rogers_comma_ and Ken Shan for helpful discussions on the topic of this paper.
E06-1048.txt,170,This work was supported in part by grant IIS 0329089 from the National Science Foundation. .
E06-1013.txt,1,algorithm based on the Generalized Hebbian Algorithm is described that allows thesingular valuedecomposition of a dataset to be learned based on single observation pairs presented serially. The algorithm has minimal memory requirements_comma_ and is therefore interesting in the natural language domain_comma_ where very large datasets are often used_comma_ and datasets quickly become intractable.
E06-1013.txt,2,The technique is demonstrated on the task of learning word and letter bigram pairs from text.
E06-1013.txt,4,Dimensionality reduction techniques are of great relevance within the field of natural language processing.
E06-1013.txt,5,A persistent problem within language processing is the over specificity of language_comma_ and the sparsity of data.
E06-1013.txt,6,Corpusbased techniques depend on a sufficiency of examples in order to model human language use_comma_ but the Zipfian nature of frequency behaviour in language means that this approach has diminishing returns with corpus size.
E06-1013.txt,7,In short_comma_ there are a large number of ways to say the same thing_comma_ and no matter how large your corpus is_comma_ you will never cover all the things that might reasonably be said.
E06-1013.txt,8,Language is often too rich for the task being performed for example it can be difficult to establish that two documents are discussing the same topic.
E06-1013.txt,9,Likewise no matter how much data your system has seen during training_comma_ it will invariably see something new at run time in a domain of any complexity.
E06-1013.txt,10,Any approach to automatic natural language processing will encounter this problem on several levels_comma_ creating a need for techniques which compensate for this.
E06-1013.txt,11,Imagine we have a set of data stored as a matrix.
E06-1013.txt,12,Techniques based on eigen decomposition allow such a matrix to be transformedinto a set of orthogonal vectors_comma_ each with an associated strength _comma_ or eigenvalue.
E06-1013.txt,13,This transformation allows the data contained in the matrix to be compressed by discarding the less significant vectors dimensions the matrix can be approximated with fewer numbers.
E06-1013.txt,14,This is what is meant by dimensionality reduction.
E06-1013.txt,15,The technique is guaranteed to return the closest least squared error approximation possible for a given number of numbers Golub and Reinsch_comma_ 1970 .
E06-1013.txt,16,In certain domains_comma_ however_comma_ the technique has even greater significance.
E06-1013.txt,17,It is effectively forcing the data through a bottleneck requiring it to describe itself using an impoverished construct set.
E06-1013.txt,18,This can allow the critical underlying features to reveal themselves.
E06-1013.txt,19,In language_comma_ for example_comma_ these features might be semantic constructs.
E06-1013.txt,20,It can also improve the data_comma_ in the case that the detail is noise_comma_ or richness not relevant to the task.
E06-1013.txt,21,Singular value decomposition SVD is a near relative of eigen decomposition_comma_ appropriate to domains where input is asymmetrical.
E06-1013.txt,22,The best known application of singular value decomposition within natural language processing is Latent Semantic Analysis Deerwester et al._comma_ 1990 .
E06-1013.txt,23,Latent Semantic Analysis LSA allows passages of text to be compared to each other in a reduced dimensionality semantic space_comma_ based on the wordsthey contain.
E06-1013.txt,24,97 The technique has been successfully applied to information retrieval_comma_ where the overspecificity of language is particularly problematic text searches often miss relevant documents where different vocabulary has been chosen in the search terms to that used in the document for example_comma_ the user searches on eigen decomposition and fails to retrieve documents on factor analysis .
E06-1013.txt,25,LSA has also been applied in language modelling Bellegarda_comma_ 2000 _comma_ where it has been used to incorporate long span semantic dependencies.
E06-1013.txt,26,Much research has been done on optimising eigen decomposition algorithms_comma_ and the extent to which they can be optimised depends on the area of application.
E06-1013.txt,27,Most natural language problems involve sparse matrices_comma_ since there are many words in a natural language and the great majority do not appear in_comma_ for example_comma_ any one document.
E06-1013.txt,28,Domains in which matrices are less sparse lend themselves to such techniques as Golub Kahan Reinsch Golub and Reinsch_comma_ 1970 and Jacobi like approaches.
E06-1013.txt,29,Techniques such as those described in Berry_comma_ 1992 are more appropriate in the natural language domain.
E06-1013.txt,30,Optimisation is an important way to increase the applicability of eigen and singular value decomposition.
E06-1013.txt,31,Designing algorithms that accommodate different requirements is another.
E06-1013.txt,32,For example_comma_ another drawback to Jacobi like approaches is that they calculate all the singular triplets singular vector pairs with associated values simultaneously_comma_ which may not be practical in a situation where only the top few are required.
E06-1013.txt,33,Consider also that the methods mentioned so far assume that the entire matrix is available from the start.
E06-1013.txt,34,There are many situations in which data may continue to become available.
E06-1013.txt,35,Berry et al._comma_ 1995 describe a number of techniques for including new data in an existing decomposition.
E06-1013.txt,36,Their techniques apply to a situation in which SVD has been performed on a collection of data_comma_ then new data becomes available.
E06-1013.txt,37,However_comma_ these techniques are either expensive_comma_ or else they are approximations which degrade in quality over time.
E06-1013.txt,38,They are useful in the context of updating an existing batch decomposition with a second batch of data_comma_ but are less applicable in the case where data are presented serially_comma_ for example_comma_ in the context of a learning system.
E06-1013.txt,39,Furthermore_comma_ there are limits to the size of matrix that can feasibly be processed using batch decomposition techniques.
E06-1013.txt,40,This is especially relevant within natural language processing_comma_ where very large corpora are common.
E06-1013.txt,41,Random Indexing Kanerva et al._comma_ 2000 provides a less principled_comma_ though very simple and efficient_comma_ alternative to SVD for dimensionality reduction over large corpora.
E06-1013.txt,42,This paper describes an approach to singular value decomposition based on the Generalized Hebbian Algorithm Sanger_comma_ 1989 .
E06-1013.txt,43,GHA calculates the eigen decomposition of a matrix based on single observations presented serially.
E06-1013.txt,44,The algorithm presented here differs in that where GHA produces the eigen decomposition of symmetrical data_comma_ our algorithm produces the singular value decomposition of asymmetrical data.
E06-1013.txt,45,It allows singular vectors to be learned from paired inputs presented serially using no more memory than is required to store the singular vector pairs themselves.
E06-1013.txt,46,It is therefore relevant in situations where the size of the dataset makes conventional batch approaches infeasible.
E06-1013.txt,47,It is also of interest in the context of adaptivity_comma_ since it has the potential to adapt to changing input.
E06-1013.txt,48,The learning update operation is very cheap computationally.
E06-1013.txt,49,Assuming a stable vector length_comma_ each update operation takes exactly as long as each previous one there is no increase with corpus size to the speed of the update.
E06-1013.txt,50,Matrix dimensions may increase during processing.
E06-1013.txt,51,The algorithm produces singular vector pairs one at a time_comma_ starting with the most significant_comma_ which means that useful data becomes available quickly many standard techniques produce the entire decomposition simultaneously.
E06-1013.txt,52,Since it is a learning technique_comma_ however_comma_ it differs from what would normally be considered an incremental technique_comma_ in that the algorithm converges on the singular value decomposition of the dataset_comma_ rather than at any one point having the best solution possible for the data it has seen so far.
E06-1013.txt,53,The method is potentially most appropriate in situations where the dataset is very large or unbounded smaller_comma_ bounded datasets may be more efficiently processed by other methods.
E06-1013.txt,54,Furthermore_comma_ our 98 approach is limited to cases where the final matrix is expressible as the linear sum of outer products of the data vectors.
E06-1013.txt,55,Note in particular that Latent Semantic Analysis_comma_ as usually implemented_comma_ is not an example of this_comma_ because LSA takes the log of the final sums in each cell Dumais_comma_ 1990 .
E06-1013.txt,56,LSA_comma_ however_comma_ does not depend on singular value decomposition Gorrell and Webb Gorrell and Webb_comma_ 2005 discuss using eigen decomposition to perform LSA_comma_ and demonstrate LSA using the Generalized Hebbian Algorithm in its unmodified form.
E06-1013.txt,57,Sanger Sanger_comma_ 1993 presents similar work_comma_ and future work will involve more detailed comparison of this approach to his.
E06-1013.txt,58,The next section describes the algorithm.
E06-1013.txt,59,Section 3 describes implementation in practical terms.
E06-1013.txt,60,Section 4 illustrates_comma_ using word n gram and letter n gram tasks as examples and section 5 concludes.
E06-1013.txt,61,2 The Algorithm This section introduces the Generalized Hebbian Algorithm_comma_ and shows how the technique can be adapted to the rectangular matrix form of singular value decomposition.
E06-1013.txt,62,Eigen decomposition requires as input a square diagonallysymmetrical matrix_comma_ that is to say_comma_ one in which the cell value at row x_comma_ column y is the same as that at row y_comma_ column x.
E06-1013.txt,63,The kind of data described by such a matrix is the correlation between data in a particular space with other data in the same space.
E06-1013.txt,64,For example_comma_ we might wish to describe how often a particular word appears with a particular other word.
E06-1013.txt,65,The data therefore are symmetrical relations between items in the same space word a appears with word b exactly as often as word b appears with word a.
E06-1013.txt,66,In singular value decomposition_comma_ rectangular input matrices are handled.
E06-1013.txt,67,Ordered word bigrams are an example of this imagine a matrix in which rows correspond to the first word in a bigram_comma_ and columns to the second.
E06-1013.txt,68,The number of times that word b appears after word a is by no means the same as the number of times that word a appears after word b.
E06-1013.txt,69,Rows and columns are different spaces rows are the space of first words in the bigrams_comma_ and columns are the space of second words.
E06-1013.txt,70,The singular value decomposition of a rectangular data matrix_comma_ A_comma_ can be presented as A U V T 1 where U and V are matrices of orthogonal left and right singular vectors columns respectively_comma_ and is a diagonal matrix of the corresponding singular values.
E06-1013.txt,71,The U and V matrices can be seen as a matched set of orthogonal basis vectors in their corresponding spaces_comma_ while the singular values specify the effective magnitude of each vector pair.
E06-1013.txt,72,By convention_comma_ these matrices are sorted such that the diagonal of is monotonically decreasing_comma_ and it is a property of SVD that preserving only the first largest N of these and hence also only the first N columns of U and V provides a least squared error_comma_ rank N approximation to the original matrix A.
E06-1013.txt,73,Singular Value Decomposition is intimately related to eigenvalue decomposition in that the singular vectors_comma_ U and V_comma_ of the data matrix_comma_ A_comma_ are simply the eigenvectors of A AT and AT A_comma_ respectively_comma_ and the singular values_comma_ _comma_ are the square roots of the corresponding eigenvalues.
E06-1013.txt,74,2.1 Generalised Hebbian Algorithm Oja and Karhunen Oja and Karhunen_comma_ 1985 demonstrated an incremental solution to finding the first eigenvector from data arriving in the form of serial data items presented as vectors_comma_ and Sanger Sanger_comma_ 1989 later generalized this to finding the first N eigenvectors with the Generalized Hebbian Algorithm.
E06-1013.txt,75,The algorithm converges on the exact eigen decomposition of the data with a probability of one.
E06-1013.txt,76,The essence of these algorithms is a simple Hebbian learning rule Un t 1 Un t  UTn Aj Aj 2 Un is the n th column of U i.e._comma_ the n th eigenvector_comma_ see equation 1 _comma_ is the learning rate and Aj is the j th column of training matrix A. t is the timestep.
E06-1013.txt,77,The only modification to this required in order to extend it to multiple eigenvectors is that each Un needs to shadow any lower ranked Um m n by removing its projection from the input Aj in order to assure both orthogonality and an ordered ranking of 99 the resulting eigenvectors.
E06-1013.txt,78,Sanger s final formulation Sanger_comma_ 1989 is cij t 1 cij t  t yi t xj t 3 yi t summationdisplay k i ckj t yk t In the above_comma_ cij is an individual element in the current eigenvector_comma_ xj is the input vector and yi is the activation that is to say_comma_ ci.xj_comma_ the dot product of the input vector with the ith eigenvector . is the learning rate.
E06-1013.txt,79,To summarise_comma_ the formula updates the current eigenvector by adding to it the input vector multiplied by the activation minus the projection of the input vector on all the eigenvectors so far including the current eigenvector_comma_ multiplied by the activation.
E06-1013.txt,80,Including the current eigenvector in the projection subtraction step has the effect of keeping the eigenvectors normalised.
E06-1013.txt,81,Note that Sanger includes an explicit learning rate_comma_ .
E06-1013.txt,82,The formula can be varied slightly by not including the current eigenvector in the projection subtraction step.
E06-1013.txt,83,In the absence of the autonormalisation influence_comma_ the vector is allowed to grow long.
E06-1013.txt,84,This has the effect of introducing an implicit learning rate_comma_ since the vector only begins to grow long when it settles in the right direction_comma_ and since further learning has less impact once the vector has become long.
E06-1013.txt,85,Weng et al. Weng et al._comma_ 2003 demonstrate the efficacy of this approach.
E06-1013.txt,86,So_comma_ in vector form_comma_ assuming C to be the eigenvector currently being trained_comma_ expanding y out and using the implicit learning rate triangleci ci.x x summationdisplay j i x.cj cj 4 Delta notation is used to describe the update here_comma_ for further readability.
E06-1013.txt,87,The subtracted element is responsible for removing from the training update any projection on previous singular vectors_comma_ thereby ensuring orthgonality.
E06-1013.txt,88,Let us assume for the moment that we are calculating only the first eigenvector.
E06-1013.txt,89,The training update_comma_ that is_comma_ the vector to be added to the eigenvector_comma_ can then be more simply described as follows_comma_ making the next steps more readable trianglec c.x x 5 2.2 Extension to Paired Data Let us begin with a simplification of 5 trianglec 1ncX X 6 Here_comma_ the upper case X is the entire data matrix. n is the number of training items.
E06-1013.txt,90,The simplification is valid in the case that c is stabilised a simplification that in our case will become more valid with time.
E06-1013.txt,91,Extension to paired data initially appears to present a problem.
E06-1013.txt,92,As mentioned earlier_comma_ the singular vectors of a rectangular matrix are the eigenvectors of the matrix multiplied by its transpose_comma_ and the eigenvectors of the transpose of the matrix multiplied by itself.
E06-1013.txt,93,Running GHA on a nonsquare non symmetrical matrix M_comma_ ie. paired data_comma_ would therefore beachievable using standard GHA as follows triangleca 1ncaMMT MMT 7 trianglecb 1ncbMTM MTM 8 In the above_comma_ ca and cb are left and right singular vectors.
E06-1013.txt,94,However_comma_ to be able to feed the algorithm with rows of the matrices MMT and MTM_comma_ we would need to have the entire training corpus available simultaneously_comma_ and square it_comma_ which we hoped to avoid.
E06-1013.txt,95,This makes it impossible to use GHA for singular value decomposition of serially presented paired input in this way without some further transformation.
E06-1013.txt,96,Equation 1_comma_ however_comma_ gives ca cbMT summationdisplay x cb.bx ax 9 cb caM summationdisplay x ca.ax bx 10 Here_comma_ is the singular value and a and b are left and right data vectors.
E06-1013.txt,97,The above is valid in the case that left and right singular vectors ca and cb have settled which will become more accurate over time and that data vectors a and b outer product and sum to M.
E06-1013.txt,98,100 Inserting 9 and 10 into 7 and 8 allows them to be reduced as follows triangleca ncbMTMMT 11 trianglecb ncaMMTM 12 triangleca  2 n c aMMT 13 trianglecb  2 n c bMTM 14 triangleca  3 n c bMT 15 trianglecb  3 n c aM 16 triangleca 3 cb.b a 17 trianglecb 3 ca.a b 18 This element can then be reinserted into GHA.
E06-1013.txt,99,To summarise_comma_ where GHA dotted the input with the eigenvector and multiplied the result by the input vector to form the training update thereby adding the input vector to the eigenvector with a length proportional to the extent to which it reflects the current direction of the eigenvector our formulation dots the right input vector with the right singular vector and multiplies the left input vector by this quantity before adding it to the left singular vector_comma_ and vice versa.
E06-1013.txt,100,In this way_comma_ the two sides cross train each other.
E06-1013.txt,101,Below is the final modification of GHA extended to cover multiple vector pairs.
E06-1013.txt,102,The original GHA is given beneath it for comparison. trianglecai cbi.b a summationdisplay j i a.caj caj 19 trianglecbi cai.a b summationdisplay j i b.cbj cbj 20 triangleci ci.x x summationdisplay j i x.cj cj 21 In equations 6 and 9 10 we introduced approximations that become accurate as the direction of the singular vectors settles.
E06-1013.txt,103,These approximations will therefore not interfere with the accuracy of the final result_comma_ though they might interfere with the rate of convergence.
E06-1013.txt,104,The constant 3 has been dropped in 19 and 20.
E06-1013.txt,105,Its relevance is purely with respect to the calculation of the singular value.
E06-1013.txt,106,Recall that in Weng et al._comma_ 2003 the eigenvalue is calculable as the average magnitude of the training update trianglec.
E06-1013.txt,107,In our formulation_comma_ according to 17 and 18_comma_ the singular value would be trianglec divided by 3.
E06-1013.txt,108,Dropping the 3 in 19 and 20 achieves that implicitly the singular value is once more the average length of the training update.
E06-1013.txt,109,The next section discusses practical aspects of implementation.
E06-1013.txt,110,The following section illustrates usage_comma_ with English language word and letter bigram data as test domains.
E06-1013.txt,111,3 Implementation Within the framework of the algorithm outlined above_comma_ there is still room for some implementation decisions to be made.
E06-1013.txt,112,The naive implementation can be summarised as follows the first datum is used to train the first singular vector pair theprojection of thefirstsingular vector pair onto this datum is subtracted from the datum the datum is then used to train the second singular vector pair and so on for all the vector pairs ensuing data items are processed similarly.
E06-1013.txt,113,The main problem with this approach is as follows.
E06-1013.txt,114,At the beginning of the training process_comma_ the singular vectors are close to the values they were initialised with_comma_ and far away from the values they will settle on.
E06-1013.txt,115,The second singular vector pair is trained on the datum minus its projection onto the first singular vector pair in order to prevent the second singular vector pair from becoming the same as the first.
E06-1013.txt,116,But if the first pair is far away from its eventual direction_comma_ then the second has a chance to move in the direction that the first will eventually take on.
E06-1013.txt,117,In fact_comma_ all the vectors_comma_ such as they can whilst remaining orthogonal to each other_comma_ will move in the strongest direction.
E06-1013.txt,118,Then_comma_ when the first pair eventually takes on the right direction_comma_ the others have difficulty recovering_comma_ since they start to receive data that they have very little projection on_comma_ meaning that they learn very 101 slowly.
E06-1013.txt,119,The problem can be addressed by waiting until each singular vector pair is relatively stable before beginning to train the next.
E06-1013.txt,120,By stable _comma_ we mean that the vector is changing little in its direction_comma_ such as to suggest it is very close to its target.
E06-1013.txt,121,Measures of stability might include the average variation in position of the endpoint of the normalised vector over a number of training iterations_comma_ or simply length of the unnormalised vector_comma_ since a long vector is one that is being reinforced by the training data_comma_ such as it would be if it was settled on the dominant feature.
E06-1013.txt,122,Termination criteria might include that a target number of singular vector pairs have been reached_comma_ or that the last vector is increasing in length only very slowly.
E06-1013.txt,123,4 Application The task of relating linguistic bigrams to each other_comma_ as mentioned earlier_comma_ is an example of a task appropriate to singular value decomposition_comma_ in that the data is paired data_comma_ in which each item is in a different space to the other.
E06-1013.txt,124,Consider word bigrams_comma_ for example.
E06-1013.txt,125,First word space is in a non symmetrical relationship to second word space indeed_comma_ the spaces are not even necessarily of the same dimensionality_comma_ since there could conceivably be words in the corpus that never appear in the first word slot they might never appear at the start of a sentence or in the second word slot they might never appear at the end. So a matrix containing word counts_comma_ in which each unique first word forms a row and each unique second word forms a column_comma_ will not be a square symmetrical matrix the value at row a_comma_ column b_comma_ will not be the same as the value at row b column a_comma_ except by coincidence.
E06-1013.txt,126,The significance of performing dimensionality reduction on word bigrams could be thought of as follows.
E06-1013.txt,127,Language clearly adheres to some extent to a rule system less rich than the individual instances that form its surface manifestation.
E06-1013.txt,128,Those rules govern which words might follow which other words although the rule system is more complex and of a longer range that word bigrams can hope to illustrate_comma_ nonetheless the rule system governs the surface form of word bigrams_comma_ and we might hope that it would be possible to discern from word bigrams something of the nature of the rules.
E06-1013.txt,129,In performing dimensionality reduction on word bigram data_comma_ we force the rules to describe themselves through a more impoverished form than via the collection of instances that form the training corpus.
E06-1013.txt,130,The hope is that the resulting simplified description will be a generalisable system that applies even to instances not encountered at training time.
E06-1013.txt,131,On a practical level_comma_ the outcome has applications in automatic language acquisition.
E06-1013.txt,132,For example_comma_ the result might be applicable in language modelling.
E06-1013.txt,133,Use of the learning algorithm presented in this paper is appropriate given the very large dimensions of any realistic corpus of language The corpus chosen for this demonstration is Margaret Mitchell s Gone with the Wind _comma_ which contains 19_comma_296 unique words 421_comma_373 in total _comma_ which fully realized as a correlation matrix with_comma_ for example_comma_ 4 byte floats would consume 1.5 gigabytes_comma_ and which in any case_comma_ within natural language processing_comma_ would not be considered a particularly large corpus.
E06-1013.txt,134,Results on the word bigram task are presented in the next section.
E06-1013.txt,135,Letter bigrams provide a useful contrasting illustration in this context an input dimensionality of 26 allows the result to be more easily visualised.
E06-1013.txt,136,Practical applications might include automatic handwriting recognition_comma_ where an estimate of the likelihood of a particular letter following another would be useful information.
E06-1013.txt,137,The fact that there are only twenty something letters in most western alphabets though makes the usefulness of the incremental approach_comma_ and indeed_comma_ dimensionality reduction techniques in general_comma_ less obvious in this domain.
E06-1013.txt,138,However_comma_ extending the space to letter trigrams and even four grams would change the requirements.
E06-1013.txt,139,Section 4.2 discusses results on a letter bigram task.
E06-1013.txt,140,4.1 Word Bigram Task Gone with the Wind was presented to the algorithm as word bigrams.
E06-1013.txt,141,Each word was mapped to a vector containing all zeros but for a one in the slot corresponding to the unique word index assigned to that word.
E06-1013.txt,142,This had the effect of making input to the algorithm a normalised vector_comma_ and of making word vectors orthogonal to each other.
E06-1013.txt,143,The singular vector pair s reaching a combined Euclidean 102 magnitude of 2000 was given as the criterion for beginning to train the next vector pair_comma_ the reasoning being that since the singular vectors only start to grow long when they settle in the approximate right direction and the data starts to reinforce them_comma_ length forms a reasonable heuristic for deciding if they are settled enough to begin training the next vector pair.
E06-1013.txt,144,2000 was chosen ad hoc based on observation of the behaviour of the algorithm during training.
E06-1013.txt,145,The data presented are the words most representative of the top two singular vectors_comma_ that is to say_comma_ the directions these singular vectors mostly point in.
E06-1013.txt,146,Table 1 shows the words with highest scores in the top two vector pairs.
E06-1013.txt,147,It says that in this vector pair_comma_ the normalised left hand vector projected by 0.513 onto the vector for the word of or in other words_comma_ these vectors have a dot product of 0.513. The normalised right hand vector has a projection of 0.876 onto the word the etc. This first table shows a left side dominated by prepositions_comma_ with a right side in which the is by far the most important word_comma_ but which also contains many pronouns.
E06-1013.txt,148,The fact that the first singular vector pair is effectively about the the right hand side points far more in the direction of the than any other word reflects its status as the most common word in the English language.
E06-1013.txt,149,What thisresult is saying is that were we to be allowed only one feature with which to describe word English bigrams_comma_ a feature describing words appearing before the and words behaving similarly to the would be the best we could choose.
E06-1013.txt,150,Other very common words in English are also prominent in this feature.
E06-1013.txt,151,Table 1 Top words in 1st singular vector pair Vector 1_comma_ Eigenvalue 0.00938 of 0.5125468 the 0.8755944 in 0.49723375 her 0.28781646 and 0.39370865 a 0.23318098 to 0.2748983 his 0.14336193 on 0.21759394 she 0.1128443 at 0.17932475 it 0.06529821 for 0.16905183 he 0.063333265 with 0.16042696 you 0.058997907 from 0.13463423 their 0.05517004 Table 2 puts she _comma_ he and it at the top on the left_comma_ and four common verbs on the right_comma_ indicating a pronoun verb pattern as the second most dominant feature in the corpus.
E06-1013.txt,152,Table 2 Top words in 2nd singular vector pair Vector 2_comma_ Eigenvalue 0.00427 she 0.6633538 was 0.58067155 he 0.38005337 had 0.50169927 it 0.30800354 could 0.2315106 and 0.18958427 would 0.17589279 4.2 Letter Bigram Task Running the algorithm on letter bigrams illustrates different properties.
E06-1013.txt,153,Because there are only 26 letters in the English alphabet_comma_ it is meaningful to examine the entire singular vector pair.
E06-1013.txt,154,Figure 1 shows the third singular vector pair derived by running the algorithm on letter bigrams.
E06-1013.txt,155,The y axis gives the projection of the vector for the given letter onto the singular vector.
E06-1013.txt,156,The left singular vector is given on the left_comma_ and the right on the right_comma_ that is to say_comma_ the first letter in the bigram is on the left and the second on the right.
E06-1013.txt,157,The first two singular vector pairs are dominated by letter frequency effects_comma_ but the third is interesting because it clearly shows that the method has identified vowels.
E06-1013.txt,158,It means that the third most useful feature for determining the likelihood of letter b following letter a is whether letter a is a vowel.
E06-1013.txt,159,If letter b is a vowel_comma_ letter a is less likely to be vowels dominate the negative end of the right singular vector .
E06-1013.txt,160,Later features could introduce subcases where a particular vowel is likely to follow another particular vowel_comma_ but this result suggests that the most dominant case is that this does not happen. Interestingly_comma_ the letter h also appears at the negative end of the right singular vector_comma_ suggesting that h for the most part does not follow a vowel in English.
E06-1013.txt,161,Items near zero k _comma_ z etc. are not strongly represented in this singular vector pair it tells us little about them.
E06-1013.txt,162,5 Conclusion An incremental approach to approximating the singular value decomposition of a correlation matrix has been presented.
E06-1013.txt,163,Use 103 Figure 1 Third Singular Vector Pair on Letter Bigram Task0.60.40.2 0 0.2 0.4 0.6 i a o e u _ q x j z n y f k p g b s d w v l c m r t h n r t s ml c d f v wp g b u k x z j q _ y a i o h e of the incremental approach means that singular value decomposition is an option in situations where data takes the form of single serially presented observations from an unknown matrix.
E06-1013.txt,164,The method is particularly appropriate in natural language contexts_comma_ where datasets are often too large to be processed by traditional methods_comma_ and situations where the dataset is unbounded_comma_ for example in systems that learn through use.
E06-1013.txt,165,The approach produces preliminary estimations of the top vectors_comma_ meaning that information becomes available early in the training process.
E06-1013.txt,166,By avoiding matrix multiplication_comma_ data of high dimensionality can be processed.
E06-1013.txt,167,Results of preliminary experiments have been discussed here on the task of modelling word and letter bigrams.
E06-1013.txt,168,Future work will include an evaluation on much larger corpora.
E06-1013.txt,169,Acknowledgements The author would like to thank Brandyn Webb for his contribution_comma_ and theGraduate Schoolof Language Technology and Vinnova for their financial support. .
E06-1022.txt,1,present results on addressee identification in four participants face to face meetings using Bayesian Network and Naive Bayes classifiers. First_comma_ we investigate how well the addressee of a dialogue act can be predicted based on gaze_comma_ utterance and conversational context features.
E06-1022.txt,2,Then_comma_ we explore whether information about meeting context can aid classifiers performances.
E06-1022.txt,3,Both classifiers perform the best when conversational context and utterance features are combined with speaker s gazeinformation.
E06-1022.txt,4,Theclassifiers show little gain from information about meeting context.
E06-1022.txt,6,Addressing is an aspect of every form of communication.
E06-1022.txt,7,It represents a form of orientation and directionality of the act the current actor performs toward the particular other s who are involved in an interaction.
E06-1022.txt,8,In conversational communication involving twoparticipants_comma_ the hearer isalways the addressee of the speech act that the speaker performs.
E06-1022.txt,9,Addressing_comma_ however_comma_ becomes a real issue in multi party conversation.
E06-1022.txt,10,The concept of addressee as well as a variety of mechanisms that people use in addressing their speech have been extensively investigated by conversational analysts and social psychologists Goffman_comma_ 1981a Goodwin_comma_ 1981 Clark and Carlson_comma_ 1982 .
E06-1022.txt,11,Recently_comma_ addressing has received considerable attention in modeling multi party interaction in various domains.
E06-1022.txt,12,Research on automatic addressee identification has been conducted in the context of mixed human human and human computer interaction Bakx et al._comma_ 2003 van Turnhout et al._comma_ 2005 _comma_ human humanrobot interaction Katzenmaier et al._comma_ 2004 _comma_ and mixed human agents and multi agents interaction Traum_comma_ 2004 .
E06-1022.txt,13,In the context of automatic analysis of multi party face to face conversation_comma_ Otsuka et al. 2005 proposed a framework for automating inference of conversational structure that is defined in terms of conversational roles speaker_comma_ addressee and unaddressed participants.
E06-1022.txt,14,In this paper_comma_ we focus on addressee identification in a special type of communication_comma_ namely_comma_ face to face meetings.
E06-1022.txt,15,Moreover_comma_ we restrict our analysis to small group meetings with four participants.
E06-1022.txt,16,Automatic analysis of recorded meetings has become an emerging domain for a range of research focusing on different aspects of interactions among meeting participants.
E06-1022.txt,17,The outcomes of this research should be combined in a targeted application that would provide users with useful information about meetings.
E06-1022.txt,18,For answering questions such as Who was asked to prepare a presentation for the next meeting or Were there any arguments between participants A and B _comma_ some sort of understanding of dialogue structure is required.
E06-1022.txt,19,In addition to identification of dialogue acts that participants perform in multi party dialogues_comma_ identification of addressees of those acts is also important for inferring dialogue structure.
E06-1022.txt,20,There are many applications related to meeting research that could benefit from studying addressing in human human interactions.
E06-1022.txt,21,The results can be used by those who develop communicative agents in interactive intelligent environments and remote meeting assistants.
E06-1022.txt,22,These agents need to recognize when they are being addressed and how they should address people in the environment.
E06-1022.txt,23,This paper presents results on addressee identi169 fication in four participants face to face meetings using Bayesian Network and Naive Bayes classifiers.
E06-1022.txt,24,The goals in the current paper are 1 to find relevant features for addressee classification in meeting conversations using information obtained from multi modal resources gaze_comma_ speech and conversational context_comma_ 2 to explore to what extent the performances of classifiers can be improved by combining different types of features obtained from these resources_comma_ 3 to investigate whether the information about meeting context can aid the performances of classifiers_comma_ and 4 to compare performances of the Bayesian Network and Naive Bayes classifiers for the task of addressee prediction over various feature sets.
E06-1022.txt,25,2 Addressing in face to face meetings Whenaspeaker contributes totheconversation_comma_ all those participants who happen to be in perceptual range of this event will have some sort of participation status relative to it .
E06-1022.txt,26,The conversational roles that the participants take in a given conversational situation make up the participation framework Goffman_comma_ 1981b .
E06-1022.txt,27,Goffman 1976 distinguished three basic kinds of hearers those who overhear_comma_ whether or not their unratified participation isunintentional or encouraged those who are ratified but are not specifically addressed by the speaker also called unaddressed recipients Goffman_comma_ 1981a and those ratified participants who are addressed.
E06-1022.txt,28,Ratified participants are those participants whoare allowed to take part in conversation.
E06-1022.txt,29,Regarding hearers roles in meetings_comma_ we are focused only on ratified participants.
E06-1022.txt,30,Therefore_comma_ the problem of addressee identification amounts to the problem of distinguishing addressed from unaddressed participants for each dialogue act that speakers perform.
E06-1022.txt,31,Goffman 1981a defined addressees as those ratified participants oriented to by the speaker in a manner to suggest that his words are particularly for them_comma_ and that some answer is therefore anticipated fromthem_comma_ moresothanfromtheother ratified participants .
E06-1022.txt,32,According to this_comma_ it is the speaker whoselects his addressee the addressee is the one who is expected by the speaker to react on what the speaker says and to whom_comma_ therefore_comma_ the speaker is giving primary attention in the present act.
E06-1022.txt,33,In meeting conversations_comma_ a speaker may address his utterance to the whole group of participants present in the meeting_comma_ or to a particular subgroup of them_comma_ or to a single participant in particular.
E06-1022.txt,34,A speaker can also just think aloud or mumble to himself without really addressing anybody e.g. What else do I want to say while trying to evoke more details about the issue that he is presenting .
E06-1022.txt,35,We excluded self addressed speech from our study.
E06-1022.txt,36,Addressing behavior is behavior that speakers show to express to whom they are addressing their speech.
E06-1022.txt,37,It depends on the course of the conversation_comma_ the status of attention of participants_comma_ their current involvement in the discussion as well as on what the participants know about each others roles and knowledge_comma_ whether explicit addressing behavior is called for.
E06-1022.txt,38,Using a vocative is the explicit verbal way to address someone.
E06-1022.txt,39,In some cases the speaker identifies the addressee of his speech by looking at the addressee_comma_ sometimes accompanying this by deictic hand gestures.
E06-1022.txt,40,Addressees can also be designated by the manner of speaking.
E06-1022.txt,41,For example_comma_ by whispering_comma_ a speaker can select a single individual or a group of people as addressees.
E06-1022.txt,42,Addressees are often designated by the content of what is being said.
E06-1022.txt,43,For example_comma_ when making the suggestion We all have to decide together about the design _comma_ the speaker is addressing the whole group.
E06-1022.txt,44,In meetings_comma_ people may perform various group actions termed as meeting actions such as presentations_comma_ discussions or monologues McCowan et al._comma_ 2003 .
E06-1022.txt,45,A type of group action that meeting participants perform may influence the speaker s addressing behavior.
E06-1022.txt,46,For example_comma_ speakers may show different behavior during a presentation than during a discussion when addressing an individual regardless of the fact that a speaker has turned his back to a participant in the audience during a presentation_comma_ he most probably addresses his speech to the group including that participant_comma_ whereas the same behavior during a discussion_comma_ in many situations_comma_ indicates that that participant is unaddressed.
E06-1022.txt,47,In this paper_comma_ we focus on speech and gaze aspects of addressing behavior as well as on contextual aspects such as conversational history and meeting actions.
E06-1022.txt,48,3 Cues for addressee identification In this section_comma_ we present our motivation for feature selection_comma_ referring also to someexisting work 170 on the examination of cues that are relevant for addressee identification.
E06-1022.txt,49,Adjacency pairs and addressing Adjacency pairs AP are minimal dialogic units that consist of pairs of utterances called first pair part or a part and the second pair part or b part that are produced by different speakers.
E06-1022.txt,50,Examples include question answers or statement agreement.
E06-1022.txt,51,In the exploration of the conversational organization_comma_ special attention has been given to the a parts that are used as one of the basic techniques for selecting a next speaker Sacks et al._comma_ 1974 .
E06-1022.txt,52,For addressee identification_comma_ the main focus is on b parts and their addressees.
E06-1022.txt,53,It is to be expected that the a part provides a useful cue for identification of addressee of the b part Galley et al._comma_ 2004 .
E06-1022.txt,54,However_comma_ itdoesnot implythat thespeaker ofthea part is always the addressee of the b part.
E06-1022.txt,55,For example_comma_ A can address a question to B_comma_ whereas B s reply to A s question is addressed to the whole group.
E06-1022.txt,56,In this case_comma_ the addressee of the b part includes the speaker of the a part.
E06-1022.txt,57,Dialogue acts and addressing When designing an utterance_comma_ a speaker intends not only to perform a certain communicative act that contributes to a coherent dialogue in the literature referred to as dialogue act _comma_ but also to perform that act towardthe particular others.
E06-1022.txt,58,Within aturn_comma_ aspeaker may perform several dialogue acts_comma_ each of those having its own addressee e.g. I agree with you agreement addressed to a previous speaker but is this what we want information request addressed to the group .
E06-1022.txt,59,Dialogue act types can provide useful information about addressing types since some types of dialogue acts such as agreements or disagreements tend to be addressed to an individual rather than to a group.
E06-1022.txt,60,More information about the addressee of a dialogue can be induced by combining the dialogue act information with some lexical markers that are used as addressee indicators e.g. you_comma_ we_comma_ everybody_comma_ all of you Jovanovic and op den Akker_comma_ 2004 .
E06-1022.txt,61,Gaze behavior and addressing Analyzing dyadic conversations_comma_ researchers into social interaction observed that gaze in social interaction is used for several purposes to control communication_comma_ to provide a visual feedback_comma_ to communicate emotions and to communicate the nature of relationships Kendon_comma_ 1967 Argyle_comma_ 1969 .
E06-1022.txt,62,Recent studies into multi party interaction emphasized the relevance of gaze as a means of addressing.
E06-1022.txt,63,Vertegaal 1998 investigated towhatextent the focus of visual attention might function as an indicator for the focus of dialogic attention in four participants face to face conversations.
E06-1022.txt,64,Dialogic attention refers to attention while listening to a person as well as attention while talking to one or more persons.
E06-1022.txt,65,Empirical findings show that when a speaker is addressing an individual_comma_ there is 77 chance that the gazed person is addressed.
E06-1022.txt,66,When addressing a triad_comma_ speaker gaze seems to be evenly distributed over the listeners in the situation where participants are seated around the table.
E06-1022.txt,67,It is also shown that on average a speaker spends significantly more time gazing at an individual when addressing the whole group_comma_ than at others when addressing a single individual.
E06-1022.txt,68,When addressing an individual_comma_ people gaze 1.6 times more while listening 62 than while speaking 40 .
E06-1022.txt,69,When addressing a triad the amount of speaker gaze increases significantly to 59 .
E06-1022.txt,70,According to all these estimates_comma_ we can expect that gaze directional cues are good indicators for addressee prediction.
E06-1022.txt,71,However_comma_ these findings cannot be generalized in the situations where some objects of interest are present in the conversational environment_comma_ since it is expected that the amount of time spent looking at the persons will decrease significantly.
E06-1022.txt,72,As shown in Bakx et al._comma_ 2003 _comma_ in a situation where auser interacts withamultimodal information system and in the meantime talks to another person_comma_ the user looks most of the time at the system_comma_ both when talking to the system 94 and when talking to the user 57 .
E06-1022.txt,73,Also_comma_ another person looks at the system in 60 of cases when talking to the user.
E06-1022.txt,74,Bakx et al. 2003 also showed that some improvement in addressee detection can be achieved by combining utterance duration with gaze.
E06-1022.txt,75,In meeting conversations_comma_ the contribution of the gaze direction to addressee prediction is also affected by the current meeting activity and seating arrangement Jovanovic and op den Akker_comma_ 2004 .
E06-1022.txt,76,For example_comma_ when giving a presentation_comma_ a speaker most probably addresses his speech to the whole audience_comma_ although he may only look at a single participant in the audience.
E06-1022.txt,77,A seating arrangement determines avisible areaforeach meeting participant.
E06-1022.txt,78,During a turn_comma_ a speaker mostly looksat theparticipants whoareinhisvisible area.
E06-1022.txt,79,171 Moreover_comma_ the speaker frequently looks at a single participant in his visual area when addressing a group.
E06-1022.txt,80,However_comma_ when he wants to address a singleparticipant outside hisvisual area_comma_ hewilloften turn his body and head toward that participant.
E06-1022.txt,81,In this paper_comma_ we explored not only the effectiveness of the speaker s gaze direction_comma_ but also the effectiveness of the listeners gaze directions as cues for addressee prediction.
E06-1022.txt,82,Meeting context and addressing As Goffman 1981a has noted_comma_ the notion of a conversational encounter does not suffice in dealing with the context in which words are spoken a social occasion involving a podium event or no speech event at all may be involved_comma_ and in any case_comma_ the whole social situation_comma_ the whole surround_comma_ must always be considered .
E06-1022.txt,83,A set of various meeting actions that participants perform in meetings is one aspect of the social situation that differentiates meetings from other contexts of talk such as ordinary conversations_comma_ interviews or trials.
E06-1022.txt,84,As noted above_comma_ it influences addressing behavior as well as the contribution of gaze to addressee identification.
E06-1022.txt,85,Furthermore_comma_ distributions of addressing types vary for different meeting actions.
E06-1022.txt,86,Clearly_comma_ the percentage of the utterances addressed to the whole group during a presentation is expected to be much higher than during a discussion.
E06-1022.txt,87,4 Data collection To train and test our classifiers_comma_ we used a small multimodal corpus developed for studying addressing behavior in meetings Jovanovic et al._comma_ 2005 .
E06-1022.txt,88,The corpus contains 12 meetings recorded at the IDIAP smart meeting room in the research program of the M41 and AMI projects2.
E06-1022.txt,89,The room has been equipped with fully synchronized multi channel audio and video recording devices_comma_ a whiteboard and a projector screen.
E06-1022.txt,90,The seating arrangement includes two participants at each of two opposite sides of the rectangular table.
E06-1022.txt,91,The total amount of the recorded data is approximately 75 minutes.
E06-1022.txt,92,For experiments presented in this paper_comma_ we have selected meetings from the M4 data collection.
E06-1022.txt,93,These meetings are scripted in terms of type and schedule of group actions_comma_ but content is natural and unconstrained.
E06-1022.txt,94,The meetings are manually annotated with dialogue acts_comma_ addressees_comma_ adjacency pairs and gaze 1http www.m4project.org 2http www.amiproject.org direction.
E06-1022.txt,95,Each type of annotation is described in detail in Jovanovic et al._comma_ 2005 .
E06-1022.txt,96,Additionally_comma_ theavailable annotations ofmeetingactions forthe M4 meetings3 were converted into the corpus format and included in the collection.
E06-1022.txt,97,The dialogue act tag set employed for the corpus creation is based on the MRDA Meeting Recorder Dialogue Act tag set Dhillon et al._comma_ 2004 .
E06-1022.txt,98,The MRDA tag set represents a modification of the SWDB DAMSL tag set Jurafsky et al._comma_ 1997 for an application to multi party meeting dialogues.
E06-1022.txt,99,The tag set used for the corpus creation is made by grouping the MRDA tags into 17 categories that are divided into seven groups acknowledgments backchannels_comma_ statements_comma_ questions_comma_ responses_comma_ action motivators_comma_ checks and politeness mechanisms.
E06-1022.txt,100,A mapping between this tag set and the MRDA tag set is given in Jovanovic et al._comma_ 2005 .
E06-1022.txt,101,Unlike MRDA where each utterance is marked with a label made up of one or more tags from the set_comma_ each utterance in the corpus is marked as Unlabeled or with exactly one tag from the set.
E06-1022.txt,102,Adjacency pairs are labeled by marking dialogue acts that occur as their a part and bpart.
E06-1022.txt,103,Since all meetings in the corpus consist of four participants_comma_ the addressee of a dialogue act is labeled as Unknown or with one of the following addressee tags individual Px_comma_ a subgroup of participants Px_comma_Py or the whole audience Px_comma_Py_comma_Pz.
E06-1022.txt,104,Labeling gaze direction denotes labeling gazed targets for each meeting participants.
E06-1022.txt,105,As the only targets of interest for addressee identification are meeting participants_comma_ the meetings were annotated with the tag set that contains tags that are linked to each participant Px and the NoTarget tag that is used when the speaker does not look at any of the participants.
E06-1022.txt,106,Meetings are annotated with a set of six meeting actions described in McCowan et al._comma_ 2003 monologue_comma_ presentation_comma_ white board_comma_ discussion_comma_ consensus_comma_ disagreement and note taking.
E06-1022.txt,107,Reliability of the annotation schema As reported in Jovanovic et al._comma_ 2005 _comma_ gaze annotation has been reproduced reliably segmentation 80.40 N 939 classification 0.95 .
E06-1022.txt,108,Table 1 shows reliability of dialogue act segmentation as well as Kappa values for dialogue act and addressee classification for two different annotation 3http mmm.idiap.ch 172 groups that annotated twodifferent sets of meeting data.
E06-1022.txt,109,Group Seg N DA ADD B E 91.73 377 0.77 0.81 M R 86.14 367 0.70 0.70 Table 1 Inter annotator agreement on DA and addressee annotation N number ofagreed segments 5 Addressee classification In this section we present the results on addressee classification in four persons face to face meetings using Bayesian Network and Naive Bayes classifiers.
E06-1022.txt,110,5.1 Classification task In a dialogue situation_comma_ which is an event which lasts as long as the dialogue act performed by the speaker in that situation_comma_ the class variable is the addressee of the dialogue act ADD .
E06-1022.txt,111,Since there are only a few instances of subgroup addressing in the data_comma_ we removed them from the data set and excluded all possible subgroups of meeting participants from the set of class values.
E06-1022.txt,112,Therefore_comma_ we define addressee classifiers to identify one of the following class values individual Px where x 0_comma_1_comma_2_comma_3 and ALLPwhich denotes the whole group.
E06-1022.txt,113,5.2 Feature set To identify the addressee of a dialogue act we initially used three sorts of features conversational context features later referred to as contextual features _comma_ utterance features and gazefeatures.
E06-1022.txt,114,Additionally_comma_ we conducted experiments with an extended feature set including a feature that conveys information about meeting context.
E06-1022.txt,115,Contextual features provide information about the preceding utterances.
E06-1022.txt,116,We experimented with using information about the speaker_comma_ the addressee and the dialogue act of the immediately preceding utterance on the same or a different channel SP1_comma_ ADD 1_comma_ DA 1 as well as information about the related utterance SP R_comma_ADD R_comma_DA R .A related utterance is the utterance that is the a part of an adjacency pair with the current utterance as the b part.
E06-1022.txt,117,Information about the speaker of the current utterance SP has also been included in the contextual feature set.
E06-1022.txt,118,As utterance features_comma_ we used a subset of lexical features presented in Jovanovic and op den Akker_comma_ 2004 as useful cues for determining whether the utterance issingle or group addressed.
E06-1022.txt,119,The subset includes the following features does the utterance contain personal pronouns we or you _comma_ both of them_comma_ or neither of them does the utterance contain possessive pronouns or possessive adjectives your yours or our ours _comma_ their combination or neither of them does the utterance contain indefinite pronouns such as somebody _comma_ someone _comma_ anybody _comma_ anyone _comma_ everybody or everyone  does the utterance contain the name of participant Px
E06-1022.txt,120,Utterance features also include information about the utterance s conversational function DA tag and information about utterance duration i.e. whether the utterance is short or long.
E06-1022.txt,121,In our experiments_comma_ an utterance is considered as a short utterance_comma_ if its duration is less than or equal to 1 sec.
E06-1022.txt,122,We experimented with a variety of gaze features.
E06-1022.txt,123,In the first experiment_comma_ for each participant Px we defined a set of features in the form Pxlooks Py and Px looks NT where x_comma_y 0_comma_1_comma_2_comma_3 and x negationslash y Px looks NT represents that participant Px does not look at any of the participants.
E06-1022.txt,124,The value set represents the number of times that speaker Px looks at Py or looks away during the time span of the utterance zerofor 0_comma_ one for 1_comma_ two for 2 and more for 3 or more times.
E06-1022.txt,125,In the second experiment_comma_ we defined a feature set that incorporates only information about gazedirection of the current speaker SP looks Px and SP looksNT with the same value set as in the first experiment.
E06-1022.txt,126,As to meeting context_comma_ we experimented with different values of the feature that represents the meeting actions MA TYPE .
E06-1022.txt,127,First_comma_ we used a full set of speech based meeting actions that was applied for the manual annotation of the meetings in the corpus monologue_comma_ discussion_comma_ presentation_comma_ white board_comma_ consensus and disagreement.
E06-1022.txt,128,As the results on modeling group actions in meetings presented in McCowan et al._comma_ 2003 indicate that consensus and disagreements were mostly misclassified as discussion_comma_ we have also conducted experiments with a set of four values for MATYPE_comma_ where consensus_comma_ disagreement and discussion meeting actions were grouped in one category marked as discussion.
E06-1022.txt,129,173 5.3 Results and Discussions To train and test the addressee classifiers_comma_ we used the hand annotated M4 data from the corpus.
E06-1022.txt,130,After we had discarded the instances labeled with Unknown orsubgroup addressee tags_comma_ therewere 781 instances left available for the experiments.
E06-1022.txt,131,The distribution of the class values in the selected data is presented in Table 2.
E06-1022.txt,132,ALLP P0 P1 P2 P3 40.20 13.83 17.03 15.88 13.06 Table 2 Distribution of addressee values For learning the Bayesian Network structure_comma_ we applied the K2 algorithm Cooper and Herskovits_comma_ 1992 .
E06-1022.txt,133,The algorithm requires an ordering ontheobservable features different ordering leads to different network structures.
E06-1022.txt,134,We conducted experiments with several orderings regarding feature types as well as with different orderings regarding features of the same type.
E06-1022.txt,135,The obtained classification results for different orderings were nearly identical.
E06-1022.txt,136,For learning conditional probability distributions_comma_ we used the algorithm implemented in theWEKAtoolbox4 that produces direct estimates of the conditional probabilities.
E06-1022.txt,137,5.3.1 Initial experiments without meeting context The performances of the classifiers are measured using different feature sets.
E06-1022.txt,138,First_comma_ we measured the performances of classifiers using utterance features_comma_ gaze features and contextual features separately.
E06-1022.txt,139,Then_comma_ we conducted experiments withall possible combinations ofdifferent types of features.
E06-1022.txt,140,For each classifier_comma_ weperformed 10 fold cross validation.
E06-1022.txt,141,Table 3 summarizes the accuracies of the classifiers with 95 confidence interval for different feature sets 1 using gaze information of all meeting participants and 2 using only information about speaker gaze direction.
E06-1022.txt,142,The results show that the Bayesian Network classifier outperforms the Naive Bayes classifier for all feature sets_comma_ although the difference is significant only for the feature sets that include contextual features.
E06-1022.txt,143,For the feature set that contains only information about gaze behavior combined with information about the speaker Gaze SP _comma_ both classifiers perform significantly better when exploiting gaze information of all meeting participants.
E06-1022.txt,144,4http www.cs.waikato.ac.nz ml weka In other words_comma_ when using solely focus of visual attention to identify the addressee of a dialogue act_comma_ listeners focus of attention provides valuable information for addressee prediction.
E06-1022.txt,145,The same conclusion can be drawn when adding information about utterance duration to the gaze feature set Gaze SP Short _comma_ although for the Bayesian Network classifier the difference is not significant.
E06-1022.txt,146,For all other feature sets_comma_ the classifiers do not perform significantly different when including or excluding thelisteners gazeinformation.
E06-1022.txt,147,Evenmore_comma_ both classifiers perform better using only speaker gaze information in all cases except when combined utterance and gaze features are exploited Utterance Gaze SP .
E06-1022.txt,148,The Bayesian network and Naive Bayes classifiers show the same changes in the performances over different feature sets.
E06-1022.txt,149,The results indicate that the selected utterance features are less informative for addressee prediction BN 52.62 _comma_ NB 52.50 compared to contextual features BN 73.11 NB 68.12 or features of gaze behavior BN 66.45 _comma_ NB 64.53 .
E06-1022.txt,150,The results also show that adding the information about the utterance duration to the gaze features_comma_ slightly increases the accuracies of the classifiers BN 67.73 _comma_ NB 65.94 _comma_ which confirms findings presented in Bakx et al._comma_ 2003 .
E06-1022.txt,151,Combining the information from the gaze and speech channels significantly improves the performances of the classifiers BN 70.68 NB 69.78 in comparison to performances obtained from each channel separately.
E06-1022.txt,152,Furthermore_comma_ higher accuracies are gained when adding contextual features to the utterance features BN 76.82 NB 72.21 and even more to the features of gaze behavior BN 80.03 _comma_ NB 77.59 .
E06-1022.txt,153,As it is expected_comma_ the best performances are achieved by combining all three types of features BN 82.59 _comma_ NB 78.49 _comma_ although not significantly better compared to combined contextual and gaze features.
E06-1022.txt,154,Wealso explored how well the addressee can be predicted excluding information about the related utterance i.e. AP information .
E06-1022.txt,155,The best performances are achieved combining speaker gaze information with contextual and utterance features BN 79.39 NB 76.06 .
E06-1022.txt,156,A small decrease in the classification accuracies when excluding AP information about 3 indicates that remaining contextual_comma_ utterance and gaze features capture most of the useful information provided by AP.
E06-1022.txt,157,174 Baysian Networks Naive Bayes Feature sets Gaze All Gaze SP Gaze All Gaze SP All Features 81.05 2.75 82.59 2.66 78.10 2.90 78.49 2.88 Context 73.11 3.11 68.12 3.27 Utterance SP 52.62 3.50 52.50 3.50 Gaze SP 66.45 3.31 62.36 3.40 64.53 3.36 59.02 3.45 Gaze SP Short 67.73 3.28 66.45 3.31 65.94 3.32 61.46 3.41 Context Utterance 76.82 2.96 72.21 3.14 Context Gaze 79.00 2.86 80.03 2.80 74.90 3.04 77.59 2.92 Utterance Gaze SP 70.68 3.19 70.04 3.21 69.78 3.22 68.63 3.25 Table 3 Classification results for Bayesian Network and Naive Bayes classifiers using gaze information of all meeting participants Gaze All and using speaker gaze information Gaze SP Error analysis Further analysis of confusion matrixes for the best performed BN and NB classifiers_comma_ show that most misclassifications were between addressing types individual vs. group each Px was more confused with ALLP than with Py.
E06-1022.txt,158,A similar type of confusion is observed between human annotators regarding addressee annotation Jovanovic et al._comma_ 2005 .
E06-1022.txt,159,Out of all misclassified cases for each classifier_comma_ individual types of addressing Px were_comma_ in average_comma_ misclassified with addressing the group ALLP in 73 cases for NB_comma_ and 68 cases for BN.
E06-1022.txt,160,5.3.2 Experiments with meeting context Weexamined whether meetingcontext information can aid the classifiers performances.
E06-1022.txt,161,First_comma_ we conducted experiments using the six values set for the MA TYPE feature.
E06-1022.txt,162,Then_comma_ we experimented with employing the reduced set of four types of meeting actions see Section 5.2 .
E06-1022.txt,163,The accuracies obtained by combining the MA TYPE feature with contextual_comma_ utterance and gaze features are presented in Table 4.
E06-1022.txt,164,Bayesian Networks Naive Bayes Features Gaze All Gaze SP Gaze All Gaze SP MA 6 All 81.82 82.84 78.74 79.90 MA 4 All 81.69 83.74 78.23 79.13 Table 4 Classification results combining MATYPE with the initial feature set The results indicate that adding meeting context information to the initial feature set improves slightly_comma_ but not significantly_comma_ the classifiers performances.
E06-1022.txt,165,The highest accuracy 83.74 is achieved using the Bayesian Network classifier by combining thefour values MA TYPEfeature with contextual_comma_ utterance and the speaker s gaze features.
E06-1022.txt,166,6 Conclusion and Future work We presented results on addressee classification in four participants face to face meetings using Bayesian Network and Naive Bayes classifiers.
E06-1022.txt,167,The experiments presented should be seen as preliminary explorations of appropriate features and models for addressee identification in meetings.
E06-1022.txt,168,We investigated how well the addressee of a dialogue act can be predicted 1 using utterance_comma_ gaze and conversational context features alone as well as 2 using various combinations of these features.
E06-1022.txt,169,Regarding gaze features_comma_ classifiers performances are measured using gaze directional cues of the speaker only as well as of all meeting participants.
E06-1022.txt,170,We found that contextual information aids classifiers performances over gaze information as well as over utterance information.
E06-1022.txt,171,Furthermore_comma_ the results indicate that selected utterance features are the most unreliable cues for addressee prediction.
E06-1022.txt,172,The listeners gaze direction provides useful information only in the situation where gaze features are used alone.
E06-1022.txt,173,Combinations of features from various resources increases classifiers performances in comparison to performances obtained from each resource separately.
E06-1022.txt,174,However_comma_ the highest accuracies for both classifiers are reached by combining contextual and utterance features with speaker s gaze BN 82.59 _comma_ NB 78.49 .
E06-1022.txt,175,We have also explored the effect of meeting context on the classification task.
E06-1022.txt,176,Surprisingly_comma_ addressee classifiers showed little gain from the information about meeting actions BN 83.74 _comma_ NB 79.90 .
E06-1022.txt,177,For all feature sets_comma_ the Bayesian Network classifier outperforms the Naive Bayes classifier.
E06-1022.txt,178,In contrast to Vertegaal 1998 and Otsuka et al. 2005 findings_comma_ where it is shown that gaze can be a good predictor for addressee in fourparticipants face to face conversations_comma_ our results 175 show that in four participants face to face meetings_comma_ gaze is less effective as an addressee indicator.
E06-1022.txt,179,This can be due to several reasons.
E06-1022.txt,180,First_comma_ they used different seating arrangements which is implicated in the organization of gaze.
E06-1022.txt,181,Second_comma_ our meeting environment contains attentional distracters such as whiteboard_comma_ projector screen and notes.
E06-1022.txt,182,Finally_comma_ during a meeting_comma_ in contrast to an ordinary conversation_comma_ participants perform various meeting actions which may influence gaze as an aspect of addressing behavior.
E06-1022.txt,183,We will continue our work on addressee identification on the large AMI data collection that is currently in production.
E06-1022.txt,184,The AMI corpus contains more natural_comma_ scenario based_comma_ meetings that involve groups focused on the design of a TV remote control.
E06-1022.txt,185,Some initial experiments on the AMI pilot data show that additional challenges for addressee identification on the AMI data are roles that participants play in the meetings e.g. project manager or marketing expert and additional attentional distracters present in the meeting room such as_comma_ the task object at first place and laptops.
E06-1022.txt,186,This means that a richer feature set should be explored to improve classifiers performances on the AMI data including_comma_ for example_comma_ the background knowledge about participants roles.
E06-1022.txt,187,We will also focus on the development of new models that better handle conditional and contextual dependencies among different types of features.
E06-1022.txt,188,Acknowledgments This work was partly supported by the European Union 6th FWPISTIntegrated Project AMI Augmented Multi party Interaction_comma_ FP6 506811_comma_ publication AMI 153 . .
E06-2001.txt,1,Web contains vast amounts of linguistic data. One key issue for linguists and language technologists is how to access it.
E06-2001.txt,2,Commercial search engines give highly compromised access.
E06-2001.txt,3,An alternative is to crawl the Web ourselves_comma_ which also allows us to remove duplicates and nearduplicates_comma_ navigational material_comma_ and a range of other kinds of non linguistic matter.
E06-2001.txt,4,We can also tokenize_comma_ lemmatise and part of speech tag the corpus_comma_ and load the data into a corpus query tool which supports sophisticated linguistic queries.
E06-2001.txt,5,We have now done this for German and Italian_comma_ with corpus sizes of over 1 billion words in each case.
E06-2001.txt,6,We provide Web access to the corpora in our query tool_comma_ the Sketch Engine.
E06-2001.txt,8,The Web contains vast amounts of linguistic data for many languages Kilgarriff and Grefenstette_comma_ 2003 .
E06-2001.txt,9,One key issue for linguists and language technologists is how to access it.
E06-2001.txt,10,The drawbacks of using commercial search engines are presented in Kilgarriff 2003 .
E06-2001.txt,11,An alternative is to crawl the Web ourselves.1 We have done this for two languages_comma_ German and Italian_comma_ and here we report on the pipeline of processes which give us reasonably well behaved_comma_ clean corpora for each language.
E06-2001.txt,12,1Another Webaccessoption isAlexa http pages. alexa.com company index.html _comma_ who allow the user for a modest fee to access their cached Web directly.
E06-2001.txt,13,Using Alexa would mean one did not need to crawl however in our experience_comma_ crawling_comma_ given free software like Heritrix_comma_ is not the bottleneck.
E06-2001.txt,14,The point at which input is required is the filtering out of non linguistic material.
E06-2001.txt,15,We use the German corpus which was developed first as our example throughout.
E06-2001.txt,16,The procedure was carried on a server running RH Fedora Core 3 with 4 GB RAM_comma_ Dual Xeon 4.3 GHz CPUs and about 2.5 TB hard disk space.
E06-2001.txt,17,We are making the tools we develop as part of the project freely available_comma_2 in the hope of stimulating public sharing of resources and know how.
E06-2001.txt,18,2 Crawl seeding and crawling We would like a balanced resource_comma_ containing a range of types of text corresponding_comma_ to some degree_comma_ to the mix of texts we find in designed linguistic corpora Atkins et al._comma_ 1992 _comma_ though also including text types found on the Web which were not anticipated in linguists corpus design discussions.
E06-2001.txt,19,We do not want a blind sample dominated by product listings_comma_ catalogues and computer scientists bulletin boards.
E06-2001.txt,20,Our pragmatic solution is to query Google through its API service for random pairs of randomly selected content words in the target language.
E06-2001.txt,21,In preliminary experimentation_comma_ we found that single word queries yielded many inappropriate pages dictionary definitions of the word_comma_ top pages of companies with the word in their name _comma_ whereas combining more than two words retrieved pages with lists of words_comma_ rather than collected text.
E06-2001.txt,22,Ueyama 2006 showed how queries for words sampled from traditional written sources such as newspaper text and published essays tend to yield public sphere pages online newspaper_comma_ government and academic sites _comma_ whereas basic vocabulary everyday life words tend to yield personal pages blogs_comma_ bulletin boards .
E06-2001.txt,23,Since we wanted both types_comma_ we obtained seed URLs with queries 2http sslmitdev online.sslmit.unibo. it wac wac.php 87 for words from both kinds of sources.
E06-2001.txt,24,For German_comma_ we sampled 2000 mid frequency words from a corpus of the S uddeutsche Zeitung newspaper and paired them randomly.
E06-2001.txt,25,Then_comma_ we found a basic vocabulary list for German learners_comma_3 removed function words and particles and built 653 random pairs.
E06-2001.txt,26,We queried Google via its API retrieving maximally 10 pages for each pair.
E06-2001.txt,27,We then collapsed the URL list_comma_ insuring maximal sparseness by keeping only one randomly selected URL for each domain_comma_ leaving a list of 8626 seed URLs.
E06-2001.txt,28,They were fed to the crawler.
E06-2001.txt,29,The crawls are performed using the Heritrix crawler_comma_4 with a multi threaded breadth first crawling strategy.
E06-2001.txt,30,The crawl is limited to pages whose URL does not end in one of several suffixes that cue non html data .pdf_comma_ .jpeg_comma_ etc. 5 For German_comma_ the crawl is limited to sites from the .de and .at domains.
E06-2001.txt,31,Heritrix default crawling options are not modified in any other respect.
E06-2001.txt,32,We let the German crawl run for ten days_comma_ retrieving gzipped archives the Heritrix output format of about 85GB.
E06-2001.txt,33,3 Filtering We undertake some post processing on the basis of the Heritrix logs.
E06-2001.txt,34,We identify documents of mime type text html and size between 5 and 200KB.
E06-2001.txt,35,As observed by Fletcher 2004 very small documents tend to contain little genuine text 5KB counts as very small because of the html code overhead and very large documents tend to be lists of various sorts_comma_ such as library indices_comma_ store catalogues_comma_ etc. The logs also contain sha1 fingerprints_comma_ allowing us to identify perfect duplicates.
E06-2001.txt,36,After inspecting some of the duplicated documents about 50 pairs _comma_ we decided for a drastic policy if a document has at least one duplicate_comma_ we discard not only the duplicate s but also the document itself.
E06-2001.txt,37,We observed that_comma_ typically_comma_ such documents came from the same site and were warning messages_comma_ copyright statements and similar_comma_ of limited or no linguistic interest.
E06-2001.txt,38,While the strategy may lose some content_comma_ one of our general principles is that_comma_ given how vast the Web is_comma_ we can afford to privilege precision over recall.
E06-2001.txt,39,All the documents that passed the pre filtering 3http mypage.bluewin.ch a z cusipage 4http crawler.archive.org 5Further work should evaluate pros and cons of retrieving documents in other formats_comma_ e.g._comma_ Adobe pdf. stage are run through a perl program that performs 1 boilerplate stripping 2 function word filtering 3 porn filtering.
E06-2001.txt,40,Boilerplate stripping By boilerplate we mean all those components of Web pages which are the same across many pages.
E06-2001.txt,41,We include stripping out HTML markup_comma_ javascript and other non linguistic material in this phase.
E06-2001.txt,42,We aimed to identify and remove sections of a document that contain link lists_comma_ navigational information_comma_ fixed notices_comma_ and other sections poor in human produced connected text.
E06-2001.txt,43,For purposes of corpus construction_comma_ boilerplate removal is critical as it will distort statistics collected from the corpus.6 Weadopted theheuristic used inthe Hyppia project BTE tool_comma_7 content rich sections of a page will have a low html tag density_comma_ whereas boilerplate is accompanied by a wealth of html because of special formatting_comma_ newlines_comma_ links_comma_ etc. The method is based on general properties of Web documents_comma_ so is relatively independent of language and crawling strategy.
E06-2001.txt,44,Function word and pornography filtering Connected text in sentences reliably contains a high proportion of function words Baroni_comma_ to appear _comma_ so_comma_ if a page does not meet this criterion we reject it.
E06-2001.txt,45,The German function word list contains 124 terms.
E06-2001.txt,46,We require that a minimum of 10 types and 30 tokens appear in a page_comma_ with a ratio of function words to total words of at least one quarter.
E06-2001.txt,47,The filter also works as a simple language identifier.8 Finally_comma_ we use a stop list of words likely to occur in pornographic Web pages_comma_ not out of prudery_comma_ but because they tend to contain randomly generated text_comma_ long keyword lists and other linguistically problematic elements.
E06-2001.txt,48,We filter out documents that have at least three types or ten tokens from a list of words highly used in pornography.
E06-2001.txt,49,The list was derived from the analysis of pornographic pages harvested in a previous crawl.
E06-2001.txt,50,This isnot entirely satisfactory_comma_ since some ofthe words 6We note that this phase currently removes the links from the text_comma_ so we can no longer explore the graph structure of the dataset.
E06-2001.txt,51,In future we may retain link structure_comma_ to support research into the relation between it and linguistic characteristics.
E06-2001.txt,52,7http www.smi.ucd.ie hyppia 8Of course_comma_ these simple methods will not filter out all machine generated text typically produced as part of search engine ranking scams or for other shady purposes sometimes this appears to have been generated with a bigram language model_comma_ and thus identifying it with automated techniques is far from trivial.
E06-2001.txt,53,88 in the list_comma_ taken in isolation_comma_ are wholly innocent fat_comma_ girls_comma_ tongue_comma_ etc. We shall revisit the strategy in due course.
E06-2001.txt,54,This filtering took 5 days and resulted in a version of the corpus containing 4.86M documents for a total of 20GB of uncompressed data.
E06-2001.txt,55,4 Near duplicate detection We use a simplified version of the shingling algorithm Broder et al._comma_ 1997 .
E06-2001.txt,56,For each document_comma_ after removing all function words_comma_ we take fingerprints of a fixed number s of randomly selected ngrams then_comma_ for each pair of documents_comma_ we count the number of shared n grams_comma_ which can be seen as an unbiased estimate of the overlap between the two documents Broder et al._comma_ 1997 Chakrabarti_comma_ 2002 .
E06-2001.txt,57,We look for pairs of documents sharing more than t n grams_comma_ and we discard one of the two.
E06-2001.txt,58,After preliminary experimentation_comma_ we chose to extract 25 5 grams from each document_comma_ and to treat as near duplicates documents that shared at least two of these 5 grams.
E06-2001.txt,59,Near duplicate spotting on the German corpus took about 4 days.
E06-2001.txt,60,2_comma_466_comma_271 near duplicates were removed.
E06-2001.txt,61,The corpus size decreased to 13GB.
E06-2001.txt,62,Most of the processing time was spent in extracting the n grams and adding the corresponding fingerprints to the database which could be parallelized .
E06-2001.txt,63,5 Part of speech tagging lemmatization and post annotation cleaning We performed German part of speech tagging and lemmatization with TreeTagger.9 Annotation took 5 days.
E06-2001.txt,64,The resulting corpus contains 2.13B words_comma_ or 34GB of data including annotation.
E06-2001.txt,65,After inspecting various documents from the annotated corpus_comma_ we decided to perform a further round of cleaning.
E06-2001.txt,66,There are two reasons for this first_comma_ we can exploit the annotation to find other anomalous documents_comma_ through observing where the distribution of parts of speech tags is very unusual and thus not likely to contain connected text.
E06-2001.txt,67,Second_comma_ the TreeTagger was not trained on Web data_comma_ and thus its performance on texts that are heavy on Web like usage e.g._comma_ texts all in lowercase_comma_ colloquial forms of inflected verbs_comma_ etc. is dismal.
E06-2001.txt,68,While a better solution to this second problem would be to re train the tagger on Web 9http www.ims.uni stuttgart.de projekte corplex TreeTagger data ultimately_comma_ the documents displaying the second problem might be among the most interesting ones to have in the corpus _comma_ for now we try to identify the most problematic documents through automated criteria and discard them.
E06-2001.txt,69,The cues we used included the number of words not recognised by the lemmatizer the proportion of words with upper case initial letters proportion of nouns_comma_ and proportion of sentence markers.
E06-2001.txt,70,After this further processing step_comma_ the corpus contains 1_comma_870_comma_259 documents from 10818 different domains_comma_ and its final size is 1.71 billion tokens 26GB of data_comma_ with annotation .
E06-2001.txt,71,The final size of the Italian corpus is 1_comma_875_comma_337 documents and about 1.9 billion tokens.
E06-2001.txt,72,6 Indexing and Web user interface We believe that matters of efficient indexing and user friendly interfacing will be crucial to the success of our initiative_comma_ both because many linguists will lack the relevant technical skills to write their own corpus access routines_comma_ and because we shall not publicly distribute the corpora for copyright reasons an advanced interface that allows linguists to do actual research on the corpus including the possibility of saving settings and results across sessions will allow us to make the corpus widely available while keeping it on our servers.10 We are using the Sketch Engine_comma_11 a corpus query tool which has been widely used in lexicography and which supports queries combining regular expressions and boolean operators over words_comma_ lemmas and part of speech tags.
E06-2001.txt,73,7 Comparison with other corpora We would like to compare the German Web corpus to an existing balanced corpus of German attempting to represent a broad range of genres and topics.
E06-2001.txt,74,Unfortunately_comma_ as far as we know no resource of this sort is publicly available which is one of the reasons why we are interested in developing the German Web corpus in the first instance. Instead_comma_ we use a corpus of newswire articles from the Austria Presse Agentur APA_comma_ kindly provided to us by OFAI as our reference 10The legal situation is of course complex.
E06-2001.txt,75,We consider that our case is equivalent to that of other search engines_comma_ and that offering linguistically encoded snippets of pages to researchers does not go beyond the fair use terms routinely invoked by search engine companies in relation to Web page caching.
E06-2001.txt,76,11http www.sketchengine.co.uk 89 WEB APA ich hier APA NATO dass wir Schlu EU und man Prozent Forts sie nicht Mill AFP ist das MRD Dollar oder sind Wien Reuters kann so Kosovo Dienstag du mir DPA Mittwoch wenn ein US Donnerstag was da am sei Table 1 Typical Web and APA words point.
E06-2001.txt,77,This corpus contains 28M tokens_comma_ and_comma_ despite its uniformity in terms of genre and restricted thematic range_comma_ it has been successfully employed as a general purpose German corpus in many projects.
E06-2001.txt,78,After basic regular expressionbased normalization and filtering_comma_ the APA contains about 500K word types_comma_ the Web corpus about 7.4M.
E06-2001.txt,79,There is a large overlap among the 30 most frequent words in both corpora 24 out of 30 words are shared.
E06-2001.txt,80,The non overlapping words occurring in the Web top 30 only are function words sie she _comma_ ich I _comma_ werden become be _comma_ oder or _comma_ sind are _comma_ er he .
E06-2001.txt,81,The words only in the APA list show a bias towards newswire specific vocabulary APA_comma_ Prozent percent _comma_ Schlu closure and temporal expressions that are also typical of newswires am at _comma_ um on the _comma_ nach after .
E06-2001.txt,82,Of the 232_comma_322 hapaxes words occurring only once in the APA corpus_comma_ 170_comma_328 73 occur in the Web corpus as well.12 89 of these APA hapaxes occur more than once in the Web corpus_comma_ suggesting how the Web data will help address data sparseness issues.
E06-2001.txt,83,Adopting the methodology of Sharoff 2006 _comma_ we then extracted the 20 words most characteristics of the Web corpus vs. APA and vice versa_comma_ based on the log likelihood ratio association measure.
E06-2001.txt,84,Results are presented in Table 1.
E06-2001.txt,85,The APA corpus has a strong bias towards newswire parlance acronyms and named entities_comma_ temporal expressions_comma_ financial terms_comma_ toponyms _comma_ whereas the terms that come out as most typical of the Web corpus are function words that are not strongly connected with any particular topic or genre.
E06-2001.txt,86,Several of these top ranked function words mark first and second person forms ich_comma_ du_comma_ wir_comma_ mir .
E06-2001.txt,87,This preliminary comparison both functioned as a sanity check _comma_ showing that there is consider12Less than 1 of the Web corpus hapaxes are attested in the APA corpus. able overlap between our corpus and a smaller corpus used in previous research_comma_ and suggested that the Web corpus has more a higher proportion of interpersonal material.
E06-2001.txt,88,8 Conclusion We have developed very large corpora from the Web for German and Italian with other languages to follow .
E06-2001.txt,89,We have filtered and cleaned the text so that the obvious problems with using the Web as a corpus for linguistic research do not hold.
E06-2001.txt,90,Preliminary evidence suggests the balance of our German corpus compares favourably with that of a newswire corpus though of course any such claim begs a number of open research questions about corpus comparability .
E06-2001.txt,91,We have lemmatised and part of speech tagged the data and loaded it into a corpus query tool supporting sophisticated linguistic queries_comma_ and made it available to all. .
E06-1015.txt,1,recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately_comma_ they show a an inherent super linear complexity and b a lower accuracy than traditional attribute value methods.
E06-1015.txt,2,In this paper_comma_ we show that tree kernels are very helpful in the processing of natural language as a we provide a simple algorithm to compute tree kernels in linear average running time and b our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
E06-1015.txt,3,Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
E06-1015.txt,5,In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks_comma_ e.g. syntactic parsing Collins and Duffy_comma_ 2002 _comma_ relation extraction Zelenko et al._comma_ 2003 _comma_ Named Entity recognition Cumby and Roth_comma_ 2003 Culotta and Sorensen_comma_ 2004 and Semantic Parsing Moschitti_comma_ 2004 .
E06-1015.txt,6,The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application.
E06-1015.txt,7,In contrast_comma_ their major drawback are a the computational time complexity which is superlinear in the number of tree nodes and b the accuracy that they produce is often lower than the one provided by linear models on manually designed features.
E06-1015.txt,8,To solve problem a _comma_ a linear complexity algorithm for the subtree ST kernel computation_comma_ was designed in Vishwanathan and Smola_comma_ 2002 .
E06-1015.txt,9,Unfortunately_comma_ the ST set is rather poorer than the one generated by the subset tree SST kernel designed in Collins and Duffy_comma_ 2002 .
E06-1015.txt,10,Intuitively_comma_ an ST rooted in a node n of the target tree always contains all n s descendants until the leaves.
E06-1015.txt,11,This does not hold for the SSTs whose leaves can be internal nodes.
E06-1015.txt,12,To solve the problem b _comma_ a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy.
E06-1015.txt,13,On the one hand_comma_ SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown_comma_ for example_comma_ in Zelenko et al._comma_ 2003 Moschitti_comma_ 2004 .
E06-1015.txt,14,On the other hand_comma_ if the SST space contains too many irrelevant features_comma_ overfitting may occur and decrease the classification accuracy Cumby and Roth_comma_ 2003 .
E06-1015.txt,15,As a consequence_comma_ the fewer features of the ST approach may be more appropriate.
E06-1015.txt,16,In this paper_comma_ we aim to solve the above problems.
E06-1015.txt,17,We present a an algorithm for the evaluation of the ST and SST kernels which runs in linear average time and b a study of the impact of diverse tree kernels on the accuracy of Support Vector Machines SVMs .
E06-1015.txt,18,Our fast algorithm computes the kernels between two syntactic parse trees in O m n average time_comma_ where m and n are the number of nodes in the two trees.
E06-1015.txt,19,This low complexity allows SVMs to carry out experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial ker113 nel_comma_ widely used on large experimentation e.g.
E06-1015.txt,20,Pradhan et al._comma_ 2004 .
E06-1015.txt,21,To confirm such hypothesis_comma_ we measured the impact of the algorithm on the time required by SVMs for the learning of about 122_comma_774 predicate argument examples annotated in PropBank Kingsbury and Palmer_comma_ 2002 and 37_comma_948 instances annotated in FrameNet Fillmore_comma_ 1982 .
E06-1015.txt,22,Regarding the classification properties_comma_ we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features Gildea and Jurafsky_comma_ 2002 .
E06-1015.txt,23,The results show that_comma_ on both PropBank and FrameNet datasets_comma_ the SST based kernel_comma_ i.e. the richest in terms of substructures_comma_ produces the highest SVM accuracy.
E06-1015.txt,24,When SSTs are combined with the manual designed features_comma_ we always obtain the best figure classifier.
E06-1015.txt,25,This suggests that the many fragments included in the SST space are relevant and_comma_ since their manual design may be problematic requiring a higher programming effort and deeper knowledge of the linguistic phenomenon _comma_ tree kernels provide a remarkable help in feature engineering.
E06-1015.txt,26,In the remainder of this paper_comma_ Section 2 describes the parse tree kernels and our fast algorithm.
E06-1015.txt,27,Section 3 introduces the predicate argument classification problem and its solution.
E06-1015.txt,28,Section 4 shows the comparative performance in term of the execution time and accuracy.
E06-1015.txt,29,Finally_comma_ Section 5 discusses the related work whereas Section 6 summarizes the conclusions.
E06-1015.txt,30,2 Fast Parse Tree Kernels The kernels that we consider represent trees in terms of their substructures fragments .
E06-1015.txt,31,These latter define feature spaces which_comma_ in turn_comma_ are mapped into vector spaces_comma_ e.g.
E06-1015.txt,33,The associated kernel function measures the similarity between two trees by counting the number of their common fragments.
E06-1015.txt,34,More precisely_comma_ a kernel function detects if a tree subpart common to both trees belongs to the feature space that we intend to generate.
E06-1015.txt,35,For such purpose_comma_ the fragment types need to be described.
E06-1015.txt,36,We consider two important characterizations the subtrees STs and the subset trees SSTs .
E06-1015.txt,37,2.1 Subtrees and Subset Trees In our study_comma_ we consider syntactic parse trees_comma_ consequently_comma_ each node with its children is associated with a grammar production rule_comma_ where the symbol at left hand side corresponds to the parent node and the symbols at right hand side are associated with its children.
E06-1015.txt,38,The terminal symbols of the grammar are always associated with the leaves of the tree.
E06-1015.txt,39,For example_comma_ Figure 1 illustrates the syntactic parse of the sentence Mary brought a cat to school .
E06-1015.txt,40,S N VP VP V NP PP PP IN N N school N school The root A leaf S N NP D N VP VMary to brought a cat PP IN A subtree Figure 1 A syntactic parse tree.
E06-1015.txt,41,We define as a subtree ST any node of a tree along with all its descendants.
E06-1015.txt,42,For example_comma_ the line in Figure 1 circles the subtree rooted in the NP node.
E06-1015.txt,43,A subset tree SST is a more general structure.
E06-1015.txt,44,The difference with the subtrees is that the leaves can be associated with non terminal symbols.
E06-1015.txt,45,The SSTs satisfy the constraint that they are generated by applying the same grammatical rule set which generated the original tree.
E06-1015.txt,46,For example_comma_ S N VP is a SST of the tree in Figure 1 which has two non terminal symbols_comma_ N and VP_comma_ as leaves.
E06-1015.txt,47,S N NP D N VP VMary brought a cat NP D N a cat N cat D a V brought N Mary NP D N VP V brought a cat Figure 2 A syntactic parse tree with its subtrees STs .
E06-1015.txt,48,NP D N a cat NP D N NP D N a NP D N NP D N VP V brought a cat cat NP D N VP V a cat NP D N VP V N cat D a V brought N Mary Figure 3 A tree with some of its subset trees SSTs .
E06-1015.txt,49,Given a syntactic tree we can use as feature representation the set of all its STs or SSTs.
E06-1015.txt,50,For example_comma_ Figure 2 shows the parse tree of the sentence Mary brought a cat together with its 6 STs_comma_ whereas Figure 3 shows 10 SSTs out of 17 of the subtree of Figure 2 rooted in VP.
E06-1015.txt,51,The 114 high different number of substructures gives an intuitive quantification of the different information level between the two tree based representations.
E06-1015.txt,52,2.2 The Tree Kernel Functions The main idea of tree kernels is to compute the number of the common substructures between two trees T1 and T2 without explicitly considering the whole fragment space.
E06-1015.txt,53,For this purpose_comma_ we slightly modified the kernel function proposed in Collins and Duffy_comma_ 2002 by introducing a parameter which enables the ST or the SST evaluation.
E06-1015.txt,54,Given the set of fragments f1_comma_f2_comma_.. F_comma_ we defined the indicator function Ii n which is equal 1 if the target fi is rooted at node n and 0 otherwise.
E06-1015.txt,55,We define K T1_comma_T2 summationdisplay n1 NT1 summationdisplay n2 NT2 n1_comma_n2 1 where NT1 and NT2 are the sets of the T1 s and T2 s nodes_comma_ respectively and n1_comma_n2 summationtext F i 1 Ii n1 Ii n2 .
E06-1015.txt,56,This latter is equal to the number of common fragments rooted in the n1 and n2 nodes.
E06-1015.txt,57,We can compute as follows 1. if the productions at n1 and n2 are different then n1_comma_n2 0 2. if the productions at n1 and n2 are the same_comma_ and n1 and n2 have only leaf children i.e. they are pre terminals symbols then n1_comma_n2 1 3. if the productions at n1 and n2 are the same_comma_ and n1 and n2 are not pre terminals then n1_comma_n2 nc n1 productdisplay j 1  cjn1_comma_cjn2 2 where  0_comma_1 _comma_ nc n1 is the number of the children of n1 and cjn is the j th child of the node n.
E06-1015.txt,58,Note that_comma_ since the productions are the same_comma_ nc n1 nc n2 .
E06-1015.txt,59,When  0_comma_ n1_comma_n2 is equal 1 only if j cjn1_comma_cjn2 1_comma_ i.e. all the productions associated with the children are identical.
E06-1015.txt,60,By recursively applying this property_comma_ it follows that the subtrees in n1 and n2 are identical.
E06-1015.txt,62,1 evaluates the subtree ST kernel.
E06-1015.txt,63,When  1_comma_ n1_comma_n2 evaluates the number of SSTs common to n1 and n2 as proved in Collins and Duffy_comma_ 2002 .
E06-1015.txt,64,Additionally_comma_ we study some variations of the above kernels which include the leaves in the fragment space.
E06-1015.txt,65,For this purpose_comma_ it is enough to add the condition 0. if n1 and n2 are leaves and their associated symbols are equal then n1_comma_n2 1_comma_ to the recursive rule set for the evaluation Zhang and Lee_comma_ 2003 .
E06-1015.txt,66,We will refer to such extended kernels as ST bow and SST bow bag ofwords .
E06-1015.txt,67,Moreover_comma_ we add the decay factor by modifying steps 2 and 3 as follows1 2.
E06-1015.txt,68,n1_comma_n2  _comma_ 3.
E06-1015.txt,69,n1_comma_n2  producttextnc n1 j 1  cjn1_comma_cjn2 .
E06-1015.txt,70,The computational complexity of Eq.
E06-1015.txt,71,1 is O NT1  NT2 .
E06-1015.txt,72,We will refer to this basic implementation as the Quadratic Tree Kernel QTK .
E06-1015.txt,73,However_comma_ as observed in Collins and Duffy_comma_ 2002 this worst case is quite unlikely for the syntactic trees of natural language sentences_comma_ thus_comma_ we can design algorithms that run in linear time on average. function Evaluate Pair Set Tree T1_comma_ T2 returns NODE PAIR SET LIST L1_comma_L2 NODE PAIR SET Np begin L1 T1.ordered list L2 T2.ordered list the lists were sorted at loading time n1 extract L1  get the head element and n2 extract L2  remove it from the list while n1 and n2 are not NULL if production of n1 production of n2 then n2 extract L2 else if production of n1 production of n2 then n1 extract L1 else while production of n1  production of n2 while production of n1  production of n2 add n1_comma_n2 _comma_ Np n2 get next elem L2  get the head element and move the pointer to the next element end n1 extract L1 reset L2  set the pointer at the first element end end return Np end Table 1 Pseudo code for fast evaluation of the node pair sets used in the fast Tree Kernel.
E06-1015.txt,74,2.3 A Fast Tree Kernel FTK To compute the kernels defined in the previous section_comma_ we sum the function for each pair n1_comma_n2 NT1 NT2 Eq. 1 .
E06-1015.txt,75,When the productions associated with n1 and n2 are different_comma_ we can avoid to evaluate n1_comma_n2 since it is 0.
E06-1015.txt,76,1To have a similarity score between 0 and 1_comma_ we also apply the normalization in the kernel space_comma_ i.e.
E06-1015.txt,77,Kprime T1_comma_T2 K T1_comma_T2 K T1_comma_T1 K T2_comma_T2 .
E06-1015.txt,78,115 S N NP D N VP VMary to brought a cat PP IN N school Arg.
E06-1015.txt,81,1 Predicate NP D N VP V brought a cat SArg1 VP V to brought PP IN N school S N VMary brought VP SArg0 SArgM Figure 4 Tree substructure space for predicate argument classification.
E06-1015.txt,82,Thus_comma_ we look for a node pair set Np n1_comma_n2 NT1 NT2 p n1 p n2 _comma_ where p n returns the production rule associated with n.
E06-1015.txt,83,To efficiently build Np_comma_ we i extract the L1 and L2 lists of the production rules from T1 and T2_comma_ ii sort them in the alphanumeric order and iii scan them to find the node pairs n1_comma_n2 such that p n1 p n2  L1 L2.
E06-1015.txt,84,Step iii may require only O NT1 NT2 time_comma_ but_comma_ if p n1 appears r1 times in T1 and p n2 is repeated r2 times in T2_comma_ we need to consider r1 r2 pairs.
E06-1015.txt,85,The formal algorithm is given in Table 1.
E06-1015.txt,86,Note that a The list sorting can be done only once at the data preparation time i.e. before training in O NT1 log NT1 . b The algorithm shows that the worst case occurs when the parse trees are both generated using only one production rule_comma_ i.e. the two internal while cycles carry out NT1 NT2 iterations.
E06-1015.txt,87,In contrast_comma_ two identical parse trees may generate a linear number of non null pairs if there are few groups of nodes associated with the same production rule.
E06-1015.txt,88,c Such approach is perfectly compatible with the dynamic programming algorithm which computes .
E06-1015.txt,89,In fact_comma_ the only difference with the original approach is that the matrix entries corresponding to pairs of different production rules are not considered.
E06-1015.txt,90,Since such entries contain null values they do not affect the application of the original dynamic programming.
E06-1015.txt,91,Moreover_comma_ the order of the pair evaluation can be established at run time_comma_ starting from the root nodes towards the children.
E06-1015.txt,92,3 A Semantic Application of Parse Tree Kernels An interesting application of the SST kernel is the classification of the predicate argument structures defined in PropBank Kingsbury and Palmer_comma_ 2002 or FrameNet Fillmore_comma_ 1982 .
E06-1015.txt,93,Figure 4 shows the parse tree of the sentence Mary brought a cat to school along with the predicate argument annotation proposed in the PropBank project.
E06-1015.txt,94,Only verbs are considered as predicates whereas arguments are labeled sequentially from ARG0 to ARG9.
E06-1015.txt,95,Also in FrameNet predicate argument information is described but for this purpose richer semantic structures called Frames are used.
E06-1015.txt,96,The Frames are schematic representations of situations involving various participants_comma_ properties and roles in which a word may be typically used.
E06-1015.txt,97,Frame elements or semantic roles are arguments of predicates called target words.
E06-1015.txt,98,For example the following sentence is annotated according to the ARREST frame Time One Saturday night Authorities police in Brooklyn Target apprehended  Suspect sixteen teenagers .
E06-1015.txt,99,The roles Suspect and Authorities are specific to the frame.
E06-1015.txt,100,The common approach to learn the classification of predicate arguments relates to the extraction of features from the syntactic parse tree of the target sentence.
E06-1015.txt,101,In Gildea and Jurafsky_comma_ 2002 seven different features2_comma_ which aim to capture the relation between the predicate and its arguments_comma_ were proposed.
E06-1015.txt,102,For example_comma_ the Parse Tree Path of the pair brought_comma_ ARG1 in the syntactic tree of Figure 4 is V VP NP.
E06-1015.txt,103,It encodes the dependency between the predicate and the argument as a sequence of nonterminal labels linked by direction symbols up or down .
E06-1015.txt,104,An alternative tree kernel representation_comma_ proposed in Moschitti_comma_ 2004 _comma_ is the selection of the minimal tree subset that includes a predicate with only one of its arguments.
E06-1015.txt,105,For example_comma_ in Figure 4_comma_ the substructures inside the three frames are the semantic syntactic structures associated with the three arguments of the verb to bring_comma_ i.e.
E06-1015.txt,106,SARG0_comma_ SARG1 and SARGM.
E06-1015.txt,107,Given a feature representation of predicate ar2Namely_comma_ they are Phrase Type_comma_ Parse Tree Path_comma_ Predicate Word_comma_ Head Word_comma_ Governing Category_comma_ Position and Voice.
E06-1015.txt,108,116 guments_comma_ we can build an individual ONE vs ALL OVA classifier Ci for each argument i.
E06-1015.txt,109,As a final decision of the multiclassifier_comma_ we select the argument type ARGt associated with the maximum value among the scores provided by the Ci_comma_ i.e. t argmaxi S score Ci _comma_ where S is the set of argument types.
E06-1015.txt,110,We adopted the OVA approach as it is simple and effective as showed in Pradhan et al._comma_ 2004 .
E06-1015.txt,111,Note that the representation in Figure 4 is quite intuitive and_comma_ to conceive it_comma_ the designer requires much less linguistic knowledge about semantic roles than those necessary to define relevant features manually.
E06-1015.txt,112,To understand such point_comma_ we should make a step back before Gildea and Jurafsky defined the first set of features for Semantic Role Labeling SRL .
E06-1015.txt,113,The idea that syntax may have been useful to derive semantic information was already inspired by linguists_comma_ but from a machine learning point of view_comma_ to decide which tree fragments may have been useful for semantic role labeling was not an easy task.
E06-1015.txt,114,In principle_comma_ the designer should have had to select and experiment all possible tree subparts.
E06-1015.txt,115,This is exactly what the tree kernels can automatically do the designer just need to roughly select the interesting whole subtree correlated with the linguistic phenomenon and the tree kernel will generate all possible syntactic features from it.
E06-1015.txt,116,The task of selecting the most relevant substructures is carried out by the kernel machines themselves.
E06-1015.txt,117,4 The Experiments The aim of the experiments is twofold.
E06-1015.txt,118,On the one hand_comma_ we show that the FTK running time is linear on the average case and is much faster than QTK.
E06-1015.txt,119,This is accomplished by measuring the learning time and the average kernel computation time.
E06-1015.txt,120,On the other hand_comma_ we study the impact of the different tree based kernels on the predicate argument classification accuracy.
E06-1015.txt,121,4.1 Experimental Set up We used two different corpora PropBank www.cis.upenn.edu ace along with PennTree bank 2 Marcus et al._comma_ 1993 and FrameNet.
E06-1015.txt,122,PropBank contains about 53_comma_700 sentences and a fixed split between training and testing which has been used in other researches_comma_ e.g.
E06-1015.txt,123,Gildea and Palmer_comma_ 2002 Pradhan et al._comma_ 2004 .
E06-1015.txt,124,In this split_comma_ sections from 02 to 21 are used for training_comma_ section 23 for testing and sections 1 and 22 as developing set.
E06-1015.txt,125,We considered a total of 122_comma_774 and 7_comma_359 arguments from ARG0 to ARG9_comma_ ARGA and ARGM in training and testing_comma_ respectively.
E06-1015.txt,126,Their tree structures were extracted from the Penn Treebank.
E06-1015.txt,127,It should be noted that the main contribution to the global accuracy is given by ARG0_comma_ ARG1 and ARGM.
E06-1015.txt,128,From the FrameNet corpus http www.icsi .berkeley.edu framenet _comma_ we extracted all 24_comma_558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 www.senseval.org .
E06-1015.txt,129,We mapped together the semantic roles having the same name and we considered only the 18 most frequent roles associated with verbal predicates_comma_ for a total of 37_comma_948 arguments.
E06-1015.txt,130,We randomly selected 30 of sentences for testing and 70 for training.
E06-1015.txt,131,Additionally_comma_ 30 of training was used as a validationset.
E06-1015.txt,132,Note that_comma_ since the FrameNet data does not include deep syntactic tree annotation_comma_ we processed the FrameNet data with Collins parser Collins_comma_ 1997 _comma_ consequently_comma_ the experiments on FrameNet relate to automatic syntactic parse trees.
E06-1015.txt,133,The classifier evaluations were carried out with the SVM light TK software available at http ai nlp.info.uniroma2.it moschitti which encodes ST and SST kernels in the SVMlight software Joachims_comma_ 1999 .
E06-1015.txt,134,We used the default linear Linear and polynomial Poly kernels for the evaluations with the standard features defined in Gildea and Jurafsky_comma_ 2002 .
E06-1015.txt,135,We adopted the default regularization parameter i.e._comma_ the average of 1 vectorx and we tried a few cost factor values i.e._comma_ j 1_comma_3_comma_7_comma_10_comma_30_comma_100 to adjust the rate between Precision and Recall on the validation set.
E06-1015.txt,136,For the ST and SST kernels_comma_ we derived that the best see Section 2.2 were 1 and 0.4_comma_ respectively.
E06-1015.txt,137,The classification performance was evaluated using the F1 measure3 for the single arguments and the accuracy for the final multiclassifier.
E06-1015.txt,138,This latter choice allows us to compare our results with previous literature work_comma_ e.g.
E06-1015.txt,139,Gildea and Jurafsky_comma_ 2002 Pradhan et al._comma_ 2004 .
E06-1015.txt,140,4.2 Time Complexity Experiments In this section we compare our Fast Tree Kernel FTK approach with the Quadratic Tree Kernel QTK algorithm.
E06-1015.txt,141,The latter refers to the naive evaluation of Eq.
E06-1015.txt,142,1 as presented in Collins and Duffy_comma_ 2002 .
E06-1015.txt,143,3F1 assigns equal importance to Precision P and Recall R_comma_ i.e.
E06-1015.txt,144,f1 2P RP R .
E06-1015.txt,145,117 Figure 5 shows the learning time4 of the SVMs using QTK and FTK over the SST structures for the classification of one large argument i.e. ARG0 _comma_ according to different percentages of training data.
E06-1015.txt,146,We note that_comma_ with 70 of the training data_comma_ FTK is about 10 times faster than QTK.
E06-1015.txt,147,With all the training data FTK terminated in 6 hours whereas QTK required more than 1 week. y 0.0006x 2 0.001x y 0.0045x 2 0.1004x 0 5 10 15 20 25 30 35 0 10 20 30 40 50 60 70 80 90 100 Training Data Ho ur s FTK QTK Figure 5 ARG0 classifier learning time according to different training percentages. y 0.04x 2 0.05x y 0.14x 0 20 40 60 80 100 120 10 15 20 25 30 35 40 45 50 55 60 Number of Tree Nodes  s ec on ds FTK QTK Figure 6 Average time in seconds for the QTK and FTK evaluations.
E06-1015.txt,148,0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0 10 20 30 40 50 60 70 80 90 100 Training Data Ac cu ra cy ST SST ST bow SST bow Linear Poly Figure 7 Multiclassifier accuracy according to different training set percentages.
E06-1015.txt,149,4We run the experiments on a Pentium 4_comma_ 2GHz_comma_ with 1 Gb ram.
E06-1015.txt,150,The above results are quite interesting because they show that 1 we can use tree kernels with SVMs on huge training sets_comma_ e.g. on 122_comma_774 instances and 2 the time needed to converge is approximately the one required by SVMs when using polynomial kernel.
E06-1015.txt,151,This latter shows the minimal complexity needed to work in the dual space.
E06-1015.txt,152,To study the FTK running time_comma_ we extracted from PennTree bank the first 500 trees5 containing exactly n nodes_comma_ then_comma_ we evaluated all 25_comma_000 possible tree pairs.
E06-1015.txt,153,Each point of the Figure 6 shows the average computation time on all the tree pairs of a fixed size n.
E06-1015.txt,154,In the figures_comma_ the trend lines which best interpolates the experimental values are also shown.
E06-1015.txt,155,It clearly appears that the training time is quadratic as SVMs have quadratic learning time complexity see Figure 5 whereas the FTK running time has a linear behavior Figure 6 .
E06-1015.txt,156,The QTK algorithm shows a quadratic running time complexity_comma_ as expected.
E06-1015.txt,157,4.3 Accuracy of the Tree Kernels In these experiments_comma_ we investigate which kernel is the most accurate for the predicate argument classification.
E06-1015.txt,158,First_comma_ we run ST_comma_ SST_comma_ ST bow_comma_ SST bow_comma_ Linear and Poly kernels over different training set size of PropBank.
E06-1015.txt,159,Figure 7 shows the learning curves associated with the above kernels for the SVMbased multiclassifier.
E06-1015.txt,160,We note that a SSTs have a higher accuracy than STs_comma_ b bow does not improve either ST or SST kernels and c in the final part of the plot SST shows a higher gradient than ST_comma_ Linear and Poly.
E06-1015.txt,161,This latter produces the best accuracy 90.5 in line with the literature findings using standard features and polynomial SVMs_comma_ e.g.
E06-1015.txt,162,87.1 6 in Pradhan et al._comma_ 2004 .
E06-1015.txt,163,Second_comma_ in tables 2 and 3_comma_ we report the results using all available training data_comma_ on PropBank and FrameNet test sets_comma_ respectively.
E06-1015.txt,164,Each row of the two tables shows the F1 measure of the individual classifiers using different kernels whereas the last column illustrates the global accuracy of the multiclassifier.
E06-1015.txt,165,5We measured also the computation time for the incomplete trees associated with the predicate argument structures see Section 3 we obtained the same results.
E06-1015.txt,166,6The small difference 2.4 is mainly due to the different treatment of ARGMs we built a single ARGM class for all subclasses_comma_ e.g.
E06-1015.txt,167,ARGM LOC and ARGM TMP_comma_ whereas in Pradhan et al._comma_ 2004 _comma_ the ARGMs_comma_ were evaluated separately.
E06-1015.txt,168,118 We note that_comma_ the F1 of the single arguments across the different kernels follows the same behavior of the global multiclassifier accuracy.
E06-1015.txt,169,On FrameNet_comma_ the bow impact on the ST and SST accuracy is higher than on PropBank as it produces an improvement of about 1.5 .
E06-1015.txt,170,This suggests that 1 to detect semantic roles_comma_ lexical information is very important_comma_ 2 bow give a higher contribution as errors in POS tagging make the word POS fragments less reliable and 3 as the FrameNet trees are obtained with the Collins syntactic parser_comma_ tree kernels seem robust to incorrect parse trees.
E06-1015.txt,171,Third_comma_ we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort Gildea and Jurafsky_comma_ 2002 .
E06-1015.txt,172,On the contrary_comma_ the choice of subtrees suitable to syntactically characterize a target phenomenon seems a easier task see Section 3 for the predicate argument case .
E06-1015.txt,173,Moreover_comma_ by combining polynomial and SST kernels_comma_ we can improve the classification accuracy Moschitti_comma_ 2004 _comma_ i.e. tree kernels provide the learning algorithm with many relevant fragments which hardly can be designed by hand.
E06-1015.txt,174,In fact_comma_ as many predicate argument structures are quite large up to 100 nodes they contain many fragments.
E06-1015.txt,175,ARGs ST SST ST bow SST bow Linear Poly ARG0 86.5 88.0 86.9 88.4 88.6 90.6 ARG1 83.1 87.4 82.8 86.7 85.9 90.8 ARG2 58.0 67.6 58.9 66.7 65.5 80.4 ARG3 35.7 37.5 39.3 41.2 51.9 60.4 ARG4 62.7 65.6 63.3 63.9 66.2 70.0 ARGM 92.0 94.2 92.0 93.7 94.9 95.3 Acc.
E06-1015.txt,176,84.6 87.7 84.8 87.5 87.6 90.7 Table 2 Evaluation of Kernels on PropBank.
E06-1015.txt,177,Roles ST SST ST bow SST bow Linear Poly agent 86.9 87.8 89.2 90.2 89.8 91.7 theme 76.1 79.2 78.5 80.7 82.9 90.4 goal 77.9 78.9 78.2 80.1 80.2 85.8 path 82.8 84.4 83.7 85.1 81.3 85.5 manner 79.9 82.0 81.3 82.5 70.8 80.5 source 85.6 87.7 86.9 87.8 86.5 89.8 time 76.3 78.3 77.0 79.1 61.8 68.3 reason 75.9 77.3 78.9 81.4 82.9 86.4 Acc.
E06-1015.txt,178,80.0 81.2 81.3 82.9 82.3 85.6 18 roles Table 3 Evaluation of the Kernels on FrameNet semantic roles.
E06-1015.txt,179,Finally_comma_ to study the combined kernels_comma_ we applied the K1 K2 formula_comma_ where K1 is either the Linear or the Poly kernel and K2 is the ST Corpus Poly ST Linear SST Linear ST Poly SST Poly PropBank 90.7 88.6 89.4 91.1 91.3 FrameNet 85.6 85.3 85.8 87.5 87.2 Table 4 Multiclassifier accuracy using Kernel Combinations. or the SST kernel.
E06-1015.txt,180,Table 4 shows the results of four kernel combinations.
E06-1015.txt,181,We note that_comma_ a STs and SSTs improve Poly about 0.5 and 2 percent points on PropBank and FrameNet_comma_ respectively and b the linear kernel_comma_ which uses fewer features than Poly_comma_ is more enhanced by the SSTs than STs for example on PropBank we have 89.4 and 88.6 vs. 87.6 _comma_ i.e.
E06-1015.txt,182,Linear takes advantage by the richer feature set of the SSTs.
E06-1015.txt,183,It should be noted that our results of kernel combinations on FrameNet are in contrast with Moschitti_comma_ 2004 _comma_ where no improvement was obtained.
E06-1015.txt,184,Our explanation is that_comma_ thanks to the fast evaluation of FTK_comma_ we could carry out an adequate parameterization.
E06-1015.txt,185,5 Related Work Recently_comma_ several tree kernels have been designed.
E06-1015.txt,186,In the following_comma_ we highlight their differences and properties.
E06-1015.txt,187,In Collins and Duffy_comma_ 2002 _comma_ the SST tree kernel was experimented with the Voted Perceptron for the parse tree reranking task.
E06-1015.txt,188,The combination with the original PCFG model improved the syntactic parsing.
E06-1015.txt,189,Additionally_comma_ it was alluded that the average execution time depends on the number of repeated productions.
E06-1015.txt,190,In Vishwanathan and Smola_comma_ 2002 _comma_ a linear complexity algorithm for the computation of the ST kernel is provided in the worst case .
E06-1015.txt,191,The main idea is the use of the suffix trees to store partial matches for the evaluation of the string kernel Lodhi et al._comma_ 2000 .
E06-1015.txt,192,This can be used to compute the ST fragments once the tree is converted into a string.
E06-1015.txt,193,To our knowledge_comma_ ours is the first application of the ST kernel for a natural language task.
E06-1015.txt,194,In Kazama and Torisawa_comma_ 2005 _comma_ an interesting algorithm that speeds up the average running time is presented.
E06-1015.txt,195,Such algorithm looks for node pairs that have in common a large number of trees malicious nodes and applies a transformation to the trees rooted in such nodes to make faster the kernel computation.
E06-1015.txt,196,The results show an increase of the speed similar to the one produced by our method.
E06-1015.txt,197,In Zelenko et al._comma_ 2003 _comma_ two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations_comma_ e.g. personaffiliation.
E06-1015.txt,198,To measure the similarity between two 119 nodes_comma_ the contiguous string kernel and the sparse string kernel Lodhi et al._comma_ 2000 were used.
E06-1015.txt,199,In Culotta and Sorensen_comma_ 2004 such kernels were slightly generalized by providing a matching function for the node pairs.
E06-1015.txt,200,The time complexity for their computation limited the experiments on data set of just 200 news items.
E06-1015.txt,201,Moreover_comma_ we note that the above tree kernels are not convolution kernels as those proposed in this article.
E06-1015.txt,202,In Shen et al._comma_ 2003 _comma_ a tree kernel based on Lexicalized Tree Adjoining Grammar LTAG for the parse reranking task was proposed.
E06-1015.txt,203,Since QTK was used for the kernel computation_comma_ the high learning complexity forced the authors to train different SVMs on different slices of training data.
E06-1015.txt,204,Our FTK_comma_ adapted for the LTAG tree kernel_comma_ would have allowed SVMs to be trained on the whole data.
E06-1015.txt,205,In Cumby and Roth_comma_ 2003 _comma_ a feature description language was used to extract structural features from the syntactic shallow parse trees associated with named entities.
E06-1015.txt,206,The experiments on the named entity categorization showed that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy.
E06-1015.txt,207,The explanation was that the complete tree fragment set contains many irrelevant features and may cause overfitting.
E06-1015.txt,208,6 Conclusions In this paper_comma_ we have shown that tree kernels can effectively be adopted in practical natural language applications.
E06-1015.txt,209,The main arguments against their use are their efficiency and accuracy lower than traditional feature based approaches.
E06-1015.txt,210,We have shown that a fast algorithm FTK can evaluate tree kernels in a linear average running time and also that the overall converging time required by SVMs is compatible with very large data sets.
E06-1015.txt,211,Regarding the accuracy_comma_ the experiments with Support Vector Machines on the PropBank and FrameNet predicate argument structures show that a the richer the kernel is in term of substructures e.g. SST _comma_ the higher the accuracy is_comma_ b tree kernels are effective also in case of automatic parse trees and c as kernel combinations always improve traditional feature models_comma_ the best approach is to combine scalar based and structured based kernels.
E06-1015.txt,212,Acknowledgments I would like to thank the AI group at the University of Rome Tor Vergata .
E06-1015.txt,213,Many thanks to the EACL 2006 anonymous reviewers_comma_ Roberto Basili and Giorgio Satta who provided me with valuable suggestions.
E06-1015.txt,214,This research is partially supported by the Presto Space EU Project FP6 507336. .
E06-1047.txt,1,Arabic language is a collection of spoken dialects with important phonological_comma_ morphological_comma_ lexical_comma_ and syntactic differences_comma_ along with a standard written language_comma_ Modern Standard Arabic MSA . Since the spoken dialects are not officially written_comma_ it is very costly to obtain adequate corpora touse fortraining dialect NLP tools such as parsers.
E06-1047.txt,2,In this paper_comma_ we address the problem of parsing transcribed spoken LevantineArabic LA .We do not assume the existence of any annotated LA corpus except for development and testing _comma_ nor of a parallel corpus LAMSA.
E06-1047.txt,3,Instead_comma_ we use explicit knowledge about the relation between LA and MSA.
E06-1047.txt,4,1 . Arabic Dialects The Arabic language is a collection of spoken dialects and a standard written language.1 The dialects show phonological_comma_ morphological_comma_ lexical_comma_ and syntactic differences comparable to those among the Romance languages.
E06-1047.txt,5,The standard written language is the same throughout the Arab world Modern Standard Arabic MSA .
E06-1047.txt,6,MSA is also used in some scripted spoken communication news casts_comma_ parliamentary debates .
E06-1047.txt,7,MSA is based on Classical Arabic and is not a native language ofanyArabic speaking people_comma_ i.e._comma_ children do not learn it from their parents but in school.
E06-1047.txt,8,1This paper is based on work done at the 2005 Johns Hopkins Summer Workshop_comma_ which was partially supported by the National Science Foundation under Grant No. 0121285.
E06-1047.txt,9,Diab_comma_ Habash_comma_ and Rambow were supported for additional work by DARPA contract HR0011 06 C 0023 under the GALE program.
E06-1047.txt,10,We wish to thank audiences at JHU for their useful feedback.
E06-1047.txt,11,The authors are listed in alphabetical order.
E06-1047.txt,12,Most native speakers of Arabic are unable to produce sustained spontaneous MSA.
E06-1047.txt,13,Dialects vary not only along a geographical continuum but also with other sociolinguistic variables such as the urban rural Bedouin dimension.
E06-1047.txt,14,The multidialectal situation has important negative consequences for Arabic natural language processing NLP since the spoken dialects are not officially written and do not have standard orthography_comma_ it is very costly to obtain adequate corpora_comma_ even unannotated corpora_comma_ to use for training NLP tools such as parsers.
E06-1047.txt,15,Furthermore_comma_ there are almost no parallel corpora involving one dialect and MSA.
E06-1047.txt,16,In this paper_comma_ we address the problem of parsing transcribed spoken Levantine Arabic LA _comma_ which we use as a representative example of the Arabic dialects.2 Our work is based on the assumption that it is easier to manually create new resources that relate LA to MSA than it is to manually create syntactically annotated corpora in LA.
E06-1047.txt,17,Our approaches do not assume the existence of any annotated LA corpus except for development and testing _comma_ nor of a parallel LA MSA corpus.
E06-1047.txt,18,Instead_comma_ we assume we have at our disposal a lexicon that relates LA lexemes to MSA lexemes_comma_ and knowledge about the morphological and syntactic differences between LA and MSA.
E06-1047.txt,19,For a single dialect_comma_ it may be argued that it is easier to create corpora than to encode all this knowledge explicitly.
E06-1047.txt,20,In response_comma_ we claim that because the dialects show important similarities_comma_ it will be easier to reuse and modify explicit linguistic resources for a new dialect_comma_ than to create a new corpus for it.
E06-1047.txt,21,The goal of this paper is to show that leveraging LA MSA 2We exclude from this study part of speech POS tagging and LA MSA lexicon induction.
E06-1047.txt,22,See Rambow et al._comma_ 2005 for these issues_comma_ as well as for more details on parsing.
E06-1047.txt,23,369 resources is feasible we do not provide a demonstration of cost effectiveness.
E06-1047.txt,24,The paper is organized as follows.
E06-1047.txt,25,After discussing related work and available corpora_comma_ we present linguistic issues in LA and MSA Section 4 .
E06-1047.txt,26,We then proceed to discuss three approaches sentence transduction_comma_ in which the LA sentence to be parsed is turned into an MSA sentence and then parsed with an MSA parser Section 5 treebank transduction_comma_ in which the MSA treebank is turned into anLAtreebank Section 6 and grammar transduction_comma_ in which an MSA grammar is turned into an LA grammar which is then used for parsing LA Section 7 .
E06-1047.txt,27,We summarize and discuss the results in Section 8.
E06-1047.txt,28,2 Related Work There has been a fair amount of interest in parsing one language using another language_comma_ see for example Smith and Smith_comma_ 2004 Hwa et al._comma_ 2004 for recent work.
E06-1047.txt,29,Much of this work uses synchronized formalisms as do we in the grammar transduction approach.
E06-1047.txt,30,However_comma_ these approaches rely on parallel corpora.
E06-1047.txt,31,For MSA and its dialects_comma_ there arenonaturally occurring parallel corpora.
E06-1047.txt,32,It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning.
E06-1047.txt,33,We refer to additional relevant work in the appropriate sections.
E06-1047.txt,34,3 Linguistic Resources We use the MSA treebanks 1_comma_ 2 and 3 ATB from theLDC Maamouri etal._comma_ 2004 .
E06-1047.txt,35,Wesplit thecorpus into 10 development data_comma_ 80 training data and 10 test data all respecting document boundaries.
E06-1047.txt,36,The training data ATB Train comprises 17_comma_617 sentences and 588_comma_244 tokens.
E06-1047.txt,37,The Levantine treebank LATB Maamouri et al._comma_ 2006 comprises 33_comma_000 words of treebanked conversational telephone transcripts collected as part of the LDC CALL HOME project.
E06-1047.txt,38,The treebanked section is primarily in the Jordanian subdialect of LA.
E06-1047.txt,39,The data is annotated by the LDC for speech effects such as disfluencies and repairs.
E06-1047.txt,40,We removed the speech effects_comma_ rendering the data more text like.
E06-1047.txt,41,The orthography and syntactic analysis chosen by the LDC for LA closely follow previous choices for MSA_comma_ see Figure 1 for two examples.
E06-1047.txt,42,The LATB is used exclusively for development and testing_comma_ not for training.
E06-1047.txt,43,We split the data in half respecting document boundaries.
E06-1047.txt,44,The resulting development data comprises 1928 sentences and 11151 tokens DEV .
E06-1047.txt,45,The test data comprises 2051 sentences and 10_comma_644 tokens TEST .
E06-1047.txt,46,For all the experiments_comma_ we use the non vocalized undiacritized version of both treebanks_comma_ as well as the collapsed POS tag set provided by the LDC for MSA and LA.
E06-1047.txt,47,Two lexicons were created a small lexicon comprising 321 LA MSA word form pairs covering LA closed class words and a few frequent open class words and a big lexicon which contains the small lexicon and an additional 1_comma_560 LA MSA word form pairs.
E06-1047.txt,48,We assign to the mappings in the two lexicons both uniform probabilities and biased probabilities using Expectation Maximization EM see Rambow et al._comma_ 2005 for details of the use of EM .
E06-1047.txt,49,We thus have four different lexicons Small lexicon with uniform probabilities SLXUN Small Lexicon with EMbased probabilities SLXEM Big Lexicon with uniform probabilities BLXUN and Big Lexicon with EM based probabilities BLXEM .
E06-1047.txt,50,4 Linguistic Facts We illustrate the differences between LA and MSA using an example3 1 a.
E06-1047.txt,51,a0a1a3a2a5a4a7a6a9a8a7a10a11a0a13a12 a14a16a15a11a17a19a18a21a20a23a22a19a24a25a27a26a28a10a11a0 LA AlrjAl the men byHbw like not Al gl the work hdA this the men do not like this work b.
E06-1047.txt,52,a4a30a29a32a31a33a10a11a0a34a0a35a36a2a37a22a19a24a25a13a26a28a10a11a0a34a38a34a17a19a39a23a40 MSA lA not yHb like AlrjAl the men h A this AlEml the work the men do not like this work Lexically_comma_ we observe that the word for work is a4a30a6a9a8a7a10a41a0 Al gl in LA but a4a30a29a42a31a9a10a11a0 AlEml in MSA.
E06-1047.txt,53,In contrast_comma_ the word for men is the same in both LA and MSA a22a19a24a25a27a26a28a10a11a0 AlrjAl.
E06-1047.txt,54,There are typically also differences in function words_comma_ in our example a12 LA and a40 lA MSA for not .
E06-1047.txt,55,Morphologically_comma_ we see that LA a14a9a15a21a17a19a18a21a20 byHbw has the same stem as MA a38a43a17a19a39 yHb_comma_ but with two additional morphemes the present aspect marker b which does not exist in MSA_comma_ and the agreement marker 3Arabic Examples are transliterated using the Buckwalter scheme http www.ldc.upenn.edu myl morph buckwalter.html 370 S NP TPC a22a19a24a25a13a26a28a10a11a0 men i VP V a14a16a15a21a17a19a18a11a20 like NEG a12 not NP SBJ ti NP OBJ N a4a30a6a9a8a7a10a41a0 work DET a0a1a36a2 this S VP NEG a40 not V a38a34a17a19a39 like NP SBJ a22a19a24a25a13a26a28a10a11a0 men NP OBJ DET a0a35a36a2 this N a4a7a29a32a31a9a10a41a0 work Figure 1 LDC style left to right phrase structure trees for LA left and MSA right for sentence 1 w_comma_ which is used in MSA only in subject initial sentences_comma_ while in LA it is always used.
E06-1047.txt,56,Syntactically_comma_ we observe three differences.
E06-1047.txt,57,First_comma_ the subject precedes the verb in LA SVO order _comma_ but follows in MSA VSO order .
E06-1047.txt,58,This is in fact not a strict requirement_comma_ but a strong preference both varieties allow both orders.
E06-1047.txt,59,Second_comma_ we see that the demonstrative determiner follows the noun in LA_comma_ but precedes it in MSA.
E06-1047.txt,60,Finally_comma_ we see that the negation marker follows the verb in LA_comma_ while it precedes the verb in MSA.4 The two phrase structure trees are shown in Figure 1 in the LDC convention.
E06-1047.txt,61,Unlike the phrase structure trees_comma_ the unordered dependency trees for the MSA and LA sentences not shown here for space considerations are isomorphic.
E06-1047.txt,62,They differ only in the node labels.
E06-1047.txt,63,5 Sentence Transduction In this approach_comma_ we parse an MSA translation of the LA sentence and then link the LA sentence to the MSA parse.
E06-1047.txt,64,Machine translation MT is not easy_comma_ especially when there are no MT resources available such as naturally occurring parallel text or transfer lexicons.
E06-1047.txt,65,However_comma_ for this task we have three encouraging insights.
E06-1047.txt,66,First_comma_ for really close languages itispossible toobtain better translation quality by means of simpler methods Hajic et al._comma_ 2000 .
E06-1047.txt,67,Second_comma_ suboptimal MSA output can still be helpful for the parsing task without necessarily being fluent or accurate since our goal is parsing LA_comma_ not translating it to MSA .
E06-1047.txt,68,And finally_comma_ translation from LA to MSA is easier than from MSA to LA.
E06-1047.txt,69,This is a result of the availability of abundant resources for MSA as compared to LA for example_comma_ text corpora and tree banks for 4Levantine also has other negation markers that precede the verb_comma_ as well as the circumfix m . language modeling and a morphological generation system Habash_comma_ 2004 .
E06-1047.txt,70,One disadvantage of this approach is the lack of structural information on the LA side for translation from LA to MSA_comma_ which means that we are limited in the techniques we can use.
E06-1047.txt,71,Another disadvantage is that the translation can add more ambiguity to the parsing problem.
E06-1047.txt,72,Some unambiguous dialect words can become syntactically ambiguous in MSA.
E06-1047.txt,73,For example_comma_ the LA words a0a2a1 mn from and a0 a18 a1 myn who both are translated into an orthographically ambiguous form in MSA a0a2a1 mn from or who .
E06-1047.txt,74,5.1 Implementation Each word in the LA sentence is translated into a bag of MSA words_comma_ producing a sausage lattice.
E06-1047.txt,75,The lattice is scored and decoded using the SRILM toolkit with a trigram language model trained on 54 million MSA words from Arabic Gigaword Graff_comma_ 2003 .
E06-1047.txt,76,The text used for language modeling was tokenized to match the tokenization of the Arabic used in the ATB and LATB.
E06-1047.txt,77,The tokenization was done using the ASVM Toolkit Diab et al._comma_ 2004 .
E06-1047.txt,78,The 1 best path in the lattice is passed on to the Bikel parser Bikel_comma_ 2002 _comma_ which was trained on the MSA training ATB.
E06-1047.txt,79,Finally_comma_ the terminal nodes in the resulting parse structure are replaced with the original LA words.
E06-1047.txt,80,5.2 Experimental Results Table 1 describes the results of the sentence transduction path on the development corpus DEV in different settings using no POS tags in the input versus using gold POS tags in the input_comma_ and using SLXUN versus BLXUN.
E06-1047.txt,81,The baseline results are obtained byparsing theLAsentence directly using the MSA parser with and without gold POS tags .
E06-1047.txt,82,The results are reported in terms of PARSEVAL s 371 No Tags Gold Tags Baseline 59.4 51.9 55.4 64.0 58.3 61.0 SLXUN 63.8 58.3 61.0 67.5 63.4 65.3 BLXUN 65.3 61.1 63.1 66.8 63.2 65.0 Table1 Sentence transduction resultsonDEV labeled precision recall F measure No Tags Gold Tags Baseline 53.5 60.2 SLXUN 57.7 64.0 Table 2 Sentence transduction results on TEST labeled F measure Precision Recall F Measure.
E06-1047.txt,83,Using SLXUN improves the F1 score for no tags and for gold tags.
E06-1047.txt,84,A further improvement is gained when using the BLXUN lexicon with no POS tags in the input_comma_ but this improvement disappears when we use BLXUN with gold POS tags.
E06-1047.txt,85,We suspect that the added translation ambiguity from BLXUN is responsible for the drop.
E06-1047.txt,86,We also experimented with the SLXEM and BLXEM lexicons.
E06-1047.txt,87,There was no consistent improvement.
E06-1047.txt,88,InTable2_comma_ wereport theF Measure score onthe test set TEST for the baseline and for SLXUN with and without gold POS tags .
E06-1047.txt,89,We see a general drop in performance between DEV and TEST for all combinations suggesting that TEST is a harder set to parse than DEV.
E06-1047.txt,90,5.3 Discussion The current implementation does not handle cases where the word order changes between MSA and LA.
E06-1047.txt,91,Since we start from an LA string_comma_ identifying constituents to permute is clearly a hard task.
E06-1047.txt,92,We experimented with identifying strings with the postverbal LA negative particle and then permuting them to obtain the MSA preverbal order.
E06-1047.txt,93,The original word positions are bread crumbed through the systems language modeling and parsing steps and then used to construct an unordered dependency parse tree labeled with the input LA words.
E06-1047.txt,94,A constituency representation is meaningless since word order changes from LA to MSA. The results were not encouraging since the effect of the positive changes was undermined by newly introduced errors.
E06-1047.txt,95,6 Treebank Transduction In this approach_comma_ the idea is to convert the MSA treebank ATB Train into anLA like treebank using linguistic knowledge of the systematic variations on the syntactic_comma_ lexical and morphological levels across the two varieties of Arabic.
E06-1047.txt,96,We then train a statistical parser on the newly transduced treebank and test the parsing performance against the gold test set of the LA treebank sentences.
E06-1047.txt,97,6.1 MSA Transformations We now list the transformations we applied to ATB Train 6.1.1 Structural Transformations Consistency checks CON These are conversions that make the ATB annotation more consistent.
E06-1047.txt,98,For example_comma_ there are many cases where SBAR and S nodes are used interchangeably in the MSA treebank.
E06-1047.txt,99,Therefore_comma_ an S clause headed by a complementizer is converted to an SBAR.
E06-1047.txt,100,Sentence Splitting TOPS A fair number of sentences in the ATB has a root node S with several embedded direct descendant S nodes_comma_ sometimes conjoined using the conjunction w.
E06-1047.txt,101,We split such sentences into several shorter sentences.
E06-1047.txt,102,6.1.2 Syntactic Transformations There are several possible systematic syntactic transformations.
E06-1047.txt,103,We focus on three major ones due to their significant distributional variation in MSA and LA.
E06-1047.txt,104,They are illustrated in Figure 1.
E06-1047.txt,105,Negation NEG In MSA negation is marked with preverbal negative particles.
E06-1047.txt,106,In LA_comma_ a negative construction is expressed in one of three possible ways m mA preceding the verb a particle suffixed onto the verb or a circumfix of a prefix mA and suffix it .
E06-1047.txt,107,We converted all negation instances in the ATB Train three ways reflecting the LA constructions for negation.
E06-1047.txt,108,VSO SVO Ordering SVO Both Verb Subject Object VSO and Subject Verb Object SVO constructions occur in MSA and LA treebanks.
E06-1047.txt,109,But pure VSO constructions where there is no pro drop occur in the LA corpus only 10 of the data_comma_ while VSO is the most frequent ordering in MSA.
E06-1047.txt,110,Hence_comma_ the goal is to skew the distributions of the SVO constructions in the MSA data.
E06-1047.txt,111,Therefore_comma_ VSO constructions are both replicated and converted to SVO constructions.
E06-1047.txt,112,Demonstrative Switching DEM In LA_comma_ demonstrative pronouns precede or_comma_ more com372 monly_comma_ follow the nouns they modify_comma_ while in MSA demonstrative pronoun only precede the noun they modify.
E06-1047.txt,113,Accordingly_comma_ we replicate the LA constructions in ATB Train and moved the demonstrative pronouns to follow their modified nouns while retaining the source MSAordering simultaneously.
E06-1047.txt,114,6.1.3 Lexical Substitution We use the four lexicons described in Section 3.
E06-1047.txt,115,These resources are created with a coverage bias from LA to MSA.
E06-1047.txt,116,As an approximation_comma_ we reversed the directionality to yield MSA to LA lexicons_comma_ retaining the assigned probability scores.
E06-1047.txt,117,Manipulations involving lexical substitution are applied only to the lexical items without altering the POS tag or syntactic structure.
E06-1047.txt,118,6.1.4 Morphological Transformations We applied some morphological rules to handle specific constructions in the LA.
E06-1047.txt,119,The POS tier as well as the lexical items were affected by these manipulations. bd Construction BD bd is an LA noun that means want .
E06-1047.txt,120,It acts like a verb in verbal constructions yielding VP constructions headed by NN.
E06-1047.txt,121,It is typically followed by a possessive pronoun.
E06-1047.txt,122,Accordingly_comma_ we translated all MSA verbs meaning want need into the noun bd and changed their POS tag to the nominal tag NN.
E06-1047.txt,123,In cases where the subject ofthe MSAverb is pro dropped_comma_ we add a clitic possessive pronoun in the first or second person singular.
E06-1047.txt,124,This was intended to bridge the genre and domain disparity between the MSA and LA data.
E06-1047.txt,125,Aspectual Marker b ASP In dialectal Arabic_comma_ present tense verbs are marked with an initial b.
E06-1047.txt,126,Therefore we add a b prefix to all verbs of POS tag type VBP.
E06-1047.txt,127,The aspectual marker is present on the verb byHbw in the LA example in Figure 1. lys Construction LYS In the MSA data_comma_ lys is interchangeably marked as a verb and as a particle.
E06-1047.txt,128,However_comma_ in the LA data_comma_ lys occurs only as a particle.
E06-1047.txt,129,Therefore_comma_ we convert all occurrences of lys into RP.
E06-1047.txt,130,6.2 Experimental Results We transform ATB Train into an LA like treebank using different strategies_comma_ and then train the Bikel parser on the resulting LA like treebank.
E06-1047.txt,131,Weparse the LA test set with the Bikel parser trained in this manner.
E06-1047.txt,132,As before_comma_ we report results on DEV and No Tags Gold Tags Baseline 59.5 52 55.5 64.2 58.4 61.1 MORPH 63.9 58 60.8 SLXEM 64.2 59.3 61.7 NEG 64.5 58.9 61.6 STRUCT 64.6 59.2 61.8 NEG 64.6 59.5 62 NEG SLXEM 62.1 55.9 58.8 65.5 61.3 63.3 Table 3 Treebank transduction results on DEV labeled precision recall F measure No Tags Gold Tags Baseline 53.5 60.2 STRUCT NEG SLXEM 57 62.1 Table 4 Treebank transduction results on TEST labeled F measure TEST sets_comma_ without POS tags and with gold POS tags_comma_ using the Parseval metrics of labeled precision_comma_ labeled recall and f measure.
E06-1047.txt,133,Table 3 summarizes the results on the LA development set.
E06-1047.txt,134,In Table 3_comma_ STRUCT refers to the structural transformations combining TOPS with CON.
E06-1047.txt,135,Of the Syntactic transformations applied_comma_ NEG is the only transformation that helps performance.
E06-1047.txt,136,Both SVO and DEM decrease the performance from the baseline with F measures of 59.4 and 59.5_comma_ respectively.
E06-1047.txt,137,Of the lexical substitutions i.e._comma_ lexicons _comma_ SLXEM helps performance the best.
E06-1047.txt,138,MORPH refers to a combination of all the morphological transformations.
E06-1047.txt,139,MORPH does not help performance_comma_ as we see a decrease from the baseline by 0.3 when applied on its own.
E06-1047.txt,140,When combining MORPH with other conditions_comma_ wesee aconsistent decrease.
E06-1047.txt,141,Forinstance_comma_ STRUCT NEG SLXEM MORPHyields an fmeasure of 62.9 compared to 63.3 yielded by STRUCT NEG SLXEM.
E06-1047.txt,142,The best results obtained are those from combining STRUCT with NEG and SLXEM for both the No Tag and Gold Tag conditions.
E06-1047.txt,143,Table 4 shows the results obtained on TEST.
E06-1047.txt,144,As for the sentence transduction case_comma_ we see an overallreduction inthe performance indicating that the test data is very different from the training data.
E06-1047.txt,145,373 6.3 Discussion The best performing condition always includes CON_comma_ TOPS and NEG.
E06-1047.txt,146,SLXEM helps as well_comma_ however_comma_ due to the inherent directionality of the resource_comma_ its impact is limited.
E06-1047.txt,147,We experimented with the other lexicons but none of them helped improve performance.
E06-1047.txt,148,We believe that the EM probabilities helped in biasing the lexical choices_comma_ playing the role of an LA language model which wedonothave .
E06-1047.txt,149,Wedonotobserve anysignificant improvement from applying MORPH.
E06-1047.txt,150,7 Grammar Transduction The grammar transduction approach uses the machinery of synchronous grammars to relate MSA andLA.Asynchronous grammarcomposes paired elementary trees_comma_ or fragments of phrase structure trees_comma_ to generate pairs of phrase structure trees.
E06-1047.txt,151,In the present application_comma_ we start with MSA elementary trees plus probabilities induced from the ATB and transform them using handwritten rules into dialect elementary trees to yield an MSAdialect synchronous grammar.
E06-1047.txt,152,This synchronous grammar can be used to parse new dialect sentences using statistics gathered from the MSA data.
E06-1047.txt,153,Thus this approach can be thought of as a variant of the treebank transduction approach in which the syntactic transformations are localized to elementary trees.
E06-1047.txt,154,Moreover_comma_ because a parsed MSA translation is produced as a byproduct_comma_ we can also think of this approach as being related to the sentence transduction approach.
E06-1047.txt,155,7.1 Preliminaries The parsing model used is essentially that of Chiang Chiang_comma_ 2000 _comma_ which is based on a highly restricted version of tree adjoining grammar.
E06-1047.txt,156,In its present form_comma_ the formalism is tree substitution grammar Schabes_comma_ 1990 with an additional operation called sister adjunction Rambow et al._comma_ 2001 .
E06-1047.txt,157,Because of space constraints_comma_ we omit discussion of the sister adjunction operation in this paper.
E06-1047.txt,158,A tree substitution grammar is a set of elementary trees.
E06-1047.txt,159,A frontier node labeled with a nonterminal label is called a substitution site.
E06-1047.txt,160,If an elementary tree has exactly one terminal symbol_comma_ that symbol is called its lexical anchor.
E06-1047.txt,161,A derivation starts with an elementary tree and proceeds by a series of composition operations.
E06-1047.txt,162,In the substitution operation_comma_ a substitution site is rewritten with an elementary tree with a matching root label.
E06-1047.txt,163,The final product is a tree with no more substitution sites.
E06-1047.txt,164,A synchronous TSG is a set of pairs of elementary trees.
E06-1047.txt,165,In each pair_comma_ there is a one to one correspondence between the substitution sites of the two trees_comma_ which we represent using boxed indices Figure 2 .
E06-1047.txt,166,The substitution operation then rewrites a pair of coindexed substitution sites with an elementary tree pair.
E06-1047.txt,167,A stochastic synchronous TSG adds probabilities to the substitution operation the probability of substituting an elementary tree pair _comma_ prime at a substitution site pair _comma_ prime is P _comma_ prime _comma_ prime .
E06-1047.txt,168,When we parse a monolingual sentence S using one side of a stochastic synchronous TSG_comma_ using a straightforward generalization of the CKY and Viterbi algorithms_comma_ we obtain the highestprobability paired derivation which includes a parse for S on one side_comma_ and a parsed translation of S on the other side.
E06-1047.txt,169,It is also straightforward to calculate inside and outside probabilities for reestimation by Expectation Maximization EM .
E06-1047.txt,170,7.2 An MSA dialect synchronous grammar We now describe how we build our MSA dialect synchronous grammar.
E06-1047.txt,171,As mentioned above_comma_ the MSA side of the grammar is extracted from the ATB in a process described by Chiang and others Chiang_comma_ 2000 Xia et al._comma_ 2000 Chen_comma_ 2001 .
E06-1047.txt,172,This process also gives us MSA only substitution probabilities P  .
E06-1047.txt,173,We then apply various transformation rules described below to the MSA elementary trees to produce a dialect grammar_comma_ at the same time assigning probabilities P prime  .
E06-1047.txt,174,The synchronoussubstitution probabilities can then be estimated as P _comma_ prime _comma_ prime P  P prime  P  P wprime_comma_tprime w_comma_t P prime  _comma_wprime_comma_tprime_comma_w_comma_t where w and t are the lexical anchor of and its POS tag_comma_ and is the equivalence class of modulo lexical anchors and their POS tags.
E06-1047.txt,175,P wprime_comma_tprime w_comma_t is assigned as described in Section 3 P prime  _comma_wprime_comma_tprime_comma_w_comma_t is initially assigned by hand.
E06-1047.txt,176,Because the full probability table for the latter would be quite large_comma_ we smooth it using a backoff model so that the number of parameters to 374       S NPi 1 VP V a14a9a15a21a17a19a18a21a20 like NP ti NP 2 _comma_ S VP V a38a43a17a19a39 like NP 1 NP 2       Figure 2 Example elementary tree pair of a synchronous TSG. be chosen is manageable.
E06-1047.txt,177,Finally_comma_ we reestimate these parameters using EM.
E06-1047.txt,178,Because of the underlying syntactic similarity between the two varieties of Arabic_comma_ we assume that every tree in the MSA grammar extracted from the MSA treebank is also an LA tree.
E06-1047.txt,179,In addition_comma_ we perform certain tree transformations on all elementary trees which match the pattern NEG and SVO Section 6.1.2 and BD Section 6.1.4 .
E06-1047.txt,180,NEG is modified so that we simply insert a negation marker postverbally_comma_ as the preverbal markers are handled by MSA trees.
E06-1047.txt,181,7.3 Experimental Results We first use DEV to determine which of the transformations are useful.
E06-1047.txt,182,The results are shown in Table 5.
E06-1047.txt,183,The baseline is the same as in the previous two approaches.
E06-1047.txt,184,We see that important improvements are obtained using lexicon SLXUN.
E06-1047.txt,185,Adding the SVO transformation does not improve the results_comma_ but the NEG and BD transformations help slightly_comma_ and their effect is partly cumulative.
E06-1047.txt,186,We did not perform these tuning experiments on input with no POS tags. We also experimented with the SLXEM and BLXEM lexicons.
E06-1047.txt,187,There was no consistent improvement.
E06-1047.txt,188,7.4 Discussion Weobserve thatthelexicon canbeusedeffectively in our synchronous grammar framework.
E06-1047.txt,189,In addition_comma_ some syntactic transformations are useful.
E06-1047.txt,190,The SVO transformation_comma_ we assume_comma_ turned out not to be useful because the SVO word order is also possible in MSA_comma_ so that the new trees were not needed and needlessly introduced new derivations.
E06-1047.txt,191,The BD transformation shows the importance not of general syntactic transformations_comma_ but rather of lexically specific syntactic transformations varieties within one language family may No Tags Gold Tags Baseline 59.4 51.9 55.4 64.0 58.3 61.0 SLXUN 63.0 60.8 61.9 66.9 67.0 66.9 SVO 66.9 66.7 66.8 NEG 67.0 67.0 67.0 BD 67.4 67.0 67.2 NEG BD 67.4 67.1 67.3 BLXUN 64.9 63.7 64.3 67.9 67.4 67.6 Table 5 Grammar transduction results on development corpus labeled precision recall Fmeasure No Tags Gold Tags Baseline 53.5 60.2 SLXUN Neg bd 60.2 67.1 Table 6 Grammar transduction results on TEST labeled F measure differ more in terms of the lexico syntactic constructions used for a specific semantic or pragmatic purpose than in their basic syntactic inventory.
E06-1047.txt,192,Notethatourtree based synchronous formalism is ideally suited for expressing such transformations since it is lexicalized_comma_ and has an extended domain of locality.
E06-1047.txt,193,8 Summary of Results and Discussion We have built three frameworks for leveraging MSA corpora and explicit knowledge about the lexical_comma_ morphological_comma_ and syntactic differences between MSA and LA for parsing LA.
E06-1047.txt,194,The results on TEST are summarized in Table 7_comma_ where performance is given as absolute and relative reduction in labeled F measure error i.e._comma_ 100 F .
E06-1047.txt,195,We see that some important improvements in parsing 375 No Tags Gold Tags Sentence Transd.
E06-1047.txt,196,4.2 9.0 3.8 9.5 Treebank Transd.
E06-1047.txt,197,3.5 7.5 1.9 4.8 Grammar Transd.
E06-1047.txt,198,6.7 14.4 6.9 17.3 Table 7 Results on test corpus absolute percent error reduction in F measure over baseline using MSA parser on LA test corpus all numbers are for best obtained results using that method quality canbeachieved.
E06-1047.txt,199,Wealso remindthereader that on the ATB_comma_ state of the art performance is currently about 75 F measure.
E06-1047.txt,200,There are several important ways in which we can expand our work.
E06-1047.txt,201,For the sentencetransduction approach_comma_ we plan to explore the use of a larger set of permutations to use improved language models on MSA such as language models built on genres closer to speech to use lattice parsing Sima an_comma_ 2000 directly on the translation lattice and to integrate this approach with the treebank transduction approach.
E06-1047.txt,202,For the treebank and grammar transduction approaches_comma_ we would like toexplore moresystematic syntactic_comma_ morphological_comma_ and lexico syntactic transformations.
E06-1047.txt,203,We would also like to explore the feasibility of inducing the syntactic and morphological transformations automatically.
E06-1047.txt,204,Specifically for the treebank transduction approach_comma_ it would be interesting to apply an LA language model for the lexical substitution phase as ameans of pruning out implausible word sequences.
E06-1047.txt,205,For all three approaches_comma_ one major impediment to obtaining better results is the disparity in genre and domain whichaffects the overall performance.
E06-1047.txt,206,This may be bridged by finding MSA data that is more in the domain of the LA test corpus than the MSA treebank. .
E06-1016.txt,1,degree of dominance of a sense of a word is the proportion of occurrences of that sense in text. We propose four new methods to accurately determine word sense dominance using raw text and a published thesaurus.
E06-1016.txt,2,Unlike the McCarthy et al. 2004 system_comma_ these methods can be used on relatively small target texts_comma_ without the need for a similarly sensedistributed auxiliary text.
E06-1016.txt,3,We perform an extensive evaluation using artificially generated thesaurus sense tagged data.
E06-1016.txt,4,In the process_comma_ we create a word category cooccurrence matrix_comma_ which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses_comma_ as well.
E06-1016.txt,6,The occurrences of the senses of a word usually have skewed distribution in text.
E06-1016.txt,7,Further_comma_ the distribution varies in accordance with the domain or topic of discussion.
E06-1016.txt,8,For example_comma_ the assertion of illegality sense of charge is more frequent in the judicial domain_comma_ while in the domain of economics_comma_ the expense cost sense occurs more often.
E06-1016.txt,9,Formally_comma_ the degree of dominance of a particular sense of a word target word in a given text target text may be defined as the ratio of the occurrences of the sense to the total occurrences of the target word.
E06-1016.txt,10,The sense with the highest dominance in the target text is called the predominant sense of the target word.
E06-1016.txt,11,Determination of word sense dominance has many uses.
E06-1016.txt,12,An unsupervised system will benefit by backing off to the predominant sense in case of insufficient evidence.
E06-1016.txt,13,The dominance values may be used as prior probabilities for the different senses_comma_ obviating the need for labeled training data in a sense disambiguation task.
E06-1016.txt,14,Natural language systems can choose to ignore infrequent senses of words or consider only the most dominant senses McCarthy et al._comma_ 2004 .
E06-1016.txt,15,An unsupervised algorithm that discriminates instances into different usages can use word sense dominance to assign senses to the different clusters generated.
E06-1016.txt,16,Sense dominance may be determined by simple counting in sense tagged data.
E06-1016.txt,17,However_comma_ dominance varies with domain_comma_ and existing sensetagged data is largely insufficient.
E06-1016.txt,18,McCarthy et al. 2004 automatically determine domainspecific predominant senses of words_comma_ where the domain may be specified in the form of an untagged target text or simply by name for example_comma_ financial domain .
E06-1016.txt,19,The system Figure 1 automatically generates a thesaurus Lin_comma_ 1998 using a measure of distributional similarity and an untagged corpus.
E06-1016.txt,20,The target text is used for this purpose_comma_ provided it is large enough to learn a thesaurus from.
E06-1016.txt,21,Otherwise a large corpus with sense distribution similar to the target text text pertaining to the specified domain must be used.
E06-1016.txt,22,The thesaurus has an entry for each word type_comma_ which lists a limited number of words neighbors that are distributionally most similar to it.
E06-1016.txt,23,Since Lin s distributional measure overestimates the distributional similarity of more frequent word pairs Mohammad and Hirst_comma_ Submitted _comma_ the neighbors of a word corresponding to the predominant sense are distributionally closer to it than those corresponding to any other sense.
E06-1016.txt,24,For each sense of a word_comma_ the distributional similarity scores of all its neighbors are summed using the semantic similarity of the word with the closest sense of the 121 TARGET A U X L A R Y I I SIMILARLY SENSE DISTRIBUTED DOMINANCE VALUES THESAURUS LIN SD C R P U S O WORDNET TEXT Figure 1 The McCarthy et al. system.
E06-1016.txt,25,TARGET A U X L A R Y I I DOMINANCE VALUES D C R P U S O WCCM TEXT PUBLISHED THESAURUS Figure 2 Our system. neighbor as weight.
E06-1016.txt,26,The sense that gets the highest score is chosen as the predominant sense.
E06-1016.txt,27,The McCarthy et al. system needs to re train create a new thesaurus every time it is to determine predominant senses in data from a different domain.
E06-1016.txt,28,This requires large amounts of partof speech tagged and chunked data from that domain.
E06-1016.txt,29,Further_comma_ the target text must be large enough to learn a thesaurus from Lin 1998 used a 64million word corpus _comma_ or a large auxiliary text with a sense distribution similar to the target text must be provided McCarthy et al. 2004 separately used 90 _comma_ 32.5 _comma_ and 9.1 million word corpora .
E06-1016.txt,30,By contrast_comma_ in this paper we present a method that accurately determines sense dominance even in relatively small amounts of target text a few hundred sentences although it does use a corpus_comma_ it does not require a similarly sense distributed corpus.
E06-1016.txt,31,Nor does our system Figure 2 need any part of speech tagged data although that may improve results further _comma_ and it does not need to generate a thesaurus or execute any such timeintensive operation at run time.
E06-1016.txt,32,Our method stands on the hypothesis that words surrounding the target word are indicative of its intended sense_comma_ and that the dominance of a particular sense is proportional to the relative strength of association between it and co occurring words in the target text.
E06-1016.txt,33,We therefore rely on first order co occurrences_comma_ which we believe are better indicators of a word s characteristics than second order co occurrences distributionally similar words .
E06-1016.txt,34,2 Thesauri Published thesauri_comma_ such as Roget s and Macquarie_comma_ divide the English vocabulary into around a thousand categories.
E06-1016.txt,35,Each category has a list of semantically related words_comma_ which we will call category terms or c terms for short.
E06-1016.txt,36,Words with multiple meanings may be listed in more than one category.
E06-1016.txt,37,For every word type in the vocabulary of the thesaurus_comma_ the index lists the categories that include it as a c term.
E06-1016.txt,38,Categories roughly correspond to coarse senses of a word Yarowsky_comma_ 1992 _comma_ and the two terms will be used interchangeably.
E06-1016.txt,39,For example_comma_ in the Macquarie Thesaurus_comma_ bark is a c term in the categories animal noises and membrane .
E06-1016.txt,40,These categories represent the coarse senses of bark.
E06-1016.txt,41,Note that published thesauri are structurally quite different from the thesaurus automatically generated by Lin 1998 _comma_ wherein a word has exactly one entry_comma_ and its neighbors may be semantically related to it in any of its senses.
E06-1016.txt,42,All future mentions of thesaurus will refer to a published thesaurus.
E06-1016.txt,43,While other sense inventories such as WordNet exist_comma_ use of a published thesaurus has three distinct advantages i coarse senses it is widely believed that the sense distinctions of WordNet are far too fine grained Agirre and Lopez de Lacalle Lekuona 2003 and citations therein ii computational ease with just around a thousand categories_comma_ the word category matrix has a manageable size iii widespread availability thesauri are available or can be created with relatively less effort in numerous languages_comma_ while WordNet is available only for English and a few romance languages.
E06-1016.txt,44,We use the Macquarie Thesaurus Bernard_comma_ 1986 for our experiments.
E06-1016.txt,45,It consists of 812 categories with around 176_comma_000 c terms and 98_comma_000 word types.
E06-1016.txt,46,Note_comma_ however_comma_ that using a sense inventory other than WordNet will mean that we cannot directly compare performance with McCarthy et al. 2004 _comma_ as that would require knowing exactly how thesaurus senses map to WordNet.
E06-1016.txt,47,Further_comma_ it has been argued that such a mapping across sense inventories is at best difficult and maybe impossible Kilgarriff and Yallop 2001 and citations therein .
E06-1016.txt,48,122 3 Co occurrence Information 3.1 Word Category Co occurrence Matrix The strength of association between a particular category of the target word and its co occurring words can be very useful calculating word sense dominance being just one application.
E06-1016.txt,49,To this end we create the word category co occurrence matrix WCCM in which one dimension is the list of all words w1BNw2BNBMBMBM in the vocabulary_comma_ and the other dimension is a list of all categories c1BNc2BNBMBMBM .
E06-1016.txt,50,c1 c2 BMBMBM cj BMBMBM w1 m11 m12 BMBMBM m1j BMBMBM w2 m21 m22 BMBMBM m2j BMBMBM ... ... ... ... BMBMBM BMBMBM wi mi1 mi2 BMBMBM mij BMBMBM ... ... ... ... ... ... A particular cell_comma_ mij_comma_ pertaining to word wi and category cj_comma_ is the number of times wi occurs in a predetermined window around any c term of cj in a text corpus.
E06-1016.txt,51,We will refer to this particular WCCM created after the first pass over the text as the base WCCM.
E06-1016.txt,52,A contingency table for any particular word w and category c see below can be easily generated from the WCCM by collapsing cells for all other words and categories into one and summing up their frequencies.
E06-1016.txt,53,The application of a suitable statistic will then yield the strength of association between the word and the category.
E06-1016.txt,54,c BMc w nwc nw BM BMw n BMc nBMBM Even though the base WCCM is created from unannotated text_comma_ and so is expected to be noisy_comma_ we argue that it captures strong associations reasonably accurately.
E06-1016.txt,55,This is because the errors in determining the true category that a word cooccurs with will be distributed thinly across a number of other categories details in Section 3.2 .
E06-1016.txt,56,Therefore_comma_ we can take a second pass over the corpus and determine the intended sense of each word using the word category co occurrence frequency from the base WCCM as evidence.
E06-1016.txt,57,We can thus create a newer_comma_ more accurate_comma_ bootstrapped WCCM by populating it just as mentioned earlier_comma_ except that this time counts of only the cooccurring word and the disambiguated category are incremented.
E06-1016.txt,58,The steps of word sense disambiguation and creating new bootstrapped WCCMs can be repeated until the bootstrapping fails to improve accuracy significantly.
E06-1016.txt,59,The cells of the WCCM are populated using a large untagged corpus usually different from the target text which we will call the auxiliary corpus.
E06-1016.txt,60,In our experiments we use a subset all except every twelfth sentence of the British National Corpus World Edition BNC Burnard_comma_ 2000 as the auxiliary corpus and a window size of A65 words.
E06-1016.txt,61,The remaining one twelfth of the BNC is used for evaluation purposes.
E06-1016.txt,62,Note that if the target text belongs to a particular domain_comma_ then the creation of the WCCM from an auxiliary text of the same domain is expected to give better results than the use of a domain free text.
E06-1016.txt,63,3.2 Analysis of the Base WCCM The use of untagged data for the creation of the base WCCM means that words that do not really co occur with a certain category but rather do so with a homographic word used in a different sense will erroneously increment the counts corresponding to the category.
E06-1016.txt,64,Nevertheless_comma_ the strength of association_comma_ calculated from the base WCCM_comma_ of words that truly and strongly co occur with a certain category will be reasonably accurate despite this noise.
E06-1016.txt,65,We demonstrate this through an example.
E06-1016.txt,66,Assume that category c has 100 c terms and each cterm has 4 senses_comma_ only one of which corresponds to c while the rest are randomly distributed among other categories.
E06-1016.txt,67,Further_comma_ let there be 5 sentences each in the auxiliary text corresponding to every c term sense pair.
E06-1016.txt,68,If the window size is the complete sentence_comma_ then words in 2_comma_000 sentences will increment co occurrence counts for c.
E06-1016.txt,69,Observe that 500 of these sentences truly correspond to category c_comma_ while the other 1500 pertain to about 300 other categories.
E06-1016.txt,70,Thus on average 5 sentences correspond to each category other than c.
E06-1016.txt,71,Therefore in the 2000 sentences_comma_ words that truly co occur with c will likely occur a large number of times_comma_ while the rest will be spread out thinly over 300 or so other categories.
E06-1016.txt,72,We therefore claim that the application of a suitable statistic_comma_ such as odds ratio_comma_ will result in significantly large association values for word category pairs where the word truly and strongly co occurs with the category_comma_ and the effect of noise 123 will be insignificant.
E06-1016.txt,73,The word category pairs having low strength of association will likely be adversely affected by the noise_comma_ since the amount of noise may be comparable to the actual strength of association.
E06-1016.txt,74,In most natural language applications_comma_ the strength of association is evidence for a particular proposition.
E06-1016.txt,75,In that case_comma_ even if association values from all pairs are used_comma_ evidence from less reliable_comma_ low strength pairs will contribute little to the final cumulative evidence_comma_ as compared to more reliable_comma_ high strength pairs.
E06-1016.txt,76,Thus even if the base WCCM is less accurate when generated from untagged text_comma_ it can still be used to provide association values suitable for most natural language applications.
E06-1016.txt,77,Experiments to be described in section 6 below substantiate this.
E06-1016.txt,78,3.3 Measures of Association The strength of association between a sense or category of the target word and its co occurring words may be determined by applying a suitable statistic on the corresponding contingency table.
E06-1016.txt,79,Association values are calculated from observed frequencies nwcBNn BMcBNnwBM BN and n BMBM _comma_ marginal frequencies nw A3 BPnwcB7nw BM n BMA3 BPn BMcB7nBMBM n A3c BP nwc B7n BMc and nA3BM BP nw BM B7n BMBM _comma_ and the sample size N BP nwc B7n BMcB7nwBM B7n BMBM .
E06-1016.txt,80,We provide experimental results using Dice coefficient Dice _comma_ cosine cos _comma_ pointwise mutual information pmi _comma_ odds ratio odds _comma_ Yule s coefficient of colligation Yule _comma_ and phi coefficient 1.
E06-1016.txt,81,4 Word Sense Dominance We examine each occurrence of the target word in a given untagged target text to determine dominance of any of its senses.
E06-1016.txt,82,For each occurrence tBC of a target word t_comma_ let TBC be the set of words tokens co occurring within a predetermined window around tBC let T be the union of all such TBC and let CGt be the set of all such TBC.
E06-1016.txt,83,Thus CYCGtCY is equal to the number of occurrences of t_comma_ and CYTCY is equal to the total number of words tokens in the windows around occurrences of t. We describe 1Measures of association Sheskin_comma_ 2003 cosB4wBNcB5BP nwc D4n wA3 A2 D4n A3c BN pmiB4wBNcB5BP log nwcA2Nn wA3 A2n A3c BN oddsB4wBNcB5BP nwcA2n BMBM nw BM A2n BMc BN YuleB4wBNcB5BP D4odds B4wBNcB5A01 D4odds B4wBNcB5B71 BN DiceB4wBNcB5BP 2A2nwcn wA3B7nA3c BN B4wBNcB5BP B4nwcA2n BMBM B5A0B4nw BM A2n BMcB5 D4n wA3A2nBMA3A2nA3cA2nA3BM UnweightedWeighted disambiguation Implicit sense Explicit sense disambiguation votingvoting DI_comma_W DE_comma_W DI_comma_U E_comma_UD Figure 3 The four dominance methods. four methods Figure 3 to determine dominance DI BNW BNDIBNUBNDEBNWBN and DEBNU and the underlying assumptions of each.
E06-1016.txt,84,DI BNW is based on the assumption that the more dominant a particular sense is_comma_ the greater the strength of its association with words that co occur with it.
E06-1016.txt,85,For example_comma_ if most occurrences of bank in the target text correspond to river bank _comma_ then the strength of association of river bank with all of bank s co occurring words will be larger than the sum for any other sense.
E06-1016.txt,86,Dominance DI BNW of a sense or category c of the target word t is DI BNWB4tBNcB5BP w BET AB4wBNcB5 cBC BEsensesB4tB5 w BET AB4wBNcBCB5 1 where A is any one of the measures of association from section 3.3.
E06-1016.txt,87,Metaphorically_comma_ words that cooccur with the target word give a weighted vote to each of its senses.
E06-1016.txt,88,The weight is proportional to the strength of association between the sense and the co occurring word.
E06-1016.txt,89,The dominance of a sense is the ratio of the total votes it gets to the sum of votes received by all the senses.
E06-1016.txt,90,A slightly different assumption is that the more dominant a particular sense is_comma_ the greater the number of co occurring words having highest strength of association with that sense as opposed to any other .
E06-1016.txt,91,This leads to the following methodology.
E06-1016.txt,92,Each co occurring word casts an equal_comma_ unweighted vote.
E06-1016.txt,93,It votes for that sense and no other of the target word with which it has the highest strength of association.
E06-1016.txt,94,The dominance DI BNU of the sense is the ratio of the votes it gets to the total votes cast for the word number of cooccurring words .
E06-1016.txt,95,DI BNUB4tBNcB5 BP CYCUw BE T Sns1B4wBNtB5BP cCVCY CYTCY 2 Sns1B4wBNtB5 BP argmax cBCBEsensesB4tB5 AB4wBNcBCB5 3 Observe that in order to determine DI BNW or DI BNU_comma_ we do not need to explicitly disambiguate 124 the senses of the target word s occurrences.
E06-1016.txt,96,We now describe alternative approaches that may be used for explicit sense disambiguation of the target word s occurrences and thereby determine sense dominance the proportion of occurrences of that sense .
E06-1016.txt,97,DE BNW relies on the hypothesis that the intended sense of any occurrence of the target word has highest strength of association with its cooccurring words.
E06-1016.txt,98,DE BNWB4tBNcB5 BP CYCUTBC BECGt Sns2B4TBCBNtB5 BP cCVCY CYCGtCY 4 Sns2B4TBCBNtB5 BP argmax cBCBEsensesB4tB5 wBETBC AB4wBNcBCB5 5 Metaphorically_comma_ words that co occur with the target word give a weighted vote to each of its senses just as in DI BNW .
E06-1016.txt,99,However_comma_ votes from co occurring words in an occurrence are summed to determine the intended sense sense with the most votes of the target word.
E06-1016.txt,100,The process is repeated for all occurrences that have the target word.
E06-1016.txt,101,If each word that co occurs with the target word votes as described for DI BNU_comma_ then the following hypothesis forms the basis of DE BNU in a particular occurrence_comma_ the sense that gets the maximum votes from its neighbors is the intended sense.
E06-1016.txt,102,DE BNUB4tBNcB5 BP CYCUTBC BECGt Sns3B4TBCBNtB5 BP cCVCY CYCGtCY 6 Sns3B4TBCBNtB5 BP argmax cBCBEsensesB4tB5 CYCUw BE T BC Sns1 B4wBNtB5 BP cBCCVCY 7 In methods DE BNW and DEBNU_comma_ the dominance of a sense is the proportion of occurrences of that sense.
E06-1016.txt,103,The degree of dominance provided by all four methods has the following properties i The dominance values are in the range 0 to 1 a score of 0 implies lowest possible dominance_comma_ while a score of 1 means that the dominance is highest.
E06-1016.txt,104,ii The dominance values for all the senses of a word sum to 1.
E06-1016.txt,105,5 Pseudo Thesaurus Sense Tagged Data To evaluate the four dominance methods we would ideally like sentences with target words annotated with senses from the thesaurus.
E06-1016.txt,106,Since human annotation is both expensive and time intensive_comma_ we present an alternative approach of artificially generating thesaurus sense tagged data following the ideas of Leacock et al. 1998 .
E06-1016.txt,107,Around 63_comma_700 of the 98_comma_000 word types in the Macquarie Thesaurus are monosemous listed under just one of the 812 categories.
E06-1016.txt,108,This means that on average around 77 c terms per category are monosemous.
E06-1016.txt,109,Pseudo thesaurus sense tagged PTST data for a non monosemous target word t for example_comma_ brilliant used in a particular sense or category c of the thesaurus for example_comma_ intelligence may be generated as follows.
E06-1016.txt,110,Identify monosemous c terms for example_comma_ clever belonging to the same category as c.
E06-1016.txt,111,Pick sentences containing the monosemous c terms from an untagged auxiliary text corpus.
E06-1016.txt,112,Hermione had a clever plan.
E06-1016.txt,113,In each such sentence_comma_ replace the monosemous word with the target word t.
E06-1016.txt,114,In theory the cterms in a thesaurus are near synonyms or at least strongly related words_comma_ making the replacement of one by another acceptable.
E06-1016.txt,115,For the sentence above_comma_ we replace clever with brilliant.
E06-1016.txt,116,This results in artificial sentences with the target word used in a sense corresponding to the desired category.
E06-1016.txt,117,Clearly_comma_ many of these sentences will not be linguistically well formed_comma_ but the non monosemous c term used in a particular sense is likely to have similar co occurring words as the monosemous cterm of the same category.2 This justifies the use of these pseudo thesaurus sense tagged data for the purpose of evaluation.
E06-1016.txt,118,We generated PTST test data for the head words in SENSEVAL 1 English lexical sample space3 using the Macquarie Thesaurus and the held out subset of the BNC every twelfth sentence .
E06-1016.txt,119,6 Experiments We evaluate the four dominance methods_comma_ like McCarthy et al. 2004 _comma_ through the accuracy of a naive sense disambiguation system that always gives out the predominant sense of the target word.
E06-1016.txt,120,In our experiments_comma_ the predominant sense is determined by each of the four dominance methods_comma_ individually.
E06-1016.txt,121,We used the following setup to study the effect of sense distribution on performance.
E06-1016.txt,122,2Strong collocations are an exception to this_comma_ and their effect must be countered by considering larger window sizes.
E06-1016.txt,123,Therefore_comma_ we do not use a window size of just one or two words on either side of the target word_comma_ but rather windows of A65 words in our experiments.
E06-1016.txt,124,3SENSEVAL 1 head words have a wide range of possible senses_comma_ and availability of alternative sense tagged data may be exploited in the future.
E06-1016.txt,125,125 phi_comma_ pmi_comma_ odds_comma_ Yule .11I_comma_UD 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 baselinebaseline Accuracy Distribution alpha Mean distance below upper bound DE_comma_W pmi_comma_ odds_comma_ Yule pmi phi_comma_ pmi_comma_ D DI_comma_U I_comma_W E_comma_U I_comma_W phi_comma_ pmi_comma_ odds_comma_ Yule .16 pmi .03 D DDE_comma_W pmi_comma_ odds_comma_ Yule .02 phi_comma_ pmi_comma_DE_comma_U upper boundupper bound odds_comma_ Yule odds_comma_ Yule lower bound lower bound Figure 4 Best results four dominance methods 6.1 Setup For each target word for which we have PTST data_comma_ the two most dominant senses are identified_comma_ say s1 and s2.
E06-1016.txt,126,If the number of sentences annotated with s1 and s2 is x and y_comma_ respectively_comma_ where x BQy_comma_ then all y sentences of s2 and the first y sentences of s1 are placed in a data bin.
E06-1016.txt,127,Eventually the bin contains an equal number of PTST sentences for the two most dominant senses of each target word.
E06-1016.txt,128,Our data bin contained 17_comma_446 sentences for 27 nouns_comma_ verbs_comma_ and adjectives.
E06-1016.txt,129,We then generate different test data sets d from the bin_comma_ where takes values 0BNBM1BNBM2BNBMBMBMBN1_comma_ such that the fraction of sentences annotated with s1 is and those with s2 is 1A0 .
E06-1016.txt,130,Thus the data sets have different dominance values even though they have the same number of sentences half as many in the bin.
E06-1016.txt,131,Each data set d is given as input to the naive sense disambiguation system.
E06-1016.txt,132,If the predominant sense is correctly identified for all target words_comma_ then the system will achieve highest accuracy_comma_ whereas if it is falsely determined for all target words_comma_ then the system achieves the lowest accuracy.
E06-1016.txt,133,The value of determines this upper bound and lower bound.
E06-1016.txt,134,If is close to 0BM5_comma_ then even if the system correctly identifies the predominant sense_comma_ the naive disambiguation system cannot achieve accuracies much higher than 50 .
E06-1016.txt,135,On the other hand_comma_ if is close to 0 or 1_comma_ then the system may achieve accuracies close to 100 .
E06-1016.txt,136,A disambiguation system that randomly chooses one of the two possible senses for each occurrence of the target word will act as the baseline.
E06-1016.txt,137,Note that no matter what the distribution of the two senses _comma_ this system will get an accuracy of 50 .
E06-1016.txt,138,DI_comma_W odds _comma_ base .08E_comma_W odds _comma_ bootstrapped .02D Mean distance below upper bound 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 upper bound upper bound baselinebaseline Accuracy Distribution alpha DE_comma_W odds _comma_ bootstrapped odds _comma_ baseDI_comma_W lower bound lower bound Figure 5 Best results base vs. bootstrapped 6.2 Results Highest accuracies achieved using the four dominance methods and the measures of association that worked best with each are shown in Figure 4.
E06-1016.txt,139,The table below the figure shows mean distance below upper bound MDUB for all values considered.
E06-1016.txt,140,Measures that perform almost identically are grouped together and the MDUB values listed are averages.
E06-1016.txt,141,The window size used was A65 words around the target word.
E06-1016.txt,142,Each dataset d _comma_ which corresponds to a different target text in Figure 2_comma_ was processed in less than 1 second on a 1.3GHz machine with 16GB memory.
E06-1016.txt,143,Weighted voting methods_comma_ DE BNW and DIBNW _comma_ perform best with MDUBs of just .02 and .03_comma_ respectively.
E06-1016.txt,144,Yule s coefficient_comma_ odds ratio_comma_ and pmi give near identical_comma_ maximal accuracies for all four methods with a slightly greater divergence in DI BNW _comma_ where pmi does best.
E06-1016.txt,145,The coefficient performs best for unweighted methods.
E06-1016.txt,146,Dice and cosine do only slightly better than the baseline.
E06-1016.txt,147,In general_comma_ results from the method measure combinations are symmetric across BP0BM5_comma_ as they should be.
E06-1016.txt,148,Marked improvements in accuracy were achieved as a result of bootstrapping the WCCM Figure 5 .
E06-1016.txt,149,Most of the gain was provided by the first iteration itself_comma_ whereas further iterations resulted in just marginal improvements.
E06-1016.txt,150,All bootstrapped results reported in this paper pertain to just one iteration.
E06-1016.txt,151,Also_comma_ the bootstrapped WCCM is 72 smaller_comma_ and 5 times faster at processing the data sets_comma_ than the base WCCM_comma_ which has many non zero cells even though the corresponding word and category never actually co occurred as mentioned in Section 3.2 earlier .
E06-1016.txt,152,126 6.3 Discussion Considering that this is a completely unsupervised approach_comma_ not only are the accuracies achieved using the weighted methods well above the baseline_comma_ but also remarkably close to the upper bound.
E06-1016.txt,153,This is especially true for values close to 0 and 1.
E06-1016.txt,154,The lower accuracies for near 0.5 are understandable as the amount of evidence towards both senses of the target word are nearly equal.
E06-1016.txt,155,Odds_comma_ pmi_comma_ and Yule perform almost equally well for all methods.
E06-1016.txt,156,Since the number of times two words co occur is usually much less than the number of times they occur individually_comma_ pmi tends to approximate the logarithm of odds ratio.
E06-1016.txt,157,Also_comma_ Yule is a derivative of odds.
E06-1016.txt,158,Thus all three measures will perform similarly in case the co occurring words give an unweighted vote for the most appropriate sense of the target as in DI BNU and DE BNU.
E06-1016.txt,159,For the weighted voting schemes_comma_ DIBNW and DE BNW_comma_ the effect of scale change is slightly higher in DI BNW as the weighted votes are summed over the complete text to determine dominance.
E06-1016.txt,160,In DE BNW the small number of weighted votes summed to determine the sense of the target word may be the reason why performances using pmi_comma_ Yule_comma_ and odds do not differ markedly.
E06-1016.txt,161,Dice coefficient and cosine gave below baseline accuracies for a number of sense distributions.
E06-1016.txt,162,This suggests that the normalization4 to take into account the frequency of individual events inherent in the Dice and cosine measures may not be suitable for this task.
E06-1016.txt,163,The accuracies of the dominance methods remain the same if the target text is partitioned as per the target word_comma_ and each of the pieces is given individually to the disambiguation system.
E06-1016.txt,164,The average number of sentences per target word in each dataset d is 323.
E06-1016.txt,165,Thus the results shown above correspond to an average target text size of only 323 sentences.
E06-1016.txt,166,We repeated the experiments on the base WCCM after filtering out setting to 0 cells with frequency less than 5 to investigate the effect on accuracies and gain in computation time proportional to size of WCCM .
E06-1016.txt,167,There were no marked changes in accuracy but a 75 reduction in size of the WCCM.
E06-1016.txt,168,Using a window equal to the complete sentence as opposed to A65 words on either side of the target resulted in a drop of accuracies.
E06-1016.txt,169,4If two events occur individually a large number of times_comma_ then they must occur together much more often to get substantial association scores through pmi or odds_comma_ as compared to cosine or the Dice coefficient.
E06-1016.txt,170,7 Related Work The WCCM has similarities with latent semantic analysis_comma_ or LSA_comma_ and specifically with work by Sch utze and Pedersen 1997 _comma_ wherein the dimensionality of a word word co occurrence matrix is reduced to create a word concept matrix.
E06-1016.txt,171,However_comma_ there is no non heuristic way to determine when the dimension reduction should stop.
E06-1016.txt,172,Further_comma_ the generic concepts represented by the reduced dimensions are not interpretable_comma_ i.e._comma_ one cannot determine which concepts they represent in a given sense inventory.
E06-1016.txt,173,This means that LSA cannot be used directly for tasks such as unsupervised sense disambiguation or determining semantic similarity of known concepts.
E06-1016.txt,174,Our approach does not have these limitations.
E06-1016.txt,175,Yarowsky 1992 uses the product of a mutual information like measure and frequency to identify words that best represent each category in the Roget s Thesaurus and uses these words for sense disambiguation with a Bayesian model.
E06-1016.txt,176,We improved the accuracy of the WCCM using simple bootstrapping techniques_comma_ used all the words that co occur with a category_comma_ and proposed four new methods to determine sense dominance two of which do explicit sense disambiguation.
E06-1016.txt,177,V eronis 2005 presents a graph theory based approach to identify the various senses of a word in a text corpus without the use of a dictionary.
E06-1016.txt,178,Highly interconnected components of the graph represent the different senses of the target word.
E06-1016.txt,179,The node word with the most connections in a component is representative of that sense and its associations with words that occur in a test instance are used as evidence for that sense.
E06-1016.txt,180,However_comma_ these associations are at best only rough estimates of the associations between the sense and co occurring words_comma_ since a sense in his system is represented by a single possibly ambiguous word.
E06-1016.txt,181,Pantel 2005 proposes a framework for ontologizing lexical resources.
E06-1016.txt,182,For example_comma_ co occurrence vectors for the nodes in WordNet can be created using the cooccurrence vectors for words or lexicals .
E06-1016.txt,183,However_comma_ if a leaf node has a single lexical_comma_ then once the appropriate co occurring words for this node are identified coup phase _comma_ they are assigned the same co occurrence counts as that of the lexical.5 5A word may have different_comma_ stronger than chance strengths of association with multiple senses of a lexical.
E06-1016.txt,184,These are different from the association of the word with the lexical.
E06-1016.txt,185,127 8 Conclusions and Future Directions We proposed a new method for creating a word category co occurrence matrix WCCM using a published thesaurus and raw text_comma_ and applying simple sense disambiguation and bootstrapping techniques.
E06-1016.txt,186,We presented four methods to determine degree of dominance of a sense of a word using the WCCM.
E06-1016.txt,187,We automatically generated sentences with a target word annotated with senses from the published thesaurus_comma_ which we used to perform an extensive evaluation of the dominance methods.
E06-1016.txt,188,We achieved near upper bound results using all combinations of the the weighted methods DI BNW and DEBNW and three measures of association odds_comma_ pmi_comma_ and Yule .
E06-1016.txt,189,We cannot compare accuracies with McCarthy et al. 2004 because use of a thesaurus instead of WordNet means that knowledge of exactly how the thesaurus senses map to WordNet is required.
E06-1016.txt,190,We used a thesaurus as such a resource_comma_ unlike WordNet_comma_ is available in more languages_comma_ provides us with coarse senses_comma_ and leads to a smaller WCCM making computationally intensive operations viable .
E06-1016.txt,191,Further_comma_ unlike the McCarthy et al. system_comma_ we showed that our system gives accurate results without the need for a large similarlysense distributed text or retraining.
E06-1016.txt,192,The target texts used were much smaller few hundred sentences than those needed for automatic creation of a thesaurus few million words .
E06-1016.txt,193,The WCCM has a number of other applications_comma_ as well.
E06-1016.txt,194,The strength of association between a word and a word sense can be used to determine the more intuitive distributional similarity of word senses as opposed to words .
E06-1016.txt,195,Conditional probabilities of lexical features can be calculated from the WCCM_comma_ which in turn can be used in unsupervised sense disambiguation.
E06-1016.txt,196,In conclusion_comma_ we provided a framework for capturing distributional properties of word senses from raw text and demonstrated one of its uses determining word sense dominance.
E06-1016.txt,197,Acknowledgments We thank Diana McCarthy_comma_ Afsaneh Fazly_comma_ and Suzanne Stevenson for their valuable feedback.
E06-1016.txt,198,This research is financially supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto. .
E06-3003.txt,1,paper describes a system that produces extractive summaries of short works of literary fiction. The ultimate purpose of produced summaries is defined as helping a reader to determine whether she would be interested in reading a particular story.
E06-3003.txt,2,To this end_comma_ the summary aims to provide a reader with an idea about the settings of a story such as characters_comma_ time and place without revealing the plot.
E06-3003.txt,3,The approach presented here relies heavily on the notion of aspect.
E06-3003.txt,4,Preliminary results show an improvement over two na ve baselines a lead baseline and a more sophisticated variant of it.
E06-3003.txt,5,Although modest_comma_ the results suggest that using aspectual information may be of help when summarizing fiction.
E06-3003.txt,6,A more thorough evaluation involving human judges is under way.
E06-3003.txt,8,In the course of recent years the scientific community working on the problem of automatic text summarization has been experiencing an upsurge.
E06-3003.txt,9,A multitude of different techniques has been applied to this end_comma_ some of the more remarkable of them being Marcu_comma_ 1997 Mani et al. 1998 Teufel and Moens_comma_ 2002 Elhadad et al._comma_ 2005 _comma_ to name just a few.
E06-3003.txt,10,These researchers worked on various text genres scientific and popular scientific articles Marcu_comma_ 1997 Mani et al._comma_ 1998 _comma_ texts in computational linguistics Teufel and Moens_comma_ 2002 _comma_ and medical texts Elhadad et al._comma_ 2002 .
E06-3003.txt,11,All these genres are examples of texts characterized by rigid structure_comma_ relative abundance of surface markers and straightforwardness.
E06-3003.txt,12,Relatively few attempts have been made at summarizing less structured genres_comma_ some of them being dialogue and speech summarization Zechner_comma_ 2002 Koumpis et al. 2001 .
E06-3003.txt,13,The issue of summarizing fiction remains largely untouched_comma_ since a few very thorough earlier works Charniak_comma_ 1972 Lehnert_comma_ 1982 .
E06-3003.txt,14,The work presented here seeks to fill in this gap.
E06-3003.txt,15,The ultimate objective of the project is stated as follows to produce indicative summaries of short works of fiction such that they be helpful to a potential reader in deciding whether she would be interested in reading a particular story or not.
E06-3003.txt,16,To this end_comma_ revealing the plot was deemed unnecessary and even undesirable.
E06-3003.txt,17,Instead_comma_ the current approach relies on the following assumption when a reader is presented with an extracted summary outlining the general settings of a story such as time_comma_ place and who it is about _comma_ she will have enough information to decide how interested she would be in reading a story.
E06-3003.txt,18,For example_comma_ a fragment of such a summary_comma_ produced by an annotator for the story The Cost of Kindness by Jerome K.
E06-3003.txt,19,Jerome is presented in Figure 1.
E06-3003.txt,20,The plot_comma_ which is a tale of how one local family decides to bid a warm farewell to Rev.
E06-3003.txt,21,Cracklethorpe and causes the vicar to change his mind and remain in town_comma_ is omitted.
E06-3003.txt,22,The data used in the experiments consisted of 23 short stories_comma_ all written in XIX early XX century by main stream authors such as Katherine Mansfield_comma_ Anton Chekhov_comma_ O.Henry_comma_ Guy de Maupassant and others 13 authors in total .
E06-3003.txt,23,The genre can be vaguely termed social fiction with the exception of a few fairy tales.
E06-3003.txt,24,Such vagueness as far as genre is concerned was deliberate_comma_ as the author wished to avoid producing a system relying on cues specific to a particular genre.
E06-3003.txt,25,Average length of a story in the corpus is 3_comma_333 tokens approximately 4.5 letter sized pages and the target compression rate is 6 .
E06-3003.txt,26,In order to separate the background of a story from events_comma_ this project relies heavily on the notion of aspect the term is explained in Section 3.1 .
E06-3003.txt,27,Each clause of every sentence is described in terms of aspect related features.
E06-3003.txt,28,This representation is then used to select salient descriptive sentences and to leave out those which describe events.
E06-3003.txt,29,55 The organization of the paper follows the overall architecture of the system.
E06-3003.txt,30,Section 2 provides a generalized overview of the preprocessing stage of the project_comma_ during which pronominal and nominal anaphoric .
E06-2017.txt,1,translation probabilities proved an effective method of semantic smoothing in the language modelling approach to information retrieval tasks. In this paper_comma_ we use Generalized Latent Semantic Analysis to compute semantically motivated term and document vectors.
E06-2017.txt,2,The normalized cosine similarity between the term vectors is used as term translation probability in the language modelling framework.
E06-2017.txt,3,Our experiments demonstrate that GLSAbased term translation probabilities capture semantic relations between terms and improve performance on document classification.
E06-2017.txt,5,Many recent applications such as document summarization_comma_ passage retrieval and question answering require a detailed analysis of semantic relations between terms since often there is no large context that could disambiguate words s meaning.
E06-2017.txt,6,Many approaches model the semantic similarity between documents using the relations between semantic classes of words_comma_ such as representing dimensions of the document vectors with distributional term clusters Bekkerman et al._comma_ 2003 and expanding the document and query vectors with synonyms and related terms as discussed in Levow et al._comma_ 2005 .
E06-2017.txt,7,They improve the performance on average_comma_ but also introduce some instability and thus increased variance Levow et al._comma_ 2005 .
E06-2017.txt,8,The language modelling approach Ponte and Croft_comma_ 1998 Berger and Lafferty_comma_ 1999 proved very effective for the information retrieval task.
E06-2017.txt,9,Berger et. al Berger and Lafferty_comma_ 1999 used translation probabilities between terms to account for synonymy and polysemy.
E06-2017.txt,10,However_comma_ their model of such probabilities was computationally demanding.
E06-2017.txt,11,Latent Semantic Analysis LSA Deerwester et al._comma_ 1990 is one of the best known dimensionality reduction algorithms.
E06-2017.txt,12,Using a bag of words document vectors Salton and McGill_comma_ 1983 _comma_ it computes a dual representation for terms and documents in a lower dimensional space.
E06-2017.txt,13,The resulting document vectors reside in the space of latent semantic concepts which can be expressed using different words.
E06-2017.txt,14,Thestatistical analysis ofthesemantic relatedness between terms is performed implicitly_comma_ in the course of a matrix decomposition.
E06-2017.txt,15,In this project_comma_ we propose to use a combination of dimensionality reduction and language modelling to compute the similarity between documents.
E06-2017.txt,16,We compute term vectors using the Generalized Latent Semantic Analysis Matveeva et al._comma_ 2005 .
E06-2017.txt,17,This method uses co occurrence based measures of semantic similarity between terms to compute low dimensional term vectors in the space of latent semantic concepts.
E06-2017.txt,18,The normalized cosine similarity between the term vectors is used as term translation probability.
E06-2017.txt,19,2 Term Translation Probabilities in Language Modelling The language modelling approach Ponte and Croft_comma_ 1998 proved very effective for the information retrieval task.
E06-2017.txt,20,This method assumes that every document defines a multinomial probability distribution p w d over the vocabulary space.
E06-2017.txt,21,Thus_comma_ given a query q q1_comma_..._comma_qm _comma_ the likelihood of the query is estimated using the document s distribution p q d producttextm1 p qi d _comma_ where 151 qi are query terms.
E06-2017.txt,22,Relevant documents maximize p d q p q d p d .
E06-2017.txt,23,Many relevant documents may not contain the same terms as the query.
E06-2017.txt,24,However_comma_ they may contain terms that are semantically related to the query terms and thus have high probability of being translations _comma_ i.e. re formulations for the query words.
E06-2017.txt,25,Berger et. al Berger and Lafferty_comma_ 1999 introduced translation probabilities between words into the document to query model as a way of semantic smoothing of the conditional word probabilities.
E06-2017.txt,26,Thus_comma_ they query document similarity is computed as p q d mproductdisplay i summationdisplay w d t qi w p w d .
E06-2017.txt,27,1 Each document word w is a translation of a query term qi with probability t qi w .
E06-2017.txt,28,This approach showed improvements over the baseline language modelling approach Berger and Lafferty_comma_ 1999 .
E06-2017.txt,29,The estimation of the translation probabilities is_comma_ however_comma_ a difficult task.
E06-2017.txt,30,Lafferty and Zhai used a Markov chain on words and documents to estimate the translation probabilities Lafferty and Zhai_comma_ 2001 .
E06-2017.txt,31,We use the Generalized Latent Semantic Analysis to compute the translation probabilities.
E06-2017.txt,32,2.1 Document Similarity We propose to use low dimensional term vectors for inducing the translation probabilities between terms.
E06-2017.txt,33,Wepostpone thediscussion ofhowtheterm vectors are computed to section 2.2.
E06-2017.txt,34,To evaluate the validity of this approach_comma_ we applied it to document classification.
E06-2017.txt,35,We used two methods of computing the similarity between documents.
E06-2017.txt,36,First_comma_ we computed the language modelling score using term translation probabilities.
E06-2017.txt,37,Once the term vectors are computed_comma_ the document vectors are generated as linear combinations of term vectors.
E06-2017.txt,38,Therefore_comma_ we also used the cosine similarity between the documents to perform classificaiton.
E06-2017.txt,39,We computed the language modelling score of a test document d relative to a training document di as p d di productdisplay v d summationdisplay w di t v w p w di .
E06-2017.txt,40,2 Appropriately normalized values of the cosine similarity measure between pairs of term vectors cos vectorv_comma_ vectorw are used as the translation probability between the corresponding terms t v w .
E06-2017.txt,41,In addition_comma_ we used the cosine similarity between the document vectors vectordi_comma_ vectordj summationdisplay w di summationdisplay v dj diw djv vectorw_comma_vectorv _comma_ 3 where diw and djv represent the weight of the terms w and v with respect to the documents di and dj_comma_ respectively.
E06-2017.txt,42,Inthis case_comma_ theinner products between the term vectors are also used to compute the similarity between the document vectors.
E06-2017.txt,43,Therefore_comma_ the cosine similarity between the document vectors also depends on the relatedness between pairs of terms.
E06-2017.txt,44,We compare these two document similarity scores to the cosine similarity between bag ofword document vectors.
E06-2017.txt,45,Our experiments show that these two methods offer an advantage for document classification.
E06-2017.txt,46,2.2 Generalized Latent Semantic Analysis We use the Generalized Latent Semantic Analysis GLSA Matveeva et al._comma_ 2005 to compute semantically motivated term vectors.
E06-2017.txt,47,TheGLSAalgorithm computes thetermvectors for the vocabulary of the document collection C with vocabulary V using a large corpus W.
E06-2017.txt,48,It has the following outline 1.
E06-2017.txt,49,Construct the weighted term document matrix D based on C 2.
E06-2017.txt,50,For the vocabulary words in V_comma_ obtain a matrix of pair wise similarities_comma_ S_comma_ using the large corpus W 3.
E06-2017.txt,51,Obtain the matrix UT of low dimensional vector space representation of terms that preserves the similarities in S_comma_ UT Rk V 4.
E06-2017.txt,52,Compute document vectors by taking linear combinations of term vectors D UTD The columns of D are documents in the kdimensional space.
E06-2017.txt,53,In step 2 we used point wise mutual information PMI as the co occurrence based measure of semantic associations between pairs of the vocabulary terms.
E06-2017.txt,54,PMI has been successfully applied to semantic proximity tests for words Turney_comma_ 2001 Terra and Clarke_comma_ 2003 and was also successfully used as a measure of term similarity to compute document clusters Pantel and Lin_comma_ 2002 .
E06-2017.txt,55,In 152 our preliminary experiments_comma_ the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio_comma_ and 2 test.
E06-2017.txt,56,PMI between random variables representing two words_comma_ w1 and w2_comma_ is computed as PMI w1_comma_w2 log P W1 1_comma_W2 1 P W 1 1 P W2 1 .
E06-2017.txt,57,4 We used the singular value decomposition SVD in step 3 to compute GLSA term vectors.
E06-2017.txt,58,LSA Deerwester et al._comma_ 1990 and some other related dimensionality reduction techniques_comma_ e.g.
E06-2017.txt,59,Locality Preserving Projections He and Niyogi_comma_ 2003 compute a dual document term representation.
E06-2017.txt,60,The main advantage of GLSA is that it focuses on term vectors which allows for a greater flexibility in the choice of the similarity matrix.
E06-2017.txt,61,3 Experiments The goal of the experiments was to understand whether the GLSA term vectors can be used to model the term translation probabilities.
E06-2017.txt,62,We used a simple k NN classifier and a basic baseline to evalute the performance.
E06-2017.txt,63,We used the GLSAbased term translation probabilities within the language modelling framework and GLSA document vectors.
E06-2017.txt,64,We used the 20 news groups data set because previous studies showed that the classification performance on this document collection can noticeably benefit from additional semantic information Bekkerman et al._comma_ 2003 .
E06-2017.txt,65,For the GLSA computations we used the terms that occurred in at least 15 documents_comma_ and had a vocabulary of 9732 terms.
E06-2017.txt,66,We removed documents with fewer than 5 words.
E06-2017.txt,67,Here we used 2 sets of 6 news groups.
E06-2017.txt,68,Groupd contained documents from dissimilar news groups1_comma_ with a total of 5300 documents.
E06-2017.txt,69,Groups contained documents from more similar news groups2 and had 4578 documents.
E06-2017.txt,70,3.1 GLSA Computation To collect the co occurrence statistics for the similarities matrix S we used the English Gigaword collection LDC .
E06-2017.txt,71,We used 1_comma_119_comma_364 New York Times articles labeled story with 771_comma_451 terms.
E06-2017.txt,72,1os.ms_comma_ sports.baseball_comma_ rec.autos_comma_ sci.space_comma_ misc.forsale_comma_ religion christian 2politics.misc_comma_ politics.mideast_comma_ politics.guns_comma_ religion.misc_comma_ religion.christian_comma_ atheism Groupd Groups L tf Glsa LM tf Glsa LM 100 0.58 0.75 0.69 0.42 0.48 0.48 200 0.65 0.78 0.74 0.47 0.52 0.51 400 0.69 0.79 0.76 0.51 0.56 0.55 1000 0.75 0.81 0.80 0.58 0.60 0.59 2000 0.78 0.83 0.83 0.63 0.64 0.63 Table 1 k NN classification accuracy for 20NG.
E06-2017.txt,73,Figure 1 k NN with 400 training documents.
E06-2017.txt,74,We used the Lemur toolkit3 to tokenize and index the document we used stemming and a list of stopwords.
E06-2017.txt,75,Unlessstated otherwise_comma_ fortheGLSA methods we report the best performance over different numbers of embedding dimensions.
E06-2017.txt,76,Theco occurrence counts can be obtained using either term co occurrence within the same document or within a sliding window of certain fixed size.
E06-2017.txt,77,In our experiments we used the windowbased approach which was shown to give better results Terra and Clarke_comma_ 2003 .
E06-2017.txt,78,We used the window of size 4.
E06-2017.txt,79,3.2 Classification Experiments We ran the k NN classifier with k 5 on ten random splits of training and test sets_comma_ with different numbers of training documents.
E06-2017.txt,80,The baseline was to use the cosine similarity between the bag ofwords document vectors weighted with term frequency.
E06-2017.txt,81,Other weighting schemes such as maximum likelihood and Laplace smoothing did not improve results.
E06-2017.txt,82,Table 1 shows the results.
E06-2017.txt,83,We computed the score between the training and test documents using two approaches cosine similarity between the GLSA document vectors according to Equation 3 denoted as GLSA _comma_ and the language modelling score which included the translation probabilities between the terms as in Equation 2 denoted as 3http www.lemurproject.org 153 LM .
E06-2017.txt,84,We used the term frequency as an estimate for p w d .
E06-2017.txt,85,To compute the matrix of translation probabilities P_comma_ where P i j t tj ti for the LMCLSA approach_comma_ we first obtained the matrix P i j cos vectorti_comma_vectortj .
E06-2017.txt,86,We set the negative and zero entries in P to a small positive value.
E06-2017.txt,87,Finally_comma_ we normalized the rows of P to sum up to one.
E06-2017.txt,88,Table 1 shows that for both settings GLSA and LM outperform the tf document vectors.
E06-2017.txt,89,As expected_comma_ the classification task was more difficult for the similar news groups.
E06-2017.txt,90,However_comma_ in this case both GLSA based approaches outperform the baseline.
E06-2017.txt,91,In both cases_comma_ the advantage is more significant with smaller sizes of the training set.
E06-2017.txt,92,GLSA and LM performance usually peaked at around 300 500 dimensions which is in line with results for other SVD based approaches Deerwester et al._comma_ 1990 .
E06-2017.txt,93,When the highest accuracy was achieved at higher dimensions_comma_ the increase after 500 dimensions was rather small_comma_ as illustrated in Figure 1.
E06-2017.txt,94,These results illustrate that the pair wise similarities between the GLSA term vectors add important semantic information which helps to go beyond term matching and deal with synonymy and polysemy.
E06-2017.txt,95,4 Conclusion and Future Work We used the GLSA to compute term translation probabilities as a measure of semantic similarity between documents.
E06-2017.txt,96,We showed that the GLSA term based document representation and GLSAbased term translation probabilities improve performance on document classification.
E06-2017.txt,97,The GLSA term vectors were computed for all vocabulary terms.
E06-2017.txt,98,However_comma_ different measures of similarity may be required for different groups of terms such as content bearing general vocabulary words and proper names as well as other named entities.
E06-2017.txt,99,Furthermore_comma_ different measures of similarity work best for nouns and verbs.
E06-2017.txt,100,To extend this approach_comma_ we will use a combination of similarity measures between terms to model the document similarity.
E06-2017.txt,101,We will divide the vocabulary into general vocabulary terms and named entities and compute a separate similarity score for each of the group of terms.
E06-2017.txt,102,The overall similarity score is a function of these two scores.
E06-2017.txt,103,In addition_comma_ we will use the GLSA based score together with syntactic similarity to compute the similarity between the general vocabulary terms. .
E06-2025.txt,1,analyze estimation methods for DataOriented Parsing_comma_ as well as the theoretical criteria used to evaluate them. We show that all current estimation methods areinconsistent inthe weight distribution test _comma_ and argue that these results force us to rethink both the methods proposed and the criteria used.
E06-2025.txt,3,Stochastic Tree Substitution Grammars henceforth_comma_ STSGs are a simple generalization of Probabilistic Context Free Grammars_comma_ where the productive elements are not rewrite rules but elementary trees of arbitrary size.
E06-2025.txt,4,The increased flexibility allows STSGs to model a variety of syntactic and statistical dependencies_comma_ using relatively complexprimitivesbut just asingle andextremelysimpleglobal rule substitution.
E06-2025.txt,5,STSGscanbeseenas Stochastic Tree Adjoining Grammars without the adjunction operation.
E06-2025.txt,6,STSGsaretheunderlying formalism of most instantiations of an approach to statistical parsing known as Data Oriented Parsing Scha_comma_ 1990 Bod_comma_ 1998 .
E06-2025.txt,7,In this approach the subtrees of the trees in a tree bank are used as elementary trees of the grammar.
E06-2025.txt,8,In most DOP models the grammar used is an STSGwith_comma_ in principle_comma_ all subtrees1 of the trees in the tree bank as elementary trees.
E06-2025.txt,9,For disambiguation_comma_ the best parse tree is taken to be the most probable parse according to the weights of the grammar.
E06-2025.txt,10,Several methods have been proposed to decide on the weights based on observed tree frequencies 1A subtree tprime of a parse tree t isa tree such that every node iprime in tprime equals a node i in t_comma_ and iprime either has no daughters or the same daughter nodes as i. inatreebank.
E06-2025.txt,11,Thefirst suchmethod isnowknown as DOP1 Bod_comma_ 1993 .
E06-2025.txt,12,In combination with someheuristic constraints on the allowed subtrees_comma_ it has been remarkably successful on small tree banks.
E06-2025.txt,13,Despite this empirical success_comma_ Johnson_comma_ 2002 argued that it is inadequate because it is biased and inconsistent.
E06-2025.txt,14,His criticism spearheaded a number of other methods_comma_ including Bonnema et al._comma_ 1999 Bod_comma_ 2003 Sima an and Buratto_comma_ 2003 Zollmann and Sima an_comma_ 2005 _comma_ and will be the starting point of our analysis.
E06-2025.txt,15,As it turns out_comma_ the DOP1 method really is biased and inconsistent_comma_ but not for the reasons Johnson gives_comma_ and it really is inadequate_comma_ but not because it is biased and inconsistent.
E06-2025.txt,16,In this note_comma_ we further show that alternative methods that have been proposed_comma_ only partly remedy the problems with DOP1_comma_ leaving weight estimation as an important open problem.
E06-2025.txt,17,2 Estimation Methods The DOP model and STSG formalism are described in detail elsewhere_comma_ for instance in Bod_comma_ 1998 .
E06-2025.txt,18,The main difference with PCFGs is that multiple derivations_comma_ using elementary trees with a variety of sizes_comma_ can yield the same parse tree.
E06-2025.txt,19,The probability of a parse p is therefore given by P p summationtextd d p P d _comma_ where d is the tree derived byderivation d_comma_P d producttextt d w t andw t gives the weights of elementary trees t_comma_ which are combined in the derivation d here treated as a multiset .
E06-2025.txt,20,2.1 DOP1 In Bod s original DOP implementation Bod_comma_ 1993 Bod_comma_ 1998 _comma_ henceforth DOP1_comma_ the weights of an elementary tree t is defined as its relative frequency relative to other subtrees with the same root label in the tree bank.
E06-2025.txt,21,That is_comma_ the weight 183 wi w ti of an elementary tree ti is given by wi fisummationtext j r tj r ti fj _comma_ 1 where fi f ti gives the frequency of subtree ti in a corpus_comma_ and r ti is the root label of ti.
E06-2025.txt,22,In his critique of this method_comma_ Johnson_comma_ 2002 considers a situation where there is an STSG G the target grammar with a specific set of subtrees t1 ...tN and specific values of the weights w1 ...wN .
E06-2025.txt,23,He evaluates an estimation procedure which produces a grammar Gprime the estimated grammar _comma_ by looking at the difference between the weights of G and the expected weights of Gprime.
E06-2025.txt,24,Johnson s test for consistency is thus based on comparing the weight distributions between target grammar and estimated grammar2.
E06-2025.txt,25,I will therefore refer to this test as the weight distribution test .
E06-2025.txt,26,t1 S A a A a t2 S A a A t3 S A A a t5 S A a t4 S A A t6 S A t7 A a Figure 1 The example of Johnson_comma_ 2002 Johnson_comma_ 2002 looks at an example grammar G STSG with the subtrees as in figure 1.
E06-2025.txt,27,Johnson considers the case where the weights of all trees of the target grammar G are 0_comma_ except for w7_comma_ which is necessarily 1_comma_ and w4 and w6 which are w4 p and w6 1 p.
E06-2025.txt,28,He finds that the expected values of the weights w4 and w6 of the estimated grammar Gprime are E wprime4 p2 2p_comma_ 2 E wprime6 1 p2 2p_comma_ 3 which are not equal to their target values for all values of p where 0 p 1.
E06-2025.txt,29,This analysis thus shows that DOP1is unable to recover the true weights of the given STSG_comma_ and hence the inconsistency of the estimator with respect to the class of STSGs.
E06-2025.txt,30,Although usually cited as showing the inadequacy of DOP1_comma_ Johnson s example is in fact 2More precisely_comma_ it is based on evaluating the estimator s behavior for any weight distribution possible in the STSG model.
E06-2025.txt,31,Prescher et al._comma_ 2003 give a more formal treatment of bias and consistency in the context of DOP. not suitable to distinguish DOP1 from alternative methods_comma_ because no possible estimation procedure can recover the true weights in the case considered.
E06-2025.txt,32,In the example there are only two complete trees that can be observed in the training data_comma_ corresponding to the trees t1 and t5.
E06-2025.txt,33,It is easy to see that when generating examples with the grammar in figure 1_comma_ the relative frequencies3 f1 ...f4 of the subtrees t1 ...t4 must all be the same_comma_ and equal to the frequency of the complete tree t1 which can be composed in the following ways from the subtrees in the original grammar t1 t2 t7 t3 t7 t4 t7 t7.
E06-2025.txt,34,4 It follows that the expected frequencies of each of these subtrees are E f1 E f2 E f3 E f4 5 w1 w2w7 w3w7 w4w7w7 Similarly_comma_ the other frequencies are given by E f5 E f6 w5 w6w7 6 E f7 2 w1 w2w7 w3w7 w4w7w7 w5 w6w7 2E f1 E f5 .
E06-2025.txt,35,7 From these equations it is immediately clear that_comma_ regardless of the amount of training data_comma_ the problem is simply underdetermined.
E06-2025.txt,36,The values of 6 weights w1 ...w6 w7 1 given only 2 frequencies f1 and f5 and the constraint thatsummationtext 6 i 1 fi 1 are not uniquely defined_comma_ and no possible estimation method will be able to reliably recover the true weights.
E06-2025.txt,37,The relevant test is whether for all possible STSGs and in the limit of infinite data_comma_ the expected relative frequencies of trees given the estimated grammar_comma_ equal the observed relative frequencies.
E06-2025.txt,38,I will refer to this test as the frequencydistribution test .
E06-2025.txt,39,As it turns out_comma_ the DOP1 method also fails this more lenient test.
E06-2025.txt,40,The easiest wayto show this_comma_ using again figure 1_comma_ isas follows.
E06-2025.txt,41,The weights wprime1 ...wprime7 of grammar Gprime will by definition be set to the relative frequencies of the corresponding subtrees wprimei braceleftBigg f iP 6 j 1 fj for i 1...6 1 for i 7.
E06-2025.txt,42,8 3Throughout this paper I take frequencies fi to be relative to the size of the corpus.
E06-2025.txt,43,184 Thegrammar Gprime will thus produce the complete trees t1 and t5 with expected frequencies E fprime1 wprime1 wprime2wprime7 wprime3wprime7 wprime4wprime7wprime7 4 f1summationtext6 j 1 fj 9 E fprime5 wprime5 wprime6wprime7 2 f5summationtext6 j 1 fj .
E06-2025.txt,44,10 Now consider the two possible complete trees t1 and t5_comma_ and the fraction of their frequencies f1 f5.
E06-2025.txt,45,In the estimated grammar Gprime this fraction becomes E fprime1 E fprime5 4n f1P6 j 1 fj 2n f5P6 j 1 fj 2f1f 5 .
E06-2025.txt,46,11 That is_comma_ in the limit of infinite data_comma_ the estimation procedure not only understandably fails to find the target grammar amongst the many grammars that could have produced the observed frequencies_comma_ it in fact chooses a grammar that could never have produced these observed frequencies at all.
E06-2025.txt,47,This example shows the DOP1 method is biased and inconsistent for the STSG class in the frequency distribution test4.
E06-2025.txt,48,2.2 Correction factor approaches Based on similar observation_comma_ Bonnema et al._comma_ 1999 Bod_comma_ 2003 propose alternative estimation methods_comma_ which involve a correction factor to move probability mass from larger subtrees to smaller ones.
E06-2025.txt,49,For instance_comma_ Bonnema et al. replace equation 1 with wi 2 N ti fisummationtext j r tj r ti fj _comma_ 12 where N ti gives the number of internal nodes in ti such that 2 N ti is inversely proportional to the number of possible derivations of ti .
E06-2025.txt,50,Similarly_comma_ Bod_comma_ 2003 changes the way frequencies fi are counted_comma_ with a similar effect.
E06-2025.txt,51,This approach solves the specific problem shown in equation 11 .
E06-2025.txt,52,However_comma_ the following example shows that the correction factor approaches cannot solve the more general problem.
E06-2025.txt,53,4Note that there are settings of the weights w1 .
E06-2025.txt,54,..w7 that generate a frequency distribution that could also have been generated with a PCFG.
E06-2025.txt,55,The example given applies to such distribution as well_comma_ and therefore also shows the inconsistency of the DOP1 method for PCFG distributions.
E06-2025.txt,56,t1 S A a A b t2 S A b A a t3 S A a A a t4 S A b A b t5 S A a A t6 S A A b t7 S A b A t8 S A A a t9 S A A t10 A a t11 A b Figure 2 Counter example to the correctionfactor approaches Consider the STSG in figure 2.
E06-2025.txt,57,The expected frequencies f1 ...f4 are here given by E f1 w1 w5w11 w6w10 w9w10w11 E f2 w2 w7w10 w8w11 w9w11w10 E f3 w3 w5w10 w8w10 w9w10w10 E f4 w4 w6w11 w7w11 w9w11w11 13 Frequencies f5 ...f11 are again simple combinations of the frequencies f1 ...f4.
E06-2025.txt,58,Observations of these frequencies therefore do not add any extra information_comma_ and the problem of finding the weights of the target grammar is in general again underdetermined.
E06-2025.txt,59,But consider the situation where f3 f4 0 and f1 0 and f2 0.
E06-2025.txt,60,This constrains the possible solutions enormously.
E06-2025.txt,61,If we solve the following equations for w3 ...w11 with the constraint that probabilities with the same root label add up to 1 i.e. summationtext9i 1 wi 1_comma_ w10 w11 1 w3 w5w10 w8w10 w9w10w10 0 w4 w6w11 w7w11 w9w11w11 0_comma_ we find_comma_ in addition to the obvious w3 w4 0_comma_ the following solutions w10 w6 w7 w9 0 w11 w5 w8 w9 0 w5 w6 w7 w8 w9 0.
E06-2025.txt,62,That is_comma_ if we observe no occurrences of trees t3 and t4 in the training sample_comma_ we know that at least one subtree in each derivation of these strings must have weight zero.
E06-2025.txt,63,However_comma_ any estimation method that uses the relative frequencies of subtrees and a nonzero correction factor that is based on the size of the subtrees_comma_ will give non zero probabilities to all weights w5 ...w11 if f1 0 and f2 0_comma_ as we assumed.
E06-2025.txt,64,In other words_comma_ these weight estimation methods for STSGs are also biased and inconsistent in the frequency distribution test.
E06-2025.txt,65,185 2.3 Shortest derivation estimators Because the STSG formalism allows elementary trees of arbitrary size_comma_ every parse tree in a tree bank could in principle be incorporated in an STSG grammar.
E06-2025.txt,66,That is_comma_ we can define a trivial estimator with the following weights wi braceleftbigg f i if ti is an observed parse tree 0 otherwise 14 Such an estimator is not particularly interesting_comma_ because it does not generalize beyond the training data.
E06-2025.txt,67,It is a point to note_comma_ however_comma_ that this estimator is unbiased and consistent in the frequencydistribution test.
E06-2025.txt,68,Prescher et al._comma_ 2003 prove that any unbiased estimator that uses the all subtrees representation has the same property_comma_ and conclude that lack of bias is not a desired property.
E06-2025.txt,69,Zollmann and Sima an_comma_ 2005 propose an estimator based on held out estimation.
E06-2025.txt,70,The training corpus is split into an estimation corpus EC and a held out corpus HC.
E06-2025.txt,71,The HC corpus is parsed by searching for the shortest derivation of each sentence_comma_ using only fragments from EC.
E06-2025.txt,72,The elementary trees of the estimated STSG are assigned weights according to their usage frequencies u1_comma_..._comma_uN in these shortest derivations wi uisummationtext j r tj r ti uj .
E06-2025.txt,73,15 This approach solves the problem with bias described above_comma_ while still allowing for consistency_comma_ as Zollmann Sima an prove.
E06-2025.txt,74,However_comma_ their proof only concerns consistency in the frequencydistribution test.
E06-2025.txt,75,As the corpus EC grows to be infinitely large_comma_ every parse tree in HC will also be found in EC_comma_ and the shortest derivation will therefore in the limit only involve a single elementary tree the parse tree itself.
E06-2025.txt,76,Target STSGs with non zero weights on smaller elementary trees will thus not be identified correctly_comma_ even with an infinitely large training set.
E06-2025.txt,77,In other words_comma_ the Zollmann Sima an method_comma_ and other methods that converge to the complete parse tree solution such as LS DOP Bod_comma_ 2003 and BackOff DOP Sima an and Buratto_comma_ 2003 _comma_ are inconsistent in the weight distribution test.
E06-2025.txt,78,3 Discussion Conclusions A desideratum for parameter estimation methods isthat they converge tothe correct parameters with infinitely many data that is_comma_ we like an estimator to be consistent.
E06-2025.txt,79,The STSG formalism_comma_ however_comma_ allows for many different derivations of the same parse tree_comma_ and for many different grammars to generate the same frequency distribution.
E06-2025.txt,80,Consistency in the weight distribution test is therefore too stringent a criterion.
E06-2025.txt,81,We have shown that DOP1 and methods based on correction factors also fail the weaker frequency distribution test.
E06-2025.txt,82,However_comma_ the only current estimation methods that are consistent in the frequency distribution test_comma_ have the linguistically undesirable property of converging to a distribution with all probability mass in complete parse trees.
E06-2025.txt,83,Although these method fail the weight distribution test for the whole class of STSGs_comma_ we argued earlier that this test is not the appropriate test either.
E06-2025.txt,84,Both estimation methods for STSGs and the criteria for evaluating them_comma_ thus require thorough rethinking.
E06-2025.txt,85,In forthcoming work we therefore study yet another estimator_comma_ and the linguistically motivated evaluation criterion of convergence to a maximally general STSG consistent with the training data5. .
E06-1045.txt,1,describe an implementation of datadriven selection of emphatic facial displays for an embodied conversational agent in a dialogue system. A corpus of sentences in the domain of the target dialogue system was recorded_comma_ and the facial displays used by the speaker were annotated.
E06-1045.txt,2,The data from those recordings was used in a range of models for generating facial displays_comma_ each model making use of a different amount of context or choosing displays differently within a context.
E06-1045.txt,3,The models were evaluated in two ways by cross validation against the corpus_comma_ and by askinguserstoratetheoutput.
E06-1045.txt,4,Thepredictions of the cross validation study differed from the actual user ratings.
E06-1045.txt,5,While the cross validation gave the highest scores to models making a majority choice within a context_comma_ the user study showed a significant preference for models that produced more variation.
E06-1045.txt,6,This preference was especially strong among the female subjects.
E06-1045.txt,8,It has long been documented that there are characteristic facial displays that accompany the emphasised parts of spoken utterances.
E06-1045.txt,9,For example_comma_ Ekman 1979 says that eyebrow raises appear to coincide with primary vocal stress_comma_ or more simply with a word that is spoken more loudly. Correlations have also been found between prosodic features and events such as head nodding and the amplitude of mouth movements.
E06-1045.txt,10,When Krahmer and Swerts 2004 performed an empirical_comma_ cross linguisticevaluationoftheinfluenceofbrow movements on the perception of prosodic stress_comma_ they found that subjects preferred eyebrow movements to be correlated with the most prominent word in an utterance and that eyebrow movements boosted the perceived prominence of the word they were associated with.
E06-1045.txt,11,While many facial displays have been shown to co occur with prosodic accents_comma_ the converse is not true in normal embodied speech_comma_ many pitch accents and other prosodic events are unaccompanied by any facial display_comma_ and when displays are used_comma_ the selection varies widely.
E06-1045.txt,12,Cassell and Th orisson 1999 demonstrated that envelope facial displays related to the process of conversation have a greater impact on successful interaction with an embodied conversational agent than do emotional displays.
E06-1045.txt,13,However_comma_ no description of face motion is sufficiently detailed that it can be used as the basis for selecting emphatic facial displays for an agent.
E06-1045.txt,14,This is therefore a task for which data driven techniques are beneficial.
E06-1045.txt,15,In this paper_comma_ we address the task of selecting emphatic facial displays for the talking head in the COMIC1 multimodal dialogue system.
E06-1045.txt,16,In the basic COMIC process for generating multimodal output Foster et al._comma_ 2005 _comma_ facial displays are selected using simple rules based only on the pitch accents specified by the text generation system.
E06-1045.txt,17,In order to make a more sophisticated and naturalistic selection of facial displays_comma_ we recorded a single speaker reading a set of sentences drawn from the COMIC domain_comma_ and annotated the facial displays that he used and the contexts in which he used them.
E06-1045.txt,18,We then created models based on the data from this corpus and used them to choose the facial displays for the COMIC talking head.
E06-1045.txt,19,1http www.hcrc.ed.ac.uk comic 353 The rest of this paper is arranged as follows.
E06-1045.txt,20,First_comma_ in Section 2_comma_ we describe previous approaches to selecting non verbal behaviour for embodied conversational agents.
E06-1045.txt,21,In Section 3_comma_ we then show how we collected and annotated a corpus of facial displays_comma_ and give some generalisations about the range of displays found in the corpus.
E06-1045.txt,22,After that_comma_ in Section 4_comma_ we outline how we implemented a range of models for selecting behaviours for the COMIC agent using the corpus data_comma_ using varying amounts of context and different selection strategies within a context.
E06-1045.txt,23,Next_comma_ we give the results of two evaluation studies comparing the quality of the output generated by the various models a cross validation study against the corpus Section 5 and a direct user evaluation of theoutput Section6 .
E06-1045.txt,24,InSection7_comma_ wediscussthe results of these two evaluations.
E06-1045.txt,25,Finally_comma_ in Section 8_comma_ we draw some conclusions from the current study and outline potential follow up work.
E06-1045.txt,26,2 Choosing Non Verbal Behaviour for Embodied Conversational Agents Embodied Conversational Agents ECAs are computer interfaces that are represented as human bodies_comma_ and that use their face and body in a human like way in conversations with the user Cassell et al._comma_ 2000 .
E06-1045.txt,27,The main benefit of ECAs is that they allow users to interact with a computer in the most natural possible setting face to face conversation.
E06-1045.txt,28,However_comma_ to realise this advantage fully_comma_ the agent must produce high quality output_comma_ both verbal and non verbal.
E06-1045.txt,29,A number of previous systems have based the choice of non verbal behaviours for an ECA on the behaviours of humans in conversational situations.
E06-1045.txt,30,The implementations vary as to how directly they use the human data.
E06-1045.txt,31,In some systems_comma_ motion specifications for the agent are created from scratch_comma_ using rules derived from studying human behaviour.
E06-1045.txt,32,For the REA agent Cassell et al._comma_ 2001a _comma_ for example_comma_ gesturing behaviour was selected to perform particular communicative functions_comma_ using rules based on studies of typical North American non verbal displays.
E06-1045.txt,33,Similarly_comma_ the Greta agent de Carolis et al._comma_ 2002 selected its performative facial displays using hand crafted rules to map from affective states to facial motions.
E06-1045.txt,34,Such implementations do not make direct use of any recorded human motions this means that they generate average behaviours from a range of people_comma_ but it is difficult to adapt them to reproduce the behaviour of an individual.
E06-1045.txt,35,In contrast_comma_ other ECA implementations have selected non verbal behaviour based directly on motion capture recordings of humans.
E06-1045.txt,36,Stone et al. 2004 _comma_ for example_comma_ recorded an actor performing scripted output in the domain of the target system.
E06-1045.txt,37,They then segmented the recordings into coherent phrases and annotated them with the relevant semantic and pragmatic information_comma_ and combined the segments at run time to produce complete performance specifications that were then played back on the agent.
E06-1045.txt,38,Cunningham et al. 2004 and Shimodaira et al. 2005 used similar techniques to base the appearance and motions of their talking heads directly on recordings of human faces.
E06-1045.txt,39,This technique is able to produce more naturalistic output than the more rule based systems described above however_comma_ capturing the motion requires specialised hardware_comma_ and the agent must be implemented in such a way that it can exactly reproduce the human motions.
E06-1045.txt,40,A middle ground is to use a purely synthetic agent one whose behaviour is controlled by high level instructions_comma_ rather than based directly on human motions but to create the instructions forthatagentusingthedatafromanannotatedcorpus of human behaviour.
E06-1045.txt,41,Like a motion capture implementation_comma_ this technique can also produce increased naturalism in the output and also allows choices to be based on the motions of a single performer if necessary.
E06-1045.txt,42,However_comma_ annotating a video corpus can be less technically demanding than capturing and directly re using real motions_comma_ especially when the corpus and the number of features under consideration are small.
E06-1045.txt,43,This approach has been taken_comma_ for example_comma_ by Cassell et al. 2001b to choose posture shifts for REA_comma_ and by Kipp 2004 to select gestures for agents_comma_ and it is also the approach that we adopt here.
E06-1045.txt,44,3 Recording and Annotation The recording script for the data collection consisted of 444 sentences in the domain of the COMIC multimodal dialogue system all of the sentences described one or more features of one or more bathroom tile designs.
E06-1045.txt,45,The sentences were generated by the full COMIC output planner_comma_ and were selected to provide coverage of all of the syntactic patterns available to the system.
E06-1045.txt,46,In addition to the surface text_comma_ each sentence included all of the contextual information from the COMIC 354 46.
E06-1045.txt,47,More about the current design they dislike the first feature_comma_ but like the second one There are GEOMETRIC SHAPES on the decorative tiles_comma_ but the tiles ARE from the ARMONIE series.
E06-1045.txt,48,Figure 1 Sample prompt slide planner the predicted pitch accents selected according to Steedman s 2000 theory of information structure and intonation along with any information from the user model and dialogue history.
E06-1045.txt,49,The sentences were presented one at a time to the speaker_comma_ who was instructed to read each sentenceoutloudasexpressivelyaspossiblewhile lookingintoacameradirectedathisface.
E06-1045.txt,50,Thesegments for which the presentation planner specified pitch accents were highlighted_comma_ and any applicable user model and dialogue history information was included.
E06-1045.txt,51,Figure 1 shows a sample prompt slide.
E06-1045.txt,52,The recorded videos were annotated by the first author_comma_ using a purpose built tool that allowed any setof facialdisplays tobe associatedwithany segmentofthesentence.
E06-1045.txt,53,First_comma_ thevideowassplitinto clips corresponding to each sentence.
E06-1045.txt,54,After that_comma_ the facial displays in each clip were annotated.
E06-1045.txt,55,The following were the displays that were considered eyebrowraisingandlowering eyesquinting head nodding up_comma_ small down_comma_ large down head leaning left and right and head turning left and right .
E06-1045.txt,56,Figure 2 shows examples of two typical display combinations.
E06-1045.txt,57,Any combination of these facial displays could be associated with any of the relevant segments in the text.
E06-1045.txt,58,The relevant segments included all mentions of tile design properties e.g._comma_ colours_comma_ designers _comma_ modifiers such as once again and also_comma_ deictic determiners this_comma_ these _comma_ and verbs in contrastive contexts e.g._comma_ are in Figure 1 .
E06-1045.txt,59,The annotation scheme treated all facial displays as batons rather than underliners Ekman_comma_ 1979 that is_comma_ each display was associated with a single segment.
E06-1045.txt,60,If a facial display spanned a longer phrase in the speech_comma_ it was annotated as a series of identical batons on each of the segments.
E06-1045.txt,61,Any predicted pitch accents and dialoguehistory and user model information from the COMIC presentation planner were also associated with each segment_comma_ as appropriate.
E06-1045.txt,62,We chose not to restrict our annotation to those segments with predicted pitch accents_comma_ because the speaker also made a large number of facial displays on segments with no predicted pitch accent instead_comma_ we incorporated the predicted accent as an additional contextual factor.
E06-1045.txt,63,For the most part_comma_ the pitch accents used by the speaker followed the specifications on the slides.
E06-1045.txt,64,We did not explicitly consider the rhetorical or syntactic structure_comma_ as did_comma_ e.g._comma_ de Carolis et al. 2000 in general_comma_ the structure was fully determined by the context.
E06-1045.txt,65,There were a total of 1993 relevant segments in the recorded sentences.
E06-1045.txt,66,Overall_comma_ the most frequent display combination was a small downward nod onitsown_comma_ whichoccurredonjustover25 ofthe segments.
E06-1045.txt,67,The second largest class was no motion at all 20 of the segments _comma_ followed by downward nods large and small accompanied by brow raises.
E06-1045.txt,68,Further down the order_comma_ the various lateral motions appear for this speaker_comma_ these were primarily turns to the right Figure 2 a and leans to the left Figure 2 b .
E06-1045.txt,69,The distribution of facial displays in specific contextsdifferedfromtheoveralldistribution.
E06-1045.txt,70,The biggest influence was the user model evaluation left leans_comma_ brow lowering_comma_ and eye squinting were all relatively more frequent on objects with negative user model evaluations_comma_ while right turns and brow raises occurred more often in positive contexts.
E06-1045.txt,71,Other factors also had an influence for example_comma_ nodding and brow raises were both more frequent on segments for which the COMIC planner specified a pitch accent.
E06-1045.txt,72,Foster 2006 gives a detailed analysis of these recordings.
E06-1045.txt,73,4 Modelling the Corpus Data We built a range of models using the data from the annotated corpus to select facial displays to accompany generated text.
E06-1045.txt,74,For each segment in the text_comma_ a model selected a display combination from among the displays used by the speaker in a similar context.
E06-1045.txt,75,All of the models used the corpus countsofdisplaysassociatedwiththesegmentsdirectly_comma_ with no back off or smoothing.
E06-1045.txt,76,The models differed from one another in two ways the amount of context that they used_comma_ and the way in which they made a selection within a context.
E06-1045.txt,77,There were three levels of context No context These models used the overall corpus counts for all segments.
E06-1045.txt,78,355 a Right turn brow raise b Left lean brow lower Figure 2 Typical speaker motions from the recording Surface only These models used only the context provided by the word s or_comma_ in some cases_comma_ a domain specific semantic class.
E06-1045.txt,79,For example_comma_ a model would use the class DECORATION rather than the specific word artwork.
E06-1045.txt,80,Full context Inadditiontothesurfaceform_comma_ these models also used the pitch accent specifications and contextual information supplied by the COMIC presentation planner.
E06-1045.txt,81,The contextual information was associated with the tile design properties included in the sentence and indicated a whether that property had been mentioned before_comma_ b whether it was explicitly contrasted with a property of a previous design_comma_ and c the expected user evaluation of that property.
E06-1045.txt,82,Within a context_comma_ there were two strategies for selecting a facial display Majority Choose the combination that occurred the largest number of times in the context.
E06-1045.txt,83,Weighted Make a random choice from all combinations seen in the context_comma_ weighting the choice according to the relative frequency.
E06-1045.txt,84,For example_comma_ in the no context case_comma_ a majoritychoice model would choose the small downward nod the majority option for every segment_comma_ while a weighted choice model would choose a small downward nod with probability 0.25_comma_ no motion with probability 0.2_comma_ and the other displays with correspondingly decreasing probabilities.
E06-1045.txt,85,These two factors produced a set of 6 models in total 3 context levels 2 selection strategies .
E06-1045.txt,86,Throughout the rest of this paper_comma_ we will use twocharacter labels to refer to the models.
E06-1045.txt,87,The first character of each label indicates the amount of g1g2 g3g2 g4g2 g1g5 g3g5 g4g5g6g7g6g6 g6g7g6g8 g6g7g9g6 g6g7g9g8 g6g7g10g6 g6g7g10g8 g6g7g11g6 g6g7g11g8 g6g7g12g6 g6g7g12g8 g6g7g8g6 g6g7g8g8 g6g7g13g6 g6g7g13g8 g6g7g13g11 g6g7g13g6 g6g7g8g13 g6g7g8g13 g6g7g8g10 g6g7g12g14 g2g15g16g17g18 g2g17 g19g20 g21g1g21 g22g23 g15g24g17 Figure 3 Mean F score for all models context that was used_comma_ while the second indicates the selection method within that context for example_comma_ SM corresponds to a model that used the surface form only and made a majority choice.
E06-1045.txt,88,5 Evaluation 1 Cross validation We first compared the performance of the models using 10 fold cross validation against the corpus.
E06-1045.txt,89,For each fold_comma_ we built models using 90 of the sentences in the corpus_comma_ and then used those models to predict the facial displays for the sentences in the other 10 of the corpus.
E06-1045.txt,90,We measured the recall and precision on a sentence by comparing the predicted facial displays for each segment to the actual displays used by the speaker and averaging those scores across the sentence.
E06-1045.txt,91,We then used the recall and precision scores for a sentence to compute a sentence level F score.
E06-1045.txt,92,Averaged across all of the cross validation folds_comma_ the NM model had the highest recall score_comma_ while the FM model scored highest for precision and F score.
E06-1045.txt,93,Figure 3 shows the average sentencelevel F score for all of the models.
E06-1045.txt,94,All but one of the differences shown are significant at the p 356 a Neutral b Right turn brow raise c Left lean brow lower Figure 4 Synthesised version of motions from Figure 2 0.01 level on a paired T test the performance of the NM and FW models was indistinguishable on F score_comma_ although the FW model scored higher on precision and the NM model on recall.
E06-1045.txt,95,That the majority choice models generally scored better on this measure than the weightedchoice models is not unexpected a weightedchoice model is more likely to choose a lesscommon display_comma_ and if it chooses it in a context where the speaker did not_comma_ the score for that sentence is decreased.
E06-1045.txt,96,It is also not surprising that_comma_ within a selection strategy_comma_ the models that take into account more of the context did better than those that use less of it this is simply an indication that there are patterns in the corpus_comma_ and that all of the contextual information contributes to the selection of displays.
E06-1045.txt,97,6 Evaluation 2 User Ratings The majority choice models performed better on the cross validation study than the weightedchoice ones did however_comma_ this does not does not mean that users will necessarily like their output in practice.
E06-1045.txt,98,A large amount of the lateral motion and eyebrow movements occurs in the second or third largest class in a number of contexts_comma_ and is therefore less likely to be selected by a majoritychoice model.
E06-1045.txt,99,If users like to see motion other than simple nodding_comma_ it might be that the schedules generated by the weighted choice models are actually preferred.
E06-1045.txt,100,To address this question_comma_ we performed a user evaluation.
E06-1045.txt,101,6.1 Experiment Design Materials For this study_comma_ we generated 30 new sentences from the COMIC system.
E06-1045.txt,102,The sentences were selected to ensure that they covered the full range of syntactic structures available to COMIC and that none of them was a duplicate of anything from the recording script.
E06-1045.txt,103,We then generated a facial schedule for each sentence using each of the six models.
E06-1045.txt,104,Note that_comma_ for some of the sentences_comma_ more than one model produced an identical sequence of facial displays_comma_ either because the majority choice in a broader context was the same as in a more narrow context_comma_ or because a weighted choice model ended up selecting the majority option in every case.
E06-1045.txt,105,All such identical schedules were retained in the set of materials in Section 6.2_comma_ we discuss their impact on the results.
E06-1045.txt,106,We then made videos of every schedule for every sentence_comma_ using the Festival speech synthesiser Clark et al._comma_ 2004 and the RUTH talking head DeCarlo et al._comma_ 2004 .
E06-1045.txt,107,Figure 4 shows synthesised versions of the facial displays from Figure 2.
E06-1045.txt,108,Procedure 33 subjects took part in the experiment 17 female subjects and 16 males.
E06-1045.txt,109,They were primarily undergraduate students_comma_ between 20 and 24 years old_comma_ native speakers of English_comma_ with an intermediate amount of computer experience.
E06-1045.txt,110,Each subject in the study was shown videos of all 30 sentences in an individually chosen random order.
E06-1045.txt,111,For each sentence_comma_ the subject saw two versions_comma_ each generated by a different model_comma_ and was asked to choose which version they liked better.
E06-1045.txt,112,The displayed versions were counterbalanced so that every subject performed each pairwise comparison of models twice_comma_ once in each order.
E06-1045.txt,113,The study was run over the web.
E06-1045.txt,114,6.2 Results2 Figure 5 a shows the overall preference rates for allofthemodels.
E06-1045.txt,115,Foreachmodel_comma_ thevalueshown 2 We do not include those trials where both videos were identical if these are included_comma_ the results are similar_comma_ but the distinctions described here just fail to reach significance.
E06-1045.txt,116,357 g1g2 g3g2 g4g2 g1g5 g3g5 g4g5g6 g6g7g6g8 g6g7g9 g6g7g9g8 g6g7g10 g6g7g10g8 g6g7g11 g6g7g11g8 g6g7g12 g6g7g12g8 g6g7g8 g6g7g8g8 g6g7g13 g6g7g8g10 g6g7g12g10 g6g7g12g11 g6g7g8g8 g6g7g8 g6g7g8g13 g2g14g15g16g17 g18g19 g16g20g21 g17g17g22g23 g22g24g25 g14g26 g16g27 a Overall preference rates g1g2g3g4g5g3g1g6 g7g2g3g4g5g3g7g6 g8g2g3g4g5g3g8g6 g9g4g10g11g12g13g13g14 g14g15g14g16 g14g15g17 g14g15g17g16 g14g15g18 g14g15g18g16 g14g15g19 g14g15g19g16 g14g15g20 g14g15g20g16 g14g15g16 g14g15g16g16 g14g15g21 g14g15g21g16 g2g12g22g23g11g24g25g26 g6g10g24g27g28g25g10g29 g30g31g23g32g33g25g3g23g34g3g35g23g33g25g10g36g25 g37g10 g12g29 g38g25g23 g38g28g10 g12g29 g3g35g28 g23g24g35 g10 b Head to head p.
E06-1001.txt,1,propose a uniform approach to the elimination of redundancy in CCG lexicons_comma_ where grammars incorporate inheritance hierarchies of lexical types_comma_ defined over a simple_comma_ feature based category description language. The resulting formalism is partially constraint based _comma_ in that the category notationis interpreted againstan underlying set of tree like feature structures.
E06-1001.txt,2,I argue that this version of CCG subsumes a number of other proposed category notations devised to allow for the construction of more efficient lexicons.
E06-1001.txt,3,The formalism retains desirable properties such as tractability and strong competence_comma_ and provides a way of approaching the problem of how to generalise CCG lexicons which have been automatically induced from treebanks.
E06-1001.txt,4,1 The CCG formalism In its most basic conception_comma_ a CCG over alphabet of terminal symbols is an ordered triple A_comma_S_comma_L _comma_ whereAis an alphabet of saturated category symbols_comma_S is a distinguished element ofA_comma_ and L is a lexicon_comma_ i.e. a mapping from to categories overA.
E06-1001.txt,8,However_comma_ in many cases development has been hindered by the absence of an agreed uniform approach to eliminating redundancy in CCG lexicons.
E06-1001.txt,9,This poses a particular problem for a radically lexicalised formalism such as CCG_comma_ where it is customary to handle bounded 1 dependency constructions such as case_comma_ agreement and binding by means of multiple lexical category assignments.
E06-1001.txt,10,Take for example_comma_ the language schematised in Table 1.
E06-1001.txt,13,Lexicons which violate the criteria of functionality and atomicity are not just inefficient in terms of storage space and development time.
E06-1001.txt,14,They also fail to capture linguistically significant generalisations about the behaviour of the relevant words or morphemes.
E06-1001.txt,15,The functionality and atomicity of a CCG lexicon can be easily quantified.
E06-1001.txt,16,The functionality ratio of the lexicon in Table 2_comma_ with 22 lexical entries for 14 distinct morphemes_comma_ is 2214 1.6.
E06-1001.txt,17,The atomicity ratio is calculated by dividing the number of saturated category symbol tokens by the number of lexical entries_comma_ i.e.
E06-1001.txt,18,3622 1.6.
E06-1001.txt,19,Various_comma_ more or less ad hoc generalisations of the basic CCG category notation have been proposed with a view to eliminating these kinds of lexical redundancy.
E06-1001.txt,20,One area of interest has involved the nature of the saturated category symbols themselves.
E06-1001.txt,21,Bozsahin 2002 presents a version of CCG where saturated category symbols are modified by unary modalities annotated with morphosyntactic features.
E06-1001.txt,22,The features are themselves ordered according to a language particular join semi lattice.
E06-1001.txt,23,This technique_comma_ along with the insistence that lexicons of agglutinating languages are necessarily morphemic_comma_ allows generalisations involving the morphological structure of nouns and verbs in Turkish to be captured in an elegant_comma_ non redundant format.
E06-1001.txt,24,Erkan 2003 generalises this approach_comma_ modelling saturated category labels as typed feature structures_comma_ constrained by underspecifiedfeaturestructuredescriptionsintheusual manner.
E06-1001.txt,25,Hoffman 1995 resolves other violations of the ideal of functionality in CCG lexicons for languages with local scrambling constructions by means of multiset notation for unsaturated categories_comma_ where scope and direction of arguments can be underspecified.
E06-1001.txt,28,One final point of note involves the project reported in Beavers 2004 _comma_ who implements CCG within the LKB system_comma_ i.e. as an application of the Typed Feature Structure Grammar formalism of Copestake 2002 _comma_ with the full apparatus of unrestricted typed feature structures_comma_ default inheritance hierarchies_comma_ and lexical rules.
E06-1001.txt,29,3 Type hierarchical CCG One of the aims of the project reported here has been to take a bottom up approach to the problem of redundancy in CCG lexicons_comma_ adding just enough formal machinery to allow the relevant generalisationstobeformulated_comma_ whilstretaininga restrictive theory of human linguistic competence which satisfies the strong competence requirement_comma_ i.e. the competence grammar and the processing grammar are identical.
E06-1001.txt,30,I start with a generalisation of the CCG formalism where the alphabet of saturated category symbols is organised into a type hierarchy in the sense of Carpenter 1992 _comma_ i.e. a weak order A_comma_subsetsqequalA _comma_ where A is an alphabet of types_comma_ subsetsqequalA is the subsumption ordering onA with a least element _comma_ and every subset ofAwith an upper bound has a least upper bound.
E06-1001.txt,31,An example type hierarchy is in Figure 2_comma_ where for example types Nomsg and NP are compatible since they have a non empty set of upper bounds_comma_ the least upper bound or unifier being NPsg .
E06-1001.txt,32,NPsgsbj NPplsbj NPsgobj NPplobj a81a81 a81 a81a81 a81 a80a80a80 a80a80a80 a16a16a16 a16a16a16 a16a16a16 a16a16a16 NPsbj NPobj NPsg NPpl Nsg Npl a35a35 a35 a76a76 a72a72 a72a72 a80a80a80 a80a80a80a80 a4a4 a12a12 a80a80a80a80a80a80 a72a72a72a72 a0a0 NP Nomsg Nompl N a16a16a16 a16a16a16 a0a0 a64a64 a80a80a80 a80a80a80 NomSa33 a33a33a33 a80a80a80a80 top Figure 2 Type hierarchy of saturated categories A type hierarchical CCG T CCG over alphabet is an ordered 4 tuple A_comma_subsetsqequalA_comma_S_comma_L _comma_ where 1http openccg.sourceforge.net A_comma_subsetsqequalA is a type hierarchy of saturated category symbols_comma_ S is a distinguished element of A_comma_ and lexicon L is a mapping from to categories over A.
E06-1001.txt,33,Given an appropriate subsetsqequalA compatibility relation on the categories over A_comma_ the combinatory projection of T CCG A_comma_subsetsqequalA_comma_S_comma_L can again be defined as the closure of L under the CCG combinatory operations_comma_ assuming that variable Y in the type raising rule 4 is restricted to maximally specified categories.
E06-1001.txt,35,Whilethelatterhasafunctionality ratio of 1.6_comma_ the former s is 1614 1.1.
E06-1001.txt,36,This improved functionality ratio results from the underspecification of saturated category symbols inherent in the subsumption relation.
E06-1001.txt,37,For example_comma_ whereas the proper noun John is assigned to two distinct categories in the lexicon in Table 2_comma_ in the T CCG lexicon it is assigned to a single non maximal type NPsg which subsumes the two maximal types NPsgsbj and NPsgobj .
E06-1001.txt,38,In other 3 words_comma_ the phenomenon of case syncretism in English proper nouns is captured by having a general singular noun phrase type_comma_ which subsumes a plurality of case distinctions.
E06-1001.txt,39,TheT CCGformalismisequivalenttothe morphosyntacticCCG formalismofBozsahin 2002 _comma_ where features are ordered in a join semi lattice.
E06-1001.txt,40,Any generalisation which can be expressed in a morphosyntactic CCG can also be expressed in a T CCG_comma_ since any lattice of morphosyntactic features can be converted into a type hierarchy.
E06-1001.txt,41,In addition_comma_ T CCG is equivalent to the formalism described in Erkan 2003 _comma_ where saturated categories are modelled as typed feature structures.
E06-1001.txt,42,Any lexicon from either of these formalisms can be translated into a T CCG lexicon whose functionality ratio is either equivalent or lower.
E06-1001.txt,43,4 Inheritance driven CCG A second generalisation of the CCG formalism involves adding a second alphabet of non terminals_comma_ in this case a set of lexical types .
E06-1001.txt,44,The lexical typesareorganisedintoan inheritancehierarchy _comma_ constrained by expressions of a simple featurebased category description language_comma_ inspired by previousattemptstointegratecategorialgrammars and unification based grammars_comma_ e.g.
E06-1001.txt,45,Uszkoreit 1986 and Zeevat et al. 1987 .
E06-1001.txt,46,4.1 Simple category descriptions The set of simple category descriptions over alphabetAof saturated category symbols is defined as the smallest set such that 1.
E06-1001.txt,47,A  2. for all  f_comma_b _comma_ SLASH  3. for all  _comma_ ARG  4. for all  _comma_ RES  Note that category descriptions may be infinitely embedded_comma_ in which case they are considered to be right associative_comma_ e.g.
E06-1001.txt,48,RES ARG RES SLASH f.
E06-1001.txt,49,A simple category description like SLASH f or SLASH b denotes the set of all expressions which seek their argument to the right left.
E06-1001.txt,50,A description of the form ARG denotes the set of expressions which take an argument of category _comma_ and one like RES denotes the set of expressions which combine with an argument to yield an expression of category .
E06-1001.txt,51,Complex category descriptions are simply sets of simple category descriptions_comma_ where the assumed semantics is simply that of conjunction.
E06-1001.txt,52,4.2 Lexical inheritance hierarchies Lexical inheritance hierarchies Flickinger_comma_ 1987 are type hierarchies where each type is associated with a set of expressions drawn from some category description language .
E06-1001.txt,53,Formally_comma_ they are ordered triples B_comma_subsetsqequalB_comma_b _comma_ where B_comma_subsetsqequalB is a type hierarchy_comma_ andbis a function fromBto .
E06-1001.txt,54,An example lexical inheritance hierarchy over the set of category descriptions over the alphabet of saturated category symbols in Table 2 is presented in Figure 4.
E06-1001.txt,55,The intuition underlying these monotonic inheritance hierarchies is that instances of a type must satisfy all the constraints associated with that type_comma_ as well as all the constraints it inherits from its supertypes. verbpl RES ARG Nompl a0a0 verbsg RES ARG Nomsg detsg ARG NomsgRES Nom sg a10a10 detpl ARG NomplRES Nom pl a66 a66 a66a66 suffixsg ARG verbplRES verb sg a1a1 suffixpl ARG NsgRES Nom pl a67 a67 a67a67 verb SLASH fARG NP objRES SLASH b RES ARG NPsbjRES RES S a24a24a24a24a24 det SLASH fARG N RES NP suffix SLASH b a72a72 a72a72 a72a72a72 top Figure 4 A lexical inheritance hierarchy This example hierarchy is a single inheritance hierarchy_comma_ since every lexical type has no more than one immediate supertype.
E06-1001.txt,56,However_comma_ multiple inheritance hierarchies are also allowed_comma_ where a given type can inherit constraints from two supertypes_comma_ neither of which subsumes the other.
E06-1001.txt,57,4.3 I CCGs An inheritance driven CCG I CCG over alphabet is an ordered 7 tuple A_comma_subsetsqequalA_comma_B_comma_subsetsqequalB_comma_b_comma_ S_comma_L _comma_ where A_comma_subsetsqequalA is a type hierarchy of saturated category symbols_comma_ B_comma_subsetsqequalB_comma_b is an inheritance hierarchy of lexical types over the set of categorydescriptionsoverA B_comma_Sisadistinguished symbol inA_comma_ and lexiconLis a function from to A B.
E06-1001.txt,58,Given an appropriate subsetsqequalA_comma_B compatibility relationonthecategoriesoverA B_comma_ thecombinatory projection of I CCG A_comma_subsetsqequalA_comma_B_comma_subsetsqequalB_comma_b_comma_S_comma_L can again be defined as the closure ofLunder the 4 CCG combinatory operations.
E06-1001.txt,59,The I CCG lexicon in Table 4_comma_ along with the type hierarchy of saturated category symbols in Figure 2 and the inheritance hierarchy of lexical types in Figure 4_comma_ generates the fragment of English in Table 1.
E06-1001.txt,60,Using this lexicon_comma_ the sentence John turnstileleft NPsg girl turnstileleft Nsg s turnstileleft suffix the turnstileleft det I_comma_we_comma_they turnstileleft NPplsbj me_comma_us_comma_them turnstileleft NPplobj you turnstileleft NPpl he turnstileleft NPsgsbj him turnstileleft NPsgobj love turnstileleft verbpl Table 4 An I CCG lexicon girls love John is derived as in Figure 5_comma_ where derivational steps involve cache ing out sets of constraints from lexical types. girl s love John Nsg suffix verbpl NPsg SLASH b RES ARG Nompl suffixpl verb ARGNsg SLASH f RESNompl ARG NPobj RES SLASH bNom pl RES ARG NPsbj T RES RES S RES S SLASH f ARG RES S ARG ARG Nompl ARG SLASH b B RES S ARG NPobj SLASH f S Figure 5 An I CCG derivation This derivation relies on a version of the CCG combinatory rules defined in terms of the I CCG category description language.
E06-1001.txt,61,For example_comma_ forward application is expressed as follows for all compex category descriptions and such that SLASH b negationslash _comma_ and  ARG    is compatible_comma_ the following is a valid inference    RES   The functionality ratio of the I CCG lexicon in Table 4 is 1414 1 and the atomicity ratio is 1414 1.
E06-1001.txt,62,In other words_comma_ the lexicon is maximally nonredundant_comma_ since all the linguistically significant generalisationsareencodablewithinthelexicalinheritance hierarchy.
E06-1001.txt,63,The optimal atomicity ratio of the I CCG lexicon is a direct result of the . of lexical types.
E06-1001.txt,65,In the I CCG s inheritance hierarchy in Figure 4_comma_ there is a lexical type verbpl which inherits six constraints whose conjunction picks out exactly the same category.
E06-1001.txt,66,It is with this atomic label that the verb is paired in the I CCG lexicon in Table 4.
E06-1001.txt,67,The lexical inheritance hierarchy also has a role to play in constructing lexicons with optimal functionality ratios.
E06-1001.txt,68,The T CCG lexicon in Table 3 assigned the definite article to two distinct categories_comma_ one for each grammatical number distinction.
E06-1001.txt,69,The I CCG utilises the disjunction inherent in inheritance hierarchies to give each of these a common supertype det _comma_ which is associated with the properties all determiners share.
E06-1001.txt,70,Finally_comma_ the I CCG formalism can be argued to subsume the multiset category notation of Hoffman 1995 _comma_ in the sense that every multiset CCG lexicon can be converted into an ICCG lexicon with an equivalent or better functionality ratio.
E06-1001.txt,72,Again it should be clear that this is just another way of representing disjunction in a categorial lexicon_comma_ and can be straightforwardly converted into a lexical inheritance hierarchy over I CCG category descriptions.
E06-1001.txt,73,5 Semantics of the category notation In the categorial grammar tradition initiated by Lambek 1958 _comma_ the standard way of providing a semantics for category notation defines the denotation of a category description as a set of strings of terminal symbols.
E06-1001.txt,75,Let s 5 start by assuming that_comma_ given some I CCG A_comma_subsetsqequalA_comma_ B_comma_subsetsqequalB_comma_b_comma_S_comma_L over alphabet _comma_ there is a denotation function ... from the maximal types in the hierarchy of saturated categories A_comma_subsetsqequalA to  .
E06-1001.txt,76,For all non maximal saturated category symbols in A_comma_ the denotation of is then the set of all strings in any of s subcategories_comma_ i.e.   uniontext subsetsqequalA  .
E06-1001.txt,78,If we define the constraint set of some lexical type  B as the set of all category descriptions either associated with or inherited by _comma_ then the denotation of is defined asintersectiontext   .
E06-1001.txt,79,Unfortunately_comma_ this approach to interpreting ICCG category descriptions is insufficient_comma_ since the logic underlying CCG is not simply the logic of string concatenation_comma_ i.e.
E06-1001.txt,80,CCG allows a limited degree of permutation by dint of the crossed composition and substitution operations.
E06-1001.txt,81,In fact_comma_ there appears to be no categorial type logic_comma_ in the sense of Moortgat 1997 _comma_ for which the CCG combinatory operations provide a sound and complete derivation system_comma_ even in the resource sensitive system of Baldridge 2002 .
E06-1001.txt,82,An alternative approach involves interpreting I CCG category descriptions against totally well typed_comma_ sort resolved feature structures_comma_ as in the HPSG formalism of Pollard and Sag 1994 .
E06-1001.txt,83,Givensometypehierarchy A_comma_subsetsqequalA ofsaturated category symbols and some lexical inheritance hierarchy B_comma_subsetsqequalB_comma_b _comma_ we define a class of category models _comma_ i.e. binary trees where every leaf node carries a maximal saturated category symbol inA_comma_ every non leaf node carries a directional slash_comma_ and every branch is labelled as either a result or an argument .
E06-1001.txt,84,In addition_comma_ nodes are optionally labelled with maximal lexical types from B.
E06-1001.txt,85,Note that since only maximal types are permitted in a model_comma_ they are by definition sort resolved.
E06-1001.txt,87,In fact_comma_ this property is restricted to those models where_comma_ if node n carries lexical type _comma_ then every category description in the constraint set of is satisfied from n.
E06-1001.txt,88,Note that the root of the model in Figure 6 carries the lexical type verbpl .
E06-1001.txt,89,Since all six constraints inherited by this type in Figure 4 are satisfied from the root_comma_ and since no other lexical types appear in the model_comma_ we can state that the model is well typed.
E06-1001.txt,90,In sum_comma_ given an appropriate satisfaction relation between well typed category models and ICCGcategorydescriptions_comma_alongwithadefinition of the CCG combinatory operations in terms of category models_comma_ it is possible to provide a formal interpretation of the language of I CCG category descriptions_comma_ in the same way as unification based formalisms like HPSG ground attribute value notation in terms of underlying totally well typed_comma_ sort resolved feature structure models.
E06-1001.txt,91,Such a semantics is necessary in order to prove the correctness of eventual I CCG implementations.
E06-1001.txt,92,6 Extending the description language The I CCG formalism described here involves a generalisation of the CCG category notation to incorporate the concept of lexical inheritance.
E06-1001.txt,93,The primary motivation for this concerns the ideal of non redundant encoding of lexical information in humanlanguagegrammars_comma_ sothatallkindsoflinguistically significant generalisation can be captured somewhere in the grammar.
E06-1001.txt,94,In order to fulfil thisgoal_comma_ thesimplecategorydescriptionlanguage defined above will need to be extended somewhat.
E06-1001.txt,95,For example_comma_ imagine that we want to specify the 6 set of all expressions which take an NPobj argument_comma_ but not necessarily as their first argument_comma_ i.e. the set of all transitive expressions ARG NPobj 7 RES ARG NPobj RES RES ARG NPobj ... It should be clear that this category is not finitely specifiable using the I CCG category notation.
E06-1001.txt,96,One way to allow such generalisations to be made involves incorporating the modal iterationoperatorusedinPropositionalDynamicLogic Harel_comma_ 1984 to denote an unbounded number of arc traversals in a Kripke structure.
E06-1001.txt,97,In other words_comma_ category description RES is satisfied from nodenin a model just in case some finite sequence of result arcs leads fromnto a node where is satisfied.
E06-1001.txt,98,In this way_comma_ the set of expressions taking an NPobj argument is specified by means of the category description RES ARG NPobj.
E06-1001.txt,99,7 Computational aspects At least as far as the I CCG category notation defined in section 4.1 is concerned_comma_ it is a straightforward task to take the standard CKY approach to parsing with CCGs Steedman_comma_ 2000 _comma_ and generalise it to take a functional_comma_ atomic I CCG lexicon and cache out the inherited constraints online.
E06-1001.txt,100,As long as the inheritance hierarchy is nonrecursive and can thus be theoretically cached out into a finite lexicon_comma_ the parsing problem remains worst case polynomial.
E06-1001.txt,101,In addition_comma_ the I CCG formalism satisfies the strong competence requirement of Bresnan 1982 _comma_ according to which the grammar used by or implicit in the human sentence processor is the competence grammar itself.
E06-1001.txt,102,In other words_comma_ although the result of cache ing out particularly common lexical entries will undoubtedly be part of a statistically optimised parser_comma_ it is not essential to the tractability of the formalism.
E06-1001.txt,103,One obvious practical problem for which the work reported here provides at least the germ of a solution involves the question of how to generaliseCCGlexiconswhichhavebeenautomatically induced from treebanks Hockenmaier_comma_ 2003 .
E06-1001.txt,104,To take a concrete example_comma_ Cakici 2005 induces a wide coverage CCG lexicon from a 6000 sentence dependency treebank of Turkish.
E06-1001.txt,106,However_comma_ data sparsity means that the automatically induced lexicon assigns only a small minority of transitive verbs to both classes.
E06-1001.txt,107,One possible way of resolving this problem would involve translating the automatically induced lexicon into sets of fully specified I CCG category descriptions_comma_ generating an inheritance hierarchy of lexical types from this lexicon Sporleder_comma_ 2004 _comma_ and applying some more precise version of the following heuristic if a critical mass of words in the automatically induced lexicon belong to both CCG categories X and Y_comma_ then in the derived I CCG lexicon assign all words belonging to either X or Y to the lexical type which functions as the greatest lower bound of X and Y in the lexical inheritance hierarchy.
E06-1001.txt,108,8 Acknowledgements The author is indebted to the following people for providing feedback on various drafts of this paper Mark Steedman_comma_ Cem Bozsahin_comma_ Jason Baldridge_comma_ and three anonymous EACL reviewers. .
E06-2013.txt,1,describe a system for automatic annotation of English text in the FrameNet standard. In addition to the conventional annotation of frame elements and their semantic roles_comma_ we annotate additional semantic information such as support verbs and prepositions_comma_ aspectual markers_comma_ copular verbs_comma_ null arguments_comma_ and slot fillers.
E06-2013.txt,2,As far as we are aware_comma_ this is the first system that finds this information automatically.
E06-2013.txt,4,Shallow semantic parsing has been an active area of research during the last few years.
E06-2013.txt,5,Semantic parsers_comma_ which are typically based on the FrameNet Baker et al._comma_ 1998 or PropBank formalisms_comma_ have proven useful in a number of NLP projects_comma_ such as information extraction and question answering.
E06-2013.txt,6,Themain reason fortheir popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness.
E06-2013.txt,7,Building English semantic parsers for the FrameNet standard has been studied widely Gildea and Jurafsky_comma_ 2002 Litkowski_comma_ 2004 .
E06-2013.txt,8,These systems typically address the task of identifying and classifying Frame Elements FEs _comma_ that is semantic arguments of predicates_comma_ for a given target word predicate .
E06-2013.txt,9,Although the FE layer is arguably the most central_comma_ the FrameNet annotation standard defines a number of additional semantic layers_comma_ which contain information about support expressions verbs and prepositions _comma_ copulas_comma_ null arguments_comma_ slotfillers_comma_ and aspectual particles.
E06-2013.txt,10,This information can for example be used in a semantic parser to refine the meaning of a predicate_comma_ to link predicates inasentence together_comma_ orpossibly toimprove detection and classification of FEs.
E06-2013.txt,11,The task of automatic reconstruction of the additional semantic layers has not been addressed by any previous system.
E06-2013.txt,12,Inthiswork_comma_ wedescribe asystem thatautomatically identifies the entities in those layers.
E06-2016.txt,1,General Ontology Discovery is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts. Operationally_comma_ it acts as a search engine returning a set of true predicates regarding the query instead of the usual ranked list of relevant documents.
E06-2016.txt,2,Our approach relies on two basic assumptions i paradigmatic relations can be established only among terms in the same Semantic Domain an ii they can be inferred from texts by analyzing the Subject Verb Object patterns where two domain specific terms co occur.
E06-2016.txt,3,A qualitative analysis of the system output shows that GOD provide true_comma_ informative and meaningful relations in a very efficient way.
E06-2016.txt,5,GOD General Ontology Discovery is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts.
E06-2016.txt,6,Operationally_comma_ it acts as a search engine returning a set of true predicates regarding the query instead of the usual ranked list of relevant documents.
E06-2016.txt,7,Such predicates can be perceived as a set of semantic relations explaining the domain of the query_comma_ i.e. a set of binary predicated involving domain specific entities and concepts.
E06-2016.txt,8,Entities and concepts are referred to by domain specific terms_comma_ and the relations among them are expressed by the verbs of which they are arguments.
E06-2016.txt,9,To illustrate the functionality of the system_comma_ below we report an example for the query God. god lord hear prayer god is creator god have mercy faith reverences god lord have mercy jesus_christ is god god banishing him god commanded israelites god was trinity abraham believed god god requires abraham god supply human_need god is holy noah obeyed god From a different perspective_comma_ GOD is first of all a general system for ontology learning from texts Buitelaar et al._comma_ 2005 .
E06-2016.txt,10,Likewise current stateof the art methodologies for non hierarchical relation extraction it exploits shallow parsing techniques to identify syntactic patterns involving domain specific entities Reinberger et al._comma_ 2004 _comma_ and statistical association measures to detect relevant relations Ciaramita et al._comma_ 2005 .
E06-2016.txt,11,In contrast to them_comma_ it does not require any domain specific collection of texts_comma_ allowing the user to describe the domain of interest by simply typing short queries.
E06-2016.txt,12,This feature is of great advantage fromapracticalpointofview itisobviouslymore easytoformulateshortqueriesthantocollecthuge amounts of domain specific texts.
E06-2016.txt,13,Even if_comma_ in principle_comma_ an ontology is supposed to represent a domain by a hierarchy of concepts and entities_comma_ in this paper we concentrate only on the non hyrarchical relation extraction process.
E06-2016.txt,14,In addition_comma_ in this work we do not address the problem of associating synonyms to the same concept e.g. god and lord in the example above .
E06-2016.txt,15,147 In this paper we just concentrate on describing our general framework for ontology learning_comma_ postponing the solution of the already mentioned problems.
E06-2016.txt,16,The good quality of the results and the well foundedness of the GOD framework motivate our future work.
E06-2016.txt,17,2 The GOD algorithm The basic assumption of the GOD model is that paradigmatic relations can be established only among terms in the same Semantic Domain_comma_ while concepts belonging to different fields are mainly unrelated Gliozzo_comma_ 2005 .
E06-2016.txt,18,Such relations can be identified by considering Subject Verb Object SVO patterns involving domain specific terms i.e. syntagmatic relations .
E06-2016.txt,19,When a query Q q1_comma_q2_comma_..._comma_qn is formulated_comma_ GOD operates as follows Domain Discovery Retrieve the ranked list dom Q  t1_comma_t2_comma_..._comma_tk of domain specific terms such that sim ti_comma_Q  prime_comma_ where sim Q_comma_t is a similarity function capturing domain proximity and prime is the domain specificity threshold.
E06-2016.txt,20,Relation Extraction For each SVO pattern involving two different terms ti dom Q and tj dom Q such that the term ti occurs in the subject position and the term tj occurs in the object position return the relation tivtj if score ti_comma_v_comma_tj  primeprime_comma_ where score ti_comma_v_comma_tj measures the syntagmatic association among ti_comma_ v and tj.
E06-2016.txt,21,In Subsection 2.1 we describe into details the Domain Discovery step.
E06-2016.txt,22,Subsection 2.2 is about the relation extraction step.
E06-2016.txt,23,2.1 Domain Discovery Semantic Domains Magnini et al._comma_ 2002 are clusters of very closely related concepts_comma_ lexicalized by domain specific terms.
E06-2016.txt,24,Word senses are determined and delimited only by the meanings of other words in the same domain.
E06-2016.txt,25,Words belonging to a limited number of domains are called domain words.
E06-2016.txt,26,Domain words can be disambiguated by simply identifying the domain of the text.
E06-2016.txt,27,As a consequence_comma_ concepts belonging to different domains are basically unrelated.
E06-2016.txt,28,This observation is crucial from a methodological point of view_comma_ allowing us to perform a large scale structural analysis of the whole lexicon of a language_comma_ otherwise computationally infeasible.
E06-2016.txt,29,In fact_comma_ restricting the attention to a particular domain is a way to reduce the complexity of the overall relation extraction task_comma_ that is evidently quadratic in the number of terms.
E06-2016.txt,30,Domain information can be expressed by exploiting Domain Models DMs Gliozzo et al._comma_ 2005 .
E06-2016.txt,31,A DM is represented by a k kprime rectangular matrix D_comma_ containing the domain relevance for each term with respect to each domain_comma_ where k is the cardinality of the vocabulary_comma_ and kprime is the size of the Domain Set.
E06-2016.txt,32,DMs can be acquired from texts in a totally unsupervised way by exploiting a lexical coherence assumption Gliozzo_comma_ 2005 .
E06-2016.txt,33,To this aim_comma_ term clustering algorithms can be adopted each cluster represents a Semantic Domain.
E06-2016.txt,34,The degree of association among terms and clusters_comma_ estimated by the learning algorithm_comma_ provides a domain relevance function.
E06-2016.txt,35,For our experiments we adopted a clustering strategy based on Latent Semantic Analysis_comma_ following the methodology described in Gliozzo_comma_ 2005 .
E06-2016.txt,36,This operation is done off line_comma_ and can be efficiently performed on large corpora.
E06-2016.txt,37,To filter out noise_comma_ we considered only those terms having a frequency higher than 5 in the corpus.
E06-2016.txt,38,Once a DM has been defined by the matrix D_comma_ the Domain Space is a kprime dimensional space_comma_ in which both texts and terms are associated to Domain Vectors DVs _comma_ i.e. vectors representing their domain relevances with respect to each domain.
E06-2016.txt,39,The DV vectortprimei for the term ti V is the ith row ofD_comma_ where V t1_comma_t2_comma_..._comma_tk is the vocabulary of the corpus.
E06-2016.txt,40,The similarity among DVs in the Domain Space is estimated by means of the cosine operation.
E06-2016.txt,41,When a query Q q1_comma_q2_comma_..._comma_qn is formulated_comma_ its DV vectorQprime is estimated by vectorQprime nsummationdisplay j 1 vectorqprimej 1 and then compared to the DVs of each term ti V by adopting the cosine similarity metric sim ti_comma_Q cos vectortprimei_comma_ vectorQprime 2 where vectortprimei and vectorqprimej are the DVs for the terms ti and qj_comma_ respectively.
E06-2016.txt,42,All those terms whose similarity with the query is above the domain specificity threshold prime are 148 thenreturnedasanoutputofthefunctiondom Q .
E06-2016.txt,43,Empirically_comma_ we fixed this threshold to 0.5.
E06-2016.txt,44,In general_comma_ the higher the domain specificity threshold_comma_ thehighertherelevanceofthediscoveredrelations for the query see Section 3 _comma_ increasing accuracy while reducing recall.
E06-2016.txt,45,In the previous example_comma_ dom god returns the terms lord_comma_ prayer_comma_ creator and mercy_comma_ among the others.
E06-2016.txt,46,2.2 Relation extraction As a second step_comma_ the system analyzes all the syntagmatic relations involving the retrieved entities.
E06-2016.txt,47,To this aim_comma_ as an off line learning step_comma_ the system acquires Subject Verb Object SVO patterns from the training corpus by using regular expressions on the output of a shallow parser.
E06-2016.txt,48,In particular_comma_ GOD extracts the relations tivtj for each ordered couple of domain specific terms ti_comma_tj such that ti dom Q _comma_ tj dom Q and score ti_comma_v_comma_tj  primeprime.
E06-2016.txt,49,The confidence score is estimated by adopting the heuristic confidence measure described in Reinberger et al._comma_ 2004 _comma_ reported below score ti_comma_v_comma_tj F ti_comma_v_comma_tj min F ti _comma_F tj F ti_comma_v F ti F v_comma_tj F tj 3 where F t is the frequency of the term t in the corpus_comma_ F t_comma_v is the frequency of the SV pattern involving both t and v_comma_ F v_comma_t is the frequency of the VO pattern involving both v and t_comma_ and F ti_comma_v_comma_tj is the frequency of the SVO pattern involving ti_comma_ v and tj.
E06-2016.txt,50,In general_comma_ augmenting primeprime is a way to filter out noisy relations_comma_ while decreasing recall.
E06-2016.txt,51,It is important to remark here that all the extractedpredicatesoccuratleastonceinthecorpus_comma_ then they have been asserted somewhere.
E06-2016.txt,52,Even if it is not a sufficient condition to guarantee their truth_comma_ it is reasonable to assume that most of the sentences in texts express true assertions.
E06-2016.txt,53,The relation extraction process is performed online for each query_comma_ then efficiency is a crucial requirement in this phase.
E06-2016.txt,54,It would be preferable to avoid an extensive search of the required SVO patterns_comma_ because the number of sentences in the corpus is huge.
E06-2016.txt,55,To solve this problem we adopted an inverted relation index_comma_ consisting of three hash tables the SV VO table report_comma_ for each term_comma_ the frequency of the SV VO patterns where it occurs as a subject object the SVO table reports_comma_ for each ordered couple of terms in the corpus_comma_ the frequency of the SVO patterns in which they co occur.
E06-2016.txt,56,All the information required to estimate Formula 3 can then be accessed in a time proportional to the frequencies of the involved terms.
E06-2016.txt,57,In general_comma_ domain specific terms are not very frequent in a generic corpus_comma_ allowing a fast computation in most of the cases.
E06-2016.txt,58,3 Evaluation Performing a rigorous evaluation of an ontology learning process is not an easy task Buitelaar et al._comma_ 2005 and it is outside the goals of this paper.
E06-2016.txt,59,Due to time constraints_comma_ we did not performed a quantitative and objective evaluation of our system.
E06-2016.txt,60,In Subsection 3.1 we describe the data and the NLP tools adopted by the system.
E06-2016.txt,61,In Subsection 3.2 we comment some example of the system output_comma_ providing a qualitative analysis of the resultsafterhavingproposedsomeevaluationguidelines.
E06-2016.txt,62,Finally_comma_ in Subsection 3.3 we discuss issues related to the recall of the system.
E06-2016.txt,63,3.1 Experimental Settings To expect high coverage_comma_ the system would be trained on WEB scale corpora.
E06-2016.txt,64,On the other hand_comma_ the analysis of very large corpora needs efficient preprocessing tools and optimized memory allocation strategies.
E06-2016.txt,65,For the experiments reported in this paper we adopted the British National Corpus BNC Consortium_comma_ 2000 _comma_ and we parsed each sentence by exploiting a shallow parser on the output of which we detected SVO patterns by means of regular expressions1.
E06-2016.txt,66,3.2 Accuracy Once a query has been formulated_comma_ and a set of relations has been extracted_comma_ it is not clear how to evaluate the quality of the results.
E06-2016.txt,67,The first four columnsoftheexamplebelowshowtheevaluation we did for the query Karl Marx.
E06-2016.txt,68,Karl Marx TRIM economic_organisation determines superstructure TRUM capitalism needs capitalists FRIM proletariat overthrow bourgeoisie TRIM marx understood capitalism E marx later marxists TRIM labour_power be production TRIM societies are class_societies RIM private_property equals exploitation TRIM primitive_societies were classless TRIM social_relationships form economic_basis TRIM max_weber criticised marxist_view 1For the experiments reported in this paper we used a memory based shallow parser developed at CNTS Antwerp and ILK Tilburg Daelemans et al._comma_ 1999 together with a set of scripts to extract SVO patterns Reinberger et al._comma_ 2004 kindly put at our disposal by the authors.
E06-2016.txt,69,149 TRIM contradictions legitimizes class_structure R E societies is political_level R E class_society where false_consciousness RUE social_system containing such_contradictions TRIM human_societies organizing production Several aspects are addressed truthfulness i.e. True vs. False in the first column _comma_ relevance for the query i.e. Relevant vs. Not relevant in the second column _comma_ information content i.e. Informative vs. Uninformative_comma_ third column and meaningfulness i.e. Meaningful vs. Error_comma_ fourth column .
E06-2016.txt,70,For most of the test queries_comma_ the majority of the retrieved predicates were true_comma_ relevant_comma_ informative and meaningful_comma_ confirming the quality of the acquired DM and the validity of the relation extraction technique2.
E06-2016.txt,71,From the BNC_comma_ GOD was able to extract good quality information for many different queries in very different domains_comma_ as for example music_comma_ unix_comma_ painting and many others.
E06-2016.txt,72,3.3 Recall An interesting aspect of the behavior of the system is that if the domain of the query is not well represented in the corpus_comma_ the domain discovery step retrieves few domain specific terms.
E06-2016.txt,73,As a consequece_comma_ just few relations and sometimes no relations have been retrieved for most of our test queries.
E06-2016.txt,74,An analysis of such cases showed that the low recall was mainly due to the low coverage of the BNC corpus.
E06-2016.txt,75,We believe that this problem can be avoided by training the system on larger scale corpora e.g. from the Web .
E06-2016.txt,76,4 Conclusion and future work In this paper we reported the preliminary results we obtained from the development of GOD_comma_ a system that dynamically acquires ontologies from texts.
E06-2016.txt,77,In the GOD model_comma_ the required domain is formulated by typing short queries in an Information Retrieval style.
E06-2016.txt,78,The system is efficient and accurate_comma_ even if the small size of the corpus prevented us from acquiring domain ontologies for many queries.
E06-2016.txt,79,For the future_comma_ we plan to evaluate the system in a more rigorous way_comma_ by contrasting its output to hand made reference ontologies for different domains.
E06-2016.txt,80,To improve the coverage of the system_comma_ we are going to train it on WEB scale 2It is worthwhile to remark here that evaluation strongly depends on the point of view from which the query has beenformulated.
E06-2016.txt,81,For example_comma_ thepredicate private property equals exploitation is true in the Marxist view_comma_ while it is obviously false with respect to the present economic system. text collections and to explore the use of supervised relation extraction techniques.
E06-2016.txt,82,In addition_comma_ we are improving relation extraction by adopting a more sophisticated syntactic analisys e.g. Sematic Role Labeling .
E06-2016.txt,83,Finally_comma_ we plan to explore the usefulness of the extracted relations into NLP systems for Question Answering_comma_ Information Extraction and Semantic Entailment.
E06-2016.txt,84,Acknowledgments This work has been supported by the ONTOTEXT project_comma_ funded by the Autonomous Province of Trento under the FUP 2004 research program.
E06-2016.txt,85,Most of the experiments have been performed during my research stage at the University of Antwerp.
E06-2016.txt,86,Thanks to Walter Daelemans and Carlo Strapparava for useful suggestions and comments and to Marie Laure Reinberger for having provided the SVO extraction scripts. .
E06-1044.txt,1,present the psycholinguistically motivatedtask ofpredictinghuman plausibility judgements for verb role argument triples and introduce a probabilistic model that solves it. We also evaluate our model on the related role labelling task_comma_ and compare it with a standard role labeller.
E06-1044.txt,2,For both tasks_comma_ our model benefits from classbased smoothing_comma_ which allows it to make correct argument specific predictions despite a severe sparse data problem.
E06-1044.txt,3,The standard labeller suffers from sparse data and a strong reliance on syntactic cues_comma_ especially in the prediction task.
E06-1044.txt,5,Computational psycholinguistics is concerned with modelling human language processing.
E06-1044.txt,6,Much work has gone into the exploration of sentence comprehension.
E06-1010.txt,1,investigate a series of graph theoretic constraints on non projective dependency parsing and their effect on expressivity_comma_ i.e. whether they allow naturally occurring syntactic constructions to be adequately represented_comma_ and efficiency_comma_ i.e. whether they reduce the search space for the parser. In particular_comma_ we define a new measure for the degree of non projectivity in an acyclic dependency graph obeying the single head constraint.
E06-1010.txt,2,The constraints are evaluated experimentally using data from the Prague Dependency Treebank and the Danish Dependency Treebank.
E06-1010.txt,3,Theresults indicate that_comma_ whereas complete linguistic coverage in principle requires unrestricted non projective dependency graphs_comma_ limiting the degree of non projectivity to at most 2 can reduce average running time from quadratic to linear_comma_ while excluding less than 0.5 of the dependency graphs found in the two treebanks.
E06-1010.txt,4,This is a substantial improvement over the commonly used projective approximation degree 0 _comma_ which excludes 15 25 of the graphs.
E06-1010.txt,6,Data driven approaches to syntactic parsing has until quite recently been limited to representations that do not capture non local dependencies.
E06-1010.txt,7,This is true regardless of whether representations are based on constituency_comma_ where such dependencies are traditionally represented by empty categories and coindexation to avoid explicitly discontinuous constituents_comma_ or on dependency_comma_ where it is more common to use a direct encoding of so called nonprojective dependencies.
E06-1010.txt,8,Whilethis surface dependency approximation Levy and Manning_comma_ 2004 may be acceptable for certain applications of syntactic parsing_comma_ it is clearly not adequate as a basis for deep semantic interpretation_comma_ which explains the growing body of research devoted to different methods for correcting this approximation.
E06-1010.txt,9,Most of this work has so far focused either on post processing to recover non local dependencies from context free parse trees Johnson_comma_ 2002 Jijkoun and De Rijke_comma_ 2004 Levy and Manning_comma_ 2004 Campbell_comma_ 2004 _comma_ or on incorporating nonlocal dependency information in nonterminal categories in constituency representations Dienes and Dubey_comma_ 2003 Hockenmaier_comma_ 2003 Cahill et al._comma_ 2004 or in the categories used to label arcs in dependency representations Nivre and Nilsson_comma_ 2005 .
E06-1010.txt,10,By contrast_comma_ there is very little work on parsing methods that allow discontinuous constructions to be represented directly in the syntactic structure_comma_ whether by discontinuous constituent structures or by non projective dependency structures.
E06-1010.txt,11,Notable exceptions are Plaehn 2000 _comma_ where discontinuous phrase structure grammar parsing is explored_comma_ and McDonald et al. 2005b _comma_ where nonprojective dependency structures are derived using spanning tree algorithms from graph theory.
E06-1010.txt,12,Onequestion that arises ifwewanttopursue the structure based approach is how to constrain the class of permissible structures.
E06-1010.txt,13,On the one hand_comma_ we want to capture all the constructions that are found in natural languages_comma_ or at least to provide a much better approximation than before.
E06-1010.txt,14,On the other hand_comma_ it must still be possible for the parser not only to search the space of permissible structures in an efficient way but also to learn to select the most appropriate structure for a given sentence with sufficient accuracy.
E06-1010.txt,15,This is the usual tradeoff 73 between expressivity and complexity_comma_ where a less restricted class of permissible structures can capture more complex constructions_comma_ but where the enlarged search space makes parsing harder with respect to both accuracy and efficiency.
E06-1010.txt,16,Whereas extensions to context free grammar have been studied quite extensively_comma_ there are very few corresponding results for dependency based systems.
E06-1010.txt,17,Since Gaifman 1965 proved that his projective dependency grammar is weakly equivalent to context free grammar_comma_ Neuhaus and Br oker 1997 have shown that the recognition problem for a dependency grammar that can define arbitrary non projective structures is NP complete_comma_ but there are no results for systems of intermediate complexity.
E06-1010.txt,18,The pseudo projective grammar proposed by Kahane et al. 1998 can be parsed in polynomial time and captures non local dependencies through a form of gap threading_comma_ but the structures generated by the grammar are strictly projective.
E06-1010.txt,19,Moreover_comma_ the study of formal grammarsisonly partially relevant for research ondatadriven dependency parsing_comma_ where most systems are not grammar based but rely on inductive inference from treebank data Yamada and Matsumoto_comma_ 2003 Nivre et al._comma_ 2004 McDonald et al._comma_ 2005a .
E06-1010.txt,20,For example_comma_ despite the results of Neuhaus and Br oker 1997 _comma_ McDonald et al. 2005b perform parsing with arbitrary non projective dependency structures in O n2 time.
E06-1010.txt,21,In this paper_comma_ we will therefore approach the problem from a slightly different angle.
E06-1010.txt,22,Instead of investigating formal dependency grammars and their complexity_comma_ we will impose a series of graphtheoretic constraints on dependency structures and see how these constraints affect expressivity and parsing efficiency.
E06-1010.txt,23,The approach is mainly experimental and we evaluate constraints using data from two dependency based treebanks_comma_ the Prague Dependency Treebank Haji c et al._comma_ 2001 and the Danish Dependency Treebank Kromann_comma_ 2003 .
E06-1010.txt,24,Expressivity is investigated by examining how large a proportion of the structures found in the treebanks are parsable under different constraints_comma_ and efficiency is addressed by considering the number of potential dependency arcs that need to be processed when parsing these structures.
E06-1010.txt,25,This is a relevant metric for data driven approaches_comma_ where parsing timeisoften dominated by thecomputation of model predictions or scores for such arcs.
E06-1010.txt,26,The parsing experiments are performed with a variant of Covington s algorithm for dependency parsing Covington_comma_ 2001 _comma_ using the treebank as an oracle in order to establish an upper bound on accuracy.
E06-1010.txt,27,However_comma_ the results are relevant for a larger class of algorithms that derive nonprojective dependency graphs by treating every possible word pair as a potential dependency arc.
E06-1010.txt,28,The paper is structured as follows.
E06-1010.txt,29,In section 2 we define dependency graphs_comma_ and in section 3 we formulate a number of constraints that can be used to define different classes of dependency graphs_comma_ ranging from unrestricted non projective to strictly projective.
E06-1010.txt,30,In section 4 we introduce the parsing algorithm used in the experiments_comma_ and in section 5 we describe the experimental setup.
E06-1010.txt,31,In section 6 we present the results of the experiments and discuss their implications for non projective dependency parsing.
E06-1010.txt,32,We conclude in section 7.
E06-1010.txt,33,2 Dependency Graphs A dependency graph is a labeled directed graph_comma_ the nodes of which are indices corresponding to the tokens of a sentence.
E06-1010.txt,34,Formally Definition 1 Given a set R of dependency types arc labels _comma_ a dependency graph for a sentence x w1_comma_..._comma_wn is a labeled directed graph G V_comma_E_comma_L _comma_ where 1.
E06-1010.txt,35,V Zn 1 2.
E06-1010.txt,36,E V V 3.
E06-1010.txt,37,L E R Definition 2 A dependency graph G is wellformed if and only if 1.
E06-1010.txt,38,The node 0 is a root ROOT .
E06-1010.txt,40,G is connected CONNECTEDNESS .1 The set of V of nodes or vertices is the set Zn 1 0_comma_1_comma_2_comma_..._comma_n n Z _comma_ i.e._comma_ the set of non negative integers up to and including n.
E06-1010.txt,41,This means that every token index i of the sentence is a node 1 i n and that there is a special node 0_comma_ which does not correspond to any token of the sentence and which will always be a root of the dependency graph normally the only root .
E06-1010.txt,42,The set E of arcs or edges is a set of ordered pairs i_comma_j _comma_ whereiandj are nodes.
E06-1010.txt,43,Since arcs are used to represent dependency relations_comma_ we will 1To be more exact_comma_ we require G to be weakly connected_comma_ which entails that the corresponding undirected graph is connected_comma_ whereas a strongly connected graph has a directed path between any pair of nodes.
E06-1010.txt,44,74 Only one of them concerns quality. 0 1 R Z Out of a7 a4 a63 AuxP 2 P nich them a7 a4 a63 Atr 3 VB je is a7 a4 a63 Pred 4 T jen only a7 a4 a63 AuxZ 5 C jedna one FEM SG a7 a4 a63 Sb 6 R na to a7 a4 a63 AuxP 7 N4 kvalitu quality a63 a7 a4Adv 8 Z . . a7 a4 a63 AuxK Figure 1 Dependency graph for Czech sentence from the Prague Dependency Treebank say that i is the head and j is the dependent of the arc i_comma_j .
E06-1010.txt,45,As usual_comma_ we will use the notation i j to mean that there is an arc connecting i and j i.e._comma_ i_comma_j E and we will use the notation i j for the reflexive and transitive closure of the arc relation E i.e._comma_ i j if and only if i j or there is a path of arcs connecting i to j .
E06-1010.txt,46,The function L assigns a dependency type arc label r R to every arc e E.
E06-1010.txt,47,Figure 1 shows a Czech sentence from the Prague Dependency Treebank with a well formed dependency graph according to Definition 1 2.
E06-1010.txt,48,3 Constraints Theonly conditions sofarimposedondependency graphs is that the special node 0 be a root and that the graph be connected.
E06-1010.txt,49,Here are three further constraints that are common in the literature 3.
E06-1010.txt,50,Every node has at most one head_comma_ i.e._comma_ if i j then there is no node k such that k negationslash i and k j SINGLE HEAD .
E06-1010.txt,52,The graph G is acyclic_comma_ i.e._comma_ if i j then not j i ACYCLICITY .
E06-1010.txt,54,The graph G is projective_comma_ i.e._comma_ if i j then i k_comma_ for every node k such that i k j or j k i PROJECTIVITY .
E06-1010.txt,55,Note that these conditions are independent in that none of them is entailed by any combination of the others.
E06-1010.txt,56,However_comma_ the conditions SINGLEHEAD and ACYCLICITY together with the basic well formedness conditions entail that the graph is a tree rooted at the node 0.
E06-1010.txt,57,These constraints are assumed in almost all versions of dependency grammar_comma_ especially in computational systems.
E06-1010.txt,58,By contrast_comma_ the PROJECTIVITY constraint is much more controversial.
E06-1010.txt,59,Broadly speaking_comma_ we can say that whereas most practical systems for dependency parsing do assume projectivity_comma_ most dependency based linguistic theories donot.
E06-1010.txt,60,More precisely_comma_ most theoretical formulations of dependency grammar regard projectivity as the norm but also recognize the need for non projective representations to capture non local dependencies Mel cuk_comma_ 1988 Hudson_comma_ 1990 .
E06-1010.txt,61,In order to distinguish classes of dependency graphs thatfallinbetweenarbitrary non projective and projective_comma_ we define a notion of degree of non projectivity_comma_ such that projective graphs have degree 0 while arbitrary non projective graphs have unbounded degree.
E06-1010.txt,62,Definition 3 LetG V_comma_E_comma_L beawell formed dependency graph_comma_ satisfying SINGLE HEAD and ACYCLICITY_comma_ and let Ge be the subgraph of G that only contains nodes between i and j for the arc e i_comma_j i.e._comma_ Ve i 1_comma_..._comma_j 1 if i j and Ve j 1_comma_..._comma_i 1 if i j .
E06-1010.txt,64,The degree of an arc e E is the number of connected components c in Ge such that the root of c is not dominated by the head of e.
E06-1010.txt,66,The degree of G is the maximum degree of any arc e E.
E06-1010.txt,67,To exemplify the notion of degree_comma_ we note that the dependency graph in Figure 1 which satisfies SINGLE HEAD and ACYCLICITY has degree 1.
E06-1010.txt,68,The only non projective arc in the graph is 5_comma_1 and G 5_comma_1 contains three connected components_comma_ each of which consists of a single root node 2_comma_ 3 and 4 .
E06-1010.txt,69,Since only one of these_comma_ 3_comma_ is not dominated by 5_comma_ the arc 5_comma_1 has degree 1.
E06-1010.txt,70,4 Parsing Algorithm Covington 2001 describes a parsing strategy for dependency representations that has been known 75 since the 1960s but not presented in the literature.
E06-1010.txt,71,The left to right or incremental version of this strategy can be formulated in the following way PARSE x w1_comma_..._comma_wn 1 for i 1 up to n 2 for j i 1 down to 1 3 LINK i_comma_ j The operation LINK i_comma_ j nondeterministically chooses between i adding the arc i j with some label _comma_ ii adding the arc j i with some label _comma_ and iii adding noarcatall.
E06-1010.txt,72,Inthis way_comma_ the algorithm builds a graph by systematically trying to link every pair of nodes i_comma_j i j .
E06-1010.txt,73,This graph will be a well formed dependency graph_comma_ provided that we also add arcs from the root node 0 to every root node in 1_comma_..._comma_n .
E06-1010.txt,74,Assuming that the LINK i_comma_ j operation can be performed in some constant time c_comma_ the running time of the algorithm issummationtextni 1 c n 1 c n22 n2 _comma_ which in terms of asymptotic complexity is O n2 .
E06-1010.txt,75,In the experiments reported in the following sections_comma_ we modify this algorithm by making the performance of LINK i_comma_ j conditional on the arcs i_comma_j and j_comma_i being permissible under the given graph constraints PARSE x w1_comma_..._comma_wn 1 for i 1 up to n 2 for j i 1 down to 1 3 if PERMISSIBLE i_comma_ j_comma_ C 4 LINK i_comma_ j The function PERMISSIBLE i_comma_ j_comma_ C returns true iff i j and j i are permissible arcs relative to the constraint C and the partially built graph G.
E06-1010.txt,76,For example_comma_ with the constraint SINGLEHEAD_comma_ LINK i_comma_ j will not be performed if both i and j already have a head in the dependency graph.
E06-1010.txt,77,We call the pairs i_comma_j i j for which LINK i_comma_ j is performed for a given sentence and set of constraints the active pairs_comma_ and we use the number of active pairs_comma_ as a function of sentence length_comma_ as an .
E06-3001.txt,1,paper describes the development of a rule based computational model that describes how a feature based representation of shared visual information combines with linguistic cues to enable effective reference resolution. This work explores a language only model_comma_ a visualonly model_comma_ and an integrated model of reference resolution and applies them to a corpus of transcribed task oriented spoken dialogues.
E06-3001.txt,2,Preliminary results from a corpus based analysis suggest that integrating information from a shared visual environment can improve the performance and quality of existing discoursebased models of reference resolution.
E06-3001.txt,4,In this paper_comma_ we present work in progress towards the development of a rule based computational model to describe how various forms of shared visual information combine with linguistic cues to enable effective reference resolution during task oriented collaboration.
E06-3001.txt,5,A number of recent studies have demonstrated that linguistic patterns shift depending on the speaker s situational context.
E06-3001.txt,6,Patterns of proximity markers e.g._comma_ this here vs. that there change according to whether speakers perceive themselves to be physically co present or remote from their partner Byron Stoia_comma_ 2005 Fussell et al._comma_ 2004 Levelt_comma_ 1989 .
E06-3001.txt,7,The use of particular forms of definite referring expressions e.g._comma_ personal pronouns vs. demonstrative pronouns vs. demonstrative descriptions varies depending on the local visual context in which they are constructed Byron et al._comma_ 2005a .
E06-3001.txt,8,And people are found to use shorter and syntactically simpler language Oviatt_comma_ 1997 and different surface realizations Cassell Stone_comma_ 2000 when gestures accompany their spoken language.
E06-3001.txt,9,More specifically_comma_ work examining dialogue patterns in collaborative environments has demonstrated that pairs adapt their linguistic patterns based on what they believe their partner can see Brennan_comma_ 2005 Clark Krych_comma_ 2004 Gergle et al._comma_ 2004 Kraut et al._comma_ 2003 .
E06-3001.txt,10,For example_comma_ when a speaker knows their partner can see their actions but will incur a small delay before doing so_comma_ they increase the proportion of full NPs used Gergle et al._comma_ 2004 .
E06-3001.txt,11,Similar work by Byron and colleagues 2005b demonstrates that the forms of referring expressions vary according to a partner s proximity to visual objects of interest.
E06-3001.txt,12,Together this work suggests that the interlocutors shared visual context has a major impact on their patterns of referring behavior.
E06-3001.txt,13,Yet_comma_ a number of discourse based models of reference primarily rely on linguistic information without regard to the surrounding visual environment e.g._comma_ see Brennan et al._comma_ 1987 Hobbs_comma_ 1978 Poesio et al._comma_ 2004 Strube_comma_ 1998 Tetreault_comma_ 2005 .
E06-3001.txt,14,Recently_comma_ multi modal models have emerged that integrate visual information into the resolution process.
E06-3001.txt,15,However_comma_ many of these models are restricted by their simplifying assumption of communication via a command language.
E06-3001.txt,16,Thus_comma_ their approaches apply to explicit interaction techniques but do not necessarily support more general communication in the presence of shared visual information e.g._comma_ see Chai et al._comma_ 2005 Huls et al._comma_ 1995 Kehler_comma_ 2000 .
E06-3001.txt,17,It is the goal of the work presented in this paper to explore the performance of languagebased models of reference resolution in contexts where speakers share a common visual space.
E06-3001.txt,18,In particular_comma_ we examine three basic hypotheses 7 regarding the likely impact of linguistic and visual salience on referring behavior.
E06-3001.txt,19,The first hypothesis suggests that visual information is disregarded and that linguistic context provides sufficient information to describe referring behavior.
E06-3001.txt,20,The second hypothesis suggests that visual salience overrides any linguistic salience in governing referring behavior.
E06-3001.txt,21,Finally_comma_ the third hypothesis posits that a balance of linguistic and visual salience is needed in order to account for patterns of referring behavior.
E06-3001.txt,22,In the remainder of this paper_comma_ we begin by presenting a brief discussion of the motivation for this work.
E06-3001.txt,23,We then describe three computational models of referring behavior used to explore the hypotheses described above_comma_ and the corpus on which they have been evaluated.
E06-3001.txt,24,We conclude by presenting preliminary results and discussing future modeling plans.
E06-3001.txt,25,2 Motivation There are several motivating factors for developing a computational model of referring behavior in shared visual contexts.
E06-3001.txt,26,First_comma_ a model of referring behavior that integrates a component of shared visual information can be used to increase the robustness of interactive agents that converse with humans in real world situated environments.
E06-3001.txt,27,Second_comma_ such a model can be applied to the development of a range of technologies to support distributed group collaboration and mediated communication.
E06-3001.txt,28,Finally_comma_ such a model can be used to provide a deeper theoretical understanding of how humans make use of various forms of shared visual information in their everyday communication.
E06-3001.txt,29,The development of an integrated multi modal model of referring behavior can improve the performance of state of the art computational models of communication currently used to support conversational interactions with an intelligent agent Allen et al._comma_ 2005 Devault et al._comma_ 2005 Gorniak Roy_comma_ 2004 .
E06-3001.txt,30,Many of these models rely on discourse state and prior linguistic contributions to successfully resolve .
E06-1027.txt,1,of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment bearing adjectives from WordNet using the Sentiment Tag Extraction Program STEP .
E06-1027.txt,2,We did 58 STEP runs on unique non intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists.
E06-1027.txt,3,The 58 runs were then collapsed into a single set of 7_comma_813 unique words.
E06-1027.txt,4,For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive.
E06-1027.txt,5,We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment the core adjectives_comma_ which had the highest Net Overlap scores_comma_ were identified most accurately both by STEP and by human annotators_comma_ while the words on the periphery of the category had the lowest scores and were associated with low rates of inter annotator agreement.
E06-1027.txt,7,Many of the tasks required for effective semantic tagging of phrases and texts rely on a list of words annotated with some lexical semantic features.
E06-1027.txt,8,Traditional approaches to the development of such lists are based on the implicit assumption of classical truth conditional theories of meaning representation_comma_ which regard all members of a category as equal no element is more of a member than any other Edmonds_comma_ 1999 .
E06-1027.txt,9,In this paper_comma_ we challenge the applicability of this assumption to the semantic category of sentiment_comma_ which consists of positive_comma_ negative and neutral subcategories_comma_ and present a dictionary based Sentiment Tag Extraction Program STEP that we use to generate a fuzzy set of English sentiment bearing words for the use in sentiment tagging systems 1.
E06-1027.txt,10,The proposed approach based on the fuzzy logic Zadeh_comma_ 1987 is used here to assign fuzzy sentiment tags to all words in WordNet Fellbaum_comma_ 1998 _comma_ thatisitassigns sentiment tagsandadegree of centrality of the annotated words to the sentiment category.
E06-1027.txt,11,This assignment is based on WordNet glosses.
E06-1027.txt,12,The implications of this approach for NLP and linguistic research are discussed.
E06-1027.txt,13,2 The Category of Sentiment as a Fuzzy Set Some semantic categories have clear membership e.g._comma_ lexical fields Lehrer_comma_ 1974 of color_comma_ body parts or professions _comma_ while others are much more difficult to define.
E06-1027.txt,14,This prompted the development ofapproaches thatregardthetransition frommembership to non membership in a semantic category asgradual rather than abrupt Zadeh_comma_ 1987 Rosch_comma_ 1978 .
E06-1027.txt,15,In this paper we approach the category of sentiment as one of such fuzzy categories where some words such as good_comma_ bad are very central_comma_ prototypical members_comma_ while other_comma_ less central words may be interpreted differently by different people.
E06-1027.txt,16,Thus_comma_ as annotators proceed from the core of the category to its periphery_comma_ word mem1Sentiment tagging is defined here as assigning positive_comma_ negative and neutral labels to words according to the sentiment they express.
E06-1027.txt,17,209 bership inthis category becomes moreambiguous_comma_ and hence_comma_ lower inter annotator agreement can be expected for more peripheral words.
E06-1027.txt,18,Under the classical truth conditional approach the disagreement between annotators is invariably viewed as a sign of poor reliability of coding and is eliminated by training annotators to code difficult and ambiguous cases in some standard way.
E06-1027.txt,19,While this procedure leads to high levels of inter annotator agreement on a list created by a coordinated team of researchers_comma_ the naturally occurring differences in the interpretation of words located on the periphery of the category can clearly be seen when annotations by two independent teams are compared.
E06-1027.txt,20,The Table 1 presents the comparison of GIH4 General Inquirer Harvard IV 4 list_comma_ Stone et al._comma_ 1966 2 and HM from Hatzivassiloglou and McKeown_comma_ 1997 study lists of words manually annotated with sentiment tags by two different research teams.
E06-1027.txt,21,GI H4 HM List composition nouns_comma_ verbs_comma_ adj._comma_ adv. adj. only Total list size 8_comma_211 1_comma_336 Total adjectives 1_comma_904 1_comma_336 Tags assigned Positiv_comma_ Negativ ornotag Positive or Negative Adj. with 1_comma_268 1_comma_336 non neutral tags Intersection 774 55 774 58  intersection of GI H4 adj of HM Agreement on tags 78.7 Table 1 Agreement between GI H4 and HM annotations on sentiment tags.
E06-1027.txt,22,The approach to sentiment as a category with fuzzy boundaries suggests that the 21.3 disagreement between the two manually annotated lists reflects a natural variability in human annotators judgment and that this variability is related to the degree of centrality and or relative importance of certain words to the category of sentiment.
E06-1027.txt,23,The attempts to address this difference 2The General Inquirer GI list used in this study was manually cleaned to remove duplicate entries for words with same part of speech and sentiment.
E06-1027.txt,24,Only the Harvard IV 4 list component of the whole GI was used in this study_comma_ since other lists included in GI lack the sentiment annotation.
E06-1027.txt,25,Unless otherwisespecified_comma_ weused the fullGI H4list including the Neutral words that were not assigned Positiv or Negativ annotations. in importance of various sentiment markers have crystallized in two main approaches automatic assignment of weights based on some statistical criterion Hatzivassiloglou and McKeown_comma_ 1997 Turney and Littman_comma_ 2002 Kim and Hovy_comma_ 2004 _comma_ and others or manual annotation Subasic and Huettner_comma_ 2001 .
E06-1027.txt,26,The statistical approaches usually employ somequantitative criterion e.g._comma_ magnitude of pointwise mutual information in Turney and Littman_comma_ 2002 _comma_ goodness for fit measure in Hatzivassiloglou and McKeown_comma_ 1997 _comma_ probability of word s sentiment given the sentiment if its synonyms in Kim and Hovy_comma_ 2004 _comma_ etc. to define the strength of the sentiment expressed by a word or to establish a threshold for the membership in the crisp sets 3 of positive_comma_ negative and neutral words.
E06-1027.txt,27,Both approaches have their limitations the first approach produces coarse results and requires large amounts of data to be reliable_comma_ while the second approach is prohibitively expensive in terms of annotator time and runs the risk of introducing a substantial subjective bias in annotations.
E06-1027.txt,28,In this paper we seek to develop an approach for semantic annotation of a fuzzy lexical category and apply it to sentiment annotation of all WordNet words.
E06-1027.txt,29,The sections that follow 1 describe the proposed approach used to extract sentiment information from WordNet entries using STEP Semantic Tag Extraction Program algorithm_comma_ 2 discuss theoverallperformance ofSTEP on WordNet glosses_comma_ 3 outline the method for defining centrality of a word to the sentiment category_comma_ and 4 comparetheresults ofbothautomatic STEP and manual HM sentiment annotations to the manually annotated GI H4 list_comma_ which was used as a gold standard in this experiment.
E06-1027.txt,30,The comparisons are performed separately for each of the subsets of GI H4 that are characterized by a different distance from the core of the lexical category of sentiment.
E06-1027.txt,31,3 Sentiment Tag Extraction from WordNet Entries Word lists for sentiment tagging applications can be compiled using different methods.
E06-1027.txt,32,Automatic methods of sentiment annotation at the word level can be grouped into two major categories 1 corpus based approaches and 2 dictionary based 3We use the term crisp set to refer to traditional_comma_ nonfuzzy sets 210 approaches.
E06-1027.txt,33,The first group includes methods that rely on syntactic or co occurrence patterns of words in large texts to determine their sentiment e.g._comma_ Turney and Littman_comma_ 2002 Hatzivassiloglou and McKeown_comma_ 1997 Yu and Hatzivassiloglou_comma_ 2003 Grefenstette et al._comma_ 2004 and others .
E06-1027.txt,34,The majority of dictionary based approaches use WordNet information_comma_ especially_comma_ synsets and hierarchies_comma_ to acquire sentiment marked words Hu and Liu_comma_ 2004 Valitutti et al._comma_ 2004 Kim and Hovy_comma_ 2004 or to measure the similarity between candidate words and sentiment bearing words such as good and bad Kamps et al._comma_ 2004 .
E06-1027.txt,35,In this paper_comma_ we propose an approach to sentiment annotation of WordNet entries that was implemented and tested in the Semantic Tag Extraction Program STEP .
E06-1027.txt,36,This approach relies both on lexical relations synonymy_comma_ antonymy and hyponymy provided in WordNet and on the WordNet glosses.
E06-1027.txt,37,It builds upon the properties of dictionary entries as a special kind of structured text such lexicographical texts are built to establish semantic equivalence between the left hand and the right hand parts of the dictionary entry_comma_ and therefore are designed to match as close as possible the components of meaning of the word.
E06-1027.txt,38,They have relatively standard style_comma_ grammar and syntactic structures_comma_ which removes a substantial source of noise common to other types of text_comma_ and finally_comma_ they have extensive coverage spanning the entire lexicon of a natural language.
E06-1027.txt,39,The STEP algorithm starts with a small set of seed words of known sentiment value positive or negative .
E06-1027.txt,40,This list is augmented during the first pass by adding synonyms_comma_ antonyms and hyponyms of the seed words supplied in WordNet.
E06-1027.txt,41,This step brings on average a 5 fold increase in the size of the original list with the accuracy of the resulting list comparable to manual annotations 78 _comma_ similar to HM vs. GI H4 accuracy .
E06-1027.txt,42,At the second pass_comma_ the system goes through all WordNet glosses and identifies the entries that contain in their definitions the sentiment bearing words from the extended seed list and adds these head words or rather_comma_ lexemes to the corresponding category positive_comma_ negative or neutral the remainder .
E06-1027.txt,43,A third_comma_ clean up pass is then performed to partially disambiguate the identified WordNet glosses with Brill s part of speech tagger Brill_comma_ 1995 _comma_ which performs with up to 95 accuracy_comma_ and eliminates errors introduced into the list by part of speech ambiguity of some words acquired in pass 1 and from the seed list.
E06-1027.txt,44,At this step_comma_ we also filter out allthose words that have been assigned contradicting_comma_ positive and negative_comma_ sentiment values within the same run.
E06-1027.txt,45,The performance of STEP was evaluated using GI H4 as a gold standard_comma_ while the HM list was used as a source of seed words fed into the system.
E06-1027.txt,46,We evaluated the performance of our system against the complete list of 1904 adjectives in GI H4 that included not only the words that were markedasPositiv_comma_ Negativ_comma_ butalso thosethatwere not considered sentiment laden by GI H4 annotators_comma_ and hence were by default considered neutral in our evaluation.
E06-1027.txt,47,For the purposes of the evaluation we have partitioned the entire HM list into 58 non intersecting seed lists of adjectives.
E06-1027.txt,48,The results of the 58 runs on these non intersecting seed lists are presented in Table 2.
E06-1027.txt,49,The Table 2 shows that the performance of the system exhibits substantial variability depending on the composition of the seed list_comma_ with accuracy ranging from 47.6 to 87.5 percent Mean 71.2 _comma_ Standard Deviation St.Dev 11.0 .
E06-1027.txt,50,Average Average run size correct ofadj StDev StDev PASS 1 103 29 78.0 10.5 WN Relations PASS 2 630 377 64.5 10.8 WN Glosses PASS 3 435 291 71.2 11.0 POS clean up Table 2 Performance statistics on STEP runs.
E06-1027.txt,51,The significant variability in accuracy of the runs Standard Deviation over 10 is attributable to the variability in the properties of the seed list words in these runs.
E06-1027.txt,52,The HM list includes some sentiment marked words where not all meanings are laden with sentiment_comma_ but also the words where some meanings are neutral and even the words where such neutral meanings are much more frequent than the sentiment laden ones.
E06-1027.txt,53,The runs where seed lists included such ambiguous adjectives were labeling a lot of neutral words as sentiment marked since such seed words were more likely to be found in the WordNet glosses in their more frequent neutral meaning.
E06-1027.txt,54,For example_comma_ run 53 had in its seed list two ambiguous adjectives 211 dim and plush_comma_ which are neutral in most of the contexts.
E06-1027.txt,55,This resulted in only 52.6 accuracy 18.6 below the average .
E06-1027.txt,56,Run 48_comma_ on the other hand_comma_ by a sheer chance_comma_ had only unambiguous sentiment bearing words in its seed list_comma_ and_comma_ thus_comma_ performed with a fairly high accuracy 87.5 _comma_ 16.3 above the average .
E06-1027.txt,57,In order to generate a comprehensive list covering the entire set of WordNet adjectives_comma_ the 58 runs werethen collapsed into asingle setofunique words.
E06-1027.txt,58,Since many of the clearly sentiment laden adjectives that form the core of the category of sentiment were identified by STEP in multiple runs and had_comma_ therefore_comma_ multiple duplicates in the list that were counted as one entry in the combined list_comma_ the collapsing procedure resulted in a lower accuracy 66.5 when GI H4 neutrals were included but much larger list of English adjectives marked as positive n 3_comma_908 or negative n 3_comma_905 .
E06-1027.txt,59,The remainder of WordNet s 22_comma_141 adjectives was not found in any STEP run and hence was deemed neutral n 14_comma_328 .
E06-1027.txt,60,Overall_comma_ the system s 66.5 accuracy on the collapsed runs is comparable to the accuracy reported in the literature for other systems run on large corpora Turney and Littman_comma_ 2002 Hatzivassiloglou and McKeown_comma_ 1997 .
E06-1027.txt,61,In order to make a meaningful comparison with the results reported in Turney and Littman_comma_ 2002 _comma_ we also did an evaluation of STEP results on positives and negatives only i.e._comma_the neutral adjectives fromGIH4 list were excluded and compared our labels to the remaining 1266 GI H4 adjectives.
E06-1027.txt,62,The accuracy on this subset was 73.4 _comma_ which is comparabletothenumbers reported byTurneyandLittman 2002 for experimental runs on 3_comma_596 sentimentmarked GI words from different parts of speech using a 2x109 corpus to compute point wise mutual information between the GI words and 14 manually selected positive and negative paradigm words 76.06 .
E06-1027.txt,63,The analysis of STEP system performance vs.GI H4 and of the disagreements between manually annotated HM and GI H4 showed that the greatest challenge with sentiment tagging of words lies at the boundary between sentimentmarked positive or negative and sentimentneutral words.
E06-1027.txt,64,The 7 performance gain from 66.5 to 73.4 associated with the removal of neutrals from the evaluation set emphasizes the importance of neutral words as a major source of sentiment extraction system errors 4.
E06-1027.txt,65,Moreover_comma_ the boundary between sentiment bearing positive or negative and neutral words in GI H4 accounts for 93 of disagreements between the labels assigned to adjectives in GI H4 and HM by two independent teams of human annotators.
E06-1027.txt,66,The view taken here is that the vast majority of such interannotator disagreements are not really errors but a reflection of the natural ambiguity of the words that are located on the periphery of the sentiment category.
E06-1027.txt,67,4 Establishing the degree of word s centrality to the semantic category The approach to sentiment category as a fuzzy set ascribes the category of sentiment some specific structural properties.
E06-1027.txt,68,First_comma_ as opposed to the words located on the periphery_comma_ more central elements of the set usually have stronger and more numerous semantic relations with other category members 5.
E06-1027.txt,69,Second_comma_ the membership of these central words in the category is less ambiguous than the membership of more peripheral words.
E06-1027.txt,70,Thus_comma_ we can estimate the centrality of a word in a given category in two ways 1.
E06-1027.txt,71,Through the density of the word s relationships with other words by enumerating its semantic ties to other words within the field_comma_ and calculating membership scores based on the number of these ties and 2.
E06-1027.txt,72,Through the degree of word membership ambiguity by assessing the inter annotator agreement on the word membership in this category.
E06-1027.txt,73,Lexicographical entries in the dictionaries_comma_ such as WordNet_comma_ seek to establish semantic equivalence between the word and its definition and provide a rich source of human annotated relationships between the words.
E06-1027.txt,74,By using a bootstrapping system_comma_ such as STEP_comma_ that follows the links between the words in WordNet to find similar words_comma_ we can identify the paths connecting members of agiven semantic category inthe dictionary.
E06-1027.txt,75,With multiple bootstrapping runs on different seed 4It is consistent with the observation by Kim and Hovy 2004 who noticed that_comma_ when positives and neutrals were collapsed into the same category opposed to negatives_comma_ the agreement between human annotators rose by 12 .
E06-1027.txt,76,5The operationalizations of centrality derived from the number of connections between elements can be found in social network theory Burt_comma_ 1980 212 lists_comma_ we can then produce a measure of the density of such ties.
E06-1027.txt,77,The ambiguity measure derived from inter annotator disagreement can then be used to validate the results obtained from the density based method of determining centrality.
E06-1027.txt,78,In order to produce a centrality measure_comma_ we conducted multiple runs with non intersecting seed lists drawn from HM.
E06-1027.txt,79,The lists of words fetched by STEP on different runs partially overlapped_comma_ suggesting that the words identified by the system many times as bearing positive or negative sentiment are more central to the respective categories.
E06-1027.txt,80,The number of times the word has been fetched by STEP runs is reflected in the Gross Overlap Measure produced by the system.
E06-1027.txt,81,In some cases_comma_ there wasadisagreement between different runs on the sentiment assigned to the word.
E06-1027.txt,82,Such disagreements were addressed by computing the Net Overlap Scores for each of the found words thetotalnumberofrunsassigning theword a negative sentiment was subtracted from the total of the runs that consider it positive.
E06-1027.txt,83,Thus_comma_ the greater the number of runs fetching the word i.e._comma_ Gross Overlap and the greater the agreement between these runs on the assigned sentiment_comma_ the higher the Net Overlap Score of this word.
E06-1027.txt,84,The Net Overlap scores obtained for each identified word were then used to stratify these words into groups that reflect positive or negative distance of these words from the zero score.
E06-1027.txt,85,The zero score was assigned to a the WordNet adjectives that were not identified by STEP as bearing positive or negative sentiment 6 and to b the words with equal number of positive and negative hits on several STEP runs.
E06-1027.txt,86,The performance measures for each of the groups were then computed to allow the comparison of STEPand human annotator performance on the words from the core and from the periphery of the sentiment category.
E06-1027.txt,87,Thus_comma_ for each of the Net Overlap Score groups_comma_ both automatic STEP and manual HM sentiment annotations were compared to human annotated GI H4_comma_ which was used as a gold standard in this experiment.
E06-1027.txt,88,On 58 runs_comma_ the system has identified 3_comma_908 English adjectives as positive_comma_ 3_comma_905 as negative_comma_ while the remainder 14_comma_428 of WordNet s 22_comma_141 adjectives was deemed neutral.
E06-1027.txt,89,Of these 14_comma_328 adjectives that STEP runs deemed neutral_comma_ 6The seed lists fed into STEP contained positive or negative_comma_ but no neutral words_comma_ since HM_comma_ which was used as a source for these seed lists_comma_ does not include any neutrals.
E06-1027.txt,90,Figure 1 Accuracy of word sentiment tagging.
E06-1027.txt,91,884 were also found in GI H4 and or HM lists_comma_ which allowed us to evaluate STEP performance and HM GI agreement on the subset of neutrals as well.
E06-1027.txt,92,The graph in Figure 1 shows the distribution of adjectives by Net Overlap scores and the average accuracy agreement rate for each group.
E06-1027.txt,93,Figure 1 shows that the greater the Net Overlap Score_comma_ and hence_comma_ the greater the distance of the word from the neutral subcategory i.e._comma_ from zero _comma_ the more accurate are STEP results and the greater is the agreement between two teams of human annotators HM and GI H4 .
E06-1027.txt,94,On average_comma_ for all categories_comma_ including neutrals_comma_ the accuracy of STEP vs.GI H4 was 66.5 _comma_ human annotated HMhad78.7 accuracy vs. GI H4.
E06-1027.txt,95,Forthewords with Net Overlap of 7 and greater_comma_ both STEP and HM had accuracy around 90 .
E06-1027.txt,96,The accuracy declined dramatically as Net Overlap scores approached zero Neutrals .
E06-1027.txt,97,In this category_comma_ human annotated HM showed only 20 agreement with GI H4_comma_ while STEP_comma_ which deemed these words neutral_comma_ rather than positive or negative_comma_ performed with 57 accuracy.
E06-1027.txt,98,These results suggest that the two measures of word centrality_comma_ Net Overlap Score based on multiple STEP runs and the inter annotator agreement HM vs. GI H4 _comma_ are directly related 7.
E06-1027.txt,99,Thus_comma_ the Net Overlap Score can serve as a useful tool in the identification of core and peripheral members of a fuzzy lexical category_comma_ as well as in predic7In our sample_comma_ the coefficient of correlation between the two was 0.68.
E06-1027.txt,100,The Absolute Net Overlap Score on the subgroups 0 to 10 was used in calculation of the coefficient of correlation.
E06-1027.txt,101,213 tion of inter annotator agreement and system performance on a subgroup of words characterized by a given Net Overlap Score value.
E06-1027.txt,102,Inorder tomake the NetOverlap Score measure usable in sentiment tagging of texts and phrases_comma_ the absolute values of this score should be normalized and mapped onto a standard 0_comma_1 interval.
E06-1027.txt,103,Since the values of the Net Overlap Score may vary depending on the number of runs used in the experiment_comma_ such mapping eliminates the variability in the score values introduced with changes in the number of runs performed.
E06-1027.txt,104,In order to accomplish this normalization_comma_ we used the value of the Net Overlap Score as a parameter in the standard fuzzy membership S function Zadeh_comma_ 1975 Zadeh_comma_ 1987 .
E06-1027.txt,105,This function maps the absolute values of the Net Overlap Score onto the interval from 0 to 1_comma_ where 0 corresponds to the absence of membership in the category of sentiment in our case_comma_ these willbe the neutral words and 1reflects the highest degree of membership in this category.
E06-1027.txt,106,The function can be defined as follows S u _comma_ _comma_    0foru  2 u  2 for u  1 2 u  2 for u  1foru  where u is the Net Overlap Score for the word and _comma_ _comma_ are the three adjustable parameters is set to 1_comma_ is set to 15 and _comma_ which represents a crossover point_comma_ is defined as    2 8.
E06-1027.txt,107,Defined this way_comma_ the S function assigns highest degree of membership 1 to words that have the the Net Overlap Score u 15.
E06-1027.txt,108,The accuracy vs. GI H4 on this subset is 100 .
E06-1027.txt,109,The accuracy goes down as the degree of membership decreases and reaches 59 for values with the lowest degrees of membership.
E06-1027.txt,110,5 Discussion and conclusions This paper contributes to the development of NLP and semantic tagging systems in several respects. The structure of the semantic category of sentiment.
E06-1027.txt,111,The analysis of the category of sentiment of English adjectives presented here suggests that this category is structured as a fuzzy set the distance from the core of the category_comma_ as measured by Net Overlap scores derived from multiple STEP runs_comma_ is shown to affect both the level of interannotator agreement and the system performance vs. human annotated gold standard. The list of sentiment bearing adjectives.
E06-1027.txt,112,The list produced and cross validated by multiple STEP runs contains 7_comma_814 positive and negative English adjectives_comma_ with an average accuracy of 66.5 _comma_ while the human annotated list HM performed at 78.7 accuracy vs. the gold standard GI H4 8.
E06-1027.txt,113,The remaining 14_comma_328 adjectives were not identified as sentiment marked and therefore were considered neutral.
E06-1027.txt,114,The stratification of adjectives by their Net Overlap Score can serve as an indicator of their degree of membership in the category of positive negative sentiment.
E06-1027.txt,115,Since low degrees of membership are associated with greater ambiguity and inter annotator disagreement_comma_ the Net Overlap Score value can provide researchers with a set of volume accuracy trade offs.
E06-1027.txt,116,For example_comma_ by including only the adjectives with the Net Overlap Score of 4 and more_comma_ the researcher can obtain a list of 1_comma_828 positive and negative adjectives with accuracy of 81 vs. GIH4_comma_ or 3_comma_124 adjectives with 75 accuracy if the threshold is set at 3.
E06-1027.txt,117,The normalization of the Net Overlap Score values for the use in phrase and text level sentiment tagging systems was achieved using the fuzzy membership function that we proposed here for the category of sentiment of English adjectives.
E06-1027.txt,118,Future work in the direction laid out by this study will concentrate on two aspects of system development.
E06-1027.txt,119,First further incremental improvements to the precision of the STEP algorithm will be made to increase the accuracy of sentiment annotation through the use of adjective noun combinatorial patterns within glosses.
E06-1027.txt,120,Second_comma_ the resulting list of adjectives annotated with sentiment and with the degree of word membership in the category as measured by the Net Overlap Score will be used in sentiment tagging of phrases and texts.
E06-1027.txt,121,This will enable us to compute the degree of importance of sentiment markers found in phrases and texts.
E06-1027.txt,122,The availability 8GI H4 contains 1268 and HM list has 1336 positive and negative adjectives.
E06-1027.txt,123,The accuracy figures reported here include the errors produced at the boundary with neutrals.
E06-1027.txt,124,214 of the information on the degree of centrality of words to the category of sentiment may improve the performance of sentiment determination systems built to identify the sentiment of entire phrases or texts. System evaluation considerations.
E06-1027.txt,125,The contribution of this paper to the development of methodology of system evaluation is twofold.
E06-1027.txt,126,First_comma_ this research emphasizes the importance of multiple runs on different seed lists for a more accurate evaluation of sentiment tag extraction system performance.
E06-1027.txt,127,We have shown how significantly the system results vary_comma_ depending on the composition of the seed list.
E06-1027.txt,128,Second_comma_ due to the high cost of manual annotation and other practical considerations_comma_ most bootstrapping and other NLP systems are evaluated on relatively small manually annotated gold standards developed for a given semantic category.
E06-1027.txt,129,The implied assumption is that such a gold standard represents a random sample drawn from the population of all category members and hence_comma_ system performance observed on this gold standard can be projected to the whole semantic category.
E06-1027.txt,130,Such extrapolation is not justified if the category is structured as a lexical field with fuzzy boundaries in this case the precision of both machine and human annotation is expected to fall when more peripheral members of the category are processed.
E06-1027.txt,131,In this paper_comma_ the sentiment bearing words identified by the system were stratified based on their Net Overlap Score and evaluated in terms of accuracy of sentiment annotation within each stratum.
E06-1027.txt,132,These strata_comma_ derived from Net Overlap scores_comma_ reflect the degree of centrality of a given word to the semantic category_comma_ and_comma_ thus_comma_ provide greater assurance that system performance on other words with the same Net Overlap Score will besimilartothe performance observed onthe intersection of system results with the gold standard. The role of the inter annotator disagreement.
E06-1027.txt,133,The results of the study presented in this paper call for reconsideration of the role of inter annotator disagreement in the development of lists of words manually annotated with semantic tags.
E06-1027.txt,134,It has been shown here that the inter annotator agreement tends to fall as we proceed from the core of a fuzzy semantic category to its periphery.
E06-1027.txt,135,Therefore_comma_ the disagreement between the annotators does not necessarily reflect a quality problem in human annotation_comma_ but rather a structural property of the semantic category.
E06-1027.txt,136,This suggests that inter annotator disagreement rates can serve as an important source of empirical information about the structural properties of the semantic category and can help define and validate fuzzy sets of semantic category members for a number of NLP tasks and applications. .
E06-1040.txt,1,consider the evaluation problem in Natural Language Generation NLG and present results for evaluating several NLG systems with similar functionality_comma_ including a knowledge based generator and several statistical systems. We compare evaluation results for these systems by human domain experts_comma_ human non experts_comma_ and several automatic evaluation metrics_comma_ including NIST_comma_ BLEU_comma_ and ROUGE.
E06-1040.txt,2,We find that NIST scores correlate best 0.8 with human judgments_comma_ but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.
E06-1040.txt,3,We conclude that automatic evaluation of NLG systems has considerable potential_comma_ in particular where high quality reference texts and only a small number of human evaluators are available.
E06-1040.txt,4,However_comma_ in general it is probably best for automatic evaluations to be supported by human based evaluations_comma_ or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.
E06-1040.txt,6,Evaluation is becoming an increasingly important topic in Natural Language Generation NLG _comma_ as in other fields of computational linguistics.
E06-1040.txt,7,Some NLG researchers are impressed by the success of the BLEU evaluation metric Papineni et al._comma_ 2002 in Machine Translation MT _comma_ which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas_comma_ algorithms_comma_ and data sets.
E06-1040.txt,8,BLEU and related metrics work by comparing the output of an MT system to a set of reference gold standard translations_comma_ and in principle this kind of evaluation could be done with NLG systems as well.
E06-1040.txt,9,Indeed NLG researchers are already starting to use BLEU Habash_comma_ 2004 Belz_comma_ 2005 in their evaluations_comma_ as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems.
E06-1040.txt,10,However_comma_ the use of such corpus based evaluation metrics is only sensible if they are known to becorrelated withtheresults ofhuman based evaluations.
E06-1040.txt,11,While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments Papineni et al._comma_ 2002 Doddington_comma_ 2002 _comma_ we are not aware of any studies that have shown that corpus based evaluation metrics of NLG systems are correlated with human judgments correlation studies have been made of individual components Bangalore et al._comma_ 2000 _comma_ but not of systems.
E06-1040.txt,12,In this paper we present an empirical study of how well various corpus based metrics agree with human judgments_comma_ when evaluating several NLG systems that generate sentences which describe changes in the wind for weather forecasts .
E06-1040.txt,13,These systems do not perform content determination theyarelimited tomicroplanning andrealisation _comma_ so our study does not address corpus based evaluation of content determination.
E06-1040.txt,14,2 Background 2.1 Evaluation of NLG systems NLG systems have traditionally been evaluated using human subjects Mellish and Dale_comma_ 1998 .
E06-1040.txt,15,NLG evaluations have tended to be of the intrinsic type Sparck Jones and Galliers_comma_ 1996 _comma_ involving subjects reading and rating texts usually subjects 313 are shown both NLG and human written texts_comma_ and the NLG system is evaluated by comparing the ratings of its texts and human texts.
E06-1040.txt,16,In some cases_comma_ subjects are shown texts generated by several NLG systems_comma_ including a baseline system which serves asanother point ofcomparison.
E06-1040.txt,17,Thismethodology was first used in NLG in the mid 1990s by Coch 1996 and Lester and Porter 1997 _comma_ and continues to be popular today.
E06-1040.txt,18,Other_comma_ extrinsic_comma_ types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance Young_comma_ 1999 _comma_ measuring how much experts postedit generated texts Sripada et al._comma_ 2005 _comma_ and measuring how quickly people read generated texts Williams and Reiter_comma_ 2005 .
E06-1040.txt,19,In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human written texts.
E06-1040.txt,20,As in other areas of NLP_comma_ the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human based evaluation_comma_ and also that it is repeatable.
E06-1040.txt,21,Corpus based evaluation was first used in NLG by Langkilde 1998 _comma_ who parsed texts from a corpus_comma_ fed the output of her parser to her NLG system_comma_ and then compared the generated texts to the original corpus texts.
E06-1040.txt,22,Similar evaluations have been used e.g. by Bangaloreetal. 2000 andMarciniak andStrube 2004 .
E06-1040.txt,23,Such corpus based evaluations have sometimes beencriticised inthe NLG community_comma_ forexample by Reiter and Sripada 2002 .
E06-1040.txt,24,Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task that texts can be very different from a corpus text but still effectively meet the system s communicative goal and that corpus texts areoften notofhigh enough quality to form a realistic test.
E06-1040.txt,25,2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts_comma_ and have shown that some of these metrics are highly correlated with human judgments.
E06-1040.txt,26,The BLEU metric Papineni et al._comma_ 2002 in MT has been particularly successful for example MT05_comma_ the 2005 NIST MT evaluation exercise_comma_ used BLEU 4 as the only method of evaluation.
E06-1040.txt,27,BLEU is a precision metric that assesses the quality of a translation in termsof the proportion of itsword ngrams n 4 has become standard that it shares with one or more high quality reference translations.
E06-1040.txt,28,BLEU scores range from 0 to 1_comma_ 1 being the highest which can only be achieved by a translation if all its substrings can be found in one of the reference texts hence a reference text will always score 1 .
E06-1040.txt,29,BLEU should be calculated on a large testsetwithseveral reference translations fourappears to be standard in MT .
E06-1040.txt,30,Properly calculated BLEU scores have been shown to correlate reliably with human judgments Papineni et al._comma_ 2002 .
E06-1040.txt,31,The NIST MT evaluation metric Doddington_comma_ 2002 is an adaptation of BLEU_comma_ but where BLEU givesequal weight toall n grams_comma_ NIST gives more importance to less frequent hence more informative n grams.
E06-1040.txt,32,BLEU s ability to detect subtle but important differences in translation quality has been questioned_comma_ some research showing NIST to be more sensitive Doddington_comma_ 2002 Riezler and Maxwell III_comma_ 2005 .
E06-1040.txt,33,The ROUGE metric Lin and Hovy_comma_ 2003 was conceived asdocument summarisation s answer to BLEU_comma_ but it does not appear to have met with the same degree of enthusiasm.
E06-1040.txt,34,There are several different ROUGE metrics.
E06-1040.txt,35,The simplest is ROUGE N_comma_ which computes the highest proportion in any reference summary of n grams that are matched by the system generated summary.
E06-1040.txt,36,A procedure is applied that averages the score across leave oneout subsets of the set of reference texts.
E06-1040.txt,37,ROUGEN is an almost straightforward n gram recall metric between two texts_comma_ and has several counterintuitive properties_comma_ including thatevenatextcomposed entirely of sentences from reference texts cannot score 1 unless there is only one reference text .
E06-1040.txt,38,There are several other variants of the ROUGE metric_comma_ and ROUGE 2_comma_ along with ROUGESU based on skip bigrams and unigrams _comma_ were among the official scores for the DUC 2005 summarisation task.
E06-1040.txt,39,2.3 SUMTIME The SUMTIME project Reiter et al._comma_ 2005 developed an NLG system which generated textual weather forecasts from numerical forecast data.
E06-1040.txt,40,The SUMTIME system generates specialist forecasts for offshore oil rigs.
E06-1040.txt,41,It has two modules a content determination module that determines the content of the weather forecast by analysing the numerical data using linear segmentation and 314 other data analysis techniques and a microplanning and realisation module which generates texts based on this content by choosing appropriate words_comma_ deciding on aggregation_comma_ enforcing the sublanguage grammar_comma_ and so forth.
E06-1040.txt,42,SUMTIME generates very high quality texts_comma_ in some cases forecast users believe SUMTIME texts are better than human written texts Reiter et al._comma_ 2005 .
E06-1040.txt,43,SUMTIME is a knowledge based NLG system.
E06-1040.txt,44,While its design was informed by corpus analysis Reiter et al._comma_ 2003 _comma_ the system is based on manually authored rules and code.
E06-1040.txt,45,As part of the project_comma_ the SUMTIME team created a corpus of 1045 forecasts from the commercial output of five different forecasters and the input data numerical predictions of wind_comma_ temperature_comma_ etc that the forecasters examined when they wrote the forecasts Sripada et al._comma_ 2003 .
E06-1040.txt,46,In other words_comma_ the SUMTIME corpus contains both the inputs numerical weather predictions and the outputs forecast texts ofthe forecast generation process.
E06-1040.txt,47,The SUMTIME team also derived a content representation called tuples from the corpus texts similar to that produced by SUMTIME s content determination module.
E06-1040.txt,48,The SUMTIME microplanner realiser can be driven by these tuples this mode combining human content determination with SUMTIME microplanning and realisation is called SUMTIME Hybrid.
E06-1040.txt,49,Table 1 includes an example of the tuples extracted from the corpus text row 1 _comma_ and a SUMTIME Hybrid text produced from the tuples row 5 .
E06-1040.txt,50,2.4 pCRU language generation Statistical NLG has focused on generate and select models a set of alternatives is generated and one is selected with a language model.
E06-1040.txt,51,This technique is computationally very expensive.
E06-1040.txt,52,Moreover_comma_ the only type of language model used in NLG are ngram models which have the additional disadvantage of a general preference for shorter realisations_comma_ which can be harmful in NLG Belz_comma_ 2005 .
E06-1040.txt,53,pCRU1 language generation Belz_comma_ 2006 is a language generation framework that was designed to facilitate statistical generation techniques that are more efficient and less biased.
E06-1040.txt,54,In pCRU generation_comma_ a base generator is encoded as a set of generation rules made up of relations with zero or more atomic arguments.
E06-1040.txt,55,The base generator 1Probabilistic Context free Representational Underspecification. is then trained on raw text corpora to provide a probability distribution over generation rules.
E06-1040.txt,56,The resulting PCRU generator can be run in several modes_comma_ including the following Random ignoring pCRU probabilities_comma_ randomly select generation rules.
E06-1040.txt,57,N gram ignoring pCRU probabilities_comma_ generate set of alternatives and select the most likely according to a given n gram language model.
E06-1040.txt,58,Greedy select the most likely among each set of candidate generation rules.
E06-1040.txt,59,Greedy roulette select rules with likelihood proportional to their pCRU probability.
E06-1040.txt,60,The greedy modes are deterministic and therefore considerably cheaper in computational terms than the equivalent n gram method Belz_comma_ 2005 .
E06-1040.txt,61,3 Experimental Procedure The main goal of our experiments was to determine how well a variety of automatic evaluation metrics correlated with human judgments of text quality in NLG.
E06-1040.txt,62,A secondary goal was to determine if there were types of NLG systems for which the correlation of automatic and human evaluation was particularly good or bad.
E06-1040.txt,63,Data We extracted from each forecast in the SUMTIME corpus the first description of wind at 10m height from every morning forecast the text shown in Table 1 is a typical example _comma_ which resulted in a set of about 500 wind forecasts.
E06-1040.txt,64,We excluded several forecasts for which we had no input data numerical weather predictions or an incomplete set of system outputs this left 465 texts_comma_ which we used in our evaluation.
E06-1040.txt,65,The inputs to the generators were tuples composed of an index_comma_ timestamp_comma_ wind direction_comma_ wind speed range_comma_ and gust speed range see examples at top of Table 1 .
E06-1040.txt,66,We randomly selected a subset of 21 forecast dates for use in human evaluations.
E06-1040.txt,67,For these 21 forecast dates_comma_ we also asked two meteorologists who had not contributed to the original SUMTIME corpus to write new forecasts texts we used these as reference texts for the automatic metrics.
E06-1040.txt,68,The forecasters created these texts byrewriting thecorpus texts_comma_ as this was a more natural task for them than writing texts based on tuples.
E06-1040.txt,69,500 wind descriptions may seem like a small corpus_comma_ but in fact provides very good coverage as 315 Input 0_comma_0600_comma_SSW_comma_16_comma_20_comma_ _comma_ _comma_ 1_comma_NOTIME_comma_SSE_comma_ _comma_ _comma_ _comma_ _comma_ 2_comma_0000_comma_VAR_comma_04_comma_08_comma_ _comma_ Corpus SSW 16 20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4 8 BY LATE EVENING Human1 SSW LY 16 20 GRADUALLY BACKING SSE LY THEN DECREASING VARIABLE 4 8 BY LATE EVENING Human2 SSW 16 20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4 8 BY LATE EVENING SumTime SSW 16 20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT pCRUgreedy SSW 16 20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4 8 BY LATE EVENINGroulette SSW 16 20 GRADUALLY BACKING SSE AND VARIABLE 4 82gram SSW 16 20 BACKING SSE VARIABLE 4 8 LATERrandom SSW 16 20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4 8 Table 1 Input tuples with corresponding forecasts in corpus_comma_ written by two experts and generated by all systems for 5 Oct 2000 . the domain language is extremely simple_comma_ involving only about 90 word forms not counting numbers and wind directions and a small handful of different syntactic structures.
E06-1040.txt,70,Systems and texts evaluated We evaluated four pCRU generators and the SUMTIME system_comma_ operating in Hybrid mode Section 2.3 for better comparability because the pCRU generators do not perform content determination.
E06-1040.txt,71,A base pCRU generator was created semiautomatically by running a chunker over the corpus_comma_ extracting generation rules and adding some higher level rules taking care of aggregation_comma_ elision etc. This base generator was then trained on 9 10 of the corpus the training data .
E06-1040.txt,72,5 different random divisions of the corpus into training and testing data were used i.e. all results were validated by 5 fold hold out cross validation .
E06-1040.txt,73,Additionally_comma_ a back off 2 gram model with GoodTuring discounting and nolexical classes wasbuilt from the same training data_comma_ using the SRILM toolkit Stolcke_comma_ 2002 .
E06-1040.txt,74,Forecasts were then generated for all corpus inputs_comma_ in all four generation modes Section 2.4 .
E06-1040.txt,75,Table1 shows anexample ofan input tothe systems_comma_ along with the three human texts Corpus_comma_ Human1_comma_ Human2 and the texts produced by all five NLG systems from this data.
E06-1040.txt,76,Automatic evaluations We used NIST2_comma_ BLEU3_comma_ and ROUGE4 to automatically evaluate the above systems and texts.
E06-1040.txt,77,We computed BLEU N for N 1..4 using BLEU 4 as our main BLEU score .
E06-1040.txt,78,We also computed NIST 5 and ROUGE 4.
E06-1040.txt,79,As a baseline we used string edit SE distance 2http cio.nist.gov esd emaildir lists mt list bin00000.bin 3ftp jaguar.ncsl.nist.gov mt resources mteval v11b.pl 4http www.isi.edu cyl ROUGE latest.html with substitution at cost 2_comma_ and deletion and insertion at cost 1_comma_ and normalised to range 0 to 1 perfect match .
E06-1040.txt,80,When multiple reference texts are used_comma_ the SE score for a generator forecast is the average of its scores against the reference texts the SE score for a set of generator forecasts is the average of scores for individual forecasts.
E06-1040.txt,81,Human evaluations We recruited 9 experts people with experience reading forecasts for offshore oil rigs and 21 non experts people with no such experience .
E06-1040.txt,82,Subjects did not have a background in NLP_comma_ and were native speakers of English.
E06-1040.txt,83,They were shown forecast texts from all the generators and from the corpus_comma_ and asked to score them on a scale of 0 to 5_comma_ for readability_comma_ clarity and general appropriateness.
E06-1040.txt,84,Experts were additionally shown the numerical weather data that the forecast text was based on.
E06-1040.txt,85,At the start_comma_ subjects were shown two practice examples.
E06-1040.txt,86,The experiments were carried out over the web.
E06-1040.txt,87,Subjects completed the experiment unsupervised_comma_ at a time and place of their choosing.
E06-1040.txt,88,Expert subjects were shown a randomly selected forecast for18ofthedates.
E06-1040.txt,89,Thenon experts were shown 21 forecast texts_comma_ in a repeated Latin squares non repeating column and row entries experimental design where each combination of date and system is assigned one evaluation.
E06-1040.txt,90,4 Results Table 2 shows evaluation scores for the five NLG systems and the corpus texts as assessed by experts_comma_ non experts_comma_ NIST 5_comma_ BLEU 4_comma_ ROUGE 4 and SE.
E06-1040.txt,91,Scores are averaged over the 18 forecasts that were used in the expert experiments for which we had scores by all metrics and humans in order to make results as directly comparable as possi316 System Experts Non experts NIST 5 BLEU 4 ROUGE 4 SE SUMTIME Hybrid 0.762 1 0.77 1 5.985 2 0.552 2 0.192 3 0.582 3 pCRU greedy 0.716 2 0.68 3 6.549 1 0.613 1 0.315 1 0.673 1 SUMTIME Corpus 0.644  0.736  8.262  0.877  0.569  0.835  pCRU roulette 0.622 3 0.714 2 5.833 3 0.478 4 0.156 4 0.571 4 pCRU 2gram 0.536 4 0.65 4 5.592 4 0.519 3 0.223 2 0.626 2 pCRU random 0.484 5 0.496 5 4.287 5 0.296 5 0.075 5 0.464 5 Table 2 Evaluation scores against 2 reference texts_comma_ for set of 18 forecasts used in expert evaluation.
E06-1040.txt,92,Experts Non experts NIST 5 BLEU 4 ROUGE 4 SE Experts 1 0.799 0.845 0.510 0.825 0.791 0.606 0.576 Non experts 0.845 0.496 1 0.609 0.836 0.812 0.534 0.627 NIST 5 0.825 0.822 0.836 0.83 1 0.991 0.973 0.884 0.911 BLEU 4 0.791 0.790 0.812 0.808 0.973 1 0.995 0.925 0.949 ROUGE 4 0.606 0.604 0.534 0.534 0.884 0.925 1 0.995 0.974 SE 0.576 0.568 0.627 0.614 0.911 0.949 0.974 1 0.984 Table 3 Pearson correlation coefficients between all scores for systems in Table 2. ble.
E06-1040.txt,93,Human scores are normalised to range 0 to 1.
E06-1040.txt,94,Systems are ranked in order of the scores given to them by experts.
E06-1040.txt,95,All ranks are shown in brackets behind the absolute scores.
E06-1040.txt,96,Both experts and non experts score SUMTIMEHybrid the highest_comma_ and pCRU 2gram and pCRUrandom the lowest.
E06-1040.txt,97,The experts have pCRUgreedy in second place_comma_ where the non experts have pCRU roulette.
E06-1040.txt,98,The experts rank the corpus forecasts fourth_comma_ the non experts second.
E06-1040.txt,99,We used approximate randomisation AR as our significance test_comma_ as recommended by Riezler and Maxwell III 2005 .
E06-1040.txt,100,Pair wise tests between results in Table 2 showed all but three differences to be significant with the likelihood of incorrectly rejecting the null hypothesis p 0.05 the standard threshold in NLP .
E06-1040.txt,101,The exceptions were the differences in NIST and SE scores for SUMTIMEHybrid pCRU roulette_comma_ and the difference in BLEU scores for SUMTIME Hybrid pCRU 2gram.
E06-1040.txt,102,Table 3 shows Pearson correlation coefficients PCC for the metrics and humans in Table 2.
E06-1040.txt,103,The strongest correlation with experts and nonexperts is achieved by NIST 5 0.82 and 0.83 _comma_ with ROUGE 4 and SE showing especially poor correlation.
E06-1040.txt,104,BLEU 4 correlates fairly well with the non experts but less with the experts.
E06-1040.txt,105,We computed another correlation statistic shown in brackets in Table 3 which measures how well scores by an arbitrary single human or runofametriccorrelate withtheaverage scores by a set of humans or runs of a metric.
E06-1040.txt,106,This is computed as the average PCC between the scores assigned by individual humans runs of a metric indexing the rows in Table 3 and the average scores assigned by a set of humans runs of a metric indexing the columns in Table 3 .
E06-1040.txt,107,For example_comma_ the PCC for non experts and experts is 0.845_comma_ but the average PCC between individual non experts and average expert judgment is only 0.496_comma_ implying that an arbitrary non expert is not very likely to correlate well with average expert judgments.
E06-1040.txt,108,Experts are better predictors for each other s judgments 0.799 than non experts 0.609 .
E06-1040.txt,109,Interestingly_comma_ it turns out that an arbitrary NIST 5 run is a better predictor 0.822 of average expert opinion than an arbitrary single expert 0.799 .
E06-1040.txt,110,The number of forecasts we were able to use in our human experiments was small_comma_ and to back up the results presented in Table 2 we report NIST 5_comma_ BLEU 4_comma_ ROUGE 4 and SE scores averaged across the five test sets from the pCRU validation runs_comma_ in Table 4.
E06-1040.txt,111,The picture is similar to results for the smaller data set the rankings assigned by all metrics are the same_comma_ except that NIST 5 and SE have swapped the ranks of SUMTIME Hybrid and pCRU roulette.
E06-1040.txt,112,Pair wise AR tests showed all differences to be significant with p 0.05_comma_ except for thedifferences in BLEU_comma_ NIST and ROUGE scores for SUMTIME Hybrid pCRUroulette_comma_ and the difference in BLEU scores for SUMTIME Hybrid pCRU 2gram.
E06-1040.txt,113,In both Tables 2 and 4_comma_ there are two major differences between the rankings assigned by hu317 System Experts NIST 5 BLEU 4 ROUGE 4 SE SUMTIME Hybrid 1 6.076 3 0.527 2 0.278 3 0.607 4 pCRU greedy 2 6.925 1 0.641 1 0.425 1 0.758 1 SUMTIME Corpus 9.317  1  1  1  pCRU roulette 3 6.175 2 0.497 4 0.242 4 0.679 3 pCRU 2gram 4 5.685 4 0.519 3 0.315 2 0.712 2 pCRU random 5 4.515 5 0.313 5 0.098 5 0.551 5 Table 4 Evaluation scores against the SUMTIME corpus_comma_ on 5 test sets from pCRU validation. man and automatic evaluation i Human evaluators prefer SUMTIME Hybrid over pCRU greedy_comma_ whereas all the automatic metrics have it the other way around and ii human evaluators score pCRU roulette highly second and third respectively _comma_ whereas theautomatic metricsscoreitvery low_comma_ second worst to random generation except for NIST which puts it second .
E06-1040.txt,114,There are two clear tendencies in scores going from left humans to right SE across Tables 2 and 4 SUMTIME Hybrid goes down in rank_comma_ and pCRU 2gram comes up.
E06-1040.txt,115,In addition to the BLEU 4 scores shown in the tables_comma_ wealso calculated BLEU 1_comma_ BLEU 2_comma_ BLEU3 scores.
E06-1040.txt,116,These give similar results_comma_ except that BLEU 1 and BLEU 2 rank pCRU roulette as highly as the human judges.
E06-1040.txt,117,It is striking how low the experts rank the corpus texts_comma_ and to what extent they disagree on their quality.
E06-1040.txt,118,This appears to indicate that corpus quality is not ideal.
E06-1040.txt,119,If an imperfect corpus is used as the gold standard for the automatic metrics_comma_ thenhighcorrelation withhumanjudgments isless likely_comma_ and this may explain the difference in human and automatic scores for SUMTIME Hybrid.
E06-1040.txt,120,5 Discussion If we assume that the human evaluation scores are the most valid_comma_ then the automatic metrics do not do a good job of comparing the knowledge based SUMTIME system to the statistical systems.
E06-1040.txt,121,One reason for this could be that there are cases where SUMTIME deliberately does not choose the most common option in the corpus_comma_ because its developers believed that it was not the best for readers.
E06-1040.txt,122,For example_comma_ in Table 1_comma_ the human forecasters and pCRU greedy use the phrase by late evening to refer to 0000_comma_ pCRU 2gram uses the phrase later_comma_ while SUMTIME Hybrid uses the phrase by midnight.
E06-1040.txt,123,The pCRU choices reflect frequency in the SUMTIME corpus later 837 instances and by late evening 327 instances are more common than by midnight 184 instances .
E06-1040.txt,124,However_comma_ forecast readers dislike this use of later because later is used to mean something else in a different type of forecast _comma_ and also dislike variants of by evening_comma_ because they are unsure how to interpret them Reiter et al._comma_ 2005 this is why SUMTIME uses by midnight.
E06-1040.txt,125,The SUMTIME system builders believe deviating from corpus frequency in such cases makes SUMTIME texts better from the reader s perspective_comma_ and it does appear to increase human ratings of the system but deviating from the corpus in such a way decreases the system s score under corpus similarity metrics.
E06-1040.txt,126,In other words_comma_ judging the output of an NLG system by comparing it to corpus texts by a method that rewards corpus similarity will penalise systems which do not base choice on highest frequency of occurrence in the corpus_comma_ even if this is motivated by careful studies of what is best for text readers.
E06-1040.txt,127,The MT community recognises that BLEU is not effective at evaluating texts which are as good as or better than the reference texts.
E06-1040.txt,128,This is not a problem for MT_comma_ because the output of current wide coverage MT systems is generally worse thanhuman translations.
E06-1040.txt,129,Butitisanissue for NLG_comma_ where systems are domain specific and can generate texts that are judged better by humans than human written texts as seen in Tables 4 and 2 .
E06-1040.txt,130,Although the automatic evaluation metrics generally replicated human judgments fairly well when comparing different statistical NLG systems_comma_ there was a discrepancy in the ranking of pCRUroulette ranked high by humans_comma_ low by several of the automatic metrics .
E06-1040.txt,131,pCRU roulette differs from the other statistical generators because it does not alwaystrytomakethemostcommonchoice maximise the likelihood of the corpus _comma_ instead it tries to vary choices.
E06-1040.txt,132,In particular_comma_ if there are several competing words and phrases with similar prob318 abilities_comma_ pCRU roulette will tend to use different words and phrases in different texts_comma_ whereas the other statistical generators will stick to those with the highest frequency.
E06-1040.txt,133,This behaviour is penalised by the automatic evaluation metrics_comma_ but the human evaluators do not seem to mind it.
E06-1040.txt,134,One of the classic rules of writing is to vary lexical and syntactic choices_comma_ in order to keep text interesting.
E06-1040.txt,135,However_comma_ this behaviour variation for variation s sake will always reduce a system s score under corpus similarity metrics_comma_ even if it enhances text quality from the perspective of readers.
E06-1040.txt,136,FosterandOberlander 2006 _comma_ intheirstudyof facial gestures_comma_ have also noted that humans do not mind and indeed in some cases prefer variation_comma_ whereas corpus based evaluations give higher ratings to systems which follow corpus frequency.
E06-1040.txt,137,Using more reference texts does counteract this tendency_comma_ but only up to a point no matter how many reference texts are used_comma_ there will still be one_comma_ or a small number of_comma_ most frequent variants_comma_ and using anything else will still worsen corpussimilarity scores.
E06-1040.txt,138,Canvassing expert opinion of text quality and averaging the results is also in a sense frequencybased_comma_ as results reflect what the majority of experts consider good variants.
E06-1040.txt,139,Expert opinions can vary considerably_comma_ as shown by the low correlation among experts in our study and as seen in corpus studies_comma_ e.g. Reiter et al._comma_ 2005 _comma_ and evaluations by a small number of experts may also be problematic_comma_ unless we have good reason to believe that expert opinions are highly correlated in the domain which was certainly not the case in our weather forecast domain .
E06-1040.txt,140,Ultimately_comma_ such disagreement between experts suggests that intrinsic judgments of the text quality whether by human or metric really should be be backed up by extrinsic judgments of the effectiveness of a text in helping real users perform tasks or otherwise achieving its communicative goal.
E06-1040.txt,141,6 Future Work We plan to further investigate the performance of automatic evaluation measures in NLG in the future i performing similar experiments to the one described here in other domains_comma_ and with more subjects and larger test sets ii investigating whether automatic corpus based techniques can evaluate content determination iii investigating how well both human ratings and corpus based measures correlate with extrinsic evaluations of the effectiveness of generated texts.
E06-1040.txt,142,Ultimately_comma_ we would like to move beyond critiques of existing corpus based metrics to proposing and validating new metrics which work well for NLG.
E06-1040.txt,143,7 Conclusions Corpus quality plays a significant role in automatic evaluation of NLG texts.
E06-1040.txt,144,Automatic metrics can be expected to correlate very highly with human judgments only if the reference texts used are of high quality_comma_ or rather_comma_ can be expected to be judged high quality by the human evaluators.
E06-1040.txt,145,This is especially important when the generated texts are of similar quality to human written texts.
E06-1040.txt,146,In MT_comma_ high quality texts vary less than generally in NLG_comma_ so BLEU scores against 4 reference translations from reputable sources as in MT 05 are a feasible evaluation regime.
E06-1040.txt,147,It seems likely thatforautomatic evaluation in NLG_comma_ alarger number of reference texts than four are needed.
E06-1040.txt,148,In our experiments_comma_ we have found NIST a more reliable evaluation metric than BLEU and in particular ROUGE which did not seem to offer any advantage over simple string edit distance.
E06-1040.txt,149,We also found individual experts judgments are not likely to correlate highly with average expert opinion_comma_ in fact less likely than NIST scores.
E06-1040.txt,150,This seems to imply that if expert evaluation can only be done with one or two experts_comma_ but a high quality reference corpus is available_comma_ then a NIST based evaluation may produce more accurate results than an expert based evaluation.
E06-1040.txt,151,It seems clear that for automatic corpus based evaluation to work well_comma_ we need high quality reference texts written by many different authors and large enough to give reasonable coverage of phenomena such as variation for variation s sake.
E06-1040.txt,152,Metrics that do not exclusively reward similarity with reference texts such as NIST are more likely to correlate well with human judges_comma_ but all of the existing metrics that we looked at still penalised generators that do not always choose the most frequent variant.
E06-1040.txt,153,The results we have reported here are for a relatively simple sublanguage and domain_comma_ and more empirical research needs to be done on how well different evaluation metrics and methodologies including different types of human evaluations correlate with each other.
E06-1040.txt,154,In order to establish reliable and trusted automatic cross system 319 evaluation methodologies_comma_ it seems likely that the NLG community will need to establish how to collect large amounts of high quality reference texts and develop new evaluation metrics specifically for NLG that correlate more reliably with human judgments of text quality and appropriateness.
E06-1040.txt,155,Ultimately_comma_ research should also look at developing new evaluation techniques that correlate reliably with the real world usefulness of generated texts.
E06-1040.txt,156,In the shorter term_comma_ we recommend that automatic evaluations of NLG systems be supported by conventional large scale human based evaluations.
E06-1040.txt,157,Acknowledgments Anja Belz s part of the research reported in this paper was supported under UK EPSRC Grant GR S24480 01.
E06-1040.txt,158,Many thanks to John Carroll_comma_ Roger Evans and the anonymous reviewers for very helpful comments. .
E06-1012.txt,1,paper presents results from the first statistical dependency parser for Turkish. Turkish is a free constituent order language with complex agglutinative inflectional and derivational morphology and presents interesting challenges for statistical parsing_comma_ as in general_comma_ dependency relations are between portions of words called inflectional groups.
E06-1012.txt,2,We have explored statistical models that use different representational units for parsing.
E06-1012.txt,3,We have used the Turkish Dependency Treebank to train and test our parser but have limited this initial exploration to that subset of the treebank sentences withonlyleft to right non crossing dependency links.
E06-1012.txt,4,Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing_comma_ and when contexts around the dependent are employed.
E06-1012.txt,6,The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks.
E06-1012.txt,7,With the emergence of the important role of word to word relations in parsing Charniak_comma_ 2000 Collins_comma_ 1996 _comma_ dependency grammars have gained acertain popularity e.g._comma_ Yamada and Matsumoto 2003 for English_comma_ Kudo and Matsumoto 2000 2002 _comma_ Sekine et al. 2000 for Japanese_comma_ Chung and Rim 2004 for Korean_comma_ Nivre et al. 2004 for Swedish_comma_ Nivre and Nilsson 2005 for Czech_comma_ among others.
E06-1012.txt,8,Dependency grammars represent the structure of the sentences by positing binary dependency relations between words.
E06-1012.txt,9,For instance_comma_ Figure 1 Figure 1 Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads.
E06-1012.txt,10,Parsers employing CFG backbones have been found to be less effective for free constituentorder languages where constituents can easily change their position in the sentence without modifying the general meaning of the sentence.
E06-1012.txt,11,Collins et al. 1999 applied the parser of Collins 1997 developed for English_comma_ to Czech_comma_ and found thatthe performance wassubstantially lower when compared to the results for English.
E06-1012.txt,12,2 Turkish Turkish is an agglutinative language where a sequence ofinflectional andderivational morphemes get affixed to a root Oflazer_comma_ 1994 .
E06-1012.txt,13,At the syntax level_comma_ the unmarked constituent order is SOV_comma_ but constituent order may vary freely as demanded by the discourse context.
E06-1012.txt,14,Essentially all constituent orders are possible_comma_ especially at the main sentence level_comma_ with very minimal formal constraints.
E06-1012.txt,15,In written text however_comma_ the unmarked order is dominant at both the main sentence and embedded clause level.
E06-1012.txt,16,Turkish morphotactics is quite complicated a given word form may involve multiple derivations and the number of word forms one can generate from a nominal or verbal root is theoretically infinite.
E06-1012.txt,17,Derivations in Turkish are very productive_comma_ and the syntactic relations that a word is in89 volved in as a dependent or head element_comma_ are determined by the inflectional properties of the one or more possibly intermediate derived forms.
E06-1012.txt,18,In this work_comma_ we assume that a Turkish word is represented as a sequence of inflectional groups IGs hereafter _comma_ separated by DBs_comma_ denoting derivation boundaries_comma_ in the following general form root IG1 DB IG2 DB  DB IGn.
E06-1012.txt,19,Here each IGi denotes relevant inflectional features including the part of speech for the root and for any of the derived forms.
E06-1012.txt,20,For instance_comma_ the derivedmodifiersa glamlas t rd g m zdaki1 would be represented as 2 sa glam strong Adj DB Verb Become DB Verb Caus Pos DB Noun PastPart A3sg P3sg Loc DB Adj Rel The five IGs in this are the feature sequences separated by the DB marker.
E06-1012.txt,21,The first IG shows the part of speech for the root which is its only inflectional feature.
E06-1012.txt,22,The second IG indicates a derivation into a verb whose semantics is to become the preceding adjective.
E06-1012.txt,23,The third IG indicates that a causative verb with positive polarity is derived from the previous verb.
E06-1012.txt,24,The fourth IG indicates the derivation of a nominal form_comma_ a past participle_comma_ with Noun as the part of speech and PastPart_comma_ as the minor part of speech_comma_ with some additional inflectional features.
E06-1012.txt,25,Finally_comma_ the fifth IG indicates a derivation into a relativizer adjective.
E06-1012.txt,26,A sentence would then be represented as a sequence of the IGs making up the words.
E06-1012.txt,27,When a word is considered as a sequence of IGs_comma_ linguistically_comma_ the last IG of a word determines its role as a dependent_comma_ so_comma_ syntactic relation links only emanate from the last IG of a dependent word_comma_ and land on one of the IGs of a head word on the right with minor exceptions _comma_ as exemplified in Figure 2.
E06-1012.txt,28,And again with minor exceptions_comma_ the dependency links between the IGs_comma_ when drawn above the IG sequence_comma_ do not cross.3 Figure 3 from Oflazer 2003 shows a dependency tree for a Turkish sentence laid on top of the words segmented along IG boundaries.
E06-1012.txt,29,With this view in mind_comma_ the dependency relations that are to be extracted by a parser should be relations between certain inflectional groups and 1Literally_comma_ the thing existing at the time we caused something to become strong .
E06-1012.txt,30,2The morphological features other than the obvious partof speech features are Become become verb_comma_ Caus causative verb_comma_ PastPart Derived past participle_comma_ P3sg 3sg possessive agreement_comma_ A3sg 3sg numberperson agreement_comma_ Loc Locative case_comma_ Pos Positive Polarity_comma_ Rel Relativizing Modifier.
E06-1012.txt,31,3Only 2.5 of the dependencies in the Turkish treebank Oflazer et al._comma_ 2003 actually cross another dependency link.
E06-1012.txt,32,Figure 2 Dependency Links and IGs not orthographic words.
E06-1012.txt,33,Since only the wordfinal inflectional groups have out going dependency links to a head_comma_ there will be IGs which do nothave anyoutgoing links e.g._comma_ thefirstIGofthe word b uy umesi in Figure 3 .
E06-1012.txt,34,We assume that such IGs are implicitly linked to the next IG_comma_ but neither represent nor extract such relationships with the parser_comma_ as it is the task of the morphological analyzer to extract those.
E06-1012.txt,35,Thus the parsing models that we will present in subsequent sections all aim to extract these surface relations between the relevant IGs_comma_ and in line with this_comma_ we will employ performance measures based on IGs and their relationships_comma_ and not on orthographic words.
E06-1012.txt,36,We use a model of sentence structure as depicted inFigure 4.
E06-1012.txt,37,Inthis figure_comma_ thetop part represents thewordsinasentence.
E06-1012.txt,38,Aftermorphological analysis and morphological disambiguation_comma_ each word is represented with the sequence of its inflectional groups_comma_ shown in the middle of the figure.
E06-1012.txt,39,The inflectional groups are then reindexed so that they are the units for the purposes of parsing.
E06-1012.txt,40,The inflectional groups marked with are those from which a dependency link will emanate from_comma_ to a head word to the right.
E06-1012.txt,41,Please note that the number of such marked inflectional groups is the same as the number of words in the sentence_comma_ and all of such IGs_comma_ except one corresponding to the distinguished head of the sentence which will not have any links _comma_ will have outgoing dependency links.
E06-1012.txt,42,Inthe rest of this paper_comma_ wefirst givea very brief overview a general model of statistical dependency parsing and then introduce three models for dependency parsing of Turkish.
E06-1012.txt,43,We then present our results for these models and for some additional experiments for the best performing model.
E06-1012.txt,44,We then close with a discussion on the results_comma_ analysis of the errors the parser makes_comma_ and conclusions.
E06-1012.txt,45,3 Parser Statistical dependency parsers first compute the probabilities of the unit to unit dependencies_comma_ and then find the most probable dependency tree T among the set of possible dependency trees.
E06-1012.txt,46,This 90 Bu eski ev de ki g l n b yle b y me si herkes i ok etkile di Mod Det Mod Subj Mod Subj Obj Mod bu Det eski Adj ev Noun A3sg Pnon Loc Adj g l Noun A3sg Pnon Gen b yle Adv b y Verb Noun Inf A3sg P3sg Nom herkes Pron A3pl Pnon Acc ok Adv etkile Verb Past A3sg This    old    house at that is   rose s   such      grow ing    everyone  very  impressed Such growing of the rose in this old house impressed everyone very much. s indicate morpheme boundaries.
E06-1012.txt,47,The rounded rectangles show the words while the inflectional groups within the words that have more than 1 IG are emphasized with the dashed rounded rectangles.
E06-1012.txt,48,The inflectional features of each inflectional group as produced by the morphological analyzer are listed below.
E06-1012.txt,49,Figure 3 Dependency links in an example Turkish sentence.
E06-1012.txt,50,w1 d4d4 d15d15 d35d35 IG1 d15d15 IG2 d15d15  IG g1 d15d15 IG1 IG2  IG g1 w2 d1d1 d15d15 d36d36 IG1 d15d15 IG2  IG g2 d15d15 IGg1 1  IG g1 g2 ... ... wn d4d4 d15d15 d35d35 IG1 IG2  IG gn d15d15  IG n i C8ik 1 gk Figure 4 Sentence Structure can be formulated as T argmax T P T_comma_S argmax T n 1productdisplay i 1 P dep wi_comma_wH i S 1 where in our case S is a sequence of units words_comma_ IGs and T_comma_ ranges over possible dependency trees consisting of left to right dependency links dep wi_comma_wH i with wH i denoting the head unit to which the dependent unit_comma_ wi_comma_ is linked to.
E06-1012.txt,51,The distance between the dependent units plays an important role in the computation of the dependency probabilities.
E06-1012.txt,52,Collins 1996 employs this distance i_comma_H i in the computation of word toword dependency probabilities P dep wi_comma_wH i S  2 P link wi_comma_wH i i_comma_H i suggesting that distance is a crucial variable when deciding whether two words are related_comma_ along with other features such as intervening punctuation.
E06-1012.txt,53,Chung and Rim 2004 propose a different method and introduce a new probability factor that takes into account the distance between the dependent and the head.
E06-1012.txt,54,The model in equation 3 takes into account the contexts that the dependent and head reside in and the distance between the head and the dependent.
E06-1012.txt,55,P dep wi_comma_wH i S  3 P link wi_comma_wH i  i H i  P wi links to some head H i i away i Here i represents the context around the dependent wi and H i _comma_ represents the context around the head word.
E06-1012.txt,56,P dep wi_comma_wH i S is the probability of the directed dependency relation between wi and wH i in the current sentence_comma_ while P link wi_comma_wH i i H i istheprobability of seeing asimilardependency withwi asthedependent_comma_ wH i as the head in a similar context in the training treebank.
E06-1012.txt,57,For the parsing models that will be described below_comma_ the relevant statistical parameters needed have been estimated from the Turkish treebank Oflazer et al._comma_ 2003 .
E06-1012.txt,58,Since this treebank is relatively smaller than the available treebanks for other languages e.g._comma_ Penn Treebank _comma_ we have 91 opted to model the bigram linkage probabilities in an unlexicalized manner that is_comma_ by just taking certain morphosyntactic properties into account _comma_ to avoid_comma_ to the extent possible_comma_ the data sparseness problem which is especially acute for Turkish.
E06-1012.txt,59,We have also been encouraged by the success of the unlexicalized parsers reported recently Klein and Manning_comma_ 2003 Chung and Rim_comma_ 2004 .
E06-1012.txt,60,For parsing_comma_ we use a version of the Backward Beam Search Algorithm Sekine et al._comma_ 2000 developed for Japanese dependency analysis adapted to our representations of the morphological structureofthewords.
E06-1012.txt,61,Thisalgorithm parses asentence by starting from the end and analyzing it towards thebeginning.
E06-1012.txt,62,Bymakingtheprojectivity assumption that the relations do not cross_comma_ this algorithm considerably facilitates the analysis.
E06-1012.txt,63,4 Details of the Parsing Models In this section we detail three models that we have experimented with for Turkish.
E06-1012.txt,64,All three models are unlexicalized and differ either in the units used for parsing or in the way contexts modeled.
E06-1012.txt,65,In all three models_comma_ we use the probability model in Equation 3.
E06-1012.txt,66,4.1 Simplifying IG Tags Our morphological analyzer produces a rather rich representation with a multitude of morphosyntactic and morphosemantic features encoded in the words.
E06-1012.txt,67,However_comma_ not all of these features are necessarily relevant in all the tasks that these analyses can be used in.
E06-1012.txt,68,Further_comma_ different subsets of these features may be relevant depending on the function of a word.
E06-1012.txt,69,In the models discussed below_comma_ we use a reduced representation of the IGs to unlexicalize the words 1.
E06-1012.txt,70,For nominal IGs_comma_4 we use two different tags depending on whether the IG is used as a dependent or as a head during different stages of parsing If the IG is used as a dependent_comma_ and_comma_ only word final IGs can be dependents _comma_ we represent that IG by a reduced tag consisting of only the case marker_comma_ as that essentially determines the syntactic function of that IG as a dependent_comma_ and only nominals have cases. If the IG is used as a head_comma_ then we use only part of speech and the possessive agreement marker in the reduced tag.
E06-1012.txt,71,4These are nouns_comma_ pronouns_comma_ and other derived forms that inflectwiththesameparadigm asnouns_comma_ includinginfinitives_comma_ past and future participles.
E06-1012.txt,73,For adjective IGs with present past future participles minor part of speech_comma_ we use the part of speech when they are used as dependents and the part of speech plus the the possessive agreement marker when used as a head.
E06-1012.txt,75,For other IGs_comma_ we reduce the IG to just the part of speech.
E06-1012.txt,76,Such a reduced representation also helps alleviate the sparse data problem as statistics from many word forms with only the relevant features are conflated.
E06-1012.txt,77,We modeled the second probability term on the right hand side of Equation 3 involving the distance between the dependent and the head unit in the following manner.
E06-1012.txt,78,First_comma_ we collected statistics over the treebank sentences_comma_ and noted that_comma_ if we count words as units_comma_ then 90 of dependency links link to a word that is less than 3 words away.
E06-1012.txt,79,Similarly_comma_ if we count distance in terms of IGs_comma_ then 90 of dependency links link to an IG that is less than 4 IGs away to the right.
E06-1012.txt,80,Thus we selected a parameter k 4 for Models 1 and 3 below_comma_ wheredistance ismeasured interms ofwords_comma_ and k 5 for Model 2where distance is measured in terms of IGs_comma_ as a threshold value at and beyond which a dependency is considered distant .
E06-1012.txt,81,During actual runs_comma_ P wi links to some head H i i away i was computed by interpolating P1 wi links to some head H i i away i estimated from the training corpus_comma_ and P2 wi links to some head H i i away the estimated probability for a length of a link when no contexts are considered_comma_ again estimated from the training corpus.
E06-1012.txt,82,When probabilities are estimated from the training set_comma_ all distances larger than k are assigned the same probability.
E06-1012.txt,83,If even after interpolation_comma_ the probability is 0_comma_ then a very small value is used.
E06-1012.txt,84,This is a modified version of the backed off smoothing used by Collins 1996 to alleviate sparse data problems.
E06-1012.txt,85,A similar interpolation isusedforthefirstcomponent ontheright hand side of Equation 3 by removing the head and the dependent contextual information all at once.
E06-1012.txt,86,4.2 Model 1 Unlexicalized Word based Model In this model_comma_ we represent each word by a reduced representation of its last IG when used as a dependent_comma_5 and by concatenation of the reduced 5Remember that other IGs in a word_comma_ if any_comma_ do not have any bearing on how this word links to its head word.
E06-1012.txt,87,92 representation of its IGs when used as a head.
E06-1012.txt,88,Since a word can be both a dependent and a head word_comma_ the reduced representation to be used is dynamically determined during parsing.
E06-1012.txt,89,Parsing then proceeds with words as units represented in this manner.
E06-1012.txt,90,Once the parser links these units_comma_ we remap these links back to IGs to recover the actual IG to IG dependencies.
E06-1012.txt,91,We already know that any outgoing link from a dependent will emanate from the last IG of that word.
E06-1012.txt,92,For the head word_comma_ we assume that the link lands on the first IG of that word.6 For the contexts_comma_ we use the following scheme.
E06-1012.txt,93,A contextual element on the left is treated as a dependent and is modeled with its last IG_comma_ while a contextual element on the right is represented as if it were a head using all its IGs.
E06-1012.txt,94,We ignore any overlaps between contexts in this and the subsequent models.
E06-1012.txt,95,In Figure 5 we show in a table the sample sentence in Figure 3_comma_ the morphological analysis for each word and the reduced tags for representing the units for the three models.
E06-1012.txt,96,For each model_comma_ we list the tags when the unit is used as a head and when it is used as a dependent.
E06-1012.txt,97,For model 1_comma_ we use the tags in rows 3 and 4.
E06-1012.txt,98,4.3 Model 2 IG based Model In this model_comma_ we represent each IG with reduced representations in the manner above_comma_ but do not concatenate them into a representation for the word.
E06-1012.txt,99,So our units for parsing are IGs.
E06-1012.txt,100,Theparser directly establishes IG to IGlinks from word final IGs to some IG to the right.
E06-1012.txt,101,The contexts that are used in this model are the IGs to the left starting with the last IG of the preceding word and the right of the dependent and the head IG.
E06-1012.txt,102,The units and the tags we use in this model are in rows 5 and 6 in the table in Figure 5.
E06-1012.txt,103,Note that the empty cells in row 4 corresponds to IGs which can not be syntactic dependents as they are not word final.
E06-1012.txt,104,4.4 Model 3 IG based Model with Word final IG Contexts This model is almost exactly like Model 2 above.
E06-1012.txt,105,The two differences are that i for contexts we only use just the word final IGs to the left and the right ignoring any non word final IGs in between except for the case that the context and the head overlap_comma_ where we use the tag of the head IG in6This choice is based on the observation that in the treebank_comma_ 85.6 of the dependency links land on the first and possibly the only IG of the head word_comma_ while 14.4 of the dependency links land on an IG other than the first one. stead of the final IG and ii the distance function is computed in terms of words.
E06-1012.txt,106,The reason this model is used is that it is the word final IGs that determine the syntactic roles of the dependents.
E06-1012.txt,107,5 Results Since in this study we are limited to parsing sentences with only left to right dependency links7 which do not cross each other_comma_ we eliminated the sentences having such dependencies even if they contain a single one and used a subset of 3398 such sentences in the Turkish Treebank.
E06-1012.txt,108,The gold standard part of speech tags are used in the experiments.
E06-1012.txt,109,The sentences in the corpus ranged between 2 words to 40 words with an average of about 8 words 8 90 of the sentences had less than or equal to 15 words.
E06-1012.txt,110,In terms of IGs_comma_ the sentences comprised 2 to 55 IGs with an average of 10 IGs per sentence 90 of the sentences had less than or equal to 15 IGs.
E06-1012.txt,111,We partitioned this set into training and test sets in 10 different ways to obtain results with 10 fold cross validation.
E06-1012.txt,112,We implemented three baseline parsers 1.
E06-1012.txt,113,The first baseline parser links a word final IG to the first IG of the next word on the right.
E06-1012.txt,115,The second baseline parser links a word final IG to the last IG of the next word on the right.9 3.
E06-1012.txt,116,The third baseline parser is a deterministic rule based parser that links each word final IGtoanIGontheright basedontheapproach of Nivre 2003 .
E06-1012.txt,117,The parser uses 23 unlexicalized linking rules and a heuristic that links any non punctuation word not linked by the parser to the last IG of the last word as a dependent.
E06-1012.txt,118,Table 1 shows the results from our experiments with these baseline parsers and parsers that are based on the three models above.
E06-1012.txt,119,The three modelshave been experimented withdifferent contexts around both the dependent unit and the head.
E06-1012.txt,120,In each row_comma_ columns 3 and 4 show the percentage of IG IG dependency relations correctly recovered for all tokens_comma_ and just words excluding punctuation from the statistics_comma_ while columns 5 and 6 show the percentage of test sentences for which all dependency relations extracted agree with the 7In 95 of the treebank dependencies_comma_ the head is the right of the dependent.
E06-1012.txt,121,8This is quite normal the equivalents of function words in English are embedded as morphemes not IGs into these words.
E06-1012.txt,122,9Note that for head words with a single IG_comma_ the first two baselines behave the same.
E06-1012.txt,123,93 Figure 5 Tags used in the parsing models relations in the treebank.
E06-1012.txt,124,Each entry presents the average and the standard error of the results on the test set_comma_ over the 10 iterations of the 10 fold crossvalidation.
E06-1012.txt,125,Our main goal is to improve the percentage of correctly determined IG to IG dependency relations_comma_ shown in the fourth column of the table.
E06-1012.txt,126,The best results in these experiments are obtained with Model 3 using 1 unit on both sides of the dependent.
E06-1012.txt,127,Although it is slightly better than Model 2 with the same context size_comma_ the difference between the means 0.4 0.2 for each 10 iterations is statistically significant.
E06-1012.txt,128,Since wehave been using unlexicalized models_comma_ we wanted to test out whether a smaller training corpus would have a major impact for our current models.
E06-1012.txt,129,Table 2 shows results for Model 3 with no context and 1 unit on each side of the dependent_comma_ obtained by using only a 1500 sentence subset of the original treebank_comma_ again using 10 fold cross validation.
E06-1012.txt,130,Remarkably the reduction in training set size has a very small impact on the results.
E06-1012.txt,131,Although all along_comma_ we have suggested that determining word to word dependency relationships is not the right approach for evaluating parser performance for Turkish_comma_ we have nevertheless performed word to word correctness evaluation so thatcomparison withother wordbased approaches can be made.
E06-1012.txt,132,In this evaluation_comma_ we assume that a dependency link is correct if we correctly determine the head word but not necessarily the correct IG .
E06-1012.txt,133,Table 3 shows the word based results for the best cases of the models in Table 1.
E06-1012.txt,134,We have also tested our parser with a pure word model where both the dependent and the head are represented by the concatenation of their IGs_comma_ that is_comma_ by their full morphological analysis except the root.
E06-1012.txt,135,Theresult forthiscase isgiven inthe lastrow of Table 3.
E06-1012.txt,136,This result is even lower than the rulebased baseline.10 Forthis model_comma_ if weconnect the 10Also lower than Model 1 with no context 79.1 1.1 dependent to the first IG of the head as we did in Model 1_comma_ the IG IG accuracy excluding punctuations becomes 69.9 3.1_comma_ which is also lower than baseline 3 70.5 .
E06-1012.txt,137,6 Discussions Our results indicate that all of our models perform better than the 3 baseline parsers_comma_ even when no contexts around the dependent and head units are used.
E06-1012.txt,138,We get our best results with Model 3_comma_ where IGs are used as units for parsing and contexts are comprised ofwordfinalIGs.
E06-1012.txt,139,Thehighest accuracy in terms of percent of correctly extracted IG to IG relations excluding punctuations 73.5 was obtained when one word is used as context on both sides of the the dependent.11 We also noted that using a smaller treebank to train our models did not result in a significant reduction in our accuracy indicating that the unlexicalized models are quite effective_comma_ but this also may hint that a larger treebank with unlexicalized modeling may not be useful for improving link accuracy.
E06-1012.txt,140,A detailed look at the results from the best performing model shown in in Table 4_comma_12 indicates that_comma_ accuracy decrases with the increasing sentence length.
E06-1012.txt,141,For longer sentences_comma_ we should employmoresophisticated modelspossibly including lexicalization.
E06-1012.txt,142,A further analysis of the actual errors made by the best performing model indicates almost 40 of the errors are attachment problems the dependent IGs_comma_ especially verbal adjuncts and arguments_comma_ link to the wrong IG but otherwise with the samemorphological features asthecorrect one except for the root word.
E06-1012.txt,143,This indicates wemay have to model distance in a more sophisticated way and 11We should also note that early experiments using different sets of morphological features that we intuitively thought should be useful_comma_ gave rather low accuracy results.
E06-1012.txt,144,12These results are significantly higher than the best baseline rule based for all the sentence length categories.
E06-1012.txt,145,94 Percentage of IG IG Percentage of Sentences Relations Correct With ALL Relations Correct Parsing Model Context Words Punc Words only Words Punc Words only Baseline 1 NA 59.9 0.3 63.9 0.7 21.4 0.6 24.0 0.7 Baseline 2 NA 58.3 0.2 62.2 0.8 20.1 0.0 22.6 0.6 Baseline 3 NA 69.6 0.2 70.5 0.8 31.7 0.7 36.6 0.8 Model 1 None 69.8 0.4 71.0 1.3 32.7 0.6 36.2 0.7 k 4 Dl 1 69.9 0.4 71.1 1.2 32.9 0.5 36.4 0.6 Dl 1 Dr 1 71.3 0.4 72.5 1.2 33.4 0.8 36.7 0.8 Hl 1 Hr 1 64.7 0.4 65.5 1.3 25.4 0.6 28.7 0.8 Both 71.4 0.4 72.6 1.1 34.2 0.7 37.2 0.6 Model 2 None 70.5 0.3 71.9 1.0 32.1 0.9 36.3 0.9 k 5 Dl 1 71.3 0.3 72.7 0.9 33.8 0.8 37.4 0.7 Dl 1 Dr 1 71.9 0.3 73.1 0.9 34.8 0.7 38.0 0.7 Hl 1 Hr 1 57.4 0.3 57.6 0.7 23.5 0.6 25.8 0.6 Both 70.9 0.3 72.2 0.9 34.2 0.8 37.2 0.9 Model 3 None 71.2 0.3 72.6 0.9 34.4 0.7 38.1 0.7 k 4 Dl 1 71.2 0.4 72.6 1.1 34.5 0.7 38.3 0.6 Dl 1 Dr 1 72.3 0.3 73.5 1.0 35.5 0.9 38.7 0.9 Hl 1 Hr 1 55.2 0.3 55.1 0.7 22.0 0.6 24.1 0.6 Both 71.1 0.3 72.4 0.9 35.5 0.8 38.4 0.9 The Context column entries show the context around the dependent and the head unit.
E06-1012.txt,146,Dl 1 and Dr 1 indicate the use of 1 unit left and the right of the dependent respectively.
E06-1012.txt,147,Hl 1 and Hr 1 indicate the use of 1 unit left and the right of the head respectively.
E06-1012.txt,148,Both indicates both head and the dependent have 1 unit of context on both sides.
E06-1012.txt,149,Table 1 Results from parsing with the baseline parsers and statistical parsers based on Models 1 3.
E06-1012.txt,150,Percentage of IG IG Percentage of Sentences Relations Correct With ALL Relations Correct Parsing Model Context Words Punc Words only Words Punc Words only Model 3 None 71.0 0.6 72.2 1.5 34.4 1.0 38.1 1.1 k 4_comma_ 1500 Sentences Dl 1 Dr 1 71.6 0.4 72.6 1.1 35.1 1.3 38.4 1.5 Table 2 Results from using a smaller training corpus.
E06-1012.txt,151,Percentage of Word Word Relations Correct Parsing Model Context Words only Baseline 1 NA 72.1 0.5 Baseline 2 NA 72.1 0.5 Baseline 3 NA 80.3 0.7 Model 1 k 4 Both 80.8 0.9 Model 2 k 5 Dl 1 Dr 1 81.0 0.7 Model 3 k 4 Dl 1 Dr 1 81.2 1.0 Pure Word Model None 77.7 3.5 Table 3 Results from word to word correctness evaluation.
E06-1012.txt,152,Sentence Length l IGs Accuracy 1 l 10 80.2 0.5 10 l 20 70.1 0.4 20 l 30 64.6 1.0 30 l 62.7 1.3 Table 4 Accuracy over different length sentences.
E06-1012.txt,153,95 perhaps use alimited lexicalization such asincluding limited non morphological information e.g._comma_ verb valency into the tags.
E06-1012.txt,154,7 Conclusions We have presented our results from statistical dependency parsing of Turkish with statistical models trained from the sentences in the Turkish treebank.
E06-1012.txt,155,The dependency relations are between sub lexical units that we call inflectional groups IGs and the parser recovers dependency relations between these IGs.
E06-1012.txt,156,Due to the modest size of the treebank available to us_comma_ we have used unlexicalized statistical models_comma_ representing IGs by reduced representations of their morphological properties.
E06-1012.txt,157,For the purposes of this work we have limited ourselves to sentences with all left to right dependency links that do not cross each other.
E06-1012.txt,158,Weget our best results 73.5 IG to IG link accuracy using a model where IGs are used as units for parsing and we use as contexts_comma_ word final IGs of the words before and after the dependent.
E06-1012.txt,159,Future work involves a more detailed understanding of the nature of the errors and see how limited lexicalization can help_comma_ as well as investigation of more sophisticated models such as SVM or memory based techniques for correctly identifying dependencies.
E06-1012.txt,160,8 Acknowledgement This research was supported in part by a research grant from TUBITAK The Scientific and Technical Research Council ofTurkey andfrom Istanbul Technical University. .
E06-2030.txt,1,report work1 in progress on adding affect detection to an existing program for virtual dramatic improvisation_comma_ monitored by a human director. To partially automate the directors functions_comma_ we have partially implemented the detection of emotions_comma_ etc. in users text input_comma_ by means of pattern matching_comma_ robust parsing and some semantic analysis.
E06-2030.txt,2,The work also involves basic research into how affect is conveyed by metaphor.
E06-2030.txt,4,Improvised drama and role play are widely used in education_comma_ counselling and conflict resolution.
E06-2030.txt,5,Researchers have explored frameworks for edrama_comma_ in which virtual characters avatars interact under the control of human actors.
E06-2030.txt,6,The springboard for our research is an existing system edrama created by Hi8us Midlands Ltd_comma_ used in schools for creative writing and teaching in various subjects.
E06-2030.txt,7,The experience suggests that e drama helps students lose their usual inhibitions_comma_ because of anonymity etc. In edrama_comma_ characters are completely human controlled_comma_ their speeches textual in speech bubbles_comma_ and their visual forms cartoon figures.
E06-2030.txt,8,The actors users are given a loose scenario within which to improvise_comma_ but are at liberty to be creative.
E06-2030.txt,9,There is also a human director_comma_ who constantly monitors the unfolding drama and can intervene by_comma_ 1 This work is supported by grant RES 328 25 0009 from the ESRC under the ESRC EPSRC DTI PACCIT programme.
E06-2030.txt,10,We are grateful to Hi8us Midlands Ltd_comma_ Maverick Television Ltd_comma_ BT_comma_ and our colleagues W.H.
E06-2030.txt,13,Lee and Z.
E06-2030.txt,15,The work is also partially supported by EPSRC grant EP C538943 1. for example_comma_ sending messages to actors_comma_ or by introducing and controlling a minor bit part character to interact with the main characters.
E06-2030.txt,16,But this places a heavy burden on directors_comma_ especially if they are_comma_ for example_comma_ teachers and unpracticed in the directorial role.
E06-2030.txt,17,One research aim is thus partially to automate the directorial functions_comma_ which importantly involve affect detection.
E06-2030.txt,18,For instance_comma_ a director may intervene when emotions expressed or discussed by characters are not as expected.
E06-2030.txt,19,Hence we have developed an affect detection module.
E06-2030.txt,20,It has not yet actually been used for direction_comma_ but instead to control a simple automated bit part actor_comma_ EmEliza.
E06-2030.txt,21,The module identifies affect in characters speeches_comma_ and makes appropriate responses to help stimulate the improvisation.
E06-2030.txt,22,Within affect we include basic and complex emotions such as anger and embarrassment meta emotions such as desiring to overcome anxiety moods such as hostility and value judgments of goodness_comma_ etc. .
E06-2030.txt,23,Although merely detecting affect is limited compared to extracting full meaning_comma_ this is often enough for stimulating improvisation.
E06-2030.txt,24,Much research has been done on creating affective virtual characters in interactive systems.
E06-2030.txt,25,Emotion theories_comma_ particularly that of Ortony et al. 1988 OCC in the following _comma_ have been used widely.
E06-2030.txt,26,Prendinger Ishizuka 2001 used OCC to reason about emotions.
E06-2030.txt,27,Mehdi et al. 2004 used OCC to generate emotional behaviour.
E06-2030.txt,28,Gratch and Marsella s 2004 model reasons about emotions.
E06-2030.txt,29,However_comma_ few systems are aimed at detecting affect as broadly as we do and in open ended utterances.
E06-2030.txt,30,Although Fa ade Mateas_comma_ 2002 included processing of open ended utterances_comma_ the broad detection of emotions_comma_ rudeness and value judgements is not covered.
E06-2030.txt,31,Zhe Boucouvalas 2002 demonstrated emotion extraction using a tagger and a chunker to help detect the speaker s own emotions.
E06-2030.txt,32,But it focuses only on emotional adjectives_comma_ considers only first person emotions and 203 neglects deep issues such as figurative expression.
E06-2030.txt,33,Our work is distinctive in several respects.
E06-2030.txt,34,Our interest is not just in a the positive first person case the affective states that a virtual character X implies that it has or had or will have_comma_ etc. _comma_ but also in b affect that X implies it lacks_comma_ c affect that X implies that other characters have or lack_comma_ and d questions_comma_ commands_comma_ injunctions_comma_ etc. concerning affect.
E06-2030.txt,35,We aim also for the software to cope partially with the important case of metaphorical conveyance of affect Fussell Moss_comma_ 1998 K vecses_comma_ 1998 .
E06-2030.txt,36,Our project does not involve using or developing deep_comma_ scientific models of how emotional states_comma_ etc._comma_ function in cognition.
E06-2030.txt,37,Instead_comma_ the deep questions investigated are on linguistic matters such as the metaphorical expression of affect.
E06-2030.txt,38,Also_comma_ in studying how people understand and talk about affect_comma_ what is of prime importance is their common sense views of how affect works_comma_ irrespective of scientific reality.
E06-2030.txt,39,Metaphor is strongly involved in such views.
E06-2030.txt,40,2 A Preliminary Approach Various characterizations of emotion are used in emotion theories.
E06-2030.txt,41,The OCC model uses emotion labels and intensity_comma_ while Watson and Tellegen 1985 use positive and negative affects as the major dimensions.
E06-2030.txt,42,Currently_comma_ we use an evaluation dimension positive and negative _comma_ affect labels and intensity.
E06-2030.txt,43,Affect labels with intensity are used when strong text clues signalling affect are detected_comma_ while the evaluation dimension with intensity is used when only weak text clues are detected.
E06-2030.txt,44,2.1 Pre processing Modules The language in the speeches created in e drama sessions_comma_ especially by excited children_comma_ severely challenges existing language analysis tools if accurate semantic information is sought.
E06-2030.txt,45,The language includes misspellings_comma_ ungrammaticality_comma_ abbreviations such as in texting _comma_ slang_comma_ use of upper case and special punctuation such as repeated exclamation marks for affective emphasis_comma_ repetition of letters or words for emphasis_comma_ and open ended onomatopoeic elements such as grrrr .
E06-2030.txt,46,The genre is similar to Internet chat.
E06-2030.txt,47,To deal with the misspellings_comma_ abbreviations and onomatopoeia_comma_ several pre processing modules are used before the detection of affect starts using pattern matching_comma_ syntactic processing by means of the Rasp parser Briscoe Carroll_comma_ 2002 _comma_ and subsequent semantic processing.
E06-2030.txt,48,A lookup table has been used to deal with abbreviations e.g. im I am _comma_ c u see you and l8r later .
E06-2030.txt,49,It includes abbreviations used in Internet chat rooms and others found in an anlysis of previous edrama sessions.
E06-2030.txt,50,We handle ambiguity e.g._comma_ 2 to_comma_ too_comma_ two in I m 2 hungry 2 walk by considering the POS tags of immediately surrounding words.
E06-2030.txt,51,Such simple processing inevitably leads to errors_comma_ but in evaluations using examples in a corpus of 21695 words derived from previous transcripts we have obtained 85.7 accuracy_comma_ which is currently adequate.
E06-2030.txt,52,The iconic use of word length corresponding roughly to imagined sound length as found both in ordinary words with repeated letters e.g. seeeee and in onomatopoeia and interjections_comma_ e.g. wheee _comma_ grr _comma_ grrrrrr _comma_ agh _comma_ aaaggghhh normally implies strong affective states.
E06-2030.txt,53,We have a small dictionary containing base forms of some special words e.g. grr and some ordinary words that often have letters repeated in e drama.
E06-2030.txt,54,Then the Metaphone spelling correction algorithm_comma_ which is based on pronunciation_comma_ works with the dictionary to locate the base forms of words with letter repetitions.
E06-2030.txt,55,Finally_comma_ the Levenshtein distance algorithm with a contemporary English dictionary deals with misspelling.
E06-2030.txt,56,2.2 Affect Detection In the first stage after the pre processing_comma_ our affect detection is based on textual patternmatching rules that look for simple grammatical patterns or phrasal templates.
E06-2030.txt,57,Thus keywords_comma_ phrases and partial sentence structures are extracted.
E06-2030.txt,58,The Jess rule based Java framework is used to implement the pattern template matching rules.
E06-2030.txt,59,This method has the robustness to deal with ungrammatical and fragmented sentences and varied positioning of sought after phraseology_comma_ but lacks other types of generality and can be fooled by suitable syntactic embedding.
E06-2030.txt,60,For example_comma_ if the input is I doubt she s really angry _comma_ rules looking for anger in a simple way will output incorrect results.
E06-2030.txt,61,The transcripts analysed to inspire our initial knowledge base and pattern matching rules had independently been produced earlier from edrama improvisations based on a school bullying scenario.
E06-2030.txt,62,We have also worked on another_comma_ distinctly different scenario concerning a serious disease_comma_ based on a TV programme produced by Maverick Television Ltd.
E06-2030.txt,63,The rule sets created for one scenario have a useful degree of applicability to another_comma_ although some changes in the specific 204 knowledge database will be needed.
E06-2030.txt,64,As a simple example of our pattern matching_comma_ when the bully character says Lisa_comma_ you Pizza Face
E06-2030.txt,65,You smell _comma_ the module detects that he is insulting Lisa.
E06-2030.txt,66,Patterns such as you smell have been used for rule implementation.
E06-2030.txt,67,The rules work out the character s emotions_comma_ evaluation dimension negative or positive _comma_ politeness rude or polite and what response EmEliza might make.
E06-2030.txt,68,Although the patterns detected are based on English_comma_ we would expect that some of the rules would require little modification to apply to other languages.
E06-2030.txt,69,Multiple exclamation marks and capitalisation of whole words are often used for emphasis in edrama.
E06-2030.txt,70,If exclamation marks or capitalisation are detected_comma_ then emotion intensity is deemed to be comparatively high and emotion is suggested even without other clues .
E06-2030.txt,71,A reasonably good indicator that an inner state is being described is the use of I see also Craggs and Wood 2004 _comma_ especially in combination with the present or future tense.
E06-2030.txt,72,In the school bullying scenario_comma_ when I is followed by a future tense verb_comma_ a threat is normally being expressed and the utterance is often the shortened version of an implied conditional_comma_ e.g._comma_ I ll scream if you stay here . When I is followed by a present tense verb_comma_ other emotional states tend to be expressed_comma_ as in I want my mum and I hate you .
E06-2030.txt,73,Another useful signal is the imperative mood_comma_ especially when used without softeners such as please strong emotions and or rude attitudes are often being expressed.
E06-2030.txt,74,There are common imperative phrases we deal with explicitly_comma_ such as shut up and mind your own business .
E06-2030.txt,75,But_comma_ to go beyond the limitations of the pattern matching we have done_comma_ we have also used the Rasp parser and semantic information in the form of the semantic profiles for the 1_comma_000 most frequently used English words Heise_comma_ 1965 .
E06-2030.txt,76,Although Rasp recognizes many simple imperatives directly_comma_ it can parse some imperatives as declaratives or questions.
E06-2030.txt,77,Therefore_comma_ further analysis is applied to Rasp s syntactic output.
E06-2030.txt,78,For example_comma_ if the subject of an input sentence is you followed by certain special verbs or verb phrases e.g. shut _comma_ calm _comma_ get lost _comma_ go away _comma_ etc _comma_ and Rasp parses a declarative_comma_ then it will be changed to imperative.
E06-2030.txt,79,If the softener please is followed by a base forms of the verb_comma_ the inputs are also deemed to be imperatives.
E06-2030.txt,80,If a singular proper noun or you is followed by a base form of the verb_comma_ the sentence is deemed to be imperative e.g. Dave bring me the menu .
E06-2030.txt,81,When you or a singular proper noun is followed by a verb whose base form equals its past tense form_comma_ ambiguity arises e.g. Lisa hit me .
E06-2030.txt,82,For one special case of this_comma_ if the direct object is me _comma_ we exploit the evaluation value of the verb from Heise s 1965 semantic profiles.
E06-2030.txt,83,Heise lists values of evaluation goodness _comma_ activation_comma_ potency_comma_ distance from neutrality_comma_ etc. for each word covered.
E06-2030.txt,84,If the evaluation value for the verb is negative_comma_ then the sentence is probably not imperative but a declarative expressing a complaint e.g Mayid hurt me .
E06-2030.txt,85,If it has a positive value_comma_ then other factors suggesting imperative are checked in this sentence_comma_ such as exclamation marks and capitalizations.
E06-2030.txt,86,Previous conversation is checked to see if there is any recent question sentence toward the speaker.
E06-2030.txt,87,If so_comma_ then the sentence is taken to be declarative.
E06-2030.txt,88,There is another type of sentence don t you base form of verb _comma_ which is often a negative version of an imperative with a you subject e.g. Don t you call me a dog .
E06-2030.txt,89,Normally Rasp regards such strings as questions.
E06-2030.txt,90,Further analysis has also been implemented for such sentence structure_comma_ which implies negative affective state_comma_ to change the sentence type to imperative.
E06-2030.txt,91,Aside from imperatives_comma_ we have also implemented simple types of semantic extraction of affect using affect dictionaries and WordNet.
E06-2030.txt,92,3 Metaphorical Expression of Affect The explicit metaphorical description of emotional states is common and has been extensively studied Fussell Moss_comma_ 1998 .
E06-2030.txt,93,Examples are He nearly exploded _comma_ and Joy ran through me. Also_comma_ affect is often conveyed implicitly via metaphor_comma_ as in His room is a cess pit _comma_ where affect associated with a source item cess pit is carried over to the corresponding target item.
E06-2030.txt,94,Physical size is often metaphorically used to emphasize evaluations_comma_ as in you are a big bully _comma_ you re a big idiot _comma_ and you re just a little bully _comma_ although the bigness may be literal as well.
E06-2030.txt,95,Big bully expresses strong disapproval Sharoff_comma_ 2005 and little bully can express contempt_comma_ although little can also convey sympathy.
E06-2030.txt,96,Such examples are not only practically important but also theoretically challenging.
E06-2030.txt,97,We have also encountered quite creative use of metaphor in e drama.
E06-2030.txt,98,For example_comma_ in a school bullying improvisation that occurred_comma_ Mayid had already insulted Lisa by calling her a pizza _comma_ developing a previous pizza face insult.
E06-2030.txt,99,Mayid then said I ll knock your topping off_comma_ Lisa a theoretically intriguing spontane205 ous creative elaboration of the pizza metaphor.
E06-2030.txt,100,Our developing approach to metaphor handling in the affect detection module is partly to look for stock metaphorical phraseology and straightforward variants of it_comma_ and partly to use a simple version of the more open ended_comma_ reasoning based techniques taken from the ATT Meta project Barnden et al._comma_ 2002 2003 2004 .
E06-2030.txt,101,ATT Meta includes a general purpose reasoning engine_comma_ and can potentially be used to reason about emotion in relation to other factors in a situation.
E06-2030.txt,102,In turn_comma_ the realities of metaphor usage in e drama sessions are contributing to our basic research on metaphor processing.
E06-2030.txt,103,4 Conclusion We have implemented a limited degree of affectdetection in an automated actor by means of pattern matching_comma_ robust parsing and some semantic analysis.
E06-2030.txt,104,Although there is a considerable distance to go in terms of the practical affectdetection that we plan to implement_comma_ the already implemented detection is able to cause reasonably appropriate contributions by the automated character.
E06-2030.txt,105,We have conducted a two day pilot user test with 39 secondary school students.
E06-2030.txt,106,We concealed the involvement of an earlier version of EmEliza in some sessions_comma_ in order to test by questionnaire whether its involvement affects user satisfaction_comma_ etc. None of the measures revealed a significant effect.
E06-2030.txt,107,Also_comma_ judging by the group debriefing sessions after the e drama sessions_comma_ nobody found out that one bit part character was sometimes computer controlled.
E06-2030.txt,108,Further user testing with students at several Birmingham schools will take place in March 2006. .
E06-2020.txt,1,describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype Natural Language Generation NLG system that produces pollen forecasts for Scotland. 1 .
E06-2020.txt,2,New monitoring devices such as remote sensing systems are generating vast amounts of spatio temporal data.
E06-2020.txt,3,These devices_comma_ coupled with the wider accessibility of the data_comma_ have spurred large amounts of research into how it can best be analysed.
E06-2020.txt,4,There has been less research however_comma_ into how the results of the data analysis can be effectively communicated.
E06-2020.txt,5,As part of a wider research project aiming to produce textual reports of complex spatio temporal data_comma_ we have developed a prototype NLG system which produces textual pollen forecasts for the general public.
E06-2020.txt,6,Pollen forecast texts describe predicted pollen concentration values for different regions of a country.
E06-2020.txt,7,Their production involves two subtasks predicting pollen concentration values for different regions of a country_comma_ and describing these numerical values textually.In our work_comma_ we focus on the later subtask_comma_ textual description of spatio temporally distributed pollen concentration values.
E06-2020.txt,8,The subtask of predicting pollen concentrations is carried out by our industrial collaborator_comma_ Aerospace and Marine International UK Ltd AMI .
E06-2020.txt,9,A fairly substantial amount of work already exists on weather forecast generation.
E06-2020.txt,10,A number of systems have been developed and are currently in commercial use with two of the most notable being FOG Goldberg et al._comma_ 1994 and MultiMeteo Coch_comma_ 1998 .
E06-2020.txt,11,2 Knowledge Acquisition Our knowledge acquisition activities consisted of corpus studies and discussions with experts.
E06-2020.txt,12,We have collected a parallel corpus 69 data text pairs of pollen concentration data and their corresponding human written pollen reports which our industrial collaborator has provided for a local commercial television station.
E06-2020.txt,13,The forecasts were written by two expert meteorologists_comma_ one of whom provided insight into how the forecasts were written.
E06-2020.txt,14,An example of a pollen forecast text is shown in Figure 1_comma_ its corresponding data is shown in table 1.
E06-2020.txt,15,A pollen forecast in the map form is shown in Figure 2.
E06-2020.txt,16,Monday looks set to bring another day of relatively high pollen counts_comma_ with values up to a very high eight in the Central Belt.
E06-2020.txt,17,Further North_comma_ levels will be a little better at a moderate to high five to six.
E06-2020.txt,18,However_comma_ even at these lower levels it will probably be uncomfortable for Hay fever sufferers. Figure 1 Human written pollen forecast text for the pollen data shown in table 1 Figure 2 Pollen forecast map for the pollen data shown in table 1 Analysis of a parallel corpus texts and their underlying data can be performed in two stages In the first stage_comma_ traditional corpus analysis procedure outlined in Reiter and Dale_comma_ 2000 and Geldof_comma_ 2003 can be used to analyse the pollen forecast texts the textual component of the parallel corpus .
E06-2020.txt,19,This stage will identify the different message types and uncover the sub language of the pollen forecasts. In the second stage the more recent analysis methods developed in the SumTime project Reiter et 163 ValidDate AreaID Value 27 06 2005 1 North 6 27 06 2005 2 North West 5 27 06 2005 3 Central 5 27 06 2005 4 North East 6 27 06 2005 5 South West 8 27 06 2005 6 South East 8 Table1 Pollen Concentration Data for Scotland Input data for Figures 1 and 2 al._comma_ 2003 which exploit the availability of the underlying pollen data corresponding to the forecast texts can be used to map messages to input data and also map parts of the sub language such as words to the input data.
E06-2020.txt,20,Due to the fact that we are modeling the task of automatically producing pollen forecast texts from predicted pollen concentration values_comma_ knowledge of how to map input data to messages and words phrases is absolutely necessary.
E06-2020.txt,21,Studies connecting language to data are useful for understanding the semantics of language in a more novel way than the traditional logic based formalisms Roy and Reiter_comma_ 2005 .
E06-2020.txt,22,We have performed the first stage of the corpus analysis and part of the second stage so far.
E06-2020.txt,23,In the first stage_comma_ we .
E06-1028.txt,1,this paper_comma_ we present an automated_comma_ quantitative_comma_ knowledge poor method to evaluate the randomness of a collection of documents corpus _comma_ with respect to a number of biased partitions. The method is based on the comparison of the word frequency distribution of the target corpus to word frequency distributions from corpora built in deliberately biased ways.
E06-1028.txt,2,We apply the method to the task of building a corpus via queries to Google.
E06-1028.txt,3,Our results indicate that this approach can be used_comma_ reliably_comma_ to discriminate biased and unbiased document collections and to choose the most appropriate query terms.
E06-1028.txt,5,The Web is a very rich source of linguistic data_comma_ and in the last few years it has been used intensively by linguists and language technologists for many tasks Kilgarriff and Grefenstette_comma_ 2003 .
E06-1028.txt,6,Among other uses_comma_ the Web allows fast and inexpensive construction of general purpose corpora_comma_ i.e._comma_ corpora that are not meant to represent a specific sub language_comma_ but a language as a whole.
E06-1028.txt,7,There are several recent studies on the extent to which Web derived corpora are comparable_comma_ in terms of variety of topics and styles_comma_ to traditional balanced corpora Fletcher_comma_ 2004 Sharoff_comma_ 2006 .
E06-1028.txt,8,Our contribution_comma_ in this paper_comma_ is to present an automated_comma_ quantitative method to evaluate the variety or randomness with respect to a number of non random partitions of a Web corpus.
E06-1028.txt,9,The more random less biased towards specific partitions a corpus is_comma_ the more it should be suitable as a general purpose corpus.
E06-1028.txt,10,We are not proposing a method to evaluate whether a sample of Web pages is a random sample of the Web_comma_ although this is a related issue Bharat and Broder_comma_ 1998 Henzinger et al._comma_ 2000 .
E06-1028.txt,11,Instead_comma_ we propose a method_comma_ based on simple distributional properties_comma_ to evaluate if a sample of Web pages in a certain language is reasonably varied in terms of the topics and_comma_ perhaps_comma_ textual types it contains.
E06-1028.txt,12,This is independent from whethertheyareactuallyproportionallyrepresenting what is out there on the Web or not.
E06-1028.txt,13,For example_comma_ although computer related technical language is probably much more common on the Web than_comma_ say_comma_ the language of literary criticism_comma_ one might prefer a biased retrieval method that fetches documents representing these and other sub languages in comparable amounts_comma_ to an unbiased method that leads to a corpus composed mostly of computer jargon.
E06-1028.txt,14,This is a new area of investigation with traditional corpora_comma_ one knows a priori their composition.
E06-1028.txt,15,As the Web plays an increasingly central role as data source in NLP_comma_ we believe that methods to efficiently characterize the nature of automatically retrieved data are becoming of central importance to the discipline.
E06-1028.txt,16,In the empirical evaluation of the method_comma_ we focus on general purpose corpora built issuing automated queries to a search engine and retrieving the corresponding pages_comma_ which has been shown to be an easy and effective way to build Web based corpora Ghani et al._comma_ 2001 Ueyama and Baroni_comma_ 2005 Sharoff_comma_ 2006 .
E06-1028.txt,17,It is natural to ask which kinds of query terms_comma_ henceforth seeds_comma_ are more appropriate to build a corpus comparable_comma_ in terms of variety_comma_ to traditional balanced corpora such as the British National Corpus_comma_ henceforth BNC Aston and Burnard_comma_ 1998 .
E06-1028.txt,18,We test our procedure to assess Web corpus randomness on corpora built 217 using seeds chosen following different strategies.
E06-1028.txt,19,However_comma_ the method per se can also be used to assesstherandomnessofcorporabuiltinotherways e.g._comma_ by crawling the Web.
E06-1028.txt,20,Our method is based on the comparison of the word frequency distribution of the target corpus to word frequency distributions constructed using queries to a search engine for deliberately biased seeds.
E06-1028.txt,21,As such_comma_ it is nearly resource free_comma_ as it only requires lists of words belonging to specific domains that can be used as biased seeds.
E06-1028.txt,22,In our experiments we used Google as the search engine of choice_comma_ but different search engines could be used as well_comma_ or other ways to obtain collections of biased documents_comma_ e.g._comma_ via a directory of precategorized Web pages.
E06-1028.txt,23,2 Relevant work Our work is related to the recent literature on building linguistic corpora from the Web using automated queries to search engines Ghani et al._comma_ 2001 Fletcher_comma_ 2004 Ueyama and Baroni_comma_ 2005 Sharoff_comma_ 2006 .
E06-1028.txt,24,Different criteria are used to select the seeds.
E06-1028.txt,25,Ghani and colleagues iteratively bootstrapped queries to AltaVista from retrieved documents in the target language and in other languages.
E06-1028.txt,26,They seeded the bootstrap procedure with manually selected documents_comma_ or with small sets of words provided by native speakers of the language.
E06-1028.txt,27,They showed that the procedure produces a corpus that contains_comma_ mostly_comma_ pages in the relevant language_comma_ but they did not evaluate the results in terms of quality or variety.
E06-1028.txt,28,Fletcher 2004 constructed a corpus of English by querying AltaVista for the 10 top frequency words from the BNC.
E06-1028.txt,29,He then conducted a qualitative analysis of frequent n grams in the Web corpus and in the BNC_comma_ highlighting the differences between the two corpora.
E06-1028.txt,30,Sharoff 2006 builtcorporaofEnglish_comma_Russian and German via queries to Google seeded with manually cleaned lists of words that are frequent in a reference corpus in the relevant language_comma_ excluding function words_comma_ while Ueyama and Baroni 2005 built corpora of Japanese using seed words from a basic Japanese vocabulary list.
E06-1028.txt,31,Both Sharoff and Ueyama and Baroni evaluated the results through a manual classification of the retrieved pages and by qualitative analysis of the words that are most typical of the Web corpora.
E06-1028.txt,32,We are also interested in evaluating the effect that different seed selection or_comma_ more in general_comma_ corpus building strategies have on the nature of the resulting Web corpus.
E06-1028.txt,33,However_comma_ rather than performing a qualitative investigation_comma_ we develop a quantitative measure that could be used to evaluateandcomparealargenumberofdifferentcorpus building methods_comma_ as it does not require manual intervention.
E06-1028.txt,34,Moreover_comma_ our emphasis is not on the corpus building methodology_comma_ nor on classifying the retrieved pages_comma_ but on assessing whether they appear to be reasonably unbiased with respect to a range of topics or other criteria.
E06-1028.txt,35,3 Measuring distributional properties of biased and unbiased collections Our goal is to create a balanced corpus of Web pages in a given language e.g._comma_ the portion composed of all Spanish Web pages.
E06-1028.txt,36,As we observed in the .
E06-3007.txt,1,paper presents a way in which a lexicalised HPSG grammar can handle word order constraints in a computational parsing system_comma_ without invoking an additional layer of representation for word order_comma_ such as Reape s Word Order Domain. The key proposal is to incorporate into lexical heads the WOC Word Order Constraints feature_comma_ which is used to constrain the word order of its projection.
E06-3007.txt,2,We also overview our parsing algorithm.
E06-3007.txt,4,It is a while since the linearisation technique was introduced into HPSG by Reape 1993 1994 as a way to overcome the inadequacy of the conventional phrase structure rule based grammars in handling freer word order of languages such as German and Japanese.
E06-3007.txt,5,In parallel in computational linguistics_comma_ it has long been proposed that more flexible parsing techniques may be required to adequately handle such languages_comma_ but hitherto a practical system using linearisation has eluded large scale implementation.
E06-3007.txt,6,There are at least two obstacles its higher computational cost accompanied with non CFG algorithms it requires_comma_ and the difficulty to state word order information succinctly in a grammar that works well with a nonCFG parsing engine.
E06-3007.txt,7,In a recent development_comma_ the cost issue has been tackled by Daniels and Meurers 2004 _comma_ who propose to narrow down on search space while using a non CFG algorithm.
E06-3007.txt,8,The underlying principle is to give priority to the full generative capacity_comma_ let the parser overgenerate at default but restrict generation for efficiency thereafter.
E06-3007.txt,9,While sharing this principle_comma_ I will attempt to further streamline the computation of linearisation_comma_ focusing mainly on the issue of grammar formalism.
E06-3007.txt,10,Specifically_comma_ I would like to show that the lexicalisation of word order constraints is possible with some conservative modifications to the standard HPSG Pollard and Sag_comma_ 1987 Pollard and Sag_comma_ 1994 .
E06-3007.txt,11,This will have the benefit of making the representation of linearisation grammar simpler and more parsing friendly than Reape s influential Word Order Domain theory.
E06-3007.txt,12,In what follows_comma_ after justifying the need for non CFG parsing and reviewing Reape s theory_comma_ I will propose to introduce into HPSG the Word Order Constraint WOC feature for lexical heads.
E06-3007.txt,13,I will then describe the parsing algorithm that refers tothis featureto constrainthe searchfor efficiency.
E06-3007.txt,14,1.1 Limitation of CFG Parsing One of the main obstacles for CFG parsing is the discontinuity in natural languages caused by interleaving of elements from different phrases Shieber_comma_ 1985 .
E06-3007.txt,15,Although there are well known syntactic techniques to enhance CFG as in GPSG Gazdar et al._comma_ 1985 _comma_ there remain constructions that show genuine discontinuity of the kind that cannot be properly dealt with by CFG.
E06-3007.txt,16,Such difficult discontinuity typically occurs when it is combined with scrambling another symptomatic phenomenon of free word order languages of a verb s complements.
E06-3007.txt,17,The following is an example from German_comma_ where scrambling and discontinuity co occur in what is called incoherent object control verb construction.
E06-3007.txt,18,1 Ich glaube_comma_ dass der Fritz dem Frank I believe Comp Fritz Nom Frank Dat das Buch zu lesen erlaubt. the book Acc to read allow I think that Fritz allows Frank to read the book 23 1 Ich glaube_comma_ dass der Fritz das Buch dem Frank zu lesen erlaubt Ich glaube_comma_ dass dem Frank das Buch der Fritz zu lesen erlaubt Ich glaube_comma_ dass das Buch dem Frank der Fritz zu lesen erlaubt ... Here 1 is in the canonical word order while the examples in 1 are its scrambled variants.
E06-3007.txt,19,In the traditional bi clausal analysis according to which the object control verb subcategorises for a zu infinitival VP complement as well as nominal complements_comma_ this embedded VP_comma_ das Buch zu lesen_comma_ becomes discontinuous in the latter examples in square brackets .
E06-3007.txt,20,One CFG response is to use mono clausal analysis or argument composition Hinrichs and Nakazawa_comma_ 1990 _comma_ according to which the higher verb and lower verb in the above example erlauben and zu lesen are combined to form a single verbal complex_comma_ which in turn subcategorises for nominal complements das Buch_comma_ der Fritz and dem Frank .
E06-3007.txt,21,Under this treatment both the verbal complex and the sequence of complements are rendered continuous_comma_ rendering all the above examples CFG parseable.
E06-3007.txt,22,However_comma_ this does not quite save the CFG parseability_comma_ in the face of the fact that you could extrapose the lower V NP_comma_ as in the following.
E06-3007.txt,23,2 Ich glaube_comma_ dass der Fritz dem Frank erlaubt _comma_ das Buch zu lesen .
E06-3007.txt,24,Now we have a discontinuity of verbal complex instead of complements the now discontinuous verbal complex is marked with square brackets .
E06-3007.txt,25,Thus either way_comma_ some discontinuity is inevitable.
E06-3007.txt,26,Such discontinuity is by no means a marginal phenomenon limited to German.
E06-3007.txt,27,Parallel phenomena are observed in the object control verbs in Korean and Japanese Sato_comma_ 2004 for examples .
E06-3007.txt,28,These languages also show a variety of genuine discontinuity of other sorts_comma_ which do not lend itself to a straightforward CFG parsing Yatabe_comma_ 1996 .
E06-3007.txt,29,TheCFG recalcitrant constructions exist in abundance_comma_ pointing to an acute need for non CFG parsing.
E06-3007.txt,30,1.2 Reape s Word Order Domain The most influential proposal to accommodate such discontinuity scrambling in HPSG is Reape s Word Order Domain_comma_ or DOM_comma_ a feature that constitutes an additional layer separate from the dominance structure of phrases Reape_comma_ 1993 Reape_comma_ 1994 .
E06-3007.txt,31,DOM encodes the phonologically realised linearised list of signs the daughter signs of a     phrase DOM angbracketleftbig 1 2 3 ... n angbracketrightbig HD DTR angbracketleftBiggbracketleftBiggphrase DOM 1 UNIONED bracketrightBiggangbracketrightBigg NHD DTRs angbracketleftBiggbracketleftBiggphrase DOM 2 UNIONED bracketrightBigg _comma_ bracketleftBigg phrase DOM 3 UNIONED bracketrightBigg ... bracketleftBigg phrase DOM n UNIONED bracketrightBiggangbracketrightBigg     Figure 1 Word Order Domain phrase in the HD DTR and NHD DTRS features are linearly ordered as in Figure 1.
E06-3007.txt,32,The feature UNIONED in the daughters indicates whether discontinuity amongst their constituents is allowed.
E06-3007.txt,33,Computationally_comma_ the positive  value of the feature dictates the DOMs of the daughters to be sequence unioned represented by the operator into the mother DOM details apart_comma_ this operation essentially merges two lists in a way that allows interleaving of their elements.
E06-3007.txt,34,In Reape s theory_comma_ LP constraints come from an entirely different source.
E06-3007.txt,35,There is nothing as yet that blocks_comma_ for instance_comma_ the ungrammatical zu lesen das Buch VP sequence.
E06-3007.txt,36,The relevant constraint_comma_ i.e.
E06-3007.txt,37,COMPS ZU INF V in German_comma_ is stated in the LP component of the theory.
E06-3007.txt,38,Thus with the interaction of the UNIONED feature and LP statements_comma_ the grammar rules out the unacceptable sequences while endorsing grammatical ones such as the examples in 1 .
E06-3007.txt,39,One important aspect of Reape s theory is that DOM is a list of whole signs rather than of any part of them such as PHON.
E06-3007.txt,40,This is necessitated by the fact that in order to determine how DOM should be constructed_comma_ the daughters internal structure need to be referred to_comma_ above all_comma_ the UNIONED feature.
E06-3007.txt,41,In other words_comma_ the internal features of the daughters must be accessible.
E06-3007.txt,42,While this is a powerful system that overcomes the inadequacies of phrase structure rules_comma_ some may feel this is a rather heavy handed way to solve the problems.
E06-3007.txt,43,Above all_comma_ much information is repeated_comma_ as all the signs are effectively stated twice_comma_ once in the phrase structure and again in DOM.
E06-3007.txt,44,Also_comma_ the fact that discontinuity and linear precedence are handled by two distinct mechanisms seems somewhat questionable_comma_ as these two factors are computationally closely related.
E06-3007.txt,45,These properties are not entirely attractive features for a computational grammar.
E06-3007.txt,46,24 2 Lexicalising Word Order Constraints 2.1 Overview Our theoretical goal is_comma_ in a nutshell_comma_ to achieve what Reape does_comma_ namely handling discontinuity and linear precedence_comma_ in a simpler_comma_ more lexicalist manner.
E06-3007.txt,47,My central proposal consists in incorporating the Word Order Constraint WOC feature into the lexical heads_comma_ rather than positing an additional tier for linearisation.
E06-3007.txt,48,Some new subfeatures will also be introduced.
E06-3007.txt,49,The value of the WOC feature is a set of wordorder related constraints.
E06-3007.txt,50,It may contain any relational constraint the grammar writer may want with the proviso of its formalisability_comma_ but for the current proposal_comma_ I include two subfeatures ADJ adjacency and LP_comma_ both of which_comma_ being binary relations_comma_ are represented as a set of ordered pairs_comma_ the members of which must either be the head itself or its sisters.
E06-3007.txt,51,Figure 2 illustrates what such feature structure looks like with an English verb provide_comma_ as in provide him with a book.
E06-3007.txt,52,We will discuss the new PHON subfeatures in the next section for now it would suffice to consider them to constitute the standard PHON list so let us focus on WOC here.
E06-3007.txt,53,The WOC feature of this verb says_comma_ for its projection VP _comma_ three constraints have to be observed.
E06-3007.txt,54,Firstly_comma_ the ADJ subfeature says that the indirect object NP has to be in the adjacent position to the verb provide yesterday him with a book is not allowed .
E06-3007.txt,55,Secondly_comma_ the first two elements of the LP value encode a head initial constraint for English VPs_comma_ namely that a head verb has to be preceded by its complements.
E06-3007.txt,56,Lastly_comma_ the last pair in the same set says the indirect object must precede the with PP provide with a book him is not allowed .
E06-3007.txt,57,Notice that this specification leaves room for some discontinuity_comma_ as there is no ADJ requirement between the indirect NP and with PP.
E06-3007.txt,58,Hence_comma_ provide him yesterday with a book is allowed.
E06-3007.txt,59,The key idea here is that since the complements of a lexical head are available in its COMPS feature_comma_ it should be possible to state the relative linear order which holds between the head and a complement_comma_ as well as between complements_comma_ inside the feature structure of the head.
E06-3007.txt,60,Admittedly word order would naturally be considered to reside in a phrase_comma_ string of words.
E06-3007.txt,61,It might be argued_comma_ on the ground that a head s COMPS feature simply consists of the categories it selects for in exclusion of the PHON feature_comma_ that with this architecture one would inevitably encounter the accessibility problem discussed in v       verb PHON  phon wd CONSTITUENTS braceleftbig provide bracerightbig CONSTRAINTS  COMPS angbracketleftbigg np bracketleftBignp case Acc bracketrightBig _comma_ pp bracketleftBigpp pform with bracketrightBigangbracketrightbigg WOC  woc ADJ braceleftBigangbracketleftbig v _comma_ np angbracketrightbigbracerightBig LP braceleftBigangbracketleftbig v _comma_ np angbracketrightbig _comma_ angbracketleftbig v _comma_ pp angbracketrightbig _comma_ angbracketleftbig np _comma_ pp angbracketrightbigbracerightBig         Figure 2 Example of lexical head with WOC feature Section 1.2 in order to ensure the enforceability of word order constraints_comma_ an access must be secured to the values of the internal features including the PHON values.
E06-3007.txt,62,However_comma_ this problem can be overcome_comma_ as we will see_comma_ if due arrangements are in place.
E06-3007.txt,63,The main benefit of this mechanism is that it paves way to an entirely lexicon based rule specification_comma_ so that_comma_ on one hand_comma_ duplication of informationbetween lexicalspecification and phrase structure rules can be reduced and on the other_comma_ a wide variety of lexical properties can be flexibly handled.
E06-3007.txt,64,If the word order constraints_comma_ which have been regarded as the bastion of rule based grammars_comma_ is shown to be lexically handled_comma_ it is one significant step further to a fully lexicalist grammar.
E06-3007.txt,65,2.2 New Head Argument Schema What is crucial for this WOC incorporated grammar is how the required word order constraints stated in WOC are passed on and enforced in its projection.
E06-3007.txt,66,I attempt to formalise this in the form of Head Argument Schema_comma_ by modifying HeadComplement Schema of Pollard and Sag 1994 .
E06-3007.txt,67,There are two key revisions an enriched PHON feature that contains word order constraints and percolation of these constraints emanating from the WOC feature in the head.
E06-3007.txt,68,The revised Schema is shown in Figure 3.
E06-3007.txt,69,For simplicity only the LP subfeature is dealt with_comma_ since the ADJ subfeature would work exactly the same way.
E06-3007.txt,70,The set notations attached underneath states the restriction on the value of WOC_comma_ namely that all the signs that appear in the constraint pairs must be relevant _comma_ i.e. must also appear as daughters included in DtrSet _comma_ the set of the head daughter and non head daughters .
E06-3007.txt,71,Naturally_comma_ they also cannot be the same signs xnegationslash y .
E06-3007.txt,72,Let me discuss some auxiliary modifications 25              head arg phrase PHON   phon CONSTITS uniontextbraceleftBigbraceleftbig ph bracerightbig _comma_ pa1 _comma_..._comma_ pai _comma_..._comma_ paj _comma_... pan bracerightBig CONSTRTS LP uniontextbraceleftbiggbraceleftBig ..._comma_ angbracketleftbig pai _comma_ paj angbracketrightbig _comma_... bracerightBig _comma_ ca1 _comma_..._comma_ cai _comma_... caj _comma_..._comma_ can bracerightbigg   ARGS HD DTR hd        word PHN bracketleftbigg CONSTITS braceleftbig ph bracerightbig CONSTRS bracketrightbigg ARGS args angbracketleftBigga1  sign PHN bracketleftbigg CONSTITS pa1 CONSTRS ca1 bracketrightbigg _comma_..._comma_ ai  sign PHN bracketleftbigg CONSTITS pai CONSTRS cai bracketrightbigg _comma_ ..._comma_ aj  sign PHN bracketleftbigg CONSTITS paj CONSTRS caj bracketrightbigg _comma_..._comma_ an bracketleftBiggsign PHN bracketleftBigCONSTITS pan CONSTRS can bracketrightBig bracketrightBigg angbracketrightBigg WOC LP wocs braceleftBig ..._comma_ angbracketleftbig ai _comma_ aj angbracketrightbig _comma_... bracerightBig        NHD DTRs args              where wocs  x_comma_y xnegationslash y_comma_ x_comma_y DtrSet DtrSet hd args Figure 3 Head Argument Schema with WOC feature first.
E06-3007.txt,73,Firstly_comma_ we change the feature name from COMPS to ARGS because we assume a nonconfigurational flat structure_comma_ as is commonly the case with linearisation grammar.
E06-3007.txt,74,Another change I propose is to make ARGS a list of underspecified signs instead of SYNSEMs as standardly assumed Pollard and Sag_comma_ 1994 .
E06-3007.txt,75,In fact_comma_ this is a position taken in an older version of HPSG Pollard and Sag_comma_ 1987 but rejected on the ground of the locality of subcategorisation.
E06-3007.txt,76,The main reason for this reversal is to facilitate the accessibility we discussed earlier.
E06-3007.txt,77,As unification and percolation of the PHON information is involved in the Schema_comma_ it is much more straightforward to formulate with signs.
E06-3007.txt,78,Though the change may not be quite defensible solely on this ground_comma_1 there is reason to leave the locality principle as an option for languages of which it holds rather than hardwire it into the Schema_comma_ since some authors raise doubt as for the universal applicability of the locality principle e.g.
E06-3007.txt,79,Meurers_comma_ 1999 .
E06-3007.txt,80,Turning to a more substantial modification_comma_ our new PHON feature consists of two subfeatures_comma_ CONSTITUENTS or CONSTITS and CONSTRAINTS or CONSTRS .
E06-3007.txt,81,The former encodes the set that comprises the phonology of words of which the string consists.
E06-3007.txt,82,Put simply_comma_ it is the un1Another potential problem is cyclicity_comma_ since the signvalued ARGS feature contains the WOC feature_comma_ which could contain the head itself.
E06-3007.txt,83,This has to be fixed for the systems that do not allow cyclicity. ordered version of the standard PHON list.
E06-3007.txt,84,The CONSTRAINTS feature represents the concatanative constraints applicable to the string.
E06-3007.txt,85,Thus_comma_ the PHON feature overall represents the legitimate word order patterns in an underspecified way_comma_ i.e. any of the possible string combinations that obey the constraints.
E06-3007.txt,86,Let me illustrate with a VP example_comma_ say_comma_ consisting of meet_comma_ often and Tom_comma_ for which we assume that the following word order patterns are acceptable_comma_ meet_comma_ Tom_comma_ often _comma_ often_comma_ meet_comma_ Tom but not the followings meet_comma_ often_comma_ Tom _comma_ Tom_comma_ often_comma_ meet _comma_ Tom_comma_ meet_comma_ often _comma_ often_comma_ Tom_comma_ meet .
E06-3007.txt,87,This situation can be captured by the following feature specification for PHON_comma_ which encodes any of the acceptable strings above in an underspecified way.    PHON    CONSTITS braceleftbig often_comma_ Tom_comma_ meet bracerightbig CONSTRS   ADJ braceleftbiggangbracketleftBigbraceleftbig meet bracerightbig _comma_ braceleftbig Tom bracerightbigangbracketrightBigbracerightbigg LP braceleftbiggangbracketleftBigbraceleftbig meet bracerightbig _comma_ braceleftbig Tom bracerightbigangbracketrightBigbracerightbigg         The key point is that now the computation of word order can be done based on the information inside the PHON feature_comma_ though indeed the CONSTR values have to come from outside the word order crucially depends on SYNSEM related values of the daughter signs.
E06-3007.txt,88,26 Let us now go back to the Schema in Figure 3 and see how to determine the CONSTR values to enter the PHON feature.
E06-3007.txt,89,This is achieved by looking up the WOC constraints in the head let s call this Step 1 and pushing the relevant constraints into the PHON feature of its mother_comma_ according to the type of constraints Step 2 .
E06-3007.txt,90,For readability Figure 3 only states explicitly a special case where one LP constraint holds of two of the arguments but the reader is asked to interpret ai and aj in the head daughter s WOC LP to represent any two signs chosen from the DTRS list including the head_comma_ hd .
E06-3007.txt,91,2 The structure sharing of ai and aj between WOC LP and ARGS indicates that the LP constraint applies to these two arguments in this order_comma_ i.e. ai aj.
E06-3007.txt,92,Thus through unification_comma_ it is determined which constraints apply to which pairs of daughter signs inside the head.
E06-3007.txt,93,This corresponds to Step 1.
E06-3007.txt,94,Now_comma_ only for these WOC applicable daughter signs_comma_ the PHON CONSTIITS valuesare pairedup for each constraint in this case pai_comma_ paj and pushed into the mother s PHON CONSTRS feature.
E06-3007.txt,95,This corresponds to Step 2.
E06-3007.txt,96,Notice alsothatthe CONSTRAINTSsubfeature is cumulatively inherited.
E06-3007.txt,97,All the non head daughters CONSTR values ca1_comma_..._comma_can the word order constraints applicable to each of these daughters are also passed up_comma_ collecting effectively all the CONSTR values of its daughters and descendants.
E06-3007.txt,98,This means the information concerning word order_comma_ as tied to particular string pairs_comma_ is never lost and passed up all the way through.
E06-3007.txt,99,Thus the WOC constraints can be enforced at any point where both members of the string pair in question are instantiated.
E06-3007.txt,100,2.3 A Worked Example Let us now go through an example of applying the Schema_comma_ again with the German subordinate clause_comma_ das Buch der Fritz dem Frank zu lesen erlaubt and other acceptable variants .
E06-3007.txt,101,Our goal is to enforce the ADJ and LP constraints in a flexible enough way_comma_ allowing the acceptable sequences such as those we saw in Section 1.2.1. while blocking the constraint violating instances.
E06-3007.txt,102,The instantiated Schema is shown in Figure 4.
E06-3007.txt,103,Let us start with a rather deeply embedded level_comma_ the embedded verb zu lesen_comma_ marked v2_comma_ found inside vp the last and largest NHD DTR as its HD2For the generality of the number of ARGS elements_comma_ which should be taken to be any number including zero_comma_ the recursive definition as detailed in Richter and Sailer_comma_ 1995 can be adopted.
E06-3007.txt,104,DTR_comma_ which I suppose to be one lexical item for simplicity.
E06-3007.txt,105,This is one of the lexical heads from which the WOC constraints emanate.
E06-3007.txt,106,Find_comma_ in this item s WOC_comma_ a general LP constraint for zuInfinitiv VPs_comma_ COMPS V_comma_ namely np3 v2.
E06-3007.txt,107,Then the PHON CONSTITS values of these signs are searched for and found in the daughters_comma_ namely pnp3 and pv2.
E06-3007.txt,108,These values are paired up and passed into the CONSTRS LP value of its mother VP.
E06-3007.txt,109,Notice also that into this value the NHDDTRs CONSTR LP values_comma_ in this case only lpnp3 das Buch _comma_ are also unioned_comma_ constituting lpvp we are here witnessing the cumulative inheritance of constraints explained earlier.
E06-3007.txt,110,Turn attention now to the percolation of ADJ subfeature no ADJ requirement is found between das Buch and zu lesen v2 s WOC ADJ is empty _comma_ though ADJ is required one node below_comma_ between das and Buch np3 s PHN CONSTR ADJ .
E06-3007.txt,111,Thus no new ADJ pair is added to the mother VP s PHON CONSTR feature.
E06-3007.txt,112,Exactly the same process is repeated for the projection of erlauben v1 _comma_ where its WOC again contains only LP requirements.
E06-3007.txt,113,With the PHON CONSTITS values of the relevant signs found and paired up Fritz_comma_der erlaubt and Frank_comma_dem erlaubt _comma_ they are pushed into its mother s PHON CONSTRS LP value_comma_ which is also unioned with the PHON CONSTRS values of the NHD DTRS.
E06-3007.txt,114,Notice this time that there is no LP requirement between the zu Infinitiv VP_comma_ das Buch zu lesen_comma_ and the higher verb_comma_ erlaubt.
E06-3007.txt,115,This is intended to allow for extraposition.3 The eventual effect of the cumulative constraint inheritance can be more clearly seen in the subAVM underneath_comma_ which shows the PHON part of the whole feature structure with its values instantiated.
E06-3007.txt,116,After a succession of applications of the Head Argument Schema_comma_ we now have a pool of WOCs sufficient to block unwanted word order patterns while endorsing legitimate ones.
E06-3007.txt,117,The representation of the PHON feature being underspecified_comma_ it corresponds to any of the appropriately constrained order patterns. der Fritz dem Frank zu lesen das Buch erlaubt would be ruled out by the violation of the last LP constraint_comma_ der Fritz erlaubt dem Frank das Buch zu lesen by the second_comma_ and so on.
E06-3007.txt,118,The reader might be led to think_comma_ because of 3The lack of this LP requirement also entails some marginally acceptable instances_comma_ such as der Fritz dem Frank das Buch erlaubt zu lesen_comma_ considered ungrammatical by many.
E06-3007.txt,119,These instances can be blocked_comma_ however_comma_ by introducing more complex WOCs.
E06-3007.txt,120,See Sato forthcoming a .
E06-3007.txt,121,27                             subordinate clause PHON  CONSTITS pv1 pnp1 pnp2 pvp CONSTRS bracketleftBigg ADJ adnp1 adnp2 adnp3 LP braceleftBigangbracketleftbig pnp1 _comma_ pv1 angbracketrightbig _comma_ angbracketleftbig pnp2 _comma_ pv1 angbracketrightbigbracerightBig lpnp1 lpnp2 lpvp bracketrightBigg  ARGS HD DTR v1   verb PHON CONSTITS pv1 braceleftbig erlaubt bracerightbig ARGS angbracketleftbig np1 _comma_ np2 _comma_ vp angbracketrightbig WOC bracketleftBigg ADJ LP braceleftBigangbracketleftbig np1 _comma_ v1 angbracketrightbig _comma_ angbracketleftbig np2 _comma_ v1 angbracketrightbigbracerightBig bracketrightBigg   NHD DTRs angbracketleftBigg np1     np PHON   CONSTITS pnp1 braceleftbig Fritz_comma_ der bracerightbig CONSTRS  ADJ adnp1 braceleftbiggangbracketleftBigbraceleftbig Fritz bracerightbig _comma_ braceleftbig der bracerightbigangbracketrightBigbracerightbigg LP lpnp1 braceleftbiggangbracketleftBigbraceleftbig der bracerightbig _comma_ braceleftbig Fritz bracerightbigangbracketrightBigbracerightbigg     SYNSEM ... CASE Nom     _comma_ np2     np PHON   CONSTITS pnp1 braceleftbig Frank_comma_ dem bracerightbig CONSTRS  ADJ adnp2 braceleftbiggangbracketleftBigbraceleftbig Frank bracerightbig _comma_ braceleftbig der bracerightbigangbracketrightBigbracerightbigg LP lpnp2 braceleftbiggangbracketleftBigbraceleftbig der bracerightbig _comma_ braceleftbig Frank bracerightbigangbracketrightBigbracerightbigg     SYNSEM ... CASE Dat     _comma_ vp              vp PHON  CONSTITS pvp pv2 pnp3 CONSTRS bracketleftBigg ADJ adnp3 LP lpvp braceleftBigangbracketleftbig pnp3 _comma_ pv2 angbracketrightbigbracerightBig lpnp3 bracketrightBigg  ARGS HD DTR v2   v PHON CONSTITS pv2 braceleftbig zu lesen bracerightbig ARGS angbracketleftbig np3 angbracketrightbig WOC bracketleftBigg ADJ LP braceleftBigangbracketleftbig np3 _comma_ v2 angbracketrightbigbracerightBig bracketrightBigg   NHD DTRS angbracketleftBigg np3     np PHON   CONSTITS pnp3 braceleftbig Buch_comma_das bracerightbig CONSTRS  ADJ adnp3 braceleftbiggangbracketleftBigbraceleftbig Buch bracerightbig _comma_ braceleftbig das bracerightbigangbracketrightBigbracerightbigg LP lpnp3 braceleftbiggangbracketleftBigbraceleftbig das bracerightbig _comma_ braceleftbig Buch bracerightbigangbracketrightBigbracerightbigg     SYNSEM ... CASE Acc     angbracketrightBigg              angbracketrightBigg                             Instantiated PHON part of the above PHON     CONSTITS braceleftbig erlaubt_comma_ Fritz_comma_ der_comma_ Frank_comma_ dem_comma_ zu lesen_comma_ Buch_comma_ das bracerightbig CONSTRS   ADJ braceleftbiggangbracketleftBigbraceleftbig Fritz bracerightbig _comma_ braceleftbig der bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig Frank bracerightbig _comma_ braceleftbig dem bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig Buch bracerightbig _comma_ braceleftbig das bracerightbigangbracketrightBigbracerightbigg LP   angbracketleftBigbraceleftbig Fritz_comma_der bracerightbig _comma_ braceleftbig erlaubt bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig Frank_comma_dem bracerightbig _comma_ braceleftbig erlaubt bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig der bracerightbig _comma_ braceleftbig Fritz bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig dem bracerightbig _comma_ braceleftbig Frank bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig das bracerightbig _comma_ braceleftbig Buch bracerightbigangbracketrightBig _comma_ angbracketleftBigbraceleftbig Buch_comma_das bracerightbig _comma_ braceleftbig zu lesen bracerightbigangbracketrightBig         Figure 4 An application of Head Argument Schema 28 the monotonic inheritance of constraints_comma_ that the WOC compliance cannot be checked until the stage of final projection.
E06-3007.txt,122,While this is generally true for freer word order languages considering various scenarios such as bottom up generation_comma_ one can conduct the WOCcheck immediatelyafter the instantiation of relevant categories in parsing_comma_ the fact we can exploit in our implementation_comma_ as we will now see.
E06-3007.txt,123,3 Constrained Free Word Order Parsing 3.1 Algorithm In this section our parsing algorithm that works with the lexicalised linearisation grammar outlined above is briefly overviewed.4 It expands on two existing ideas bitmasks for non CFG parsing and dynamic constraint application.
E06-3007.txt,124,Bitmasks are used to indicate the positions of a parsed words_comma_ wherever they have been found.
E06-3007.txt,125,Reape 1991 presents a non CFG tabular parsing algorithm using them_comma_ for permutation complete language_comma_ which accepts all the permutations and discontinuous realisations of words.
E06-3007.txt,126,To take for an example a simple English NP that comprises the_comma_ thick and book_comma_ this parser accepts not only their 3 permutations but discontinuous realisations thereof in a longer string_comma_ such as book_comma_ _comma_ the_comma_ _comma_ thick  indicates the positions of constituents from other phrases .
E06-3007.txt,127,Clearly_comma_ the problem here is overgeneration and in efficiency.
E06-3007.txt,128,In the current form the worstcase complexity will be exponential O n 2n _comma_ n length of string .
E06-3007.txt,129,In response_comma_ Daniels and Meurers 2004 propose to restrict search space during the parse with two additional bitmasks_comma_ positive and negative masks_comma_ which encode the bits that must be and must not be occupied_comma_ respectively_comma_ based on what has been found thus far and the relevant word order constraints.
E06-3007.txt,130,For example_comma_ given the constraints that Det precedes Nom and Det must be adjacent to Nom and supposing the parser has found Det in the third position of a five word string like above_comma_ the negative mask x_comma_ x_comma_ the_comma_ _comma_ is created_comma_ where x indicates the position that cannot be occupied by Nom_comma_ as well as the positive mask  _comma_ das_comma_ _comma_ _comma_ where indicates the positions that must be occupied by Nom.
E06-3007.txt,131,Thus_comma_ you can stop the parser from searching the positions the categories yet to be found cannot occupy_comma_ or force it to search only the positions they have to occupy.
E06-3007.txt,132,4For full details see Sato forthcoming b .
E06-3007.txt,133,A remaining important job is to how to state the constraints themselves in a grammar that works with this architecture_comma_ and Daniels and Meurers answer is a rather traditional one stating them in phrase structure rules as LP attachments.
E06-3007.txt,134,They modify HPSG rather extensively in a way similar to GPSG_comma_ in what they call Generalised ID LP Grammar .
E06-3007.txt,135,However_comma_ as we have been arguing_comma_ this is not an inevitable move.
E06-3007.txt,136,It is possible to keep the general contour of the standard HPSG largely intact.
E06-3007.txt,137,The way our parser interacts with the grammar is fundamentally different.
E06-3007.txt,138,We take full advantage of the information that now resides in lexical heads.
E06-3007.txt,139,Firstly_comma_ rules are dynamically generated from the subcategorisation information ARGS feature in the head.
E06-3007.txt,140,Secondly_comma_ the constraints are picked up from the WOC feature when lexical heads are encountered and carried in edges_comma_ eliminating the need for positive negative masks.
E06-3007.txt,141,When an active edge is about to embrace the next category_comma_ these constraints are checked and enforced_comma_ limiting the search space thereby.
E06-3007.txt,142,After the lexicon lookup_comma_ the parser generates rules from the found lexical head and forms lexical edges.
E06-3007.txt,143,It is also at this stage that the WOC is picked up and pushed into the edge_comma_ along with the rule generated Mum Hd Dtr Nhd1 Nhd2...Nhdn WOCs where WOCs is the set of ADJ and LP constraints picked up_comma_ if any.
E06-3007.txt,144,This edge now tries to find the rest non head daughters.
E06-3007.txt,145,The following is the representation of an edge when the parsing proceeds to the stage where some non head daughter_comma_ in this representation Dtri_comma_ has been parsed_comma_ and Dtrj is to be searched for.
E06-3007.txt,146,Mum Dtr1 Dtr2...Dtri Dtrj...Dtrn WOCs When Dtrj is found_comma_ the parser does not immediately move the dot.
E06-3007.txt,147,At this point the WOC compliance check with the relevant WOC constraint the one s involving Dtri and Dtrj is conducted on these two daughters.
E06-3007.txt,148,The compliance check is a simple list operation.
E06-3007.txt,149,It picks the bitmasks of the two daughters in question and checks whether the occupied positions of one daughter precede are adjacent to those of the other.
E06-3007.txt,150,The failure of this check would prevent the dot move from taking place.
E06-3007.txt,151,Thus_comma_ edges that violate the word order constraints would not be created_comma_ thereby preventing wasteful search.
E06-3007.txt,152,This is the same feature as Daniels and Meurers _comma_ and therefore the efficiency in terms of the number of edges is identical.
E06-3007.txt,153,The main difference is that we use 29 the information inside the feature structure without having media like positive negative masks.
E06-3007.txt,154,3.2 Implementation I have implemented the algorithm in Prolog and coded the HPSG feature structure in the way described using ProFIT Erbach_comma_ 1995 .
E06-3007.txt,155,It is a headcorner_comma_ bottom up chart parser_comma_ roughly based on Gazdar and Mellish 1989 .
E06-3007.txt,156,The main modification consists of introducing bitmasks and the word order checking procedure described above.
E06-3007.txt,157,I created small grammars for Japanese and German and put them to the parser_comma_ to confirm that linearisation heavy constructions such as object control construction can be successfully parsed_comma_ with the WOC constraints enforced.
E06-3007.txt,158,4 Future Tasks What we have seen is an outline of my initial proposal and there are numerous tasks yet to be tackled.
E06-3007.txt,159,First of all_comma_ now that the constraints are written in individual lexical items_comma_ we are in need of appropriate typing in terms of word order constraints_comma_ in order to be able to state succinctly general constraints such as the head final initial constraint.
E06-3007.txt,160,In other words_comma_ it is crucial to devise an appropriate type hierarchy.
E06-3007.txt,161,Another potential problem concerns the generality of our theoretical framework.
E06-3007.txt,162,I have focused on the Head Argument structure in this paper_comma_ but if the present theory were to be of general use_comma_ non argument constructions_comma_ such as the Head Modifier structure_comma_ must be accounted for.
E06-3007.txt,163,Also_comma_ the cases where the head of a phrase is itself a phrase may pose a challenge_comma_ if such a phrasal head were to determine the word order of its projection.
E06-3007.txt,164,Since it is desirable for computational transparencynot touse emergentconstraints_comma_Iwill attempt to get all the word order constraints ultimately propagated and monotonically inherited from the lexical level.
E06-3007.txt,165,Though some word order constraints may turn out to have to be written into the phrasal head directly_comma_ I am confident that the majority_comma_ if not all_comma_ of the constraints can be stated in the lexicon.
E06-3007.txt,166,These issues are tackled in a separate paper Sato_comma_ forthcoming a .
E06-3007.txt,167,In terms of efficiency_comma_ more study has to be requiredto identify the exactcomplexity of myalgorithm.
E06-3007.txt,168,Also_comma_ with a view to using it for a practical system_comma_ an evaluation of the efficiency on the actual machine will be crucial. .
E06-3009.txt,1,recognition is generally approached with complex algorithms that rely heavily on the manual annotation of training and test data. This paper will relieve this complexity in two ways.
E06-3009.txt,2,First_comma_ it will show that the results of the current learning algorithms can be replicated by the lazy algorithm of Memory Based Learning.
E06-3009.txt,3,This approach simply stores all training instances to its memory and classifies a test instance by comparing it to all training examples.
E06-3009.txt,4,Second_comma_ this paper will argue that the number of labelled training examples that is currently used in the literature can be reduced drastically.
E06-3009.txt,5,This finding can help relieve the knowledge acquisition bottleneck in metonymy recognition_comma_ and allow the algorithms to be applied on a wider scale.
E06-3009.txt,7,Metonymy is a figure of speech that uses one entity to refer to another that is related to it Lakoff and Johnson_comma_ 1980_comma_ p.35 .
E06-3009.txt,8,In example 1 _comma_ for instance_comma_ China and Taiwan stand for the governments of the respective countries 1 China has always threatened to use force if Taiwan declared independence.
E06-3009.txt,9,BNC Metonymy resolution is the task of automatically recognizing these words and determining their referent.
E06-3009.txt,10,It is therefore generally split up into two phases metonymy recognition and metonymy interpretation Fass_comma_ 1997 .
E06-3009.txt,11,The earliest approaches to metonymy recognition identify a word as metonymical when it violates selectional restrictions Pustejovsky_comma_ 1995 .
E06-3009.txt,12,Indeed_comma_ in example 1 _comma_ China and Taiwan both violate the restriction that threaten and declare require an animate subject_comma_ and thus have to be interpreted metonymically.
E06-3009.txt,13,However_comma_ it is clear that many metonymies escape this characterization.
E06-3009.txt,14,Nixon in example 2 does not violate the selectional restrictions of the verb to bomb_comma_ and yet_comma_ it metonymically refers to the army under Nixon s command.
E06-3009.txt,15,2 Nixon bombed Hanoi.
E06-3009.txt,16,This example shows that metonymy recognition should not be based on rigid rules_comma_ but rather on statistical information about the semantic and grammatical context in which the target word occurs.
E06-3009.txt,17,This statistical dependency between the reading of a word and its grammatical and semantic context was investigated by Markert and Nissim 2002a and Nissim and Markert 2003 2005 .
E06-3009.txt,18,The key to their approach was the insightthatmetonymyrecognition isbasically asubproblem of Word Sense Disambiguation WSD .
E06-3009.txt,19,Possibly metonymical words are polysemous_comma_ and they generally belong to one of a number of predefinedmetonymical categories.
E06-3009.txt,20,Hence_comma_ like WSD_comma_ metonymy recognition boils down to the automatic assignment of a sense label to a polysemous word.
E06-3009.txt,21,This insight thus implied that all machine learning approaches to WSD can also be applied to metonymy recognition.
E06-3009.txt,22,There are_comma_ however_comma_ two differences between metonymy recognition and WSD.
E06-3009.txt,23,First_comma_ theoretically speaking_comma_ the set of possible readings of a metonymical word is open ended Nunberg_comma_ 1978 .
E06-3009.txt,24,In practice_comma_ however_comma_ metonymies tend to stick to a small number of patterns_comma_ and their labels can thus be defined a priori.
E06-3009.txt,25,Second_comma_ classic 71 WSD algorithms take training instances of one particular word as their input and then disambiguate test instances of the same word.
E06-3009.txt,26,By contrast_comma_ since all words of the same semantic class may undergo the same metonymical shifts_comma_ metonymy recognition systems can be built for an entire semantic class instead of one particular word Markert and Nissim_comma_ 2002a .
E06-3009.txt,27,To this goal_comma_ Markert and Nissim extracted from the BNC a corpus of possibly metonymical words from two categories country names Markert and Nissim_comma_ 2002b and organization names Nissim and Markert_comma_ 2005 .
E06-3009.txt,28,All these words were annotated with a semantic label either literal or the metonymical category they belonged to.
E06-3009.txt,29,For the country names_comma_ Markert and Nissim distinguished between place for people_comma_ place for event and place for product.
E06-3009.txt,30,For the organization names_comma_ the most frequent metonymies are organization for members and organization for product.
E06-3009.txt,31,In addition_comma_ Markert and Nissim used a label mixed for examples that had two readings_comma_ and othermet for examples that did not belong to any of the pre defined metonymical patterns.
E06-3009.txt,32,For both categories_comma_ the results were promising.
E06-3009.txt,33,The best algorithms returned an accuracy of 87 for the countries and of 76 for the organizations.
E06-3009.txt,34,Grammatical features_comma_ which gave the function of a possibly metonymical word and its head_comma_ proved indispensable for the accurate recognition of metonymies_comma_ but led to extremely low recall values_comma_ due to data sparseness.
E06-3009.txt,35,Therefore Nissim and Markert 2003 developed an algorithm that also relied on semantic information_comma_ and tested it on the mixed country data.
E06-3009.txt,36,This algorithm used Dekang Lin s 1998 thesaurus of semantically similar words in order to search the training data for instances whose head was similar_comma_ and not just identical_comma_ to the test instances.
E06-3009.txt,37,Nissim and Markert 2003 showed that a combination of semantic and grammatical information gave the most promising results 87 .
E06-3009.txt,38,However_comma_ Nissim and Markert s 2003 approach has two major disadvantages.
E06-3009.txt,39,The first of these is its complexity the best performing algorithm requires smoothing_comma_ backing off to grammatical roles_comma_ iterative searches through clusters of semantically similar words_comma_ etc. In section 2_comma_ Iwill therefore investigate ifametonymy recognition algorithm needs to be that computationally demanding.
E06-3009.txt,40,In particular_comma_ I will try and replicate Nissim and Markert s results with the lazy algorithm of Memory Based Learning.
E06-3009.txt,41,The second disadvantage of Nissim and Markert s 2003 algorithms is their supervised nature.
E06-3009.txt,42,Because they rely so heavily on the manual annotation of training and test data_comma_ an extension of the classifiers to more metonymical patterns is extremely problematic.
E06-3009.txt,43,Yet_comma_ such an extension is essential for many tasks throughout the field of Natural Language Processing_comma_ particularly Machine Translation.
E06-3009.txt,44,This knowledge acquisition bottleneck is a well known problem in NLP_comma_ and many approaches havebeendeveloped toaddress it.
E06-3009.txt,45,One of these is active learning_comma_ or sample selection_comma_ a strategy that makes it possible to selectively annotate those examples that are most helpful to the classifier.
E06-3009.txt,46,It has previously been applied to NLP tasks such as parsing Hwa_comma_ 2002 Osborne and Baldridge_comma_ 2004 and Word Sense Disambiguation Fujii et al._comma_ 1998 .
E06-3009.txt,47,In section 3_comma_ I will introduce active learning into the field of metonymy recognition.
E06-3009.txt,48,2 Example based metonymy recognition As I have argued_comma_ Nissim and Markert s 2003 approach to metonymy recognition is quite complex.
E06-3009.txt,49,I therefore wanted to see if this complexity can be dispensed with_comma_ and if it can be replaced with the much more simple algorithm of MemoryBased Learning.
E06-3009.txt,50,The advantages of MemoryBased Learning MBL _comma_ which is implemented in the TiMBL classifier Daelemans et al._comma_ 2004 1_comma_ are twofold.
E06-3009.txt,51,First_comma_ it is based on a plausible psychological hypothesis of human learning.
E06-3009.txt,52,It holds that people interpret new examples of a phenomenon by comparing them to stored representations of earlier experiences Daelemans et al._comma_ 2004_comma_ p.19 .
E06-3009.txt,53,This contrasts to many other classification algorithms_comma_ such as Naive Bayes_comma_ whose psychological validity is an object of heavy debate.
E06-3009.txt,54,Second_comma_ as a result of this learning hypothesis_comma_ an MBL classifier such as TiMBL eschews the formulation of complex rules or the computation of probabilities during its training phase.
E06-3009.txt,55,Instead itstores alltraining vectors toits memory_comma_ together with their labels.
E06-3009.txt,56,In the test phase_comma_ it computes the distance between thetest vector and allthese train1This software package is freely available and can be downloaded from http ilk.uvt.nl software.html.
E06-3009.txt,57,72 ing vectors_comma_ and simply returns the most frequent label of the most similar training examples.
E06-3009.txt,58,One of the most important challenges in Memory Based Learningisadapting thealgorithm to one s data.
E06-3009.txt,59,This includes finding a representative seed set as well as determining the right distance measures.
E06-3009.txt,60,For my purposes_comma_ however_comma_ TiMBL s default settings proved more than satisfactory.
E06-3009.txt,61,TiMBL implements the IB1 and IB2 algorithms that were presented inAhaet al. 1991 _comma_ but adds a broad choice of distance measures.
E06-3009.txt,62,Its default implementation of the IB1 algorithm_comma_ which is called IB1 IG in full Daelemans and Van den Bosch_comma_ 1992 _comma_ proved most successful in my experiments.
E06-3009.txt,63,It computes the distance between two vectors X and Y by adding up the weighted distances between their corresponding feature values xi and yi X_comma_Y  nsummationdisplay i 1 wi xi_comma_yi 3 The most important element in this equation is the weight that is given to each feature.
E06-3009.txt,64,In IB1 IG_comma_ features are weighted by their Gain Ratio equation 4 _comma_ the division of the feature s Information Gain by its split info.
E06-3009.txt,65,Information Gain_comma_ the numerator in equation 4 _comma_ measures how much information it feature i contributes to our knowledge of the correct class label ... by computing the difference in uncertainty i.e. entropy between the situations without and with knowledge of the value of that feature Daelemans et al._comma_ 2004_comma_ p.20 .
E06-3009.txt,66,In order not to overestimate the relevance of features with large numbers of values Daelemans et al._comma_ 2004_comma_ p.21 _comma_ this Information Gain is then divided by the split info_comma_ the entropy of the feature values equation 5 .
E06-3009.txt,67,In the following equations_comma_ C is the set of class labels_comma_ H C is the entropy of that set_comma_ and Vi is the set of values for feature i. wi H C summationtext v Vi P v H C v si i 4 si i  summationdisplay v Vi P v log2P v 5 The IB2 algorithm wasdeveloped alongside IB1 in order to reduce storage requirements Aha et al._comma_ 1991 .
E06-3009.txt,68,It iteratively saves only those instances that are misclassified by IB1.
E06-3009.txt,69,This isbecause these will likely lie close to the decision boundary_comma_ and hence_comma_ be most informative to the classifier.
E06-3009.txt,70,My experiments showed_comma_ however_comma_ that IB2 s best performance lay more than 2 below that of IB1.
E06-3009.txt,71,It will therefore not be treated any further here.
E06-3009.txt,72,2.1 Experiments with grammatical information only In order to see if Memory Based Learning is able to replicate Nissim and Markert s 2003 2005 results_comma_ I used their corpora for a number of experiments.
E06-3009.txt,73,These corpora consist of one set with about 1000 mixed country names_comma_ another with 1000 occurrences of Hungary_comma_ and a final set with about 1000 mixed organization names.2 Evaluation was performed with ten fold cross validation.
E06-3009.txt,74,The first round of experiments used only grammatical information.
E06-3009.txt,75,The experiments for the location data were similar to Nissim and Markert s 2003 _comma_ and took the following features into account the grammatical function of the word subj_comma_ obj_comma_ iobj_comma_ pp_comma_ gen_comma_ premod_comma_ passive subj_comma_ other  its head the presence of a second head the second head if present .
E06-3009.txt,76,The experiments for the organization names used the same features as Nissim and Markert 2005  the grammatical function of the word its head its type of determiner if present def_comma_ indef_comma_ bare_comma_ demonst_comma_ other  its grammatical number sing_comma_ plural  its number of grammatical roles if present .
E06-3009.txt,77,The number of words in the organization name_comma_ which Nissim and Markert used as a sixth and final feature_comma_ led to slightly worse results in my experiments and was therefore dropped.
E06-3009.txt,78,The results of these first experiments clearly beat the baselines of 79.7 countries and 63.4 organizations .
E06-3009.txt,79,Moreover_comma_ despite its extremely 2This data is publicly available and can be downloaded from http homepages.inf.ed.ac.uk mnissim mascara.
E06-3009.txt,80,73 Acc P R F TiMBL 86.6 80.2 49.5 61.2 N M 87.0 81.4 51.0 62.7 Table 1 Results for the mixed country data.
E06-3009.txt,81,TiMBL my TiMBL results N M Nissim and Markert s 2003 results simple learning phase_comma_ TiMBL is able to replicate the results from Nissim and Markert 2003 2005 .
E06-3009.txt,82,As table 1 shows_comma_ accuracy for the mixed country data is almost identical to Nissim and Markert s figure_comma_ and precision_comma_ recall and F score for the metonymical class lie only slightly lower.3 TiMBL s results for the Hungary data were similar_comma_ and equally comparable to Markert and Nissim s Katja Markert_comma_ personal communication .
E06-3009.txt,83,Note_comma_ moreover_comma_ that these results were reached with grammatical information only_comma_ whereas Nissim and Markert s 2003 algorithm relied on semantics as well.
E06-3009.txt,84,Next_comma_ table 2 indicates that TiMBL s accuracy forthemixedorganization dataliesabout1.5 below Nissim and Markert s 2005 figure.
E06-3009.txt,85,This result should be treated with caution_comma_ however.
E06-3009.txt,86,First_comma_ Nissim and Markert s available organization data had not yet been annotated for grammatical features_comma_ and my annotation may slightly differ from theirs.
E06-3009.txt,87,Second_comma_ Nissim and Markert used several feature vectors for instances with more than one grammatical role and filtered all mixed instances from thetraining set.
E06-3009.txt,88,Atestinstance wastreated as mixedonly when its several feature vectors were classified differently.
E06-3009.txt,89,My experiments_comma_ in contrast_comma_ were similar to those for the location data_comma_ in that each instance corresponded to one vector.
E06-3009.txt,90,Hence_comma_ the slightly lower performance of TiMBL is probably due to differences between the two experiments.
E06-3009.txt,91,These first experiments thus demonstrate that Memory Based Learning can give state of the art performance in metonymy recognition.
E06-3009.txt,92,In this respect_comma_ it is important to stress that the results for the country data were reached without any semantic information_comma_ whereas Nissim and Markert s 2003 algorithm used Dekang Lin s 1998 clusters of semantically similar words in order to deal with data sparseness.
E06-3009.txt,93,This fact_comma_ together 3Precision_comma_ recall and F score are given for the metonymical class only_comma_ since this isthe category that metonymy recognition is concerned with.
E06-3009.txt,94,Acc P R F TiMBL 74.63 78.65 55.53 65.10 N M 76.0  Table 2 Results for the mixed organization data.
E06-3009.txt,95,TiMBL my TiMBL results N M Nissim and Markert s 2005 results with the psychological plausibility and the simple learning phase_comma_ adds tothe attractivity ofMemoryBased Learning.
E06-3009.txt,96,2.2 Experiments with semantic and grammatical information It is still intuitively true_comma_ however_comma_ that the interpretation of a possibly metonymical word depends mainly on the semantics of its head.
E06-3009.txt,97,The question is if this information is still able to improve the classifier s performance.
E06-3009.txt,98,Itherefore performed a second round of experiments with the location data_comma_ in which I also made use of semantic information.
E06-3009.txt,99,In this round_comma_ I extracted the hypernym synsets of the head s first sense from WordNet.
E06-3009.txt,100,WordNet s hierarchy of synsets makes it possible to quantify the semantic relatedness of two words the more hypernyms two words share_comma_ the more closely related they are.
E06-3009.txt,101,I therefore used the ten highest hypernyms of the first head as features 5 to 14.
E06-3009.txt,102,For those heads with fewer than ten hypernyms_comma_ a copy of their lowest hypernym filled the empty features.
E06-3009.txt,103,As a result_comma_ TiMBL would first look for training instances with ten identical hypernyms_comma_ then with nine_comma_ etc. It would thus comparethetestexampletothesemantically mostsimilar training examples.
E06-3009.txt,104,However_comma_ TiMBL did not perform better with this semantic information.
E06-3009.txt,105,Although F scores for the metonymical category went up slightly_comma_ the system s accuracy hardly changed.
E06-3009.txt,106,This result was not due to the automatic selection of the first most frequent WordNet sense.
E06-3009.txt,107,By manually disambiguating all the heads in the training and test set of the country data_comma_ I observed that this first sense was indeed often incorrect_comma_ but that choosing the correct sense did not lead to a more robust system.
E06-3009.txt,108,Clearly_comma_ the classifier did not benefit from WordNet information as Nissim and Markert s 2003 did from Lin s 1998 thesaurus.
E06-3009.txt,109,The learning curves for the country set allow us to compare the two types of feature vectors 74 Figure 1 Accuracy learning curves for the mixed country data with and without semantic information. in more detail.4 As figure 1 indicates_comma_ with respect to overall accuracy_comma_ semantic features have a negative influence the learning curve with both features climbs much more slowly than that with only grammatical features.
E06-3009.txt,110,Hence_comma_ contrary to my expectations_comma_ grammatical features seem to allow a better generalization from a limited number of training instances.
E06-3009.txt,111,With respect to the F score on the metonymical category in figure 2_comma_ the differences are much less outspoken.
E06-3009.txt,112,Both features give similar learning curves_comma_ but semantic features lead to a higher final F score.
E06-3009.txt,113,In particular_comma_ the use of semantic features results in a lower precision figure_comma_ but a higher recall score.
E06-3009.txt,114,Semantic features thus cause the classifier to slightly overgeneralize from the metonymic training examples.
E06-3009.txt,115,There are two possible reasons for this inability of semantic information to improve the classifier s performance.
E06-3009.txt,116,First_comma_ WordNet s synsets do not always map well to one of our semantic labels many are rather broad and allow for several readings of the target word_comma_ while others are too specific to make generalization possible.
E06-3009.txt,117,Second_comma_ there is the predominance of prepositional phrases in our data.
E06-3009.txt,118,With their closed set of heads_comma_ the number of examples that benefits from semantic information about its head is actually rather small.
E06-3009.txt,119,Nevertheless_comma_ my first round of experiments has indicated that Memory Based Learning is a simple but robust approach to metonymy recognition.
E06-3009.txt,120,It is able to replace current approaches that needsmoothing oriterative searches through athesaurus_comma_ with a simple_comma_ distance based algorithm.
E06-3009.txt,121,4These curves were obtained by averaging the results of 10 experiments.
E06-3009.txt,122,They show performance on a test set of 40 of the data_comma_ with the other 60 as training data.
E06-3009.txt,123,Figure 2 F score learning curves for the mixed country data with and without semantic information.
E06-3009.txt,124,Moreover_comma_ in contrast to some other successful classifiers_comma_ it incorporates a plausible hypothesis of human learning.
E06-3009.txt,125,3 Distance based sample selection Theprevious section has shownthat asimple algorithm that compares test examples to stored training instances is able to produce state of the art results in the field of metonymy recognition.
E06-3009.txt,126,This leads to the question of how many examples we actually need to arrive at this performance.
E06-3009.txt,127,After all_comma_ the supervised approach that we explored requires the careful manual annotation of a large number of training instances.
E06-3009.txt,128,This knowledge acquisition bottleneck compromises the extrapolation of this approach to a large number of semantic classes and metonymical patterns.
E06-3009.txt,129,This section will therefore investigate if it is possible to automatically choose informative examples_comma_ sothatannotation effort can be reduced drastically.
E06-3009.txt,130,For this round of experiments_comma_ two small changes were made.
E06-3009.txt,131,First_comma_ since we are focusing on metonymy recognition_comma_ I replaced all specific metonymical labels with the label met_comma_ so that only three labels remain lit_comma_ met and mixed.
E06-3009.txt,132,Second_comma_ whereas the results in theprevious section were obtained with ten fold cross validation_comma_ I ran these experiments with a training and a test set.
E06-3009.txt,133,On each run_comma_ I used a random 60 of the data for training 40 was set aside for testing.
E06-3009.txt,134,All curves give the average of twenty test runs that use grammatical information only.
E06-3009.txt,135,In general_comma_ sample selection proceeds on the basis of the confidence that the classifier has in its classification.
E06-3009.txt,136,Commonly used metrics are the probability of the most likely label_comma_ or the entropy 75 Figure 3 Accuracy learning curves for the country data with random and maximum distance selection of training examples. over all possible labels.
E06-3009.txt,137,The algorithm then picks those instances with the lowest confidence_comma_ since these will contain valuable information about the training set and hopefully also the test set that is still unknown to the system.
E06-3009.txt,138,One problem with Memory Based Learning algorithms is that they do not directly output probabilities.
E06-3009.txt,139,Since they are example based_comma_ they can only give the distances between the unlabelled instance and all labelled training instances.
E06-3009.txt,140,Nevertheless_comma_ these distances can be used as a measure of certainty_comma_ too we can assume that the system is most certain about the classification of test instances that lie very close to one or more of its training instances_comma_ and less certain about those that are further away.
E06-3009.txt,141,Therefore the selection function that minimizes the probability of the most likely label can intuitively be replaced by one that maximizes the distance from the labelled training instances.
E06-3009.txt,142,However_comma_ figure 3 shows that for the mixed country instances_comma_ this function is not an option.
E06-3009.txt,143,Both learning curves give the results of an algorithm that starts with fifty random instances_comma_ and then iteratively adds ten new training instances to this initial seed set.
E06-3009.txt,144,Thealgorithm behind the solid curve chooses these instances randomly_comma_ whereas the one behind the dotted line selects those that are most distant from the labelled training examples.
E06-3009.txt,145,In the first half of the learning process_comma_ both functions are equally successful in the second the distance based function performs better_comma_ but only slightly so.
E06-3009.txt,146,There are two reasons for this bad initial performance of the active learning function.
E06-3009.txt,147,First_comma_ it is not able to distinguish between informative and Figure 4 Accuracy learning curves for the country data with random and maximum minimumdistance selection of training examples. unusual training instances.
E06-3009.txt,148,This is because a large distance from the seed set simply means that the particular instance s feature values are relatively unknown.
E06-3009.txt,149,This does not necessarily imply that the instance is informative to the classifier_comma_ however.
E06-3009.txt,150,After all_comma_ it may be so unusual and so badly representative of the training and test set that the algorithm had better exclude it something that is impossible on the basis of distances only.
E06-3009.txt,151,This biastowards outliers isawell known disadvantage of many simple active learning algorithms.
E06-3009.txt,152,A second type of bias is due to the fact that the data has beenannotated withafewfeatures only.
E06-3009.txt,153,Moreparticularly_comma_ the present algorithm will keep adding instances whose head is not yet represented in the training set.
E06-3009.txt,154,This entails that it will put off adding instances whose function is pp_comma_ simply because other functions subj_comma_ gen_comma_ ... have a wider variety in heads.
E06-3009.txt,155,Again_comma_ the result is a labelled set that is not very representative of the entire training set.
E06-3009.txt,156,There are_comma_ however_comma_ a few easy ways to increase the number of prototypical examples in the training set.
E06-3009.txt,157,In a second run of experiments_comma_ I used an active learning function that added not only those instances that were most distant from the labelled training set_comma_ but also those that were closest to it.
E06-3009.txt,158,After a few test runs_comma_ I decided to add six distant andfourcloseinstances oneachiteration.
E06-3009.txt,159,Figure4 showsthatsuch afunction isindeed fairly successful.
E06-3009.txt,160,Because it builds a labelled training set that is more representative of the test set_comma_ this algorithm clearly reduces the number of annotated instances that is needed to reach a given performance.
E06-3009.txt,161,Despite its success_comma_ this function is obviously notyet asophisticated wayof selecting good train76 Figure 5 Accuracy learning curves for the organization data with random and distance based AL selection of training examples with a random seed set. ing examples.
E06-3009.txt,162,The selection of the initial seed set in particular can be improved upon ideally_comma_ this seed set should take into account the overall distribution of the training examples.
E06-3009.txt,163,Currently_comma_ the seeds are chosen randomly.
E06-3009.txt,164,This flaw in the algorithm becomes clear if it is applied to another data set figure 5 shows that it does not outperform random selection on the organization data_comma_ for instance.
E06-3009.txt,165,As I suggested_comma_ the selection of prototypical or representative instances as seeds can be used to make the present algorithm more robust.
E06-3009.txt,166,Again_comma_ it is possible to use distance measures to do this before the selection of seed instances_comma_ the algorithm can calculate for each unlabelled instance its distance from each of the other unlabelled instances.
E06-3009.txt,167,In this way_comma_ it can build a prototypical seed set by selecting those instances with the smallest distance on average.
E06-3009.txt,168,Figure 6 indicates that such an algorithm indeed outperforms random sample selection on the mixed organization data.
E06-3009.txt,169,For the calculation of the initial distances_comma_ each feature received the same weight.
E06-3009.txt,170,The algorithm then selected 50 random samples from the most prototypical half of the training set.5 The other settings were the same as above.
E06-3009.txt,171,With thepresent small number of features_comma_ however_comma_ such a prototypical seed set is not yet always as advantageous as it could be.
E06-3009.txt,172,A few experiments indicated that it did not lead to better performance on the mixed country data_comma_ for instance.
E06-3009.txt,173,However_comma_ as soon as a wider variety of features is taken into account as with the organization data _comma_ the advan5Of course_comma_ the random algorithm in figure 6 still randomly selected its seeds from the entire training set.
E06-3009.txt,174,Figure 6 Accuracy learning curves for the organization data with random and distance based AL selection of training examples with a prototypical seed set. tages of a prototypical seed set will definitely become more obvious.
E06-3009.txt,175,In conclusion_comma_ it has become clear that a careful selection of training instances may considerably reduce annotation effort in metonymy recognition.
E06-3009.txt,176,Functions that construct a prototypical seed set and then use MBL s distance measures to select informative as well as typical samples are extremely promising in this respect and can already considerably reduce annotation effort.
E06-3009.txt,177,In order to reach an accuracy of 85 on the country data_comma_ for instance_comma_ the active learning algorithm above needs 44 fewertraining instances thanitsrandom competitor on average .
E06-3009.txt,178,On the organisation data_comma_ reduction is typically around 30 .
E06-3009.txt,179,These relatively simple algorithms thus constitute a good basis for the future development of robust active learning techniques for metonymy recognition.
E06-3009.txt,180,I believe in particular that research in this field should go hand in hand with an investigation of new informative features_comma_ since the present_comma_ limited feature set does not yet allow us to measure the classifier s confidence reliably.
E06-3009.txt,181,4 Conclusions and future work Inthis paper Ihave explored anexample based approach to metonymy recognition.
E06-3009.txt,182,Memory Based Learning does away with the complexity of current supervised metonymy recognition algorithms.
E06-3009.txt,183,Even without semantic information_comma_ it is able to give state of the art results similar to those in the literature.
E06-3009.txt,184,Moreover_comma_ not only is the complexity of current learning algorithms unnecessary the number of labelled training instances can be reduced drastically_comma_ too.
E06-3009.txt,185,I have argued that selective sam77 pling can help choose those instances that aremost helpful to the classifier.
E06-3009.txt,186,A few distance based algorithms were able to drastically reduce the number of training instances that is needed for a given accuracy_comma_ both forthecountry and the organization names.
E06-3009.txt,187,If current metonymy recognition algorithms are to be used in a system that can recognize all possible metonymical patterns across a broad variety of semantic classes_comma_ it is crucial that the required number of labelled training examples be reduced.
E06-3009.txt,188,This paper has taken the first steps along this path and has set out some interesting questions for future research.
E06-3009.txt,189,This research should include the investigation of new features that can make classifiers more robust and allow us to measure their confidence more reliably.
E06-3009.txt,190,This confidence measurement can then also be used in semi supervised learning algorithms_comma_ for instance_comma_ where the classifier itself labels the majority of training examples.
E06-3009.txt,191,Only with techniques such as selective sampling and semi supervised learning can the knowledge acquisition bottleneck in metonymy recognition be addressed.
E06-3009.txt,192,Acknowledgements I would like to thank Mirella Lapata_comma_ Dirk Geeraerts and Dirk Speelman for their feedback on this project.
E06-3009.txt,193,I am also very grateful to Katja Markert and Malvina Nissim for their helpful information about their research. .
E06-3006.txt,1,paper addresses the problem of automatically retrieving answers for how to questions_comma_ focusing on those that inquire about the procedure for achieving a specific goal. For such questions_comma_ typical information retrieval methods_comma_ based on key word matching_comma_ are better suited to detecting the content of the goal e.g._comma_ installing a Windows XP server than the general nature of the desired information i.e._comma_ procedural_comma_ a series of steps for achieving this goal .
E06-3006.txt,2,We suggest dividing the process of retrieving answers for such questions into two stages_comma_ with each stage focusing on modeling one aspect of a how to question.
E06-3006.txt,3,We compare the two stage approach with two alternative approaches a baseline approach that only uses the content of the goal to retrieve relevant documents and another approach that explores the potential of automatic query expansion.
E06-3006.txt,4,The result of the experiment shows that the two stage approach significantly outperforms the baseline but achieves similar result with the systems using automatic query expansion techniques.
E06-3006.txt,5,We analyze the reason and also present some future work.
E06-3006.txt,7,How To questions constitute a large proportion of questions on the Web.
E06-3006.txt,8,Many how to questions inquire about the procedure for achieving a specific goal.
E06-3006.txt,9,For such questions_comma_ typical information retrieval IR methods_comma_ based on key word matching_comma_ are better suited to detecting the content of the goal e.g._comma_ installing a Windows XP server than the general nature of the desired information i.e._comma_ procedural_comma_ a series of steps for achieving this goal .
E06-3006.txt,10,The reasons are given as below.
E06-3006.txt,11,First_comma_ documents that describe a procedure often do not contain the word procedure itself_comma_ but we are able to .
E06-1029.txt,1,propose a method for compiling bilingual terminologies of multi word terms MWTs for given translation pairs of seed terms. Traditional methods for bilingual terminology compilation exploit parallel texts_comma_ while the more recent ones have focused on comparable corpora.
E06-1029.txt,2,We use bilingual corpora collected from the web and tailor made for the seed terms.
E06-1029.txt,3,For each language_comma_ we extract from the corpus a set of MWTs pertaining to the seed s semantic domain_comma_ and use a compositional method to align MWTs from both sets.
E06-1029.txt,4,We increase the coverage of our system by using thesauri and by applying a bootstrap method.
E06-1029.txt,5,Experimental results show high precision and indicate promising prospects for future developments.
E06-1029.txt,7,Bilingual terminologies have been the center of much interest in computational linguistics.
E06-1029.txt,8,Their applications in machine translation have proven quite effective_comma_ and this has fuelled research aiming at automating terminology compilation.
E06-1029.txt,9,Early developments focused on their extraction from parallel corpora Daille et al. 1994 _comma_ Fung 1995 _comma_ which works well but is limited by the scarcity of such resources.
E06-1029.txt,10,Recently_comma_ the focus has changed to utilizing comparable corpora_comma_ which are easier to obtain in many domains.
E06-1029.txt,11,Most of the proposed methods use the fact that words have comparable contexts across languages.
E06-1029.txt,12,Fung 1998 and Rapp 1999 use so called context vector methods to extract translations of general words.
E06-1029.txt,13,Chiao and Zweigenbaum 2002 and D jean and Gaussier 2002 apply similar methods to technical domains.
E06-1029.txt,14,Daille and Morin 2005 use specialized comparable corpora to extract translations of multi word terms MWTs .
E06-1029.txt,15,These methods output a few thousand terms and yield a precision of more or less 80 on the first 10 20 candidates.
E06-1029.txt,16,We argue for the need for systems that output fewer terms_comma_ but with a higher precision.
E06-1029.txt,17,Moreover_comma_ all the above were conducted on language pairs including English.
E06-1029.txt,18,It would be possible_comma_ albeit more difficult_comma_ to obtain comparable corpora for pairs such as French Japanese.
E06-1029.txt,19,We will try to remove the need to gather corpora beforehand altogether.
E06-1029.txt,20,To achieve this_comma_ we use the web as our only source of data.
E06-1029.txt,21,This idea is not new_comma_ and has already been tried by Cao and Li 2002 for base noun phrase translation.
E06-1029.txt,22,They use a compositional method to generate a set of translation candidates from which they select the most likely translation by using empirical evidence from the web.
E06-1029.txt,23,The method we propose takes a translation pair of seed terms in input.
E06-1029.txt,24,First_comma_ we collect MWTs semantically similar to the seed in each language.
E06-1029.txt,25,Then_comma_ we work out the alignments between the MWTs in both sets.
E06-1029.txt,26,Our intuition is that both seeds have the same related terms across languages_comma_ and we believe that this will simplify the alignment process.
E06-1029.txt,27,The alignment is done by generating a set of translation candidates using a compositional method_comma_ and by selecting the most probable translation from that set.
E06-1029.txt,28,It is very similar to Cao and Li s_comma_ except in two respects.
E06-1029.txt,29,First_comma_ the generation makes use of thesauri to account for lexical divergence between MWTs in the source and target language.
E06-1029.txt,30,Second_comma_ we validate candidate translations using a set of terms collected from the web_comma_ rather than using empirical evidence from the web as a whole.
E06-1029.txt,31,Our research further differs from Cao and Li s in that they focus only on finding valid translations for given base noun phrases.
E06-1029.txt,32,We attempt to both collect appropriate sets of related MWTs and to find their respective translations.
E06-1029.txt,33,The initial output of the system contains 9.6 pairs on average_comma_ and has a precision of 92 .
E06-1029.txt,34,We use this high precision as a bootstrap to augment the set of Japanese related terms_comma_ and obtain a final output of 19.6 pairs on average_comma_ with a precision of 81 .
E06-1029.txt,35,2 Related Term Collection Given a translation pair of seed terms s f _comma_ s j _comma_ we use a search engine to gather a set F of French terms related to s f _comma_ and a set J of Japanese terms related to s j .
E06-1029.txt,36,The methods applied for both languages use the framework proposed by Sato and Sasaki 2003 _comma_ outlined in Figure 1.
E06-1029.txt,37,We proceed in three steps corpus collection_comma_ automatic term recognition ATR _comma_ and filtering.
E06-1029.txt,38,2.1 Corpus Collection For each language_comma_ we collect a corpus C from web pages by selecting passages that contain the seed.
E06-1029.txt,39,Web page collection In French_comma_ we use Google to find relevant web pages by entering the following three queries s f _comma_ s f est s f is _comma_ and s f sont s f are .
E06-1029.txt,40,In Japanese_comma_ we do the same with queries s j _comma_ s j  _comma_ s j _comma_ s j  _comma_ and s j _comma_ where toha_comma_ ha_comma_  toiu_comma_ and no are Japanese functional words that are often used for defining or explaining a term.
E06-1029.txt,41,We retrieve the top pages for each query_comma_ and parse those pages looking for hyperlinks whose anchor text contain the seed.
E06-1029.txt,42,If such links exist_comma_ we retrieve the linked pages as well.
E06-1029.txt,43,Sentence extraction From the retrieved web pages_comma_ we remove html tags and other noise.
E06-1029.txt,44,Then_comma_ we keep only properly structured sentences containing the seed_comma_ as well as the preceding and following sentences that is_comma_ we use a window of three sentences around the seed.
E06-1029.txt,45,2.2 Automatic Term Recognition The next step is to extract candidate related terms from the corpus.
E06-1029.txt,46,Because the sentences composing the corpus are related to the seed_comma_ the same should be true for the terms they contain.
E06-1029.txt,47,The process of extracting terms is highly language dependent.
E06-1029.txt,48,French ATR We use the C value method Frantzi and Ananiadou 2003 _comma_ which extracts compound terms and ranks them according to their termhood.
E06-1029.txt,49,It consists of a linguistic part_comma_ followed by a statistical part.
E06-1029.txt,50,The linguistic part consists in applying a linguistic filter to constrain the structure of terms extracted.
E06-1029.txt,51,We base our filter on a morphosyntactic pattern for the French language proposed by Daille et al. It defines the structure of multi word units MWUs that are likely to be terms.
E06-1029.txt,52,Although their work focused on MWUs limited to two content words nouns_comma_ adjectives_comma_ verbs or adverbs _comma_ we extend our filter to MWUs of greater length.
E06-1029.txt,53,The pattern is defined as follows    NumNounDetPrepAdjNumNoun
E06-1029.txt,54,The statistical part measures the termhood of each compound that matches the linguistic pattern.
E06-1029.txt,55,It is given by the C value                 otherwise T b aaa nestednotisaif aa a a Tb a P f f f log _comma_ flog valueC 2 2 where a is the candidate string_comma_ f a is its frequency of occurrence in all the web pages retrieved_comma_ T a is the set of extracted candidate terms that contain a_comma_ and P T a is the number of these candidate terms.
E06-1029.txt,56,The nature of our variable length pattern is such that if a long compound matches the pattern_comma_ all the shorter compounds it includes also match.
E06-1029.txt,57,For example_comma_ consider the N Prep N related term sets F_comma_ J the Web ATR Filtering Corpus collection corpora C f _comma_ C j term sets X f _comma_ X j seed terms s f _comma_ s j Figure 1 Related term collection Prep N structure in syst me base de connaissances knowledge based system .
E06-1029.txt,58,The shorter candidate syst me base based system also matches_comma_ although we would prefer not to extract it.
E06-1029.txt,59,Fortunately_comma_ the strength of the C value is the way it effectively handles nested MWTs.
E06-1029.txt,60,When we calculate the termhood of a string_comma_ we subtract from its total frequency its frequency as a substring of longer candidate terms.
E06-1029.txt,61,In other words_comma_ a shorter compound that almost always appears nested in a longer compound will have a comparatively smaller C value_comma_ even if its total frequency is higher than that of the longer compound.
E06-1029.txt,62,Hence_comma_ we discard MWTs whose C value is smaller than that of a longer candidate term in which it is nested.
E06-1029.txt,63,Japanese ATR Because compound nouns represent the bulk of Japanese technical MWTs_comma_ we extract them as candidate related terms.
E06-1029.txt,64,As opposed to Sato and Sasaki_comma_ we ignore single nouns.
E06-1029.txt,65,Also_comma_ we do not limit the number of candidates output by ATR as they did.
E06-1029.txt,66,2.3 Filtering Finally_comma_ from the output set of ATR_comma_ we select only the technical terms that are part of the seed s semantic domain.
E06-1029.txt,67,Numerous measures have been proposed to gauge the semantic similarity between two words van Rijsbergen 1979 .
E06-1029.txt,68,We choose the Jaccard coefficient_comma_ which we calculate based on search engine hit counts.
E06-1029.txt,69,The similarity between a seed term s and a candidate term x is given by  xsH xsH Jac  where H s x is the hit count of pages containing both s and x_comma_ and H s x is the hit count of pages containing s or x.
E06-1029.txt,70,The latter can be calculated as follows   xsHxHsHxsH   Candidates that have a high enough coefficient are considered related terms of the seed. 3 Term Alignment Once we have collected related terms in both French and Japanese_comma_ we must link the terms in the source language to the terms in the target language. Our alignment procedure is twofold. First_comma_ we first generate Japanese translation candidates for each collected French term. Second_comma_ we select the most likely translation s from the set of candidates.
E06-1029.txt,71,This is similar to the generation and selection procedures used in the literature Baldwin and Tanaka 2004 _comma_ Cao and Li_comma_ Langkilde and Knight 1998 .
E06-1029.txt,72,3.1 Translation Candidates Generation Translation candidates are generated using a compositional method_comma_ which can be divided in three steps.
E06-1029.txt,73,First_comma_ we decompose the French MWTs into combinations of shorter MWU elements.
E06-1029.txt,74,Second_comma_ we look up the elements in bilingual dictionaries.
E06-1029.txt,75,Third_comma_ we recompose translation candidates by generating different combinations of translated elements.
E06-1029.txt,76,Decomposition In accordance with Daille et al._comma_ we define the length of a MWU as the number of content words it contains.
E06-1029.txt,77,Let n be the length of the MWT to decompose.
E06-1029.txt,78,We produce all the combinations of MWU elements of length less or equal to n.
E06-1029.txt,79,For example_comma_ consider the French translation of knowledge based system It has a length of three and yields the following four combinations 1 Note the treatment given to the prepositions and determiners we leave them in place when they are interposed between content words within elements_comma_ otherwise we remove them.
E06-1029.txt,80,Dictionary Lookup We look up each element in bilingual dictionaries.
E06-1029.txt,81,Because some words appear in their inflected forms_comma_ we use their lemmata.
E06-1029.txt,82,In the example given above_comma_ we look up connaissance lemma rather than connaissances inflected .
E06-1029.txt,83,Note that we do not lemmatize MWUs such as base de connaissances.
E06-1029.txt,84,This is due to the complexity of gender and number agreements of French compounds.
E06-1029.txt,85,However_comma_ only a small part of the MWTs are collected in their inflected forms_comma_ and French Japanese bilingual dictionaries do not contain that many MWTs to begin with.
E06-1029.txt,86,The performance hit should therefore be minor.
E06-1029.txt,87,Already at this stage_comma_ we can anticipate problems arising from the insufficient coverage of 1 A MWT of length n produces 2 n 1 combinations_comma_ including itself. syst me base de connaissances Noun Prep Noun Prep Noun syst me base de connaissances syst me base de connaissances syst me base connaissances syst me base connaissances French Japanese lexicon resources.
E06-1029.txt,88,Bilingual dictionaries may not have enough entries_comma_ and existing entries may not include a great variety of translations for every sense.
E06-1029.txt,89,The former problem has no easy solution_comma_ and is one of the reasons we are conducting this research.
E06-1029.txt,90,The latter can be partially remedied by using thesauri we augment each element s translation set by looking up in thesauri all the translations obtained with bilingual dictionaries.
E06-1029.txt,91,Recomposition To recompose the translation candidates_comma_ we simply generate all suitable combinations of translated elements for each decomposition.
E06-1029.txt,92,The word order is inverted to take into account the different constraints in French and Japanese.
E06-1029.txt,93,In the example above_comma_ if the lookup phase gave  chishiki _comma_  dodai_comma_  besu and  t aikei_comma_  shisutemu as respective translation sets for syst me_comma_ base and connaissance_comma_ the fourth decomposition given above would yield the following candidates connaissance base syst me           If we do not find any translation for one of the elements_comma_ the generation fails.
E06-1029.txt,94,3.2 Translation Selection Selection consists of picking the most likely translation from the translation candidates we have generated.
E06-1029.txt,95,To discern the likely from the unlikely_comma_ we use the empirical evidence provided by the set of Japanese terms related to the seed.
E06-1029.txt,96,We believe that if a candidate is present in that set_comma_ it could well be a valid translation_comma_ as the French MWT in consideration is also related to the seed.
E06-1029.txt,97,Accordingly_comma_ our selection process consists of picking those candidates for which we find a complete match among the related terms.
E06-1029.txt,98,3.3 Relevance of Compositional Methods The automatic translation of MWTs is no simple task_comma_ and it is worthwhile asking if it is best tackled with a compositional method.
E06-1029.txt,99,Intricate problems have been reported with the translations of compounds Daille and Morin_comma_ Baldwin and Tanaka _comma_ notably fertility source and target MWTs can be of different lengths.
E06-1029.txt,100,For example_comma_ table de v rit truth table contains two content words and translates into  shinri chi hyo lit. truth value table _comma_ which contains three. variability of forms in the translations MWTs can appear in many forms.
E06-1029.txt,101,For example_comma_ champ electromagn tique electromagnetic field translates both into  denji ba lit. electromagnetic field  denji kai lit. electromagnetic region . constructional variability in the translations source and target MWTs have different morphological structures.
E06-1029.txt,102,For example_comma_ in the pair apprentissage automatique   kikai gakushu machine learning we have NAdj N N .
E06-1029.txt,103,In the pair programmation par contraintes   patan ninshiki pattern recognition we have N par N N N . non compositional compounds some compounds meaning cannot be derived from the meaning of their components.
E06-1029.txt,104,For example_comma_ the Japanese term  aka ten failing grade_comma_ lit. red point translates into French as note d chec lit. failing grade or simply chec lit. failure . lexical divergence source and target MWTs can use different lexica to express a concept.
E06-1029.txt,105,For example_comma_ traduction automatique machine translation_comma_ lit. automatic translation translates as  kikai honyaku lit. machine translation .
E06-1029.txt,106,It is hard to imagine any method that could address all these problems accurately.
E06-1029.txt,107,Tanaka and Baldwin 2003 found that 48.7 of English Japanese Noun Noun compounds translate compositionality.
E06-1029.txt,108,In a preliminary experiment_comma_ we found this to be the case for as much as 75.1 of the collected MWTs.
E06-1029.txt,109,If we are to maximize the coverage of our system_comma_ it is sensible to start with a compositional approach.
E06-1029.txt,110,We will not deal with the problem of fertility and non compositional compounds in this paper.
E06-1029.txt,111,Nonetheless_comma_ lexical divergence and variability issues will be partly tackled by broader translations and related words given by thesauri.
E06-1029.txt,112,4 Evaluation 4.1 Linguistic Resources The bilingual dictionaries used in the experiments are the Crown French Japanese Dictionary Ohtsuki et al. 1989 _comma_ and the French Japanese Scientific Dictionary French Japanese Scientific Association 1989 .
E06-1029.txt,113,The former contains about 50_comma_000 entries of general usage single words.
E06-1029.txt,114,The latter contains about 50_comma_000 entries of both single and multi word scientific terms.
E06-1029.txt,115,These two complement each other_comma_ and by combining both entries we form our base dictionary to which we refer as Dic FJ .
E06-1029.txt,116,The main thesaurus used is Bunrui Goi Hyo National Institute for Japanese Language 2004 .
E06-1029.txt,117,It contains about 96_comma_000 words_comma_ and each entry is organized in two levels a list of synonyms and a list of more loosely related words.
E06-1029.txt,118,We augment the initial translation set by looking up the Japanese words given by Dic FJ .
E06-1029.txt,119,The expanded bilingual dictionary comprised of the words from Dic FJ combined with their synonyms is denoted Dic FJJ .
E06-1029.txt,120,The dictionary resulting of Dic FJJ combined with the more loosely related words is denoted Dic FJJ2 .
E06-1029.txt,121,Finally_comma_ we build another thesaurus from a Japanese English dictionary.
E06-1029.txt,122,We use Eijiro Electronic Dictionary Project 2004 _comma_ which contains 1_comma_290_comma_000 entries.
E06-1029.txt,123,For a given Japanese entry_comma_ we look up its English translations.
E06-1029.txt,124,The Japanese translations of the English intermediaries are used as synonyms related words of the entry.
E06-1029.txt,125,The resulting thesaurus is expected to provide even more loosely related translations and also many irrelevant ones .
E06-1029.txt,126,We denote it Dic FJEJ .
E06-1029.txt,127,4.2 Notation Let F and J be the two sets of related terms collected in French and Japanese.
E06-1029.txt,128,F is the subset of F for which Jac 0.01 01.0  fJacFfF F is the subset of valid related terms in F _comma_ as determined by human evaluation. P is the set of all potential translation pairs among the collected terms P F J .
E06-1029.txt,129,P is the set of pairs containing either a French term or a Japanese term with Jac 0.01  01.0 01.0 _comma_   jJacfJacJjFfP P is the subset of valid translation pairs in P _comma_ determined by human evaluation. These pairs need to respect three criteria 1 contain valid terms_comma_ 2 be related to the seed_comma_ and 3 constitute a valid translation.
E06-1029.txt,130,M is the set of all translations selected by our system.
E06-1029.txt,131,M is the subset of pairs in M with Jac 0.01 for either the French or the Japanese term.
E06-1029.txt,132,It is also the output of our system  01.0 01.0 _comma_   jJacfJacMjfM M is the intersection of M and P _comma_ or in other words_comma_ the subset of valid translation pairs output by our system. 4.3 Baseline Method Our starting point is the simplest possible alignment_comma_ which we refer to as our baseline. It is worked out by using each of the aforementioned dictionaries independently. The output set obtained using Dic FJ is denoted FJ_comma_ the one using Dic FJJ is denoted FJJ_comma_ and so on. The experiment is made using the eight seed pairs given in Table 1. On average_comma_ we have F 74.3_comma_ F 51.0 and P 24.0. Table 2 gives a summary of the key results. The precision and the recall are given by  M M precision _comma_  P M recall Dic FJ contains only Japanese translations corresponding to the strict sense of French elements. Such a dictionary generates only a few translation candidates which tend to be correct when present in the target set. On the other hand_comma_ the lookup in Dic FJJ2 and Dic FJEJ interprets French Set M M Prec. Recall FJ 10.5 9.6 92 40 FJJ 15.3 12.6 83 53 FJJ2 20.5 13.4 65 56 FJEJ 30.9 14.1 46 59 Table 2 Results for the baseline Id French Japanese English 1 analyse vectorielle   bekutoru kaiseki vector analysis 2 circuit logique  ronri kairo logic circuit 3 intelligence artificielle    jinko chinou artificial intelligence 4 linguistique informatique  keisan gengogaku computational linguistics 5 reconnaissance des formes   patan ninshiki pattern recognition 6 reconnaissance vocale  onsei ninshiki speech recognition 7 science cognitive  ninchi kagaku cognitive science 8 traduction automatique  kikai honyaku machine translation Table 1 Seed pairs MWT elements with more laxity_comma_ generating more translations and thus more alignments_comma_ at the cost of some precision.
E06-1029.txt,133,4.4 Incremental Selection The progressive increase in recall given by the increasingly looser translations is in inverse proportion to the decrease in precision_comma_ which hints that we should give precedence to the alignments obtained with the more accurate methods.
E06-1029.txt,134,Consequently_comma_ we start by adding the alignments in FJ to the output set.
E06-1029.txt,135,Then_comma_ we augment it with the alignments from FJJ whose terms are not already in FJ.
E06-1029.txt,136,The resulting set is denoted FJJ .
E06-1029.txt,137,We then augment FJJ with the pairs from FJJ2 whose terms are not in FJJ _comma_ and so on_comma_ until we exhaust the alignments in FJEJ.
E06-1029.txt,138,For instance_comma_ let FJ contain synth se de la parole  onsei gousei speech synthesis and FJJ contain this pair plus synth se de la parole  onsei kaiseki speech analysis .
E06-1029.txt,139,In the first iteration_comma_ the pair in FJ is added to the output set.
E06-1029.txt,140,In the second iteration_comma_ no pair is added because the output set already contains an alignment with synth se de la parole.
E06-1029.txt,141,Table 3 gives the results for each incremental step.
E06-1029.txt,142,We can see an increase in precision for FJJ _comma_ FJJ2 and FJEJ of respectively 5 _comma_ 9 and 8 _comma_ compared to FJJ_comma_ FJJ2 and FJEJ.
E06-1029.txt,143,We are effectively filtering output pairs and_comma_ as expected_comma_ the increase in precision is accompanied by a slight decrease in recall.
E06-1029.txt,144,Note that_comma_ because FJEJ is not a superset of FJJ2_comma_ we see an increase in both precision and recall in FJEJ over FJEJ.
E06-1029.txt,145,Nonetheless_comma_ the precision yielded by FJEJ is not sufficient_comma_ which is why Dic FJEJ is left out in the next experiment.
E06-1029.txt,146,4.5 Bootstrapping The coverage of the system is still shy of the 20 pairs seed objective we gave ourselves.
E06-1029.txt,147,One cause for this is the small number of valid translation pairs available in the corpora.
E06-1029.txt,148,From an average of 51 valid related terms in the source set_comma_ only 24 have their translation in the target set.
E06-1029.txt,149,To counter that problem_comma_ we increase the coverage of Japanese related terms and hope that by doing so_comma_ we will also increase the coverage of the system as a whole.
E06-1029.txt,150,Once again_comma_ we utilize the high precision of the baseline method.
E06-1029.txt,151,The average 10.5 pairs in FJ include 92 of Japanese terms semantically similar to the seed.
E06-1029.txt,152,By inputting these terms in the term collection system_comma_ we collect many more terms_comma_ some of which are probably the translations of our French MWTs.
E06-1029.txt,153,The results for the baseline method with bootstrapping are given in Table 4.
E06-1029.txt,154,The ones using incremental selection and bootstrapping are given in Table 5.
E06-1029.txt,155,FJ consists of the alignments given by a generation process using Dic FJ and a selection performed on the augmented set of related terms.
E06-1029.txt,156,FJJ and FJJ2 are obtained in the same way using Dic FJJ and Dic FJJ2 .
E06-1029.txt,157,FJ  contains the alignments from FJ_comma_ augmented with those from FJ whose terms are not in FJ.
E06-1029.txt,158,FJJ  contains FJ _comma_ incremented with terms from FJJ.
E06-1029.txt,159,FJJ  contains FJJ _comma_ incremented with terms from FJJ _comma_ and so on.
E06-1029.txt,160,The bootstrap mechanism grows the target term set tenfold_comma_ making it very laborious to identify all the valid translation pairs manually.
E06-1029.txt,161,Consequently_comma_ we only evaluate the pairs output by the system_comma_ making it impossible to calculate recall.
E06-1029.txt,162,Instead_comma_ we use the number of valid translation pairs as a makeshift measure.
E06-1029.txt,163,Bootstrapping successfully allows for many more translation pairs to be found.
E06-1029.txt,164,FJ _comma_ FJJ _comma_ and FJJ2 respectively contain 7.6_comma_ 8.7 and 8.5 more valid alignments on average than FJ_comma_ FJJ and FJJ2.
E06-1029.txt,165,The augmented target term set is noisier than the initial set_comma_ and it produces many more invalid alignments as well.
E06-1029.txt,166,Fortunately_comma_ the incremental selection effectively filters out most of the unwanted_comma_ restoring the precision to acceptable levels.
E06-1029.txt,167,Set M M Prec.
E06-1029.txt,168,Recall FJJ 14.0 12.3 88 51 FJJ2 16.1 12.8 79 53 FJEJ 29.1 15.5 53 65 Table 3 Results for the incremental selection Set M M Prec.
E06-1029.txt,169,FJ  19.5 16.1 83 FJJ  22.5 18.6 83 FJJ  24.3 19.6 81 FJJ2  25.6 20.1 79 FJJ2  28.6 20.6 72 Table 5 Results for the incremental selection with bootstrap expansion Set M M Prec.
E06-1029.txt,170,FJ 20.9 16.8 80 FJJ 30.9 21.3 69 FJJ2 45.8 22.6 49 Table 4 Results for the baseline method with bootstrap expansion 4.6 Analysis A comparison of all the methods is illustrated in the precision valid alignments curves of Figure 2.
E06-1029.txt,171,The points on the four curves are taken from Tables 2 to 5.
E06-1029.txt,172,The gap between the dotted and filled curves clearly shows that bootstrapping increases coverage.
E06-1029.txt,173,The respective positions of the squares and crosses show that incremental selection effectively filters out erroneous alignments.
E06-1029.txt,174,FJJ  _comma_ with 19.6 valid alignments and a precision of 81 _comma_ is at the rightmost and uppermost position in the graph.
E06-1029.txt,175,The detailed results for each seed are presented in Table 6_comma_ and the complete output for the seed logic circuit is given in Table 7.
E06-1029.txt,176,From the average 4.7 erroneous pairs seed_comma_ 3.2 68 were correct translations but were judged unrelated to the seed.
E06-1029.txt,177,This is not surprising_comma_ considering that our set of French related terms contained only 69 51 74.3 of valid related terms.
E06-1029.txt,178,Also note that_comma_ of the 24.3 pairs seed output_comma_ 5.25 are listed in the French Japanese Scientific Dictionary.
E06-1029.txt,179,However_comma_ only 3.9 of those pairs are included in M .
E06-1029.txt,180,The others were deemed unrelated to the seed.
E06-1029.txt,181,In the output set of machine translation _comma_   shizen gengo shori natural language processing is aligned to both traitement du language naturel and traitement des langues naturelles.
E06-1029.txt,182,The system captures the term s variability around langue language.
E06-1029.txt,183,Lexical divergence is also taken into account to some extent.
E06-1029.txt,184,The seed computational linguistics yields the alignment of langue maternelle mother tongue with  bokoku go literally mothercountry language .
E06-1029.txt,185,The usage of thesauri enabled the system to include the concept of country in the translated MWT_comma_ even though it is not present in any of the French elements.
E06-1029.txt,186,5 Conclusion and future work We have proposed a method for compiling bilingual terminologies of compositionally translated MWTs.
E06-1029.txt,187,As opposed to previous work_comma_ we use the web rather than comparable corpora as a source of bilingual data.
E06-1029.txt,188,Our main insight is to constrain source and target candidate MWTs to only those strongly related to the seed.
E06-1029.txt,189,This allows us to achieve term alignment with high precision.
E06-1029.txt,190,We showed that coverage reaches satisfactory levels by using thesauri and bootstrapping.
E06-1029.txt,191,Due to the difference in objectives and in corpora_comma_ it is very hard to compare results our method produces a rather small set of highly accurate alignments_comma_ whereas extraction from comparable corpora generates much more candidates_comma_ but with an inferior precision.
E06-1029.txt,192,These two approaches have very different applications.
E06-1029.txt,193,Our method does however eliminate the requirement of comparable corpora_comma_ which means that we can use seeds from any domain_comma_ provided we have reasonably rich dictionaries and thesauri.
E06-1029.txt,194,Let us not forget that this article describes only a first attempt at compiling French Japanese terminology_comma_ and that various sources of improvement have been left untapped.
E06-1029.txt,195,In particular_comma_ our alignment suffers from the fact that we do not discriminate between different candidate translations.
E06-1029.txt,196,This could be achieved by using any of the more sophisticated selection methods proposed in the literature.
E06-1029.txt,197,Currently_comma_ corpus features are used solely for the collection of related terms.
E06-1029.txt,198,These could also be utilized in the translation selection_comma_ which Baldwin and Tanaka have shown to be quite effective.
E06-1029.txt,199,We could also make use of bilingual dictionary features as they did.
E06-1029.txt,200,Lexical context is another resource we have not exploited.
E06-1029.txt,201,Context vectors have successfully been applied in translation selection by Fung as well as Daille and Morin.
E06-1029.txt,202,On a different level_comma_ we could also apply the bootstrapping to expand the French set of related terms.
E06-1029.txt,203,Finally_comma_ we are investigating the possibilseed F F  P  M M Prec.
E06-1029.txt,204,1 89 40 14 26 13 50 2 64 55 24 14 14 100 3 72 59 38 40 33 83 4 67 49 22 23 18 78 5 85 70 22 21 17 81 6 67 50 27 22 21 95 7 36 27 16 20 17 85 8 114 58 29 28 24 86 avg 74.3 51.0 24.0 24.3 19.6 81 Table 6 Detailed results for FJJ  70 80 90 100 25 Pr e c ision 0 10 20 30 40 50 60 0 5 10 15 20 Baseline Baseline with bootstrap Incremental Incremental with bootstrap Number of Valid Alignments Figure 2 Precision Valid Alignments curves ity of resolving the alignments in the opposite direction from Japanese to French.
E06-1029.txt,205,Surely the constructional variability of French MWTs would present some difficulties_comma_ but we are confident that this could be tackled using translation templates_comma_ as proposed by Baldwin and Tanaka. .
E06-1021.txt,1,sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms_comma_ for tasks such as summarization or paraphrasing. We present here a new monolingual sentence alignment algorithm_comma_ combining a sentence based TF IDF score_comma_ turned into a probability distribution using logistic regression_comma_ with a global alignment dynamic programming algorithm.
E06-1021.txt,2,Our approach provides a simpler and more robust solution achieving a substantial improvement in accuracy over existing systems.
E06-1021.txt,4,Sentence aligned bilingual corpora are a crucial resource for training statistical machine translation systems.
E06-1021.txt,5,Several authors have suggested that large scale aligned monolingual corpora could be similarly used to advance the performance of monolingual text to text rewriting systems_comma_ for tasks including summarization Knight and Marcu_comma_ 2000 Jing_comma_ 2002 and paraphrasing Barzilay and Elhadad_comma_ 2003 Quirk et al._comma_ 2004 .
E06-1021.txt,6,Unlike bilingual corpora_comma_ such as the Canadian Hansard corpus_comma_ whicharerelatively rare_comma_ itis now fairly easy to amass corpora of related monolingual documents.
E06-1021.txt,7,For instance_comma_ with the advent of news aggregator services such as Google News _comma_ one can readily collect multiple news stories covering the same news item Dolan et al._comma_ 2004 .
E06-1021.txt,8,Utilizing such a resource requires aligning related documents at a finer level of resolution_comma_ identifying which sentences from one document align with which sentences from the other.
E06-1021.txt,9,Previous work has shown that aligning related monolingual documents is quite different from the well studied multi lingual alignment task.
E06-1021.txt,10,Whereas documents in a bilingual corpus are typically very closely aligned_comma_ monolingual corpora exhibit a much looser level of alignment_comma_ with similar content expressed using widely divergent wording_comma_ grammatical form_comma_ and sentence order.
E06-1021.txt,11,Consequently_comma_ many of the simple surface based methods that have proven to be so successful in bilingual sentence alignment_comma_ such as correlation of sentence length_comma_ linearity of alignment_comma_ and a predominance of one to one sentence mapping_comma_ are much less likely to be effective for monolingual sentence alignment.
E06-1021.txt,12,Barzilay and Elhadad 2003 suggested that these disadvantages could be at least partially offset by the recurrence of the same lexical items in document pairs.
E06-1021.txt,13,Indeed_comma_ they showed that a simple cosine word overlap score is a good baseline for the task_comma_ outperforming much more sophisticated methods.
E06-1021.txt,14,They also observed that context is a powerful factor in determining alignment.
E06-1021.txt,15,They illustrated this on a corpus of Encyclopedia Britannica entries describing world cities_comma_ where each entry comes in two flavors_comma_ the comprehensive encyclopedia entry_comma_ and a shorter and simpler elementary version.
E06-1021.txt,16,Barzilay and Elhadad used context in two different forms.
E06-1021.txt,17,First_comma_ using interdocument context_comma_ they took advantage of commonalities in the topical structure of the encyclopedia entries to identify paragraphs that are likely to be about the same topic.
E06-1021.txt,18,They then took advantage of intra document context by using dynamic programming to locally align sequences of sentences belonging to paragraphs about the same topic_comma_ yielding improved accuracy on the corpus.
E06-1021.txt,19,While powerful_comma_ such commonalities in document structure appear to be a special feature of the Britannica corpus_comma_ and therefore cannot be relied upon for other corpora.
E06-1021.txt,20,In this paper we present a novel algorithm for sentence alignment in monolingual corpora.
E06-1021.txt,21,At the core of the algorithm is a classical similar161 ity score based on differentially weighting words according to their Term Frequency Inverse Document Frequency TF IDF Sp arck Jones_comma_ 1972 Salton and Buckley_comma_ 1988 .
E06-1021.txt,22,We treat sentences as documents_comma_ and the collection of sentences in the two documents being compared as the document collection_comma_ and use this score to estimate the probability that two sentences are aligned using logistic regression.
E06-1021.txt,23,Surprisingly_comma_ this approach by itself yields competitive accuracy_comma_ yielding thesame level of accuracy as Barzilay and Elhadad s algorithm_comma_ and higher than all previous approaches on the Britannica corpus.
E06-1021.txt,24,Such matching_comma_ however_comma_ is still noisy.
E06-1021.txt,25,We further improve accuracy by using a global alignment dynamic programming algorithm_comma_ which prunes many spurious matches.
E06-1021.txt,26,Our approach validates Barzilay and Elhadad s observation regarding the utility of incorporating context.
E06-1021.txt,27,In fact_comma_ we are able to extract more information out oftheintra document context.
E06-1021.txt,28,First_comma_ by using TF IDF at the level of sentences_comma_ we weigh words inasentence withrespect toother sentences of the document.
E06-1021.txt,29,Second_comma_ global alignment takes advantage of noisy linear order of sentences.
E06-1021.txt,30,We makenouseofinter document context_comma_ and inparticular make no assumptions about common topical structure that are unique to the Britannica corpus_comma_ thus ensuring the scalability of the approach.
E06-1021.txt,31,Indeed_comma_ we successfully apply our algorithm to a very different corpus_comma_ the three Synoptic gospels of the New Testament Matthew_comma_ Mark_comma_ and Luke.
E06-1021.txt,32,Putting aside any religious or theological significance of these texts_comma_ they offer an excellent data source for studying alignment_comma_ since they contain many parallels_comma_ which have been conveniently annotated by bible scholars Aland_comma_ 1985 .
E06-1021.txt,33,Ouralgorithm achieves a significant improvement over the baseline for this corpus as well_comma_ demonstrating the general applicability of our approach.
E06-1021.txt,34,2 Related work Several authors have tackled the monolingual sentence correspondence problem.
E06-1021.txt,35,SimFinder Hatzivassiloglou et al._comma_ 1999 Hatzivassiloglou et al._comma_ 2001 examined 43 different features that could potentially help determine the similarity of two short text units sentences or paragraphs .
E06-1021.txt,36,Of these_comma_ they automatically selected 11 features_comma_ including word overlap_comma_ synonymy as determined by WordNet Fellbaum_comma_ 1998 _comma_ matching proper nouns and noun phrases_comma_ and sharing semantic classes of verbs Levin_comma_ 1993 .
E06-1021.txt,37,The Decomposition method Jing_comma_ 2002 relies on the observation that document summaries are often constructed by extracting sentence fragments from the document.
E06-1021.txt,38,It attempts to identify such extracts_comma_ using a Hidden Markov Model of the process of extracting words.
E06-1021.txt,39,The HMM uses features of word identity and document position_comma_ in which transition probabilities are based on locality assumptions.
E06-1021.txt,40,For instance_comma_ after a word is extracted_comma_ an adjacent word or one that belongs to a nearby sentence is more likely to be extracted than one that is further away.
E06-1021.txt,41,Barzilay and Elhadad 2003 apply a 4 step algorithm 1.
E06-1021.txt,42,Cluster the paragraphs of the training documents into topic specific clusters_comma_ based on word overlap.
E06-1021.txt,43,For instance_comma_ paragraphs in the Britannica city entries describing climate might cluster together.
E06-1021.txt,45,Learn mapping rules between paragraphs of the full and elementary versions_comma_ taking the word overlap and the clusters as features.
E06-1021.txt,47,Given a new pair of texts_comma_ identify sentence pairs with high overlap_comma_ and take these to be aligned.
E06-1021.txt,48,Then_comma_ classify paragraphs according to the clusters learned in Step 1_comma_ and use the mapping rules of Step 2 to match pairs of paragraphs between the documents.
E06-1021.txt,50,Finally_comma_ take advantage of the paragraph clustering and mapping_comma_ by locally aligning only sentences belonging to mapped paragraph pairs.
E06-1021.txt,51,Dolan et al. 2004 used Web aggregated news stories to learn both sentence level and word level alignments.
E06-1021.txt,52,Having collected a large corpus of clusters of related news stories from Google and MSN news aggregator services_comma_ they first seek related sentences_comma_ using two methods.
E06-1021.txt,53,First_comma_ using a high Levenshtein distance score they identify 139K sentence pairs of which about 16.7 are estimatedtobeunrelated using human evaluation of asample .
E06-1021.txt,54,Second_comma_ assuming that the firsttwosentences of related news stories should be matched_comma_ provided they have a high enough word overlap_comma_ yields 214K sentence pairs of which about 40 are estimated to be unrelated.
E06-1021.txt,55,No recall estimates 162 are provided however_comma_ with the release of the annotated Microsoft Research Paraphrase Corpus_comma_1 it is apparent that Dolan et al. are seeking much more tightly related pairs of sentences than Barzilay and Elhadad_comma_ ones that are virtually semantically equivalent.
E06-1021.txt,56,In subsequent work_comma_ the same authors Quirk et al._comma_ 2004 used such matched sentence pairs to train Giza Och and Ney_comma_ 2003 on word level alignment.
E06-1021.txt,57,The recent PASCAL Recognizing Textual Entailment RTE challenge Dagan et al._comma_ 2005 focused on the problem of determining whether one sentence entails another.
E06-1021.txt,58,Beyond the difference in the definition of the required relation between sentences_comma_ the RTE challenge focuses on isolated sentence pairs_comma_ as opposed to sentences within a document context.
E06-1021.txt,59,The task was judged to be quite difficult_comma_ with many of the systems achieving relatively low accuracy.
E06-1021.txt,60,3 Data The Britannica corpus_comma_ collected and annotated by Barzilay and Elhadad 2003 _comma_ consists of 103 pairsofcomprehensive andelementary encyclopedia entries describing major world cities.
E06-1021.txt,61,Twenty of these document pairs were annotated by human judges_comma_ who were asked to mark sentence pairs thatcontain atleastoneclause expressing thesame information_comma_ and further split into a training and testing set.
E06-1021.txt,62,As a rough indication of the diversity of the dataset and the difference of the task from bilingual alignment_comma_ we define the alignment diversity measure ADM for two texts_comma_ T1 a0 T2_comma_ to be 2a1 matchesa2 T1 a3 T2 a4 a5T 1 a5a6a7a5T 2 a5 _comma_ where matches is the number of matching sentence pairs.
E06-1021.txt,63,Intuitively_comma_ for closely aligned document pairs_comma_ as prevalent in bilingual alignment_comma_ one would expect an ADM value close to 1.
E06-1021.txt,64,The average ADM value for the training document pairs of the Britannica corpus is 0a826.
E06-1021.txt,65,For the gospels_comma_ we use the King James version_comma_ available electronically from the Sacred Text Archive.2 The gospels lengths span from 678 verses Mark to 1151 verses Luke _comma_ where we treat verses as sentences.
E06-1021.txt,66,For training and evaluation purposes_comma_ we use the list of parallels given by Aland 1985 .3 We use the pair Matthew Mark 1http research.microsoft.com research downloads 2http www.sacred texts.com 3The parallels are available online from http www. bible researcher.com parallels.html. for training and the two pairs Matthew Luke and Mark Luke fortesting.
E06-1021.txt,67,Whereas for the Britannica corpus parallels were marked at the resolution of sentences_comma_ Aland s annotation presents parallels as matched sequences of verses_comma_ known as pericopes.
E06-1021.txt,68,For instance_comma_ Matthew 4.1 11 matches Mark 1.1213.
E06-1021.txt,69,We write v a9 p to indicate that verse v belongs to pericope p.4 4 Algorithm We now describe the algorithm_comma_ starting with the TF IDF similarity score_comma_ followed by our use of logistic regression_comma_ and the global alignment.
E06-1021.txt,70,4.1 From word overlap to TF IDF Barzilay and Elhadad 2003 use a cosine measure of word overlap as a baseline for the task.
E06-1021.txt,71,As can be expected_comma_ word overlap is a relatively effective indicator of sentence similarity and relatedness Marcu_comma_ 1999 .
E06-1021.txt,72,Unfortunately_comma_ plain word overlap assigns all words equal importance_comma_ not even distinguishing between function and content words.
E06-1021.txt,73,Thus_comma_ once the overlap threshold is decreased to improve recall_comma_ precision degrades rapidly.
E06-1021.txt,74,For instance_comma_ if a pair of sentences has one or two words in common_comma_ this is inconclusive evidence of their similarity or difference.
E06-1021.txt,75,One way to address this problem is to differentially weight words using the TF IDF scoring scheme_comma_ which has become standard in Information Retrieval Salton and Buckley_comma_ 1988 .
E06-1021.txt,76,IDF wasalso used for the similar task of directional entailment by Monz and de Rijke 2001 .
E06-1021.txt,77,To apply this scheme for the task at hand we diverge from the standard IDF definition by viewing each sentence as a document_comma_ and the pair of documents as a combined collection of N single sentence documents.
E06-1021.txt,78,For a term t in sentence s_comma_ we define TFs a10 ta11 to be a binary indicator of whether t occurs in s_comma_5 and DF a10 ta11 to be the number of sentences in which t occurs.
E06-1021.txt,79,The TF IDF weight is ws a10 ta11a13a12 def TFs a10 ta11a15a14 log a16 N DF a10 ta11a18a17 .
E06-1021.txt,80,4The annotation of matched pericopes induces a partial segmentation of each gospel into paragraph like segments.
E06-1021.txt,81,Since this segmentation is part of the gold annotation_comma_ we do not use it in our algorithm.
E06-1021.txt,82,5Using a binary indicator rather than the more typical number of occurrences yielded better accuracy on the Britannica training set.
E06-1021.txt,83,This is probably due to the documents being only of sentence length.
E06-1021.txt,84,163 1 0.8 0.6 0.5 0.4 0.276 0.2 0 1 0.8 0.6 0.4 0.2 0 probability similarity probabilityprobability Figure 1 Logistic Regression for Britannica training data We use these scores as the basis of a standard cosine similarity measure_comma_ sim a10 s1 a0 s2a11a13a12 s1a0s2a5s1a5a5s2a5 a12 t ws1 a2 t a4a1a0 ws2a2 t a4 a2 t w2s1a2 t a4 t w2s2a2 t a4 .
E06-1021.txt,85,We normalize terms by using Porter stemming Porter_comma_ 1980 .
E06-1021.txt,86,Forthe Britannica corpus_comma_ we also normalized British American spelling differences using a small manually constructed lexicon.
E06-1021.txt,87,4.2 Logistic regression TF IDF scores provide a numeric measure of sentence similarity.
E06-1021.txt,88,To use them for choosing sentence pairs_comma_ we proceeded to learn a probability of two sentences being matched_comma_ given their TF IDF similarity score_comma_ pr a10 match a12 1 a3 sima11 .
E06-1021.txt,89,We expect this probability to follow a sigmoid shaped curve.
E06-1021.txt,90,While it is always monotonically increasing_comma_ the rate of ascent changes for very low or very high values it is not as steep as for middle values.
E06-1021.txt,91,This reflects the intuition that while we always prefer a higher scoring pair over a lower scoring pair_comma_ this preference ismorepronounced inthemiddlerange than in the extremities.
E06-1021.txt,92,Indeed_comma_ Figure 1 shows a graph of this distribution on the training part of the Britannica corpus_comma_ where point a10 x a0 ya11 represents the fraction y of correctly matched sentences of similarity x.
E06-1021.txt,93,Overlayed on top of the points is a logistic regression model of this distribution_comma_ defined as the function p a12 e aa6 bx 1a4 eaa6 bx _comma_ where a and b are parameters.
E06-1021.txt,94,We used Weka Witten and Frank_comma_ 1999 to automatically learn the parameters of the distribution on the training data.
E06-1021.txt,95,These are set to a a12a6a5 7a889 and b a12 27a856 for the Britannica corpus.
E06-1021.txt,96,1 2 3 4 a b c pg2 pg1 Figure 2 Reciprocal best hit example.
E06-1021.txt,97,Arrows indicate the best hit for each verse.
E06-1021.txt,98,The pairs considered correct are a7 2 a0 ba8 and a7 4 a0 ca8 .
E06-1021.txt,99,Logistic regression scales the similarity scores monotonically but non linearly.
E06-1021.txt,100,In particular_comma_ it changes the density of points at different score levels.
E06-1021.txt,101,In addition_comma_ we can use this distribution to choose a threshold_comma_ th_comma_ for when a similarity score is indicative of a match.
E06-1021.txt,102,Optimizing the F measure on the training data using Weka_comma_ we choose a threshold value of th a12 0a8276.
E06-1021.txt,103,Note that since the logistic regression transformation is monotonic_comma_ the existence of a threshold on probabilities implies the existence of a threshold on the original sim scores.
E06-1021.txt,104,Moreover_comma_ such a threshold might be obtained by means other than logistic regression.
E06-1021.txt,105,The scaling_comma_ however_comma_ will become crucial once we do additional calculations with these probabilities in Section 4.4.
E06-1021.txt,106,Applying logistic regression to the gospels is complicated by the fact that we only have a correct alignment at the resolution of pericopes_comma_ and not individual verses.
E06-1021.txt,107,Verse pairs that do not belong to a matched pericope pair can be safely considered unaligned_comma_ but for amatched pericope pair_comma_ pg1 a0 pg2_comma_ we do not know which verse is matched with which.
E06-1021.txt,108,We solve this by searching for the reciprocal best hit_comma_ a method often used to find orthologous genes in related species Mushegian and Koonin_comma_ 1996 .
E06-1021.txt,109,For each verse in each pericope_comma_ we find the top matching verse in the other pericope.
E06-1021.txt,110,We take as correct all and only pairs of verses x a0 y_comma_ such that x is y s best match and y is x s best match.
E06-1021.txt,111,An example is shown in Figure 2.
E06-1021.txt,112,Taking these pairs as matched yields an ADM value of 0a834 for the training pair of documents.
E06-1021.txt,113,We used the reciprocally best matched pairs of the training portion of the gospels to find logistic regression parameters a10 a a12a9a5 9a860 a0 b a12 25a800a11 _comma_ and 164 athreshold_comma_ a10 th a12 0a8250a11 .
E06-1021.txt,114,Notethat werely on this matching only for training_comma_ but not for evaluation see Section 5.2 .
E06-1021.txt,115,4.3 Method 1 TF IDF As a simple method for choosing sentence pairs_comma_ we just select all sentence pairs with pr a10 matcha11a1a0 th.
E06-1021.txt,116,We use the following additional heuristics a2 We unconditionally match the first sentence of one document with the first sentence of the other document.
E06-1021.txt,117,As noted by Quirk et al. 2004 _comma_ these are very likely to be matched_comma_ as verified on our training set as well.
E06-1021.txt,118,a2 We allow many to one matching of sentences_comma_ but limit them to at most 2 to 1 sentences in both directions by allowing only the top two matches per sentence to be chosen _comma_ since such multiple matchings often arise due to splitting a sentence into two_comma_ or conversely_comma_ merging two sentences into one.
E06-1021.txt,119,4.4 Method 2 TF IDF Global alignment Matching sentence pairs according to TF IDF ignores sentence ordering completely.
E06-1021.txt,120,For bilingual texts_comma_ Gale and Church 1991 demonstrated the extraordinary effectiveness of a global alignment dynamic programming algorithm_comma_ where the basic similarity score wasbasedonthedifference insentence lengths_comma_ measured in characters.
E06-1021.txt,121,Such methods fail to work in the monolingual case.
E06-1021.txt,122,Gale and Church s algorithm using the implementation of Danielsson and Ridings 1997 yields 2 precision at 2.85 recall on the Britannica corpus.
E06-1021.txt,123,Moore s algorithm 2002 _comma_ which augments sentence length alignment with IBM Model 1 alignment_comma_ reports zero matching sentence pairs regardless of threshold .
E06-1021.txt,124,Nevertheless_comma_ we expect sentence ordering can provide important clues for monolingual alignment_comma_ bearing in mind two main differences from the bilingual case.
E06-1021.txt,125,First_comma_ as can be expected by the ADM value_comma_ there are many gaps in the alignment.
E06-1021.txt,126,Second_comma_ there can be large segments that diverge from the linear order predicted by a global alignment_comma_ as illustrated by the oval in Figure 3 Figure 2_comma_ Barzilay and Elhadad_comma_ 2003 .
E06-1021.txt,127,To model these features of the data_comma_ we use a variant of Needleman Wunsch alignment 1970 .
E06-1021.txt,128,We compute the optimal alignment between sentences 1a8 a8 iofthecomprehensive textandsentences 1a8 a8 j of the elementary version by 0 50 100 150 200 250 0 5 10 15 20 25 30 Sentences in comprehensive version Sentences in elementary version Manual alignment Sentences in comprehensive version Figure 3 Gold alignment for a text from the Britannica corpus. s a10 i a0 ja11 a12 max a3 a4a6a5 s a10 i a5 1 a0 j a5 1a11 a4 pr a10 match a10 i a0 ja11 a11 s a10 i a5 1 a0 ja11 a4 pr a10 match a10 i a0 ja11 a11 s a10 i a0 j a5 1a11 a4 pr a10 match a10 i a0 ja11 a11 Note that the dynamic programming sums match probabilities_comma_ rather than the original sim scores_comma_ making crucial use of the calibration induced by the logistic regression.
E06-1021.txt,129,Starting from the first pair of sentences_comma_ wefind the best path through the matrix indexed by i and j_comma_ using dynamic programming.
E06-1021.txt,130,Unlike the standard algorithm_comma_ weassign no penalty to off diagonal matches_comma_ allowing manyto one matches as illustrated schematically in Figure 4.
E06-1021.txt,131,This is because for the loose alignment exhibited by the data_comma_ being off diagonal is not indicative of a bad match.
E06-1021.txt,132,Instead_comma_ we prune the complete path generated by the dynamic programming using two methods.
E06-1021.txt,133,First_comma_ as in Section 4.3_comma_ we limit many to one matches to 2 to 1_comma_ by allowing just the two best matches per sentence to be included.
E06-1021.txt,134,Second_comma_ we eliminate sentence pairs with very low match probabilities a10 pr a10 matcha11a8a7 0a8005a11 _comma_ a value learned on the training data.
E06-1021.txt,135,Finally_comma_ to deal with the divergences from the linear order_comma_ we add the top n pairs with very high match probability_comma_ above a higher threshold_comma_ tha9 .
E06-1021.txt,136,Optimizing on the training data_comma_ we set n a12 5 and tha9 a12 0a865 for both corpora.
E06-1021.txt,137,Note that although Barzilay and Elhadad also used an alignment algorithm_comma_ they restricted it only to sentences judged to belong to topically related paragraphs.
E06-1021.txt,138,As noted above_comma_ this restriction relies on a special feature of the corpus_comma_ the fact that encyclopedia entries follow a relatively regular structure of paragraphs.
E06-1021.txt,139,By not relying on such 165 a14 a14 a14 a14 Figure 4 Global alignment corpus specific features_comma_ our approach gains in robustness.
E06-1021.txt,140,5 Evaluation 5.1 Britannica corpus Precision recall curves for both methods_comma_ aggregated over all the documents of the testing portion of the Britannica corpus are given in Figure 5.
E06-1021.txt,141,To obtain different precision recall points_comma_ we vary the threshold above which a sentence pair is deemed matched.
E06-1021.txt,142,Of course_comma_ when practically applying the algorithm_comma_ we have to pick a particular threshold_comma_ as we have done by choosing th.
E06-1021.txt,143,Precision recall values atthis threshold are also indicated in the figure.6 1 0.9 0.8 0.7 0.6 0.7 0.6 0.558 0.5 0.4 0.3 Precision Recall TF IDF Align Precision TF IDF Precision Precision 55.8 Recall PrecisionPrecision Precision Recall th Figure 5 Precision Recall curves for the Britannica corpus Comparative results with previous algorithms are given in Table 1_comma_ in which the results for Barzilay and Elhadad s algorithm and previous ones are taken from Barzilay and Elhadad 2003 .
E06-1021.txt,144,The paper reports the precision at 55.8 recall_comma_ since the Decomposition method Jing_comma_ 2002 only produced results at this level of recall_comma_ as some of the method s parameters were hard coded.
E06-1021.txt,145,Interestingly_comma_ the TF IDF method is highly competitive in determining sentence similarity.
E06-1021.txt,146,6Decreasing the threshold to 0.0 does not yield all pairs_comma_ since we only consider pairs with similarity strictly greater than 0.0_comma_ and restrict many to one matches to 2 to 1.
E06-1021.txt,147,Algorithm Precision SimFinder 24 Word Overlap 57.9 Decomposition 64.3 Barzilay Elhadad 76.9 TF IDF 77.0 TF IDF Align 83.1 Table 1 Precision at 55.8 Recall Despite its simplicity_comma_ it achieves the same performance as Barzilay and Elhadad s algorithm_comma_7 and is better than all previous ones.
E06-1021.txt,148,Significant improvement is achieved by adding the global alignment.
E06-1021.txt,149,Clearly_comma_ the method is inherently limited in that it can only match sentences with some lexical overlap.
E06-1021.txt,150,For instance_comma_ the following sentence pair that should have been matched was missed a2 Population soared_comma_ reaching 756_comma_000 by 1903_comma_ and urban services underwent extensive modification.
E06-1021.txt,151,a2 At the beginning of the 20th century_comma_ Warsaw had about 700_comma_000 residents.
E06-1021.txt,152,Matching 1903 with the beginning of the 20th century goes beyond the scope of any method relying predominantly on word identity.
E06-1021.txt,153,The hope is_comma_ however_comma_ that such mappings could be learned by amassing a large corpus of accurately sentence aligned documents_comma_ and then applying a word alignment algorithm_comma_ as proposed by Quirk et al. 2004 .
E06-1021.txt,154,Incidentally_comma_ examining sentence pairs withhigh TF IDFsimilarity scores_comma_ there are some striking cases that appear to have been missed by the human judges.
E06-1021.txt,155,Of course_comma_ we faithfully and conservatively relied on the human annotation in the evaluation_comma_ ignoring such cases.
E06-1021.txt,156,5.2 Gospels For evaluating our algorithm s accuracy on the gospels_comma_ we again have to contend with the fact that the correct alignments are given at the resolution of pericopes_comma_ not verses.
E06-1021.txt,157,We cannot rely on the reciprocal best hit method we used for training_comma_ since itrelies ontheTF IDFsimilarity scores_comma_ which we are attempting to evaluate.
E06-1021.txt,158,We therefore devise an alternative evaluation criterion_comma_ counting 7We discount the minor difference as insignificant.
E06-1021.txt,159,166 a pair of verses as correctly aligned if they belong to a matched pericope in the gold annotation.
E06-1021.txt,160,Let Gold a10 g1 a0 g2 a11 be the set of matched pericope pairsforgospels g1 a0 g2_comma_according toAland 1985 .
E06-1021.txt,161,For each pair of matched verses_comma_ vg1 a0 vg2_comma_ we count the pair as a true positive if and only if there is a pericope pair a7 pg1 a0 pg2 a8 a9 Gold a10 g1 a0 g2 a11 such that vgi a9 pgi a0 i a12 1 a0 2.
E06-1021.txt,162,Otherwise_comma_ it is a false positive.
E06-1021.txt,163,Precision is defined as usual P a12 tpa0 a10 tp a4 f pa11 .
E06-1021.txt,164,For recall_comma_ we note that not all the verses of a matched pericope should be matched_comma_ especially when one pericope has substantially more verses than the other.
E06-1021.txt,165,In general_comma_ wemayexpect the number of verses to be matched to be the minimum of a3 pg1 a3 and a3 pg2 a3.
E06-1021.txt,166,We thus define recall as R a12 tpa0 a1a2 a3 p g1 a3 pg2 a4a6a5 Gold a2 g1 a3 g2 a4 min a10 a3 pg1 a3 a0 a3 pg2 a3 a11a8a7a9 .
E06-1021.txt,167,The results are given in Figure 6_comma_ including the word overlap baseline_comma_ TF IDF ranking with logistic regression_comma_ and the added global alignment.
E06-1021.txt,168,Once again_comma_ TF IDF yields a substantial improvement over the baseline_comma_ and results are further improved by adding the global alignment.
E06-1021.txt,169,1 0.9 0.8 0.7 0.6 0.5 0.4 0.6 0.5 0.4 0.3 0.2 0.1 0 Precision Recall TF IDF Align Precision TF IDF Precision Overlap Figure 6 Precision Recall curves for the gospels 6 Conclusions and future work For monolingual alignment to achieve its full potential for text rewriting_comma_ huge amounts of text would need to be accurately aligned.
E06-1021.txt,170,Since monolingual corpora are so noisy_comma_ simple but effective methods as described in this paper willbe required to ensure scalability.
E06-1021.txt,171,We have presented a novel algorithm for aligning the sentences of monolingual corpora of comparable documents.
E06-1021.txt,172,Our algorithm not only yields substantially improved accuracy_comma_ but is also simpler and more robust than previous approaches.
E06-1021.txt,173,The efficacy of TF IDF ranking is remarkable in the face of previous results.
E06-1021.txt,174,In particular_comma_ TF IDF was not chosen by the feature selection algorithm of Hatzivassiloglou et al. 2001 _comma_ who directly experimented and rejected TF IDF measures as being less effective in determining similarity.
E06-1021.txt,175,Webelieve this striking difference can be attributed to the source of the weights.
E06-1021.txt,176,Recall that our TF IDF weights treat each sentence as a separate document for the purpose of weighting.
E06-1021.txt,177,TF IDFscores used in previous work are likely to have been obtained either by aggregation over the full document corpus_comma_ or by comparison with an external general collection_comma_ which is bound to yield lower discriminative power.
E06-1021.txt,178,To illustrate this_comma_ consider two words_comma_ such as the name of a city_comma_ and the name of a building in that city.
E06-1021.txt,179,Viewed globally_comma_ both words are likely to belong to the long tail of the Zipf distribution_comma_ having almost indistinguishable logarithmic IDF.
E06-1021.txt,180,However_comma_ in the encyclopedia entry describing the city_comma_ the city s name is likely to appear in many sentences_comma_ while the building name may appear only in the single sentence that refers to it_comma_ and thus the latter should be scored higher.
E06-1021.txt,181,Conversely_comma_ a word that is relativelyfrequent ingeneral usage_comma_ e.g._comma_ river might be highly discriminative between sentences.
E06-1021.txt,182,We further improve on the TF IDF results by using a global alignment algorithm.
E06-1021.txt,183,We expect that more sophisticated sequence alignment techniques_comma_ as studied for biological sequence analysis_comma_ might yield improved results_comma_ in particular for comparing loosely matched document pairs involving non linear text transformations such as inversions and translocations.
E06-1021.txt,184,Such methods could still modularly rely on the TF IDF scoring.
E06-1021.txt,185,We reiterate Barzilay and Elhadad s conclusion about the effectiveness of using the document context for the alignment of text.
E06-1021.txt,186,In fact_comma_ we are able totake better advantage ofthe intra document context_comma_ while not relying on any assumptions about inter document context that might be specific to one particular corpus.
E06-1021.txt,187,Identifying scalable principles for the use of inter document context poses a challenging topic for future research.
E06-1021.txt,188,We have restricted our attention here to preannotated corpora_comma_ allowing better comparison with previous work_comma_ and sidestepping the laborintensive task of human annotation.
E06-1021.txt,189,Having es167 tablished a simple and robust document alignment method_comma_ we leave its application to much largerscale document sets for future work.
E06-1021.txt,190,Acknowledgments We thank Regina Barzilay and Noemie Elhadad for providing access to the annotated Britannica corpus_comma_ and for discussion.
E06-1021.txt,191,This work was supported in part by National Science Foundation grant BCS 0236592. .
E06-1025.txt,1,mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about_comma_ but with the opinion it expresses. To aid the extraction of opinions from text_comma_ recent work has tackled the issue of determining the orientation of subjective terms contained in text_comma_ i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation.
E06-1025.txt,2,This is believed to be of key importance for identifying the orientation of documents_comma_ i.e. determining whether a document expresses a positive or negative opinion about its subject matter.
E06-1025.txt,3,We contend that the plain determination of the orientation of terms is not a realistic problem_comma_ since it starts from the nonrealistic assumption that we already know whether a term is subjective or not this would imply that a linguistic resource that marks terms as subjective or objective is available_comma_ which is usually not the case.
E06-1025.txt,4,In this paper we confront the task of deciding whether a given term has a positive connotation_comma_ or a negative connotation_comma_ or has no subjective connotation at all this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation.
E06-1025.txt,5,We tackle this problem by testing three different variants of a semi supervised method previously proposed for orientation detection.
E06-1025.txt,6,Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.
E06-1025.txt,8,Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about_comma_ but with the opinion it expresses.
E06-1025.txt,9,Opinion driven content management has several important applications_comma_ such as determining critics opinions about a given product by classifying online product reviews_comma_ or tracking the shifting attitudes ofthegeneral public towardapolitical candidate by mining online forums.
E06-1025.txt,10,Within opinion mining_comma_ several subtasks can be identified_comma_ all of them having to do with tagging a given document according to expressed opinion 1. determining document subjectivity_comma_ as in deciding whether a given text has a factual nature i.e. describes a given situation or event_comma_ without expressing a positive or a negative opinion on it or expresses an opinion on its subject matter.
E06-1025.txt,11,This amounts to performing binary text categorization under categories Objective and Subjective Pang and Lee_comma_ 2004 Yu and Hatzivassiloglou_comma_ 2003 2. determining document orientation or polarity _comma_ as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter Pang and Lee_comma_ 2004 Turney_comma_ 2002 3. determining the strength of document orientation_comma_ as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive_comma_ Mildly Positive_comma_ or Strongly Positive Wilson et al._comma_ 2004 .
E06-1025.txt,12,To aid these tasks_comma_ recent work Esuli and Sebastiani_comma_ 2005 Hatzivassiloglou and McKeown_comma_ 1997 Kamps et al._comma_ 2004 Kim and Hovy_comma_ 2004 Takamura et al._comma_ 2005 Turney and Littman_comma_ 2003 has tackled the issue of identifying the orientation ofsubjective terms contained in text_comma_ i.e.determiningwhether atermthat carries opinionated content has a positive or a negative connotation e.g. deciding that using Turney and Littman s 2003 examples honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation .
E06-1025.txt,13,193 This is believed to be of key importance for identifying the orientation of documents_comma_ since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1_comma_ 2 and 3 above.
E06-1025.txt,14,The conceptually simplest approach to this latter problem is probably Turney s 2002 _comma_ who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to but more sophisticated approaches arealsopossible Hatzivassiloglou and Wiebe_comma_ 2000 Riloff et al._comma_ 2003 Wilson et al._comma_ 2004 .
E06-1025.txt,15,Implicit in most works dealing with term orientation is the assumption that_comma_ for many languages for which one would like to perform opinion mining_comma_ there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation_comma_ and that in the absence of such a resource the only available route is to generate such a resource automatically.
E06-1025.txt,16,However_comma_ we think this approach lacks realism_comma_ since it is also true that_comma_ for the very same languages_comma_ there is no available lexical resource where terms are tagged as having either a Subjective or an Objective connotation.
E06-1025.txt,17,Thus_comma_ the availability of an algorithm that tags Subjective terms as being either Positive or Negative is of little help_comma_ since determining if a term is Subjective is itself non trivial.
E06-1025.txt,18,In this paper we confront the task of determining whether a given term has a Positive connotation e.g. honest_comma_ intrepid _comma_ or a Negative connotation e.g. disturbing_comma_ superfluous _comma_ or has instead no Subjective connotation at all e.g. white_comma_ triangular this problem thus subsumes the problem of deciding between Subjective and Objective and the problem of deciding between Positive and Negative.
E06-1025.txt,19,We tackle this problem by testing three different variants of the semi supervised method for orientation detection proposed in Esuli and Sebastiani_comma_ 2005 .
E06-1025.txt,20,Our results show that determining subjectivity and orientation is amuch harder problem than determining orientation alone.
E06-1025.txt,21,1.1 Outline of the paper The rest of the paper is structured as follows.
E06-1025.txt,22,Section 2 reviews related work dealing with term orientation and or subjectivity detection.
E06-1025.txt,23,Section 3 briefly reviews the semi supervised method for orientation detection presented in Esuli and Sebastiani_comma_ 2005 .
E06-1025.txt,24,Section 4 describes in detail three different variants of itwe propose for determining_comma_ at the same time_comma_ subjectivity and orientation_comma_ and describes the general setup of our experiments.
E06-1025.txt,25,In Section 5 we discuss the results we have obtained.
E06-1025.txt,26,Section 6 concludes.
E06-1025.txt,27,2 Related work 2.1 Determining term orientation Most previous works dealing with the properties of terms within an opinion mining perspective have focused on determining term orientation.
E06-1025.txt,28,Hatzivassiloglou and McKeown 1997 attempt to predict the orientation of subjective adjectives by analysing pairs of adjectives conjoined by and_comma_or_comma_but_comma_either or_comma_orneither nor extracted from a large unlabelled document set.
E06-1025.txt,29,The underlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the adjectives involved e.g. and usually conjoins adjectives of equal orientation_comma_ while but conjoins adjectives of opposite orientation.
E06-1025.txt,30,The authors generate a graph where terms are nodes connected by equal orientation or opposite orientation edges_comma_ depending on the conjunctions extracted from the document set.
E06-1025.txt,31,A clustering algorithm then partitions the graph into a Positive cluster and a Negative cluster_comma_ based on a relation of similarity induced by the edges.
E06-1025.txt,32,Turney and Littman 2003 determine term orientation by bootstrapping from two small sets of subjective seed terms with the seed set for Positive containing terms such as good and nice_comma_ and the seed set for Negative containing terms such as bad and nasty .
E06-1025.txt,33,Their method is based on computing the pointwise mutual information PMI of the target term t with each seed term ti as a measure of their semantic association.
E06-1025.txt,34,Given a target term t_comma_ its orientation value O t where positive value means positive orientation_comma_ and higher absolute value means stronger orientation is given by the sum of the weights of its semantic association with the seed positive terms minus the sum of the weights of its semantic association with the seed negative terms.
E06-1025.txt,35,For computing PMI_comma_ term frequencies and co occurrence frequencies are measured by querying a document set by means of the AltaVista search engine1 with a t query_comma_ a ti query_comma_ and a t NEARti query_comma_ and using the number of matching documents returned by the search engine as estimates of the probabilities needed for the computation of PMI.
E06-1025.txt,36,Kamps et al. 2004 consider instead the graph defined on adjectives by the WordNet2 synonymy relation_comma_ and determine the orientation of a target 1http www.altavista.com 2http wordnet.princeton.edu 194 adjective t contained in the graph by comparing the lengths of i the shortest path between t and the seed term good_comma_ and ii the shortest path between t and the seed term bad if the former is shorter than the latter_comma_ than t is deemed to be Positive_comma_ otherwise it is deemed to be Negative.
E06-1025.txt,37,Takamura et al. 2005 determine term orientation for Japanese according to a spin model _comma_ i.e. a physical model of a set of electrons each endowed with one between two possible spin directions_comma_ and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration.
E06-1025.txt,38,The authors equate terms with electrons and term orientation to spin direction.
E06-1025.txt,39,They build a neighbourhood matrix connecting each pair of terms if one appears in the gloss ofthe other_comma_ and iteratively apply thespin model on the matrix until a minimum energy configuration is reached.
E06-1025.txt,40,The orientation assigned to a term then corresponds to the spin direction assigned to electrons.
E06-1025.txt,41,ThesystemofKimandHovy 2004 tackles orientation detection by attributing_comma_ to each term_comma_ a positivity score and a negativity score interestingly_comma_ terms may thus be deemed to have both a positive and a negative correlation_comma_ maybe with different degrees_comma_ and some terms may be deemed to carry a stronger positive or negative orientation than others.
E06-1025.txt,42,Their system starts from a set of positive and negative seed terms_comma_ and expands the positive resp. negative seed set by adding to it the synonyms of positive resp. negative seed termsandtheantonyms ofnegative resp. positive seed terms.
E06-1025.txt,43,The system classifies then a target term t into either Positive or Negative by means of two alternative learning free methods based on the probabilities that synonyms of t also appear in the respective expanded seed sets.
E06-1025.txt,44,A problem with this method is that it can classify only terms that share somesynonyms withtheexpanded seed sets.
E06-1025.txt,45,Kim and Hovy also report an evaluation of human inter coder agreement.
E06-1025.txt,46,We compare this evaluation with our results in Section 5.
E06-1025.txt,47,The approach we have proposed for determining term orientation Esuli and Sebastiani_comma_ 2005 is described in more detail in Section 3_comma_ since it will be extensively used in this paper.
E06-1025.txt,48,All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms_comma_ i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative.
E06-1025.txt,49,When tested on the same benchmarks_comma_ the methods of Esuli and Sebastiani_comma_ 2005 Turney and Littman_comma_ 2003 have performed with comparable accuracies however_comma_ the method of Esuli and Sebastiani_comma_ 2005 is much more efficient than the one of Turney and Littman_comma_ 2003 _comma_ and have outperformed the method of Hatzivassiloglou and McKeown_comma_ 1997 by a wide margin and the one by Kamps et al._comma_ 2004 by a very wide margin.
E06-1025.txt,50,The methods described in Hatzivassiloglou and McKeown_comma_ 1997 is also limited by the fact that it can only decide the orientation of adjectives_comma_ while the method of Kamps et al._comma_ 2004 is further limited in that it can only work on adjectives that are present in WordNet.
E06-1025.txt,51,The methods of Kim and Hovy_comma_ 2004 Takamura et al._comma_ 2005 are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets.
E06-1025.txt,52,2.2 Determining term subjectivity Riloff et al. 2003 develop a method to determine whether a term has a Subjective or an Objective connotation_comma_ based on bootstrapping algorithms.
E06-1025.txt,53,The method identifies patterns for the extraction of subjective nouns from text_comma_ bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extracted.
E06-1025.txt,54,The results of this method are not easy to compare with the ones we present in this paper because of the different evaluation methodologies.
E06-1025.txt,55,While we adopt the evaluation methodology used in all of the papers reviewed so far i.e. checking how good our system is at replicating an existing_comma_ independently motivated lexical resource _comma_ the authors do not test their method on an independently identified set of labelled terms_comma_ butonthesetoftermsthatthealgorithm itself extracts.
E06-1025.txt,56,This evaluation methodology only allows to test precision_comma_ and not accuracy tout court_comma_ since no quantification can be made of false negatives i.e. the subjective terms that the algorithm should have spotted but has not spotted .
E06-1025.txt,57,In Section 5 this will prevent us from drawing comparisons between this method and our own.
E06-1025.txt,58,Baroni and Vegnaduzzo 2004 apply the PMI method_comma_ first used by Turney and Littman 2003 to determine term orientation_comma_ to determine term subjectivity.
E06-1025.txt,59,Their method uses a small set Ss of 35 adjectives_comma_ marked as subjective by human judges_comma_ toassign asubjectivity score toeachadjective to be classified.
E06-1025.txt,60,Therefore_comma_ their method_comma_ unlike our own_comma_ does not classify terms i.e. take firm classification decisions _comma_ but ranks them according to a subjectivity score_comma_ on which they evaluate precision at various level of recall.
E06-1025.txt,61,195 3 Determining term subjectivity and term orientation by semi supervised learning The method we use in this paper for determining term subjectivity and term orientation is a variant of the method proposed in Esuli and Sebastiani_comma_ 2005 for determining term orientation alone.
E06-1025.txt,62,This latter method relies on training_comma_ in a semisupervised way_comma_ a binary classifier that labels terms as either Positive or Negative.
E06-1025.txt,63,A semisupervised method is a learning process whereby only a small subset L Tr of the training data Tr are human labelled.
E06-1025.txt,64,In origin the training data in U Tr L are instead unlabelled it is the process itself that labels them_comma_ automatically_comma_ by using L with the possible addition of other publicly available resources as input.
E06-1025.txt,65,The method of Esuli and Sebastiani_comma_ 2005 starts from two small seed i.e. training sets Lp and Ln of known Positive and Negativeterms_comma_ respectively_comma_ and expands them into the two final training sets Trp Lp andTrn Ln byadding them new sets of terms Up and Un found by navigating the WordNet graph along the synonymy and antonymy relations3.
E06-1025.txt,66,This process is based on the hypothesis that synonymy and antonymy_comma_ in addition to defining a relation of meaning_comma_ also define a relation of orientation_comma_ i.e. that two synonyms typically have the same orientation and two antonyms typically have opposite orientation.
E06-1025.txt,67,The method is iterative_comma_ generating two sets Trkp and Trkn at each iteration k_comma_ where Trkp Trk 1p ... Tr1p Lp and Trkn Trk 1n ... Tr1n Ln.
E06-1025.txt,68,At iteration k_comma_ Trkp is obtained by adding to Trk 1p all synonyms of terms in Trk 1p and all antonyms of terms in Trk 1n similarly_comma_ Trkn is obtained by adding to Trk 1n all synonyms of terms in Trk 1n and allantonyms oftermsinTrk 1p .
E06-1025.txt,69,Ifatotal ofK iterations are performed_comma_ then Tr TrKp TrKn .
E06-1025.txt,70,The second main feature of the method presented in Esuli and Sebastiani_comma_ 2005 is that terms are given vectorial representations based on their WordNet glosses i.e. textual definitions .
E06-1025.txt,71,For each term ti in Tr Te Te being the test set_comma_ i.e. thesetoftermstobeclassified _comma_ atextual representation of ti is generated by collating all the glosses of ti as found in WordNet4.
E06-1025.txt,72,Each such represen3Several other WordNet lexical relations_comma_ and several combinations of them_comma_ are tested in Esuli and Sebastiani_comma_ 2005 .
E06-1025.txt,73,In the present paper we only use the best performing such combination_comma_ as described in detail in Section 4.2.
E06-1025.txt,74,The version of WordNet used here and in Esuli and Sebastiani_comma_ 2005 is 2.0.
E06-1025.txt,75,4In general a term ti may have more than one gloss_comma_ since tation is converted into vectorial form by standard text indexing techniques in Esuli and Sebastiani_comma_ 2005 and in the present work_comma_ stop words are removed and the remaining words are weighted by cosine normalized tfidf no stemming is performed 5.
E06-1025.txt,76,This representation method is based on the assumption that terms with a similar orientation tend to have similar glosses for instance_comma_ that the glosses of honest and intrepid will both contain appreciative expressions_comma_ while the glosses of disturbing and superfluous will both contain derogative expressions.
E06-1025.txt,77,Note that this method allows to classify any term_comma_ independently of its POS_comma_ provided there is a gloss for it in the lexical resource.
E06-1025.txt,78,Once the vectorial representations for all terms inTr Tehavebeengenerated_comma_ thosefortheterms in Tr are fed to a supervised learner_comma_ which thus generates a binary classifier.
E06-1025.txt,79,This latter_comma_ once fed with the vectorial representations of the terms in Te_comma_ classifies each of them as either Positive or Negative.
E06-1025.txt,80,4 Experiments In this paper we extend the method of Esuli and Sebastiani_comma_ 2005 tothedetermination oftermsubjectivity and term orientation altogether.
E06-1025.txt,81,4.1 Test sets The benchmark i.e. test set we use for our experiments is the General Inquirer GI lexicon Stone et al._comma_ 1966 .
E06-1025.txt,82,This is a lexicon of terms labelled according to a large set of categories6_comma_ each one denoting the presence of a specific trait in the term.
E06-1025.txt,83,The two main categories_comma_ and the ones we will be concerned with_comma_ are Positive Negative_comma_ which contain 1_comma_915 2_comma_291 terms having a positive negative orientation in what follows we will also refer to the category Subjective_comma_ which we define as the union of the two categories Positive and Negative .
E06-1025.txt,84,In opinion mining research the GI was first used by Turney and Littman 2003 _comma_ who reduced the list of terms to 1_comma_614 1_comma_982 entries afit may have more than one sense dictionaries normally associate one gloss to each sense.
E06-1025.txt,85,5Several combinations of subparts of a WordNet gloss are tested as textual representations of terms in Esuli and Sebastiani_comma_ 2005 .
E06-1025.txt,86,Of all those combinations_comma_ in the present paper we always use the DGS combination_comma_ since this is the one that has been shown to perform best in Esuli and Sebastiani_comma_ 2005 .
E06-1025.txt,87,DGS corresponds to using the entire gloss and performing negation propagation on itstext_comma_ i.e. replacing allthe terms that occur after a negation in a sentence with negated versions of the term see Esuli and Sebastiani_comma_ 2005 for details .
E06-1025.txt,88,6The definitions of all such categories are available at http www.webuse.umd.edu 9090 196 terremoving 17termsappearing inboth categories e.g. deal and reducing all the multiple entries of the same term in a category_comma_ caused by multiple senses_comma_ to a single entry.
E06-1025.txt,89,Likewise_comma_ we take all the 7_comma_582 GI terms that are not labelled as either Positive or Negative_comma_ as being implicitly labelled as Objective_comma_ and reduce them to 5_comma_009 terms after combining multiple entries of the same term_comma_ caused by multiple senses_comma_ to a single entry.
E06-1025.txt,90,The effectiveness of our classifiers will thus be evaluated in terms of their ability to assign the total 8_comma_605 GI terms to the correct category among Positive_comma_ Negative_comma_ and Objective7.
E06-1025.txt,91,4.2 Seed sets and training sets Similarly to Esuli and Sebastiani_comma_ 2005 _comma_ our training set is obtained by expanding initial seed sets by means of WordNet lexical relations.
E06-1025.txt,92,The main difference is that our training set is now the union of three sets of training terms Tr TrKp TrKn TrKo obtained byexpanding_comma_ through K iterations_comma_ three seed sets Tr1p_comma_Tr1n_comma_Tr1o_comma_ one for each of the categories Positive_comma_ Negative_comma_ and Objective_comma_ respectively.
E06-1025.txt,93,Concerning categories Positive and Negative_comma_ we have used the seed sets_comma_ expansion policy_comma_ and number of iterations_comma_ that have performed best in the experiments of Esuli and Sebastiani_comma_ 2005 _comma_ i.e. the seed sets Tr1p good and Tr1n bad expanded by using the union of synonymy and indirect antonymy_comma_ restricting the relations only to terms with the same POS of the original terms i.e. adjectives _comma_ for a total of K 4 iterations.
E06-1025.txt,94,The final expanded sets contain 6_comma_053 Positive terms and 6_comma_874 Negative terms.
E06-1025.txt,95,Concerning the category Objective_comma_ the process we have followed is similar_comma_ but with a few key differences.
E06-1025.txt,96,These are motivated by the fact that the Objective category coincides with the complement of the union of Positive and Negative therefore_comma_ Objective terms are more varied and diverse in meaning than the terms in the other two categories.
E06-1025.txt,97,To obtain a representative expanded set TrKo _comma_ we have chosen the seed set Tr1o entity and we have expanded it by using_comma_ along with synonymy and antonymy_comma_ the WordNet relation of hyponymy e.g. vehicle car _comma_andwithout imposing the restriction that the two related terms must have the same POS.
E06-1025.txt,98,These choices are strictly related to each other the term entityis the root term of the largest generalization hierarchy in WordNet_comma_ with more than 40_comma_000 7We make this labelled term set available for download at http patty.isti.cnr.it esuli software SentiGI.tgz. terms Devitt and Vogel_comma_ 2004 _comma_ thus allowing to reach a very large number of terms by using the hyponymy relation8.
E06-1025.txt,99,Moreover_comma_ it seems reasonable to assume that terms that refer to entities are likely to have an objective nature_comma_ and that hyponyms and also synonyms and antonyms of an objective term are also objective.
E06-1025.txt,100,Note that_comma_ at each iteration k_comma_ a given term t is added to Trko only if it does not already belong to either Trp or Trn.
E06-1025.txt,101,We experiment with two different choices for the Tro set_comma_ corresponding to the sets generated in K 3 and K 4 iterations_comma_ respectively this yields sets Tr3o and Tr4o consisting of 8_comma_353 and 33_comma_870 training terms_comma_ respectively.
E06-1025.txt,102,4.3 Learning approaches and evaluation measures We experiment with three philosophically different learning approaches to the problem of distinguishing between Positive_comma_ Negative_comma_ and Objective terms.
E06-1025.txt,103,Approach I is a two stage method which consists in learning two binary classifiers the first classifier places terms into either Subjective or Objective_comma_ while the second classifier places terms that have been classified as Subjective by thefirstclassifier into either Positive orNegative.
E06-1025.txt,104,In the training phase_comma_ the terms in TrKp TrKn are used as training examples of category Subjective.
E06-1025.txt,105,Approach II is again based on learning two binary classifiers.
E06-1025.txt,106,Here_comma_ one of them must discriminate between terms that belong to the Positive category and ones that belong to its complement not Positive _comma_ while the other must discriminate between terms that belong to the Negative category and ones that belong to its complement not Negative .
E06-1025.txt,107,Terms that have been classified both into Positive by the former classifier and into not Negative by the latter are deemed to be positive_comma_ and terms that have been classified both into not Positive by the former classifier and into Negative by the latter are deemed to be negative.
E06-1025.txt,108,The terms that have been classified i into both not Positive and not Negative _comma_ or ii into both Positive and Negative_comma_ are taken to be Objective.
E06-1025.txt,109,In the training phase of Approach II_comma_ the terms in TrKn TrKo are used as training examples of category not Positive _comma_ and the terms in TrKp TrKo are used as training examples of category not Negative .
E06-1025.txt,110,Approach III consists instead in viewing Positive_comma_ Negative_comma_ and Objective as three categories 8The synonymy relation connects instead only 10_comma_992 terms at most Kamps et al._comma_ 2004 .
E06-1025.txt,111,197 with equal status_comma_ and in learning a ternary classifier that classifies each term into exactly one among the three categories.
E06-1025.txt,112,There are several differences among these three approaches.
E06-1025.txt,113,A first difference_comma_ of a conceptual nature_comma_ is that only Approaches I and III view Objective as a category_comma_ or concept_comma_ in its own right_comma_ while Approach II views objectivity as a nonexistent entity_comma_ i.e. as the absence of subjectivity in fact_comma_ in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative .
E06-1025.txt,114,Asecond difference isthatApproaches Iand II are based on standard binary classification technology_comma_ while Approach III requires multiclass i.e. 1 of m classification.
E06-1025.txt,115,As a consequence_comma_ while for the former we use well known learners for binary classification the naive Bayesian learner using the multinomial model McCallum and Nigam_comma_ 1998 _comma_ support vector machines using linear kernels Joachims_comma_ 1998 _comma_ the Rocchio learner_comma_ and its PrTFIDFprobabilistic version Joachims_comma_ 1997 _comma_ for Approach III we use their multiclass versions9.
E06-1025.txt,116,Before running our learners we make a pass of feature selection_comma_ with the intent of retaining only those features that are good at discriminating our categories_comma_ while discarding those which are not.
E06-1025.txt,117,Feature selection is implemented by scoring each feature fk i.e. each term that occurs in the glosses of at least one training term by means of the mutual information MI function_comma_ defined as MI fk summationdisplay c c1_comma_..._comma_cm _comma_ f fk_comma_fk Pr f_comma_c log Pr f_comma_c Pr f Pr c 1 and discarding the x features fk that minimize it.
E06-1025.txt,118,We will call x the reduction factor.
E06-1025.txt,119,Note that theset c1_comma_..._comma_cm fromEquation 1isinterpreted differently in Approaches I to III_comma_ and always consistently with who the categories at stake are.
E06-1025.txt,120,Since the task we aim to solve is manifold_comma_ we will evaluate our classifiers according to two evaluation measures SO accuracy_comma_ i.e. the accuracy of a classifier inseparating SubjectivefromObjective_comma_i.e. in deciding term subjectivity alone PNO accuracy_comma_ the accuracy of a classifier in discriminating among Positive_comma_ Negative_comma_ 9The naive Bayesian_comma_ Rocchio_comma_ and PrTFIDF learners we have used are from Andrew McCallum s Bow package http www 2.cs.cmu.edu mccallum bow _comma_ while the SVMs learner we have used is Thorsten Joachims SV Mlight http svmlight.joachims.org _comma_ version 6.01.
E06-1025.txt,121,Both packages allow the respective learners to be run in multiclass fashion.
E06-1025.txt,122,Table 1 Average and best accuracy values over the four dimensions analysed in the experiments.
E06-1025.txt,123,Dimension SO accuracy PNO accuracy Avg  Best Avg  Best Approach I .635 .020 .668 .595 .029 .635 II .636 .033 .676 .614 .037 .660 III .635 .036 .674 .600 .039 .648 Learner NB .653 .014 .674 .619 .022 .647 SVMs .627 .033 .671 .601 .037 .658 Rocchio .624 .030 .654 .585 .033 .616 PrTFIDF .637 .031 .676 .606 .042 .660 TSR 0 .649 .025 .676 .619 .027 .660 50 .650 .022 .670 .622 .022 .657 80 .646 .023 .674 .621 .021 .647 90 .642 .024 .667 .616 .024 .651 95 .635 .027 .671 .606 .031 .658 99 .612 .036 .661 .570 .049 .647 TrKo set Tr3o .645 .006 .676 .608 .007 .658 Tr4o .633 .013 .674 .610 .018 .660 and Objective_comma_ i.e. in deciding both term orientation and subjectivity.
E06-1025.txt,124,5 Results We present results obtained from running every combination of i the three approaches to classification described in Section 4.3_comma_ ii the four learners mentioned in the same section_comma_ iii five different reduction factors for feature selection 0 _comma_ 50 _comma_ 90 _comma_ 95 _comma_ 99 _comma_ and iv the two different training sets Tr3o and Tr4o for Objective mentioned in Section 4.2.
E06-1025.txt,125,We discuss each of these four dimensions of the problem individually_comma_ for each one reporting results averaged across all the experiments we have run see Table 1 .
E06-1025.txt,126,The first and most important observation is that_comma_ with respect to a pure term orientation task_comma_ accuracy drops significantly.
E06-1025.txt,127,In fact_comma_ the best SOaccuracy and the best PNO accuracy results obtained across the 120 different experiments are .676 and .660_comma_ respectively these were obtained by using Approach II with the PrTFIDF learner and no feature selection_comma_ with Tro Tr3o for the .676 SO accuracy result and Tro Tr4o for the .660 PNO accuracy result this contrasts sharply with the accuracy obtained in Esuli and Sebastiani_comma_ 2005 on discriminating Positive from Negative where the best run obtained .830 accuracy _comma_ on the same benchmarks and essentially the same algorithms.
E06-1025.txt,128,This suggests that good performance at orientation detection as e.g. in Esuli and Sebastiani_comma_ 2005 Hatzivassiloglou and McKeown_comma_ 1997 Turney and Littman_comma_ 2003 may not be a 198 Table 2 Human inter coder agreement values reported by Kim and Hovy 2004 .
E06-1025.txt,129,Agreement Adjectives 462 Verbs 502 measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity detection_comma_ quite evidently a harder and_comma_ as we have suggested_comma_ more realistic task.
E06-1025.txt,130,This hypothesis is confirmed by an experiment performed by Kim and Hovy 2004 on testing the agreement of two human coders at tagging words with the Positive_comma_ Negative_comma_ and Objective labels.
E06-1025.txt,131,The authors define two measures of such agreement strict agreement_comma_ equivalent to our PNO accuracy_comma_ and lenient agreement_comma_ which measures the accuracy at telling Negative against the rest.
E06-1025.txt,132,For any experiment_comma_ strict agreement values are then going to be_comma_ by definition_comma_ lower or equal than the corresponding lenient ones.
E06-1025.txt,133,Theauthors use two sets of 462 adjectives and 502 verbs_comma_ respectively_comma_ randomly extracted from the basic English word list of the TOEFL test.
E06-1025.txt,134,The intercoder agreement results see Table 2 show a deterioration in agreement from lenient to strict of 16.77 for adjectives and 36.42 for verbs.
E06-1025.txt,135,Following this_comma_ we evaluated our best experiment according to these measures_comma_ and obtained a strict accuracy value of .660 and a lenient accuracy value of .821_comma_ with a relative deterioration of 24.39 _comma_ in line with Kim and Hovy s observation10.
E06-1025.txt,136,This confirms that determining subjectivity and orientation is a much harder task than determining orientation alone.
E06-1025.txt,137,The second important observation is that there is very little variance in the results across all 120 experiments_comma_ average SO accuracy and PNOaccuracy results were .635 with standard deviation  .030 and .603  .036 _comma_ a mere 6.06 and 8.64 deterioration from the best results reported above.
E06-1025.txt,138,This seems to indicate that the levels of performance obtained may be hard to improve upon_comma_ especially if working in a similar framework.
E06-1025.txt,139,Let us analyse the individual dimensions of the problem.
E06-1025.txt,140,Concerning the three approaches to classification described in Section 4.3_comma_ Approach II outperforms the other two_comma_ but by an extremely narrow margin.
E06-1025.txt,141,As for the choice of learners_comma_ on average the best performer is NB_comma_ but again by a very small margin wrt the others.
E06-1025.txt,142,On average_comma_ the 10We observed this trend in all of our experiments. best reduction factor for feature selection turns out to be 50 _comma_ but the performance drop we witness in approaching 99 a dramatic reduction factor is extremely graceful.
E06-1025.txt,143,As for the choice of TrKo _comma_ we note that Tr3o and Tr4o elicit comparable levels of performance_comma_ with the former performing best at SO accuracy and the latter performing best at PNO accuracy.
E06-1025.txt,144,An interesting observation on the learners we have used is that NB_comma_ PrTFIDF and SVMs_comma_ unlike Rocchio_comma_ generate classifiers that depend on P ci _comma_ the prior probabilities of the classes_comma_ which are normally estimated as the proportion of training documents that belong to ci.
E06-1025.txt,145,In many classification applications this is reasonable_comma_ as we may assume that the training data are sampled from the samedistribution fromwhichthetestdataaresampled_comma_ and that these proportions are thus indicative of the proportions that we are going to encounter in the test data.
E06-1025.txt,146,However_comma_ in our application this is not the case_comma_ since we do not have a natural sample of training terms.
E06-1025.txt,147,What we have is one human labelled training term for each category in Positive_comma_Negative_comma_Objective _comma_ and as many machine labelled terms as we deem reasonable to include_comma_ in possibly different numbers for the different categories and we have no indication whatsoever as to what the natural proportions among the three might be.
E06-1025.txt,148,This means that the proportions of Positive_comma_ Negative_comma_ and Objective terms we decide to include in the training set will strongly bias the classification results if the learner is one of NB_comma_ PrTFIDF and SVMs.
E06-1025.txt,149,We may notice this by looking at Table 3_comma_ which shows the average proportion of test terms classified as Objective by each learner_comma_ depending on whether we have chosen Tro to coincide with Tr3o or Tr4o note that the former resp. latter choice means having roughly as many resp. roughly five times as many Objective training terms as there are Positive and Negative ones.
E06-1025.txt,150,Table 3 shows that_comma_ the more Objective training terms there are_comma_ the more test terms NB_comma_ PrTFIDF and in particular SVMs will classify as Objective this is not true for Rocchio_comma_ which is basically unaffected by the variation in size of Tro.
E06-1025.txt,151,6 Conclusions We have presented a method for determining both term subjectivity and term orientation for opinion mining applications.
E06-1025.txt,152,This is a valuable advance with respect to the state of the art_comma_ since past work in this area had mostly confined to determining term orientation alone_comma_ a task that as we have ar199 Table 3 Average proportion of test terms classified as Objective_comma_ for each learner and for each choice of the TrKo set. Learner Tr3o Tr4o Variation NB .564  .069 .693 .069 23.0 SVMs .601 .108 .814 .083 35.4 Rocchio .572 .043 .544 .061 4.8 PrTFIDF .636 .059 .763 .085 20.0 gued has limited practical significance in itself_comma_ given the generalized absence of lexical resources that tag terms as being either Subjective or Objective.
E06-1025.txt,153,Our algorithms have tagged by orientation and subjectivity the entire General Inquirer lexicon_comma_ a complete general purpose lexicon that is the de facto standard benchmark for researchers in this field.
E06-1025.txt,154,Our results thus constitute_comma_ for this task_comma_ the first baseline for other researchers to improve upon.
E06-1025.txt,155,Unfortunately_comma_ our results have shown that an algorithm that had shown excellent_comma_ stateof the art performance in deciding term orientation Esuli and Sebastiani_comma_ 2005 _comma_ once modified for the purposes of deciding term subjectivity_comma_ performs more poorly.
E06-1025.txt,156,This has been shown by testing several variants of the basic algorithm_comma_ some of them involving radically different supervised learning policies.
E06-1025.txt,157,The results suggest that deciding term subjectivity is a substantially harder task that deciding term orientation alone. .
E06-2015.txt,1,resolution system with a feature capturing automatically generated information about semantic roles improves its performance. 1 .
E06-2015.txt,2,The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems Soon et al._comma_ 2001 Ng Cardie_comma_ 2002 Kehler et al._comma_ 2004_comma_ inter alia .
E06-2015.txt,3,Similarly_comma_ many researchers have explored techniques for robust_comma_ broad coverage semantic parsing in terms of semantic role labeling Gildea Jurafsky_comma_ 2002 Carreras M arquez_comma_ 2005_comma_ SRL henceforth .
E06-2015.txt,4,This paper explores whether coreference resolution can benefit from SRL_comma_ more specifically_comma_ which phenomena are affected by such information.
E06-2015.txt,5,The motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features_comma_ such as the distance between the coreferent expressions_comma_ string matching_comma_ and linguistic form.
E06-2015.txt,6,On the other hand_comma_ the literature emphasizes since the very beginning the relevance of world knowledge and inference Charniak_comma_ 1973 .
E06-2015.txt,7,As an example_comma_ consider a sentence from the Automatic Content Extraction ACE 2003 data.
E06-2015.txt,8,1 A state commission of inquiry into the sinking of the Kursk will convene in Moscow on Wednesday_comma_ the Interfax news agency reported.
E06-2015.txt,9,It said that the diving operation will be completed by the end of next week.
E06-2015.txt,10,It seems that in this example_comma_ knowing that the Interfax news agency is the AGENT of the report predicate_comma_ and It being the AGENT of say_comma_ could trigger the semantic parallelism based inference required to correctly link the two expressions_comma_ in contrast to anchoring the pronoun to Moscow.
E06-2015.txt,11,SRL provides the semantic relationships that constituents have with predicates_comma_ thus allowing us to include document level event descriptive information into the relations holding between referring expressions REs .
E06-2015.txt,12,This layer of semantic context .
E06-3004.txt,1,Named Entity Recognition systems suffer from the lack of hand tagged data as well as degradation when moving to other domain. This paper explores two aspects the automatic generation of gazetteerlistsfromunlabeleddata andthe building of a Named Entity Recognition system with labeled and unlabeled data.
E06-3004.txt,3,Automatic information extraction and information retrieval concerning particular person_comma_ location_comma_ organization_comma_ title of movie or book_comma_ juxtaposes to the Named Entity Recognition NER task.
E06-3004.txt,4,NER consists in detecting the most silent and informative elements in a text such as names of people_comma_ company names_comma_ location_comma_ monetary currencies_comma_ dates.
E06-3004.txt,5,Early NER systems Fisher et al._comma_ 1997 _comma_ Black et al._comma_ 1998 etc._comma_ participating in Message Understanding Conferences MUC _comma_ used linguistic tools and gazetteer lists.
E06-3004.txt,6,However these are difficult to develop and domain sensitive.
E06-3004.txt,7,To surmount these obstacles_comma_ application of machine learning approaches to NER became a research subject.
E06-3004.txt,8,Various state of the art machine learning algorithms such as Maximum Entropy Borthwick_comma_ 1999 _comma_ AdaBoost Carreras et al._comma_ 2002 _comma_ Hidden Markov Models Bikel et al._comma_ _comma_ Memory based Based learning Tjong Kim Sang_comma_ 2002b _comma_ have been used1.
E06-3004.txt,9,Klein et al._comma_ 2003 _comma_ Mayfield et al._comma_ 2003 _comma_ Wu et al._comma_ 2003 _comma_ Kozareva et al._comma_ 2005c among others_comma_ combined several classifiers to obtain better named entity coverage rate.
E06-3004.txt,10,1For other machine learning methods_comma_ consult http www.cnts.ua.ac.be conll2002 ner http www.cnts.ua.ac.be conll2003 ner Nevertheless all these machine learning algorithms rely on previously hand labeled training data.
E06-3004.txt,11,Obtaining such data is labor intensive_comma_ time consuming and even might not be present for languages with limited funding.
E06-3004.txt,12,Resource limitation_comma_ directed NER research Collins and Singer_comma_ 1999 _comma_ Carreras et al._comma_ 2003 _comma_ Kozareva et al._comma_ 2005a toward the usage of semi supervised techniques.
E06-3004.txt,13,These techniques are needed_comma_ as we live in a multilingualsocietyandaccesstoinformationfromvarious language sources is reality.
E06-3004.txt,14,The development of NER systems for languages other than English commenced.
E06-3004.txt,15,This paper presents the development of a Spanish Named Recognition system based on machine learning approach.
E06-3004.txt,16,For it no morphologic or syntactic information was used.
E06-3004.txt,17,However_comma_ we propose and incorporate a very simple method for automatic gazetteer2 construction.
E06-3004.txt,18,Such method can be easily adapted to other languages and it is low costly obtained as it relies on n gram extraction from unlabeled data.
E06-3004.txt,19,We compare the performance of our NER system when labeled and unlabeled training data is present.
E06-3004.txt,20,The paper is organized in the following way brief explanation about NER process is represented in Section 2.
E06-3004.txt,21,In Section 3 follows feature extraction.
E06-3004.txt,22,The experimental evaluation for the Named Entity detection and classification tasks withand without labeled data are in Sections 4 and 5.
E06-3004.txt,23,We conclude in Section 6.
E06-3004.txt,24,2 The NER how to A Named Entity Recognition task can be described as composition of two subtasks_comma_ entity de2specialized lists of names for location and person names_comma_ e.g.
E06-3004.txt,25,Madrid is in the location gazetteer_comma_ Mary is in the person gazetteer 15 tection and entity classification.
E06-3004.txt,26,Entity delimitation consist in determining the boundaries of the entity e.g. the place from where it starts and the place it finishes .
E06-3004.txt,27,This is important for tracing entities composed of two or more words such as Presidente de los Estados Unidos 3_comma_ Universidad Politecnica de Catalu na 4.
E06-3004.txt,28,For this purpose_comma_ the BIO scheme was incorporated.
E06-3004.txt,29,In this scheme_comma_ tag B denotes the start of an entity_comma_ tag I continues the entity and tag O marks words that do not form part of an entity.
E06-3004.txt,30,This scheme was initially introduced in CoNLL s Tjong Kim Sang_comma_ 2002a and Tjong Kim Sang and De Meulder_comma_ 2003 NER competitions_comma_ and we decided to adapt it for our experimental work.
E06-3004.txt,31,Once all entities in the text are detected_comma_ they are passed for classification in a predefined set of categories such as location_comma_ person_comma_ organization or miscellaneous5 names.
E06-3004.txt,32,This task is known as entity classification.
E06-3004.txt,33,The final NER performance is measured considering the entity detection and classification tasks together.
E06-3004.txt,34,Our NER approach is based on machine learning.
E06-3004.txt,35,The two algorithms we used for the experiments were instance based and decision trees_comma_ implemented by Daelemans et al._comma_ 2003 .
E06-3004.txt,36,They were used with their default parameter settings.
E06-3004.txt,37,We selected the instance based model_comma_ because it is known to be useful when the amount of training data is not sufficient.
E06-3004.txt,38,Important part in the NE process takes the location and person gazetteer lists which were automatically extracted from unlabeled data.
E06-3004.txt,39,More detailed explanation about their generation can be found in Section 3.
E06-3004.txt,40,To explore the effect of labeled and unlabeled trainingdatatoourNER_comma_twotypesofexperiments were conducted.
E06-3004.txt,41,For the supervised approach_comma_ the labels in the training data were previously known.
E06-3004.txt,42,Forthe semi supervised approach_comma_ the labels in the training data were hidden.
E06-3004.txt,43,We used bootstrapping Abney_comma_ 2002 which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data_comma_ and the task is to induce a classifier. Goals utilize a minimal amount of supervised examples 3 President of the United States 4 Technical University of Catalu na 5book titles_comma_ sport events_comma_ etc. obtain learning from many unlabeled examples General scheme initial supervision seed examples for training an initial model corpus classification with seed model add most confident classifications to training data and iterate.
E06-3004.txt,44,In our bootstrapping_comma_ a newly labeled example was added into the training data L_comma_ if the two classifiers C1 and C2 agreed on the class of that example.
E06-3004.txt,45,The number n of iterations for our experiments is set up to 25 and when this bound is reached the bootstrapping stops.
E06-3004.txt,46,The scheme we follow is described below.
E06-3004.txt,47,1. for iteration 0...n do 2. pool 1000 examples from unlabeled data 3. annotate all 1000 examples with classifier C1 and C2 4. for each of the 1000 examples compare classes of C1 and C2 5. add example into L only if classes of C1 and C2 agree 6. train model with L 7. calculate result 8. end for Bootstrapping was previously used by Carreras et al._comma_ 2003 _comma_ who were interested in recognizing Catalan names using Spanish resources.
E06-3004.txt,48,Becker et al._comma_ 2005 employed bootstrapping in an active learning method for tagging entities in an astronomic domain.
E06-3004.txt,49,Yarowsky_comma_ 1995 and Mihalcea and Moldovan_comma_ 2001 utilized bootstrapping for word sense disambiguation.
E06-3004.txt,50,Collins and Singer_comma_ 1999 classified NEs through co training_comma_ Kozareva et al._comma_ 2005a used self training and cotraining to detect and classify named entities in news domain_comma_ Shen et al._comma_ 2004 conducted experimentswithmulti criteria basedactivelearning for biomedical NER.
E06-3004.txt,51,The experimental data we work with is taken from the CoNLL 2002 competition.
E06-3004.txt,52,The Spanish 16 corpus6 comes from news domain and was previously manually annotated.
E06-3004.txt,53,The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities.
E06-3004.txt,54,We decided to work with available NE annotatedcorporainordertoconductanexhaustiveand comparative NER study when labeled and unlabeld data is present.
E06-3004.txt,55,For our bootstrapping experiment_comma_ we simply ignored the presence of the labels inthetrainingdata.
E06-3004.txt,56,Ofcoursethisapproachcanbe applied to other domain or language_comma_ the only need is labeled test data to conduct correct evaluation.
E06-3004.txt,57,TheevaluationiscomputedperNEclassbythe help of conlleval7 script.
E06-3004.txt,58,The evaluation measures are Precision numberof correctanswersfoundbythesystemnumberof answersgivenbythesystem 1 Recall numberof correctanswersfoundbythesystemnumberof correctanswersinthetestcorpus 2 F 1 2 Precision RecallPrecision Recall 3 3 Feature extraction Recently diverse machine learning techniques are utilized to resolve various NLP tasks.
E06-3004.txt,59,For all of them crucial role plays the feature extraction and selection module_comma_ which leads to optimal classifier performance.
E06-3004.txt,60,This section describes the features used for our Named Entity Recognition task.
E06-3004.txt,61,Feature vectors i f1_comma_..._comma_fn are constructed.
E06-3004.txt,62,The total number of features is denoted by n_comma_ and i corresponds to the number of examples in the data.
E06-3004.txt,63,In our experiment features represent contextual_comma_ lexical and gazetteer information.
E06-3004.txt,64,Here we number each feature and its corresponding argument.
E06-3004.txt,65,f1 all letters of w08 are in capitals f2 f8 w 3_comma_w 2_comma_w 1_comma_w0_comma_w 1_comma_w 2_comma_w 3 initiate in capitals f9 position of w0 in the current sentence f10 frequency of w0 f11 f17 word forms of w0 and the words in 3_comma_ 3 window f18 first word making up the entity f19 second word making up the entity_comma_ if present 6http www.cnts.ua.ac.be conll2002 ner data 7http www.cnts.ua.ac.be conll2002 ner bin 8w0 indicates the word to be classified.
E06-3004.txt,66,f20 w 1 is trigger word for location_comma_ person or organization f21 w 1 is trigger word for location_comma_ person or organization f22 w0 belongs to location gazetteer list f23 w0 belongs to first person name gazetteer list f24 w0 belongs to family name gazetteer list f25 0 if the majority of the words in an entity are locations_comma_ 1 if the majority of the words in an entity are persons and 2 otherwise.
E06-3004.txt,67,Features f22_comma_ f23_comma_ f24 were automatically extracted by a simple pattern validation method we propose below.
E06-3004.txt,68,The corpus from where the gazetteer lists were extracted_comma_ forms part of Efe94 and Efe95 Spanish corpora provided for the CLEF9 competitions.
E06-3004.txt,69,We conducted a simple preprocessing_comma_ where all sgml documents were merged in a single file and only the content situated among the text tags was extracted and considered for further processing.
E06-3004.txt,70,As a result_comma_ we obtained 1 Gigabyte of unlabeled data_comma_ containing 173468453 words.
E06-3004.txt,71,The text was tokenized and the frequency of all unigrams in the corpus was gathered.
E06-3004.txt,72,The algorithm we propose and use to obtain location and person gazetteer lists is very simple.
E06-3004.txt,73,It consists in finding and validating common patterns_comma_ which can be constructed and utilized also for languages other than Spanish.
E06-3004.txt,74,The location pattern prepi_comma_ wj _comma_ looks for preposition i which indicates location in the Spanish language and all corresponding right capitalized context words wj for preposition i.
E06-3004.txt,75,The dependency relation between prepi and wj_comma_ conveys the semantic information on the selection restrictions imposed by the two related words.
E06-3004.txt,76,In a walk through example the pattern en_comma_ _comma_ extracts all right capitalized context words wj as Argentina_comma_ Barcelona_comma_ Madrid_comma_ Valencia placed next to preposition en .
E06-3004.txt,77,These words are taken as location candidates.
E06-3004.txt,78,The selection restriction implies searching for words appearing after the preposition en e.g. en Madrid and not before the preposition e.g. Madrid en .
E06-3004.txt,79,Theterminationofthepatternextraction en_comma_ _comma_ initiates the extraction phase for the next prepositions in prepi en_comma_ En_comma_ desde_comma_ Desde_comma_ hacia_comma_ Hacia .
E06-3004.txt,80,This processes is repeated until the complete set of words in the preposition set are validated.
E06-3004.txt,81,Table 1 represents the number of entities extracted 9http www.clef campaign.org 17 by each one of the preposition patterns. pi en En desde Desde hacia Hacia wj 15567 2381 1773 320 1336 134 Table 1 Extracted entities The extracted capitalized words are passed through a filtering process.
E06-3004.txt,82,Bigrams prepi Capitalized wordj with frequency lower than 20 were automatically discarded_comma_ because we saw that this threshold removes words that do not tend to appear very often with the location prepositions.
E06-3004.txt,83,In this way misspelled words as Bacelona instead of Barcelona were filtered.
E06-3004.txt,84,From another side_comma_ every capitalized word composed of two or three characters_comma_ for instance La_comma_ Las was initiated in a trigram prepi_comma_Capitalized wordj_comma_Capitalized wordj 1 validation pattern.
E06-3004.txt,85,If these words were seen in combination with other capitalized words and their trigram frequency was higher then 20 they were included in the location gazetteer file.
E06-3004.txt,86,With this trigram validation pattern_comma_ locations as Los Angeles _comma_ Las Palmas _comma_ La Coru na _comma_ Nueva York 10 were extracted.
E06-3004.txt,87,In total 16819 entities with no repetition were automatically obtained.
E06-3004.txt,88,The words represent countries around the world_comma_ European capitals and mostly Spanish cities.
E06-3004.txt,89,Some noisy elements found in the file were person names_comma_ which were accompanied by the preposition en .
E06-3004.txt,90,As person names werecapitalizedandhadfrequencyhigherthanthe threshold we placed_comma_ it was impossible for these names to be automatically detected as erroneous and filtered.
E06-3004.txt,91,However we left these names_comma_ since the gazetteer attributes we maintain are mutually nonexclusive.
E06-3004.txt,92,This means the name Jordan can be seen in location gazetteer indicating the country Jordan and in the same time can be seen in the person name list indicating the person Jordan.
E06-3004.txt,93,In a real NE application such case is reality_comma_ but for the determination of the right category name entity disambiguation is needed as in Pedersen et al._comma_ 2005 .
E06-3004.txt,94,Person gazetteer is constructed with graph exploration algorithm.
E06-3004.txt,95,The graph consists of 1. two kinds of nodes First Names Family Names 10New York 2. undirected connections between First Names and Family Names.
E06-3004.txt,96,The graph connects Family Names with First Names_comma_ and vice versa.
E06-3004.txt,97,In practice_comma_ such a graph is not necessarily connected_comma_ as there can be unusual first names and surnames which have no relation with other names in the corpus.
E06-3004.txt,98,Though_comma_ the corpus is supposed to contain mostly common names in one and the same language_comma_ names from other languages might be present too.
E06-3004.txt,99,In this case_comma_ if the foreign name is not connected with a Spanish name_comma_ it will never be included in the name list.
E06-3004.txt,100,Therefore_comma_ starting from some common Spanish name will very probably place us in the largest connected component11.
E06-3004.txt,101,If there exist other different connected components in the graph_comma_ these will be outliers_comma_ corresponding to names pertaining to someotherlanguage_comma_ orcombinationsofbothvery unusual first name and family name.
E06-3004.txt,102,The larger the corpus is_comma_ the smaller the presence of such additional connected components will be.
E06-3004.txt,104,As the graph is not a tree_comma_ the stop condition occurs when no more nodes are found.
E06-3004.txt,105,Nodes and connections are found following the pattern First name_comma_Family name .
E06-3004.txt,106,The node from which we start the search can be a common Spanish first or family name.
E06-3004.txt,107,In our example we started from the Spanish common first name Jos e.
E06-3004.txt,108,The notation i_comma_j C refers to finding in the corpus C the regular expression12 A Z a z A Z a z This regular expression indicates a possible relation between first name and family name.
E06-3004.txt,109,The scheme of the algorithm is the following Let C be the corpus_comma_ F be the set of first names_comma_ and S be the set of family names.
E06-3004.txt,111,F  Jos e 2.
E06-3004.txt,112,i F do Snew Snew j _comma_ j i_comma_j C 3.
E06-3004.txt,113,S S Snew 4. j S do Fnew Fnew i _comma_ i i_comma_j C 11A connected component refers to a maximal connected subgraph_comma_ in graph theory.
E06-3004.txt,114,A connected graph_comma_ is a graph containing only one connected component.
E06-3004.txt,115,12For Spanish some other characters have to be added to the regular expression_comma_ such as n and accents.
E06-3004.txt,116,18 Manolo Jose Maria Garcia Martinez Fernandez John Lennon First FamilyRelationsnamenodes namenodes Connected Component Connected Component Figure 1 An example of connected components.
E06-3004.txt,118,F F Fnew 6. if Fnew negationslash  Snew negationslash  then goto 2. else finish.
E06-3004.txt,119,Suppose we have a corpus containing the following person names Jos e Garc a _comma_ Jos e Mart nez _comma_ Manolo Garc a _comma_ Mar a Mart nez _comma_ Mar a Fern andez _comma_ John Lennon  C.
E06-3004.txt,120,Initially we have F  Jos e and S .
E06-3004.txt,121,After the 3rd step we would have S  Garc a _comma_ Mart nez _comma_ and after the 5th step F  Jos e _comma_ Manolo _comma_ Mar a .
E06-3004.txt,122,During the next iteration Fern andez would also be added to S_comma_ as Mar a is already present in F.
E06-3004.txt,123,Neither John _comma_ nor Lennon are connected to the rest of the names_comma_ so these will never be added to the sets.
E06-3004.txt,124,This can be seen in Figure 1 as well.
E06-3004.txt,125,In our implementation_comma_ we filtered relations appearing less than 10 times.
E06-3004.txt,126,Thus rare combinations like Jose Madrid_comma_ Mercedes Benz are filtered.
E06-3004.txt,127,Noise was introduced from names related to both person and organization names.
E06-3004.txt,128,For example the Spanish girl name Mercedes_comma_ lead to the node Benz_comma_ and as Mercedes Benz refers also to the car producing company_comma_ noisy elements started to be added through the node Benz .
E06-3004.txt,129,In total 13713 fist names and 103008 surnames have been automatically extracted.
E06-3004.txt,130,We believe and prove that constructing automatic location and person name gazetteer lists with the pattern search and validation model we propose is a very easy and practical task.
E06-3004.txt,131,With our approach thousands of names can be obtained_comma_ especially given the ample presence of unlabeled data and the World Wide Web.
E06-3004.txt,132,The purpose of our gazetteer construction was not to make complete gazetteer lists_comma_ but rather generate in a quick and automatic way lists of names that can help during our feature construction module.
E06-3004.txt,133,4 Experiments for delimitation process In this section we describe the conducted experiments for named entity detection.
E06-3004.txt,134,Previously Kozareva et al._comma_ 2005b demonstrated that in supervised learning only superficial features as context and ortografics are sufficient to identify the boundaries of a Named Entity.
E06-3004.txt,135,In our experiment the superficial features f1 f10 were used by the supervised and semi supervised classifiers.
E06-3004.txt,136,Table 2 shows the obtained results for Begin and Inside tags_comma_ whichactuallydetecttheentitiesandthetotal BIO tag performance. experiment B I BIO Supervised 94.40 85.74 91.88 Bootstrapped 87.47 68.95 81.62 Table 2 F score of detected entities.
E06-3004.txt,137,On the first row are the results of the supervised method and on the second row are the highest results of the bootstrapping achieved in its seventeenth iteration.
E06-3004.txt,138,For the supervised learning 91.88 of the entity boundaries were correctly identified and for the bootstrapping 81.62 were correctly detected.
E06-3004.txt,139,The lower performance ofbootstrappingisduetothenoiseintroducedduring the learning.
E06-3004.txt,140,Some examples were learned with the wrong class and others didn t introduce new information in the training data.
E06-3004.txt,141,Figure 2 presents the learning curve of the bootstrapping processes for 25 iterations.
E06-3004.txt,142,On each iteration 1000 examples were tagged_comma_ but only the examples having classes that coincide by the two classifiers were later included in the training data.
E06-3004.txt,143,We should note that for each iteration the same amount of B_comma_ I and O classes was included.
E06-3004.txt,144,Thus thebalanceamongthethreedifferentclassesinthe training data is maintained.
E06-3004.txt,145,According to zprime statistics Dietterich_comma_ 1998 _comma_ the highest score reached by bootstrapping cannot outperform the supervised method_comma_ however if both methods were evaluated on small amount of data the results were similar.
E06-3004.txt,146,19 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 2560 65 70 75 80 85 iterations f score Figure 2 Bootstrapping performance 5 Experiments for classification process In a Named Entity classification process_comma_ to the previously detected Named Entities a predefined category of interest such as name of person_comma_ organization_comma_ location or miscellaneous names should be assigned.
E06-3004.txt,147,To obtain a better idea of the performanceoftheclassificationmethods_comma_severalexperiments were conducted.
E06-3004.txt,148,The influence of the automatically extracted gazetteers was studied_comma_ and a comparison of the supervised and semi supervised methods was done. experiment PER LOC ORG MISC NoGazetteerSup.
E06-3004.txt,149,80.98 71.66 73.72 49.94 GazetteerSup.
E06-3004.txt,150,84.32 75.06 77.83 53.98 Bootstrapped 62.59 51.19 50.18 33.04 Table 3 F score of classified entities.
E06-3004.txt,151,Table 3 shows the obtained results for each one of the experimental settings.
E06-3004.txt,152,The first row indicates the performance of the supervised classifier when no gazetteer information is present.
E06-3004.txt,153,The classifier used f1_comma_ f2_comma_ f3_comma_ f4_comma_ f5_comma_ f6_comma_ f7_comma_ f8_comma_ f18_comma_ f19_comma_ f20_comma_ f21 attributes.
E06-3004.txt,154,The performance of the second row concerns the same classifier_comma_ but including the gazetteer information by adding f22_comma_ f23_comma_ f24 and f25 attributes.
E06-3004.txt,155,The third row relates to the bootstrapping process.
E06-3004.txt,156,The attributes used for the supervised and semi supervised learning were the same.
E06-3004.txt,157,Results show that among all classes_comma_ miscellaneousistheonewiththelowestperformance.
E06-3004.txt,158,This is related to the heterogeneous information of the category.
E06-3004.txt,159,The other three categories performed above 70 .
E06-3004.txt,160,As expected gazetteer information contributed for better distinction of person and location names.
E06-3004.txt,161,Organizationnames benefitted from the contextual information_comma_ the organization trigger words and the attribute validating if an entity is not a person or location then is treated as an organization.
E06-3004.txt,162,Bootstrapping performance was not high_comma_ due to the previously 81 correctly detected named entity boundaries and from another side to the training examples which were incorrectly classified and included into the training data.
E06-3004.txt,163,In our experiment_comma_ unlabeled data was used to construct in an easy and effective way person and location gazetteer lists.
E06-3004.txt,164,By their help supervised and semi supervised classifiers improved performance.
E06-3004.txt,165,Although one semi supervised method cannot reach the performance of a supervised classifier_comma_ we can say that results are promising.
E06-3004.txt,166,We call them promising in the aspect of constructing NE recognizer for languages with no resources or even adapting the present Spanish Named Entity system to other domain.
E06-3004.txt,167,6 Conclusions and future work In this paper we proposed and implemented a pattern validation search in an unlabeled corpus though which gazetteer lists were automatically generated.
E06-3004.txt,168,The gazetteers were used as features by a Named Entity Recognition system.
E06-3004.txt,169,The performance of this NER system_comma_ when labeled and unlabeled training data was available_comma_ was measured.
E06-3004.txt,170,A comparative study for the information contributed by the gazetteers in the entity classification process was shown.
E06-3004.txt,171,In the future we intend to develop automatic gazetteers for organization and product names.
E06-3004.txt,172,It is also of interest to divide location gazetteers in subcategories as countries_comma_ cities_comma_ rivers_comma_ mountains as they are useful for Geographic Information Retrieval systems.
E06-3004.txt,173,To explore the behavior of named entity bootstrapping_comma_ other domains as bioinformatics will be explored.
E06-3004.txt,174,Acknowledgements Many thanks to the three anonymous reviewers for their useful comments and suggestions.
E06-3004.txt,175,This research has been partially funded by the SpanishGovernmentunderprojectCICyTnumber TIC2003 0664 C02 02 and PROFIT number FIT340100 2004 14 and by the Valencia Government under project numbers GV04B 276 and GV04B268.
E06-3002.txt,1,verbal jokes_comma_ like garden path sentences_comma_ pose difficulties to models of discourse since the initially primed interpretation needs to be discarded and a new one created based on subsequent statements. The effect of the joke depends on the fact that the second correct interpretation was not visible earlier.
E06-3002.txt,2,Existing models of discourse semantics in principle generate all interpretations of discourse fragments and carry these until contradicted_comma_ and thus the dissonance criteria in humour cannot be met.
E06-3002.txt,3,Computationally_comma_ maintaining all possible worlds in a discourse is very inefficient_comma_ thus computing only the maximum likelihood interpretation seems to be a more efficient choice on average.
E06-3002.txt,4,In this work we outline a probabilistic lexicon based lexical semantics approach which seems to be a reasonable construct for discourse in general and use some examples from humour to demonstrate its working.
E06-3002.txt,6,Consider the following 1 I still miss my ex wife_comma_ but my aim is improving.
E06-3002.txt,7,2 The horse raced past the barn fell.
E06-3002.txt,8,In a discourse structure common to many jokes_comma_ the first part of 1 has a default set of interpretations_comma_ say P1_comma_ for which no consistent interpretation can be found when the second part of the joke is uttered.
E06-3002.txt,9,After a search_comma_ the listener reaches P2P 1 J2J1 time t TP I still miss my ex wife_comma_ but my aim is improving search gap Figure 1 Cognitive model of destructive dissonance as in joke 1 .
E06-3002.txt,10,The initial sentence primes the possible world P1 where miss is taken in an emotional sense.
E06-3002.txt,11,After encountering the word aim this is destroyed and eventually a new world P2 arises where miss is taken in the physical sense. the alternate set of interpretations P2 Figure 1 .
E06-3002.txt,12,A similar process holds for garden path sentences such as 2 _comma_ where the default interpretation created in the first part upto the word barn has to be discarded when the last part is heard.
E06-3002.txt,13,The search involved in identifying the second interpretation is an important indicator of human communication_comma_ and linguistic impairment such as autism often leads to difficulty in identifying jokes.
E06-3002.txt,14,Yet_comma_ this aspect of discourse is not sufficiently emphasized in most computational work.
E06-3002.txt,15,Cognitively_comma_ this is a form of dissonance_comma_ a violation of expectation.
E06-3002.txt,16,However_comma_ unlike some forms of dissonance which may be constructive_comma_ leading to metaphoric or implicature shifts_comma_ where part of the original interpretation may be retained_comma_ these discourse structures are destructive_comma_ and the original interpretation has to be completely abandoned_comma_ and a new one searched out Figure 2 .
E06-3002.txt,17,Often this is because the default interpretation involves a sense association that has very high coherence in the immediate context_comma_ but is nullified by later 31 P1 P2 P1 P2 P2P1 P1 P2 a b d c Figure 2 Cognitive Dissonance in Discourse a c can be Constructive_comma_ where the interpretation P1 does not disappear completely after the dissonant utterance_comma_ or d Destructive_comma_ where P2 has to be arrived at afresh and P1 is destroyed completely. utterances.
E06-3002.txt,18,While humour may involve a number of other mechanisms such as allusion or stereotypes Shibles_comma_ 1989 Gruner_comma_ 1997 _comma_ a wide class of verbal humour exhibits destructive dissonance.
E06-3002.txt,19,For a joke to work_comma_ the resulting interpretation must result in an incongruity_comma_ what Freud_comma_ 1960 calls an energy release that breaks the painful barriers we have around forbidden thoughts.
E06-3002.txt,20,Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics.
E06-3002.txt,21,Computational theories such as the General Theory of Verbal Humour Attardo and Raskin_comma_ 1991 have avoided this difficult problem by adopting extra linguistic knowledge in the form of scripts_comma_ which encode different oppositions that may arise in jokes.
E06-3002.txt,22,Others Minsky_comma_ 1986 posit a general mechanism without considering specifics.
E06-3002.txt,23,Other models in computation have attempted to generate jokes using templates Attardo and Raskin_comma_ 1994 Binsted and Ritchie_comma_ 1997 or recognize jokes using machine learning models Mihalcea and Strapparava_comma_ 2005 .
E06-3002.txt,24,Computationally_comma_ the fact that other less likely interpretations such as P2 are not visible initially_comma_ may also result in considerably efficiency in more common situations_comma_ where ambiguities are not generated to begin with.
E06-3002.txt,25,For example_comma_ in joke 1 the interpretation after reading the first clause_comma_ has the word miss referring to the .
E06-3008.txt,1,paper presents results from experiments in automatic classification of animacy for Norwegian nouns using decision tree classifiers. The method makes use of relative frequency measures for linguistically motivated morphosyntactic features extracted from an automatically annotated corpus of Norwegian.
E06-3008.txt,2,The classifiers are evaluated using leave oneout training and testing and the initial results are promising approaching 90 accuracy forhighfrequency nouns_comma_ however deteriorate gradually as lower frequency nouns are classified.
E06-3008.txt,3,Experiments attempting to empirically locate a frequency threshold for the classification method indicate that a subset of the chosen morphosyntactic features exhibit a notable resilience to data sparseness.
E06-3008.txt,4,Results will be presented which show that the classification accuracy obtained for high frequency nouns with absolute frequencies 1000 can be maintained for nouns with considerably lower frequencies 50 by backing off to a smaller set of features at classification.
E06-3008.txt,6,Animacy is a an inherent property of the referents of nouns which has been claimed to figure as an influencing factor in a range of different grammatical phenomena in various languages and it is correlated with central linguistic concepts such as agentivity and discourse salience.
E06-3008.txt,7,Knowledge about the animacy of a noun is therefore relevant for several different kinds of NLP problems ranging from coreference resolution toparsing and generation.
E06-3008.txt,8,In recent years a range of linguistic studies have examined the influence of argument animacy in grammatical phenomena such as differential object marking Aissen_comma_ 2003 _comma_ the passive construction Dingare_comma_ 2001 _comma_ the dative alternation Bresnan et al._comma_ 2005 _comma_ etc. A variety of languages are sensitive to the dimension of animacy in the expression and interpretation of core syntactic arguments Lee_comma_ 2002 vrelid_comma_ 2004 .
E06-3008.txt,9,A key generalisation or tendency observed there is that prominent grammatical features tend to attract other prominent features 1 subjects_comma_ for instance_comma_ will tend to be animate and agentive_comma_ whereas objects prototypically are inanimate and themes patients.
E06-3008.txt,10,Exceptions to this generalisation express a more marked structure_comma_ a property which has consequences_comma_ for instance_comma_ for the distributional properties of the structure in question.
E06-3008.txt,11,Even though knowledge about the animacy of a noun clearly has some interesting implications_comma_ little work has been done within the field of lexical acquisition in order to automatically acquire such knowledge.
E06-3008.txt,12,Or asan and Evans 2001 make useofhyponym relations takenfrom theWordNet resource Fellbaum_comma_ 1998 in order to classify animate referents.
E06-3008.txt,13,However_comma_ such a method is clearly restricted to languages for which large scale lexical resources_comma_ such as the Word Net_comma_ are available.
E06-3008.txt,14,Merlo and Stevenson 2001 present a method for verb classification which relies only on distributional statistics taken from corpora inorder totrain a decision tree classifier to distinguish between three groups of intransitive verbs.
E06-3008.txt,15,1The notion of prominence has been linked to several properties such as most likely as topic_comma_ agent_comma_ most available referent_comma_ etc. 47 This paper presents experiments in automatic classification of the animacy of unseen Norwegian common nouns_comma_ inspired by the method for verb classification presented in Merlo and Stevenson 2001 .
E06-3008.txt,16,The learning task is_comma_ for a given common noun_comma_ to classify it as either belonging to the class animate or inanimate.
E06-3008.txt,17,Based on correlations between animacy and other linguistic dimensions_comma_ a set of morphosyntactic features is presented and shown to differentiate common nouns along the binary dimension of animacy with promising results.
E06-3008.txt,18,Themethodreliesonaggregated relative frequencies for common noun lemmas_comma_ hence might be expected to seriously suffer from data sparseness.
E06-3008.txt,19,Experiments attempting to empirically locate a frequency threshold for the classification method will therefore be presented.
E06-3008.txt,20,It turns out that a subset of the chosen morphosyntactic approximators of animacy show a resilience to data sparseness which can be exploited in classification.
E06-3008.txt,21,By backing off to this smaller set of features_comma_ we show that we can maintain the same classification accuracy also for lower frequency nouns.
E06-3008.txt,22,The rest of the paper is structured as follows.
E06-3008.txt,23,Section 2identifies andmotivates thesetofchosen features for the classification task and describes how these features are approximated through feature extraction from an automatically annotated corpus of Norwegian.
E06-3008.txt,24,In section 3_comma_ a group of experiments testing the viability of the method and chosen features is presented.
E06-3008.txt,25,Section 4 goes on to investigate the effect of sparse data on the classification performance and present experiments which address possible remedies for the sparse data problem.
E06-3008.txt,26,Section 5 sums up the main findings of the previous sections and outlines a few suggestions for further research.
E06-3008.txt,27,2 Features of animacy As mentioned above_comma_ animacy is highly correlated with a number of other linguistic concepts_comma_ such as transitivity_comma_ agentivity_comma_ topicality and discourse salience.
E06-3008.txt,28,The expectation is that marked configurations along these dimensions_comma_ e.g. animate objects or inanimate agents_comma_ are less frequent in the data.
E06-3008.txt,29,However_comma_ these are complex notions to translate into extractable features from a corpus.
E06-3008.txt,30,In the following we will present some morphological andsyntactic features which_comma_ indifferent ways_comma_ approximate the multi faceted property of animacy Transitive subject and direct object As mentioned earlier_comma_ a prototypical transitive relation involves an animate subject and an inanimate object.
E06-3008.txt,31,In fact_comma_ a corpus study of animacy distribution in simple transitive sentences in Norwegian revealed that approximately 70 of the subjects of these types of sentences were animate_comma_ whereas as many as 90 of the objects were inanimate vrelid_comma_ 2004 .
E06-3008.txt,32,Although this corpus study involved all types of nominal arguments_comma_ including pronouns and proper nouns_comma_ it still seems that the frequency with which a certain noun occurs as a subject or an object of a transitive verb might be an indicator of its animacy.
E06-3008.txt,33,Demoted agent in passive Agentivity is another related notion to that of animacy_comma_ animate beings are usually inherently sentient_comma_ capable of acting volitionally and causing an event to take place all properties of the prototypical agent Dowty_comma_ 1991 .
E06-3008.txt,34,The passive construction_comma_ or rather the property of being expressed as the demoted agent in a passive construction_comma_ is a possible approximator of agentivity.
E06-3008.txt,35,It is well known that transitive constructions tend to passivize better hence more frequently if the demoted subject bears a prominent thematic role_comma_ preferably agent.
E06-3008.txt,36,Anaphoric reference by personal pronoun Anaphoric reference is a phenomenon where the animacy of areferent isclearly expressed.
E06-3008.txt,37,The Norwegian personal pronouns distinguish their antecedents along the animacy dimension animate han hun he she vs. inanimate den det it MASC NEUT .
E06-3008.txt,38,Anaphoric reference by reflexive pronoun Reflexive pronouns represent another form of anaphoric reference_comma_ and_comma_ may_comma_ in contrast to the personal pronouns locate their antecedent locally_comma_ i.e. within the same clause.
E06-3008.txt,39,In the prototypical reflexive construction the subject and the reflexive object are coreferent and it describes an action directed at oneself.
E06-3008.txt,40,Although the reflexive pronoun in Norwegian does not distinguish for animacy_comma_ the agentive semantics of the construction might still favour an animate subject.
E06-3008.txt,41,Genitive s There is no extensive case system for common nouns in Norwegian and the only 48 distinction that is explicitly marked on the noun is the genitive case by addition of s.
E06-3008.txt,42,The genitive construction typically describes possession_comma_ arelation whichoften involves an animate possessor.
E06-3008.txt,43,2.1 Feature extraction In order to train a classifier to distinguish between animate and inanimate nouns_comma_ training data consisting of distributional statistics on the above features were extracted from a corpus.
E06-3008.txt,44,For this end_comma_ a 15 million word version of the Oslo Corpus_comma_ a corpus of Norwegian texts of approximately 18.5 million words_comma_ wasemployed.2 Thecorpus ismorphosyntactically annotated and assigns an underspecified dependency style analysis to each sentence.3 For each noun_comma_ relative frequencies for the different morphosyntactic features described above were computed from the corpus_comma_ i.e. the frequency of the feature relative to this noun is divided by the total frequency of the noun.
E06-3008.txt,45,For transitive subjects SUBJ _comma_ we extracted the number of instances where the noun in question was unambiguously tagged as subject_comma_ followed by a finite verb and an unambiguously tagged object.4 The frequency of direct objects OBJ for a given noun was approximated to the number of instances where the noun in question was unambiguously tagged as object.
E06-3008.txt,46,We here assume that an unambiguously tagged object implies an unambiguously tagged subject.
E06-3008.txt,47,However_comma_ by not explicitly demanding that the object is preceded by a subject_comma_ we also capture objects with a missing subject_comma_ such as objects occurring in relative clauses and infinitival clauses.
E06-3008.txt,48,Asmentioned earlier_comma_ another context where animate nouns might be predominant is in the byphrase expressing the demoted agent of a passive verb PASS .
E06-3008.txt,49,Norwegian has two ways of expressing the passive_comma_ a morphological passive verb s and a periphrastic passive bli past participle .
E06-3008.txt,50,The counts for passive by phrases allow for both types ofpassives to precede the by phrase containing the noun in question.
E06-3008.txt,51,2The corpus is freely available for research purposes_comma_ see http www.hf.uio.no tekstlab for more information.
E06-3008.txt,52,3The actual framework is that of Constraint Grammar Karlsson et al._comma_ 1995 _comma_ and the analysis is underspecified as the nodes are labelled only with their dependency function_comma_ e.g. subject or prepositional object_comma_ and their immediate heads are not uniquely determined.
E06-3008.txt,53,4The tagger works in an eliminative fashion_comma_ so tokens may bear two or more tags when they have not been fully disambiguated.
E06-3008.txt,54,With regard to the property of anaphoric reference by personal pronouns_comma_ the extraction was bound to be a bit more difficult.
E06-3008.txt,55,The anaphoric personal pronoun is never in the same clause as the antecedent_comma_ and often not even in the same sentence.
E06-3008.txt,56,Coreference resolution is a complex problem_comma_ and certainly not one that we shall attempt to solve in the present context.
E06-3008.txt,57,However_comma_ we might attempt to come up with a metric that approximates the coreference relation in a manner adequate for our purposes_comma_ that is_comma_ which captures the different coreference relation for animate as opposed to inanimate nouns.
E06-3008.txt,58,To this end_comma_ we make useofthecommonassumption thatapersonal pronoun usually refers to a discourse salient element which is fairly recent in the discourse.
E06-3008.txt,59,Now_comma_ if a sentence only contains one core argument i.e. an intransitive subject and it is followed by a sentence initiated byapersonal pronoun_comma_ itseems reasonable to assume that these are coreferent Hale and Charniak_comma_ 1998 .
E06-3008.txt,60,For each of the nouns then_comma_ we count the number of times it occurs as a subject with no subsequent object and an immediately following sentence initiated by i an animate personal pronoun ANAAN and ii an inanimate personal pronouns ANAIN .
E06-3008.txt,61,The feature of reflexive coreference is easier to approximate_comma_ as this coreference takes place within the same clause.
E06-3008.txt,62,For each noun_comma_ the number of occurrences as a subject followed by a verb and the 3.person reflexive pronoun seg him her itself are counted and its relative frequency recorded.
E06-3008.txt,63,The genitive feature GEN simply contains relative frequencies ofthe occurrence of each noun with genitive case marking_comma_ i.e. the suffix s.
E06-3008.txt,64,3 Method viability In order to test the viability of the classification method for this task_comma_ and in particular_comma_ the chosen features_comma_ a set of forty highly frequent nouns were selected twenty animate and twenty inanimate nouns.
E06-3008.txt,65,A frequency threshold of minimum one thousand occurrences ensured sufficient data for all the features_comma_ as shown in table 1_comma_ which reports the mean values along with the standard deviation for each class and feature.
E06-3008.txt,66,The total data points for each feature following the data collection are as follows SUBJ 16813_comma_ OBJ 24128_comma_ GEN 7830_comma_ PASS 577_comma_ ANAANIM 989_comma_ ANAINAN 944_comma_ REFL 558.
E06-3008.txt,67,As we can see_comma_ quite a few of the features express morphosyntactic cues that are 49 SUBJ OBJ GEN PASS ANAAN ANAIN REFL Class Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD Mean SD A 0.14 0.05 0.11 0.03 0.04 0.02 0.006 0.005 0.009 0.006 0.003 0.003 0.005 0.0008 I 0.07 0.03 0.23 0.10 0.02 0.03 0.002 0.002 0.003 0.002 0.006 0.003 0.001 0.0008 Table 1 Mean relative frequencies and standard deviation for each class A nimate vs. I nanimate from feature extraction SUBJ Transitive Subject_comma_ OBJ Object_comma_ GEN Genitive s_comma_ PASS Passive byphrase_comma_ ANAAN Anaphoric reference by animate pronoun_comma_ ANAIN Anaphoric reference by inanimate pronoun_comma_ REFL Anaphoric reference by reflexive pronoun .
E06-3008.txt,68,Feature Accuracy SUBJ 85.0 OBJ 72.5 GEN 72.5 PASS 62.5 ANAAN 67.5 ANAIN 50.0 REFL 82.5 Table 2 Accuracy for the individual features using leaveone out training and testing Features used Feature Not Used Accuracy 1.
E06-3008.txt,69,SUBJ OBJ GEN PASS ANAAN ANAIN REFL 87.5 2.
E06-3008.txt,70,OBJ GEN PASS ANAAN ANAIN REFL SUBJ 85.0 3.
E06-3008.txt,71,SUBJ GEN PASS ANAAN ANAIN REFL OBJ 87.5 4.
E06-3008.txt,72,SUBJ OBJ PASS ANAAN ANAIN REFL GEN 85.0 5.
E06-3008.txt,73,SUBJ OBJ GEN ANAAN ANAIN REFL PASS 82.5 6.
E06-3008.txt,74,SUBJ OBJ GEN PASS ANAIN REFL ANAAN 82.5 7.
E06-3008.txt,75,SUBJ OBJ GEN PASS ANAAN REFL ANAIN 87.5 8.
E06-3008.txt,76,SUBJ OBJ GEN PASS ANAAN ANAIN REFL 75.0 9.
E06-3008.txt,77,OBJ PASS ANAAN ANAIN SUBJ GEN REFL 77.5 Table 3 Accuracy for all features and all minus one using leave one out training and testing rather rare.
E06-3008.txt,78,This isin particular true for the passive feature and the anaphoric features ANAAN_comma_ ANAIN and REFL.
E06-3008.txt,79,There is also quite a bit of variation in the data represented by the standard deviation for each class feature combination _comma_ a property which is to be expected as all the features represent approximations of animacy_comma_ gathered from an automatically annotated_comma_ possibly quite noisy_comma_ corpus.
E06-3008.txt,80,Even so_comma_ the features all express a difference between the two classes in terms of distributional properties the difference between the mean feature values for the two classes range from double to five times the lowest class value.
E06-3008.txt,81,3.1 Experiment 1 Based on the data collected on seven different features for our 40 nouns_comma_ a set of feature vectors are constructed for each noun.
E06-3008.txt,82,They contain the relative frequencies for each feature along with the name of the noun and its class animate or inanimate .
E06-3008.txt,83,Note that the vectors do not contain the mean values presented in Table 1 above_comma_ but rather the individual relative frequencies for each noun.
E06-3008.txt,84,The experimental methodology chosen for the classification experiments is similar to the one described in Merlo and Stevenson 2001 for verb classification.
E06-3008.txt,85,We also make use of leave oneout training and testing of the classifiers and the same software package for decision tree learning_comma_ C5.0 Quinlan_comma_ 1998 _comma_ isemployed.
E06-3008.txt,86,Inaddition_comma_ all our classifiers employ the boosting option for constructing classifiers Quinlan_comma_ 1993 .
E06-3008.txt,87,For calculation of the statistical significance of differences in the performance of classifiers tested on the same data set_comma_ McNemar s test is employed.
E06-3008.txt,88,Table 2 shows the performance of each individual feature in the classification of animacy.
E06-3008.txt,89,As we can see_comma_ the performance of the features differ quite a bit_comma_ ranging from mere baseline performance ANAIN to a 70 improvement of the baseline SUBJ .
E06-3008.txt,90,Thefirstline ofTable3showsthe performance using all the seven features collectively where we achieve an accuracy of 87.5 _comma_ a 75 improvement of the baseline.
E06-3008.txt,91,The SUBJ_comma_ GEN and REFL features employed individually are the best performing individual features and their classification performance do not differ significantly from the performance of the combined classifier_comma_ whereas the rest of the individual features do at the p .05 level .
E06-3008.txt,92,The subsequent lines 2 8 of Table 3 show the accuracy results for classification using all features except one at a time.
E06-3008.txt,93,This provides an indication of the contribution of each feature to the classification task.
E06-3008.txt,94,In general_comma_ the removal of a feature causes a 0 12.5 deterioration of results_comma_ however_comma_ only the difference in performance caused by the removal of the REFL feature is significant at the p 0.05 level .
E06-3008.txt,95,Since this feature is one of the best performing features individually_comma_ it is not surprising that its removal causes a notable difference in performance.
E06-3008.txt,96,The removal of the 50 ANAIN feature_comma_ on the other hand_comma_ does not have any effect on accuracy whatsoever.
E06-3008.txt,97,This feature was the poorest performing feature with a baseline_comma_ or mere chance_comma_ performance.
E06-3008.txt,98,We also see_comma_ however_comma_ that the behaviour of the features incombination is not strictly predictable from their individual performance_comma_ as presented in table 2.
E06-3008.txt,99,The SUBJ_comma_ GEN and REFL features were the strongest features individually with a performance that did not differ significantly from that of the combined classifier.
E06-3008.txt,100,However_comma_ as line 9 in Table 3 shows_comma_ the classifier as a whole is not solely reliant on these three features.
E06-3008.txt,101,When they are removed from the feature pool_comma_ the performance 77.5 accuracy does not differ significantly p .05 from that of the classifier employing all features collectively.
E06-3008.txt,102,4 Data sparseness and back off The classification experiments reported above impose a frequency constraint absolute frequencies 1000 on the nouns used for training and testing_comma_ in order to study the interaction of the different features without the effects of sparse data.
E06-3008.txt,103,In the light of the rather promising results from these experiments_comma_ however_comma_ it might be interesting to further test theperformance ofour features inclassification as the frequency constraint is gradually relaxed.
E06-3008.txt,104,To this end_comma_ three sets of common nouns each counting 40 nouns 20 animate and 20 inanimate nouns were randomly selected from groups of nouns with approximately the same frequency in the corpus.
E06-3008.txt,105,The first set included nouns with an absolute frequency of 100 20 100 _comma_ the second of50 5 50 and thethird of10 2 10 .
E06-3008.txt,106,Feature extraction followed the same procedure as in experiment 1_comma_ relative frequencies for all seven features were computed and assembled into feature vectors_comma_ one for each noun.
E06-3008.txt,107,4.1 Experiment 2 Effect of sparse data on classification In order to establish how much of the generalizing power of the old classifier is lost when the frequency ofthenouns islowered_comma_ anexperiment was conducted which tested the performance of the old classifier_comma_ i.e. a classifier trained on all the more frequent nouns_comma_ on the three groups of less frequent nouns.
E06-3008.txt,108,As we can see from the first column in Table 4_comma_ this resulted in a clear deterioration of results_comma_ from our earlier accuracy of 87.5 to new accuracies ranging from 70 to 52.5 _comma_ barely above the baseline.
E06-3008.txt,109,Not surprisingly_comma_ the results decline steadily as the absolute frequency of the classified noun is lowered.
E06-3008.txt,110,Accuracy results provide an indication that the classification is problematic.
E06-3008.txt,111,However_comma_ it does not indicate what the damage is to each class as such.
E06-3008.txt,112,A confusion matrix is in this respect more informative.
E06-3008.txt,113,Confusion matrices for the classification of the three groups of nouns_comma_ 100_comma_ 50 and 10_comma_ are provided in table 5.
E06-3008.txt,114,These clearly indicate that it is the animate class which suffers when data becomes more sparse.
E06-3008.txt,115,The percentage of misclassified animate nouns drop drastically from 50 at 100 to 80 at 50 and finally 95 at 10.
E06-3008.txt,116,The classification of the inanimate class remains pretty stable throughout.
E06-3008.txt,117,The fact that a majority of our features SUBJ_comma_ GEN_comma_ PASS_comma_ ANAAN and REFL target animacy_comma_ in the sense that a higher proportion of animate than inanimate nouns exhibit the feature_comma_ gives a possible explanation for this.
E06-3008.txt,118,As data gets more limited_comma_ this differentiation becomes harder to make_comma_ and the animate feature profiles come to resemble the inanimate more and more.
E06-3008.txt,119,Because the inanimate nouns are expected to have low proportions compared to the animate for all these features_comma_ the data sparseness is not as damaging.
E06-3008.txt,120,In order to examine the effect on each individual feature of the lowering of the frequency threshold_comma_ wealso ran classifiers trained on the high frequency nouns with only individual features on the three groups of new nouns.
E06-3008.txt,121,These resultsaredepicted inTable4.
E06-3008.txt,122,Inourearlier experiment_comma_ the performance of a majority of the individual features OBJ_comma_ PASS_comma_ ANAAN_comma_ ANAIN was significantly worse at the p 0.05 level than the performance of the classifier including all the features.
E06-3008.txt,123,Three ofthe individual features SUBJ_comma_ GEN_comma_ REFL had a performance which did not differ significantly from that of the classifier employing all the features in combination.
E06-3008.txt,124,As the frequency threshold is lowered_comma_ however_comma_ the performance of the classifiers employing all features and those trained only on individual features become more similar.
E06-3008.txt,125,For the 100 nouns_comma_ only the two anaphoric features ANAAN andthe reflexivefeature REFL_comma_ have aperformance that differs significantly p 0.05 from the classifier employing all features.
E06-3008.txt,126,For the 50 and 10 nouns_comma_ there are no significant differences between the classifiers employing individual fea51 Freq All SUBJ OBJ GEN PASS ANAAN ANAIN REFL 100 70.0 75.0 80.0 72.5 65.0 52.5 50.0 60.0 50 57.5 75.0 62.5 77.5 62.5 57.5 50.0 55.0 10 52.5 52.5 65.0 50.0 57.5 50.0 50.0 50.0 Table 4 Accuracy obtained when employing the old classifier on new lower frequency nouns with leaveone out training and testing all and individual features 100 nouns a b classified as 10 10 a class animate 2 18 b class inanimate 50 nouns a b classified as 4 16 a class animate 1 19 b class inanimate 10 nouns a b classified as 1 19 a class animate 20 b class inanimate Table 5 Confusion matrices for classification of lower frequency nouns with old classifier tures only and the classifiers trained on the feature set as a whole.
E06-3008.txt,127,This indicates that the combined classifiers no longer exhibit properties that are not predictable from the individual features alone and they do not generalize over the data based on the combinations of features.
E06-3008.txt,128,Interms ofaccuracy_comma_ afew ofthe individual features even outperform the collective result.
E06-3008.txt,129,On average_comma_ the three most frequent features_comma_ the SUBJ_comma_ OBJ and GEN features_comma_ improve the performance by 9.5 for the 100 nouns and 24.6 for the 50 nouns.
E06-3008.txt,130,For the lowest frequency nouns 10 we see that the object feature alone improves the result by almost 24 _comma_ from 52.5 to 65 accuracy.
E06-3008.txt,131,In fact_comma_ the object feature seems to be the most stable feature of all the features.
E06-3008.txt,132,When examining the means of the results extracted for the different features_comma_ the object feature is the feature which maintains thelargest difference between the two classes as the frequency threshold is lowered.
E06-3008.txt,133,The second most stable feature in this respect is the subject feature.
E06-3008.txt,134,Thegroup of experiments reported above shows that thelowering ofthe frequency threshold forthe classified nouns causes a clear deterioration of results in general_comma_ and most gravely when all the features are employed together.
E06-3008.txt,135,4.2 Experiment 3 Back off features The three most frequent features_comma_ the SUBJ_comma_ OBJ and GEN features_comma_ were the most stable in the two experiments reported above and had a performance which did not differ significantly from the combined classifiers throughout.
E06-3008.txt,136,In light of this we ran some experiments where all possible combinations ofthese morefrequent features wereemployed.
E06-3008.txt,137,The results for each of the three groups of nouns ispresented inTable 6.
E06-3008.txt,138,The exclusion ofthe less frequent features has a clear positive effect on the accuracy results_comma_ as we can see in table 6.
E06-3008.txt,139,For the 100 and 50nouns_comma_ theperformance has improved compared to the classifier trained both on all the features and on the individual features.
E06-3008.txt,140,The classification performance for these nouns is now identical or only slightly worse than the performance for the high frequency nouns in experiment 1.
E06-3008.txt,141,For the 10 group of nouns_comma_ the performance is_comma_ at best_comma_ the same as for all the features and at worse fluctuating around baseline.
E06-3008.txt,142,In general_comma_ the best performing feature combinations are SUBJ OBJ GEN and SUBJ OBJ .
E06-3008.txt,143,These two differ significantly at the p .05 level from the results obtained by employing all the features collectively for both the 100 and the 50 nouns_comma_ hence indicate a clear improvement.
E06-3008.txt,144,The feature combinations both contain the two most stable features one feature which targets the animate class SUBJ and another which target the inanimate class OBJ _comma_ a property which facilitates differentiation even as the marginals between the two decrease.
E06-3008.txt,145,It seems_comma_ then_comma_ that backing off to the most frequent features might constitute a partial remedy for the problems induced by data sparseness in the classification.
E06-3008.txt,146,The feature combinations SUBJ OBJ GEN and SUBJ OBJ both significantly improve the classification performance and actually enable us to maintain the same accuracy for both the 100 and 50 nouns as for the higher frequency nouns_comma_ as reported in experiment 1.
E06-3008.txt,147,52 Freq SUBJ OBJ GEN SUBJ OBJ SUBJ GEN OBJ GEN 100 87.5 87.5 77.5 85.0 50 82.5 90.0 70.0 77.5 10 57.5 50.0 50.0 47.5 Table 6 Accuracy obtained when employing the old classifier on new lower frequency nouns combinations of the most frequent features 4.3 Experiment 4 Back off classifiers Another option_comma_ besides a back off to more frequent features in classification_comma_ is to back off to another classifier_comma_ i.e. a classifier trained on nouns with a similar frequency.
E06-3008.txt,148,An approach of this kind will attempt to exploit any group similarities that these nouns may have in contrast to the mores frequent ones_comma_ hopefully resulting in a better classification.
E06-3008.txt,149,In this experiment classifiers were trained and tested using leave one out cross validation on the three groups of lower frequency nouns and employing individual_comma_ as well as various other feature combinations.
E06-3008.txt,150,The results for all features as well as individual features are summarized in Table 7.
E06-3008.txt,151,As we can see_comma_ the result for the classifier employing allthefeatures hasimproved somewhat compared to the corresponding classifiers in experiment 3 as reported above in Table 4 for all ourthreegroups ofnouns.
E06-3008.txt,152,Thisindicates thatthere is a certain group similarity for the nouns of similar frequency that is captured in the combination of the seven features.
E06-3008.txt,153,However_comma_ backing off to a classifier trained on nouns that are more similar frequency wise does not cause an improvement in classification accuracy.
E06-3008.txt,154,Apart from the SUBJ feature for the 100 nouns_comma_ none of the other classifiers trained on individual or all features for the three different groups differ significantly p .05 from their counterparts in experiment 3.
E06-3008.txt,155,As before_comma_ combinations of the most frequent features were employed in the new classifiers trained and tested on each of the three frequencyordered groups of nouns.
E06-3008.txt,156,In the terminology employed above_comma_ this amounts to a backing off both classifier and feature wise.
E06-3008.txt,157,The accuracy measures obtained for these experiments are summarized in table 8.
E06-3008.txt,158,For these classifiers_comma_ the backed off feature combinations do not differ significantly at the p .05 level from their counterparts in experiment 3_comma_ where the classifiers were trained on the more frequent nouns with feature back off.
E06-3008.txt,159,5 Conclusion The above experiments have shown that the classification of animacy for Norwegian common nouns is achievable using distributional data from a morphosyntactically annotated corpus.
E06-3008.txt,160,The chosen morphosyntactic features of animacy have proven to differentiate well between the two classes.
E06-3008.txt,161,As we have seen_comma_ the transitive subject_comma_ direct object andmorphological genitive provide stable features for animacy even when the data is sparse r .
E06-3008.txt,162,Four groups of experiments have been reported above which indicate that a reasonable remedy for sparse data in animacy classification consists of backing off to a smaller feature set in classification.
E06-3008.txt,163,These experiments indicate that a classifier trained on highly frequent nouns experiment 1 backed off to the most frequent features experiment 3 sufficiently capture generalizations which pertain to nouns with absolute frequencies down to approximately fifty occurrences and enables an unchanged performance approaching 90 accuracy.
E06-3008.txt,164,Even so_comma_ there are certainly still possibilities for improvement.
E06-3008.txt,165,As is well known_comma_ singleton occurrences of nouns abound and the above classification method is based on data for lemmas_comma_ rather than individual instances or tokens.
E06-3008.txt,166,One possibility to be explored is token based classification of animacy_comma_ possibly in combination with a lemmabased approach like the one outlined above.
E06-3008.txt,167,Such an approach might also include a finer subdivision of the nouns.
E06-3008.txt,168,We have chosen to classify along a binary dimension_comma_ however_comma_ it might be argued that this is an artificial dichotomy.
E06-3008.txt,169,Zaenen et al._comma_ 2004 describe an encoding scheme for the manual encoding of animacy information in part of the English Switchboard corpus.
E06-3008.txt,170,They make a three way distinction between human_comma_ other animates_comma_ and inanimates_comma_ where the other animates category describes a rather heterogeneous group of entities organisations_comma_ animals_comma_ intelligent machines and vehicles.
E06-3008.txt,171,However_comma_ what these seem to have in common is that they may all be construed linguistically as ani53 Freq All SUBJ OBJ GEN PASS ANAAN ANAIN REFL 100 85.0 52.5 87.5 65.0 70.0 50.0 57.5 50.0 50 77.5 77.5 75.0 75.0 50.0 50.0 50.0 50.0 10 52.5 50.0 62.5 50.0 50.0 50.0 50.0 50.0 Table 7 Accuracy obtained when employing a new classifier on new lower frequency nouns all and individual features Freq SUBJ OBJ GEN SUBJ OBJ SUBJ GEN OBJ GEN 100 85.0 85.0 67.5 82.5 50 75.0 80.0 75.0 70.0 10 62.5 62.5 50.0 62.5 Table 8 Accuracy obtained when employing a new classifier on new lower frequency nouns combinations of the most frequent features mate beings_comma_ even though they_comma_ in the real world_comma_ are not.
E06-3008.txt,172,Interestingly_comma_ the two misclassified inanimate nouns in experiment 1_comma_ were bil car and fly air plane _comma_ both vehicles.
E06-3008.txt,173,A token based approach to classification might better capture the contextdependent and dual nature of these types of nouns.
E06-3008.txt,174,Automatic acquisition of animacy in itself is not necessarily the primary goal.
E06-3008.txt,175,By testing the use of acquired animacy information in various NLP applications such as parsing_comma_ generation or coreference resolution_comma_ we might obtain an extrinsic evaluation measure for the usefulness of animacy information.
E06-3008.txt,176,Since very frequent nouns are usually well described in other lexical resources_comma_ it is important that a method for animacy classification is fairly robust to data sparseness.
E06-3008.txt,177,This paper suggests that a method based on seven morphosyntactic features_comma_ in combination with feature back off_comma_ can contribute towards such a classification. .
E06-1011.txt,1,this paper we extend the maximum spanning tree MST dependency parsing framework of McDonald et al. 2005c to incorporate higher order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable_comma_ but that the intractability can be circumvented with new approximate parsing algorithms.
E06-1011.txt,2,We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
E06-1011.txt,4,Dependency representations of sentences Hudson_comma_ 1984 Me l cuk_comma_ 1988 model head dependent syntactic relations as edges in a directed graph.
E06-1011.txt,5,Figure 1 displays a dependency representation for the sentence John hit the ball with the bat.
E06-1011.txt,6,This sentence is an example of a projective or nested tree representation_comma_ in which all edges can be drawn in the plane with none crossing.
E06-1011.txt,7,Sometimes a non projective representations are preferred_comma_ as in the sentence in Figure 2.1 In particular_comma_ for freer word order languages_comma_ non projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid.
E06-1011.txt,8,The dependency structures in Figures 1 and 2 satisfy the tree constraint they are weakly connected graphs with a unique root node_comma_ and each non root node has a exactly one parent.
E06-1011.txt,9,Though trees are 1Examples are drawn from McDonald et al. 2005c . more common_comma_ some formalisms allow for words to modify multiple parents Hudson_comma_ 1984 .
E06-1011.txt,10,Recently_comma_ McDonald et al. 2005c have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree MST in a graph yields efficient algorithms for both projective and non projective trees.
E06-1011.txt,11,When combined with a discriminative online learning algorithm and a rich feature set_comma_ these models provide state of the art performance across multiple languages.
E06-1011.txt,12,However_comma_ the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges.
E06-1011.txt,13,This first order factorization is very restrictive since it only allows for features to be defined over single attachment decisions.
E06-1011.txt,14,Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy Yamada and Matsumoto_comma_ 2003 Charniak_comma_ 2000 .
E06-1011.txt,15,In this paper we extend the MST parsing framework to incorporate higher order feature representations of bounded size connected subgraphs.
E06-1011.txt,16,We also present an algorithm for acyclic dependency graphs_comma_ that is_comma_ dependency graphs in which a word maydepend on multiple heads.
E06-1011.txt,17,In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable.
E06-1011.txt,18,We evaluate these algorithms within an online learning framework_comma_ which has been shown to be robust with respect approximate inference_comma_ and describe experiments displaying that these new models lead to state of the art accuracy for English and the best accuracy we know of for Czech and Danish.
E06-1011.txt,19,2 Maximum Spanning Tree Parsing Dependency tree parsing as the search for the maximum spanning tree MST in a graph was 81 root John saw a dog yesterday which was a Yorkshire Terrier Figure 2 An example non projective dependency structure. root hit John ball with the bat the root0 John1 hit2 the3 ball4 with5 the6 bat7 Figure 1 An example dependency structure. proposed byMcDonald etal. 2005c .
E06-1011.txt,20,Thisformulation leads to efficient parsing algorithms for both projective and non projective dependency trees with the Eisner algorithm Eisner_comma_ 1996 and the Chu Liu Edmonds algorithm Chu and Liu_comma_ 1965 Edmonds_comma_ 1967 respectively.
E06-1011.txt,21,The formulation works by defining the score of a dependency tree to be the sum of edge scores_comma_ s x_comma_y summationdisplay i_comma_j y s i_comma_j where x x1 xn is an input sentence and y a dependency tree for x.
E06-1011.txt,22,We can view y as a set of tree edges and write i_comma_j y to indicate an edge in y from word xi to word xj.
E06-1011.txt,23,Consider the example from Figure 1_comma_ where the subscripts index the nodes of the tree.
E06-1011.txt,24,The score of this tree would then be_comma_ s 0_comma_2 s 2_comma_1 s 2_comma_4 s 2_comma_5 s 4_comma_3 s 5_comma_7 s 7_comma_6 We call this first order dependency parsing since scores are restricted to a single edge in the dependency tree.
E06-1011.txt,25,The score of an edge is in turn computed as the inner product of a high dimensional feature representation of the edge with a corresponding weight vector_comma_ s i_comma_j w f i_comma_j This is a standard linear classifier in which the weight vector w are the parameters to be learned during training.
E06-1011.txt,26,We should note that f i_comma_j can be based on arbitrary features of the edge and the input sequence x.
E06-1011.txt,27,Given a directed graph G V_comma_E _comma_ the maximum spanning tree MST problem is to find the highest scoring subgraph of G that satisfies the tree constraint over the vertices V .
E06-1011.txt,28,By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above_comma_ McDonald et al. 2005c showed that dependency parsing is equivalent to finding the MST in this graph.
E06-1011.txt,29,Furthermore_comma_ it was shown that this formulation can lead to state of the art results when combined with discriminative learning algorithms.
E06-1011.txt,30,Although the MST formulation applies to any directed graph_comma_ our feature representations andone oftheparsing algorithms Eisner s rely onalinear ordering of the vertices_comma_ namely the order of the words in the sentence.
E06-1011.txt,31,2.1 Second Order MST Parsing Restricting scores to a single edge in a dependency tree gives a very impoverished view of dependency parsing.
E06-1011.txt,32,Yamadaand Matsumoto 2003 showed that keeping a small amount of parsing history was crucial to improving parsing performance for their locally trained shift reduce SVM parser.
E06-1011.txt,33,It is reasonable to assume that other parsing models might benefit from features over previous decisions.
E06-1011.txt,34,Here we will focus on methods for parsing second order spanning trees.
E06-1011.txt,35,These models factor the score of the tree into the sum of adjacent edge pair scores.
E06-1011.txt,36,To quantify this_comma_ consider again the example from Figure 1.
E06-1011.txt,37,In the second order spanning tree model_comma_ the score would be_comma_ s 0_comma_ _comma_2 s 2_comma_ _comma_1 s 2_comma_ _comma_4 s 2_comma_4_comma_5 s 4_comma_ _comma_3 s 5_comma_ _comma_7 s 7_comma_ _comma_6 Here we use the second order score function s i_comma_k_comma_j _comma_ which is the score of creating a pair of adjacent edges_comma_ from word xi to words xk and xj.
E06-1011.txt,38,For instance_comma_ s 2_comma_4_comma_5 is the score of creating the edges from hit to with and from hit to ball.
E06-1011.txt,39,The score functions are relative to the left or right of the parent and we never score adjacent edges that are on different sides of the parent for instance_comma_ 82 there is no s 2_comma_1_comma_4 for the adjacent edges from hit to John and ball .
E06-1011.txt,40,This independence between left and right descendants allow us to use a O n3 second order projective parsing algorithm_comma_ as we will see later.
E06-1011.txt,41,We write s xi_comma_ _comma_xj when xj is the first left or first right dependent of word xi.
E06-1011.txt,42,For example_comma_ s 2_comma_ _comma_4 is the score of creating a dependency from hit to ball_comma_ since ball is the first child to the right of hit.
E06-1011.txt,43,More formally_comma_ if the word xi0 has the children shown in this picture_comma_ xi0 xi1 ... xij xij 1 ... xim the score factors as follows summationtextj 1 k 1 s i0_comma_ik 1_comma_ik s i0_comma_ _comma_ij s i0_comma_ _comma_ij 1 summationtextm 1k j 1s i0_comma_ik_comma_ik 1 This second order factorization subsumes the first order factorization_comma_ since the score function could just ignore the middle argument to simulate first order scoring.
E06-1011.txt,44,The score of a tree for secondorder parsing is now s x_comma_y summationdisplay i_comma_k_comma_j y s i_comma_k_comma_j where k and j are adjacent_comma_ same side children of i in the tree y.
E06-1011.txt,45,The second order model allows us to condition onthe mostrecent parsing decision_comma_ thatis_comma_ the last dependent picked up by a particular word_comma_ which is analogous to the the Markov conditioning of in the Charniak parser Charniak_comma_ 2000 .
E06-1011.txt,46,2.2 Exact Projective Parsing For projective MST parsing_comma_ the first order algorithm can be extended to the second order case_comma_ as was noted by Eisner 1996 .
E06-1011.txt,47,The intuition behind the algorithm is shown graphically in Figure 3_comma_ which displays both the first order and secondorder algorithms.
E06-1011.txt,48,In the first order algorithm_comma_ a word will gather its left and right dependents independently by gathering each half of the subtree rooted by its dependent in separate stages.
E06-1011.txt,49,By splitting up chart items into left and right components_comma_ the Eisner algorithm only requires 3 indices to be maintained at each step_comma_ as discussed in detail elsewhere Eisner_comma_ 1996 McDonald et al._comma_ 2005b .
E06-1011.txt,50,For the second order algorithm_comma_ the key insight is to delay the scoring of edges until pairs 2 order non proj approx x_comma_s Sentence x x0 ...xn_comma_ x0 root Weight function s i_comma_k_comma_j R 1.
E06-1011.txt,51,Let y 2 order proj x_comma_s 2. while true 3. m  _comma_c 1_comma_p 1 4. for j 1 n 5. for i 0 n 6. yprime y i j 7. if tree yprime or k i_comma_k_comma_j y continue 8.  s x_comma_yprime s x_comma_y 9. if  m 10. m _comma_c j_comma_p i 11. end for 12. end for 13. if m 0 14. y y p c 15. else return y 16. end while Figure 4 Approximate second order nonprojective parsing algorithm. of dependents have been gathered.
E06-1011.txt,52,This allows for the collection of pairs of adjacent dependents in a single stage_comma_ which allows for the incorporation of second order scores_comma_ while maintaining cubictime parsing.
E06-1011.txt,53,The Eisner algorithm can be extended to an arbitrary mth order model with a complexity of O nm 1 _comma_ for m 1.
E06-1011.txt,54,An mth order parsing algorithm willworksimilarly tothe second order algorithm_comma_ except that wecollect mpairs of adjacent dependents in succession before attaching them to their parent.
E06-1011.txt,55,2.3 Approximate Non projective Parsing Unfortunately_comma_ second order non projective MST parsing is NP hard_comma_ as shown in appendix A.
E06-1011.txt,56,To circumvent this_comma_ we designed an approximate algorithm based on the exact O n3 second order projective Eisner algorithm.
E06-1011.txt,57,The approximation works by first finding the highest scoring projective parse.
E06-1011.txt,58,It then rearranges edges in the tree_comma_ one at a time_comma_ as long as such rearrangements increase the overall score and do not violate the tree constraint.
E06-1011.txt,59,We can easily motivate this approximation by observing that even in non projective languages like Czech and Danish_comma_ most trees are primarily projective with just a few non projective edges Nivre and Nilsson_comma_ 2005 .
E06-1011.txt,60,Thus_comma_ by starting with the highest scoring projective tree_comma_ we are typically only a small number of transformations away from the highest scoring non projective tree.
E06-1011.txt,61,The algorithm is shown in Figure 4.
E06-1011.txt,62,The expression y i j denotes the dependency graph identical to y except that xi s parent is xi instead 83 FIRST ORDER h 1 h3 h1 r r 1 h3 A h1 h3 h1 h3 B SECOND ORDER h 1 h2 h2 h3 h1 h2 h2 r r 1 h3 A h1 h2 h2 h3 h1 h2 h2 h3 B h1 h3 h1 h3 C Figure 3 A O n3 extension of the Eisner algorithm to second order dependency parsing.
E06-1011.txt,63,This figure shows how h1 creates a dependency to h3 with the second order knowledge that the last dependent of h1 was h2.
E06-1011.txt,64,This is done through the creation of a sibling item in part B .
E06-1011.txt,65,In the first order model_comma_ the dependency to h3 is created after the algorithm has forgotten that h2 was the last dependent. of what it was in y.
E06-1011.txt,66,The test tree y is true iff the dependency graph y satisfies the tree constraint.
E06-1011.txt,67,In more detail_comma_ line 1 of the algorithm sets y to the highest scoring second order projective tree.
E06-1011.txt,68,The loop of lines 2 16 exits only when no further score improvement is possible.
E06-1011.txt,69,Each iteration seeks the single highest scoring parent change to y that does not break the tree constraint.
E06-1011.txt,70,To that effect_comma_ the nested loops starting in lines 4 and 5 enumerate all i_comma_j pairs.
E06-1011.txt,71,Line 6 sets yprime to the dependency graph obtained from y by changing xj s parent to xi.
E06-1011.txt,72,Line 7 checks that the move from y to yprime is valid by testing that xj s parent was not already xi and that yprime is a tree.
E06-1011.txt,73,Line 8 computes the score change from y to yprime.
E06-1011.txt,74,If this change is larger than the previous best change_comma_ we record how this new tree was created lines 9 10 .
E06-1011.txt,75,After considering all possible valid edge changes to the tree_comma_ the algorithm checks to see that the best new tree does have a higher score.
E06-1011.txt,76,If that is the case_comma_ we change the tree permanently and re enter the loop.
E06-1011.txt,77,Otherwise we exit since there are no single edge switches that can improve the score.
E06-1011.txt,78,This algorithm allows for the .
E06-1042.txt,1,this paper we present TroFi Trope Finder _comma_ a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
E06-1042.txt,2,It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.
E06-1042.txt,3,We adapt a word sense disambiguation algorithm to our task and augment it with multiple seed set learners_comma_ a voting schema_comma_ and additional features like SuperTags and extrasentential context.
E06-1042.txt,4,Detailed experiments on hand annotated data show that our enhanced algorithm outperforms the baseline by 24.4 .
E06-1042.txt,5,Using the TroFi algorithm_comma_ we also build the TroFi Example Base_comma_ an extensible resource of annotated literal nonliteral examples which is freely available to the NLP research community.
E06-1042.txt,7,In this paper_comma_ we propose TroFi Trope Finder _comma_ a nearly unsupervised clustering method for separating literal and nonliteral usages of verbs.
E06-1042.txt,8,For example_comma_ given the target verb pour _comma_ we would expect TroFi to cluster the sentence Custom demands that cognac be poured from a freshly opened bottle as literal_comma_ and the sentence Salsa and rap music pour out of the windows as nonliteral_comma_ which_comma_ indeed_comma_ it does.
E06-1042.txt,9,We call our method nearly unsupervised.
E06-1042.txt,10,See Section 3.1 for why we use this terminology.
E06-1042.txt,11,We reduce the problem of nonliteral language recognition to one of word sense disambiguation This research was partially supported by NSERC_comma_ Canada RGPIN 264905 .
E06-1042.txt,12,We would like to thank Bill Dolan_comma_ Fred Popowich_comma_ Dan Fass_comma_ Katja Markert_comma_ Yudong Liu_comma_ and the anonymous reviewers for their comments. by redefining literal and nonliteral as two different senses of the same word_comma_ and we adapt an existing similarity based word sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.
E06-1042.txt,13,This paper focuses on the algorithmic enhancements necessary to facilitate this transformation from word sense disambiguation tononliteral language recognition.
E06-1042.txt,14,Theoutput ofTroFiisanexpandable examplebase of literal nonliteral clusters which is freely available to the research community.
E06-1042.txt,15,Many systems that use NLP methods such as dialogue systems_comma_ paraphrasing and summarization_comma_ language generation_comma_ information extraction_comma_ machine translation_comma_ etc. would benefit from being able to recognize nonliteral language.
E06-1042.txt,16,Consider an example based on a similar example from an automated medical claims processing system.
E06-1042.txt,17,We must determine that the sentence she hit the ceiling is meant literally before it can be marked up as an ACCIDENT claim.
E06-1042.txt,18,Note that the typical use of hit the ceiling stored in a list of idioms cannot help us.
E06-1042.txt,19,Onlyusing thecontext_comma_ Shebroke her thumb while she was cheering for the Patriots and_comma_ in her excitement_comma_ she hit the ceiling_comma_ can we decide.
E06-1042.txt,20,We further motivate the usefulness of the ability to recognize literal vs. nonliteral usages using an example from the Recognizing Textual Entailment RTE 1 challenge of 2005.
E06-1042.txt,21,This is just an example we do not compute entailments. In the challenge data_comma_ Pair1959was KerryhitBushhard on his conduct on the war in Iraq. Kerry shot Bush.
E06-1042.txt,22,The objective was to report FALSE since the second statement in this case is not entailed from the first one.
E06-1042.txt,23,In order to do this_comma_ it is crucial to know that hit is being used nonliterally in the first sentence.
E06-1042.txt,24,Ideally_comma_ we would like to look at TroFi as a first step towards an unsupervised_comma_ scalable_comma_ widely applicable approach to nonliteral language processing that works on real world data from any domain in any language.
E06-1042.txt,25,329 2 Previous Work The foundations of TroFi lie in a rich collection of metaphor and metonymy processing systems everything from hand coded rule based systems to statistical systems trained on large corpora.
E06-1042.txt,26,Rule based systems some using a type of interlingua Russell_comma_ 1976 others using complicated networks and hierarchies often referred to as metaphor maps e.g. Fass_comma_ 1997 Martin_comma_ 1990 Martin_comma_ 1992 must be largely hand coded and generally work well on an enumerable set of metaphors or in limited domains.
E06-1042.txt,27,Dictionarybased systems use existing machine readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information e.g. Dolan_comma_ 1995 .
E06-1042.txt,28,Corpus based systems primarily extract or learn the necessary metaphor processing information from large corpora_comma_ thus avoiding the need for manual annotation or metaphor map construction.
E06-1042.txt,29,Examples of suchsystems canbefound in Murata et. al._comma_ 2000 Nissim Markert_comma_ 2003 Mason_comma_ 2004 .
E06-1042.txt,30,Thework on supervised metonymy resolution by Nissim Markert and the work on conceptual metaphors by Mason come closest to what we are trying to do with TroFi.
E06-1042.txt,31,Nissim Markert 2003 approach metonymy resolution with machine learning methods_comma_ which exploit the similarity between examples of conventional metonymy Nissim Markert_comma_ 2003 _comma_ p.
E06-1042.txt,33,They see metonymy resolution as a classification problem between the literal use of a word and a number of pre defined metonymy types.
E06-1042.txt,34,They use similarities between possibly metonymic words PMWs and known metonymies as well as context similarities to classify the PMWs.
E06-1042.txt,35,The main difference between the Nissim Markert algorithm and the TroFi algorithm besides the fact that Nissim Markert deal with specific types of metonymy and not a generalized category of nonliteral language is that Nissim Markert use a supervised machine learning algorithm_comma_ as opposed to the primarily unsupervised algorithm used by TroFi.
E06-1042.txt,36,Mason 2004 presents CorMet_comma_ a corpusbased system for discovering metaphorical mappings between concepts Mason_comma_ 2004 _comma_ p.
E06-1042.txt,38,His system finds the selectional restrictions of given verbs in particular domains by statistical means.
E06-1042.txt,39,It then finds metaphorical mappings between domains based on these selectional p.
E06-2026.txt,1,this paper_comma_ we present a formalization of grammatical role labeling within the frameworkofInteger Linear Programming ILP . We focus on the integration of subcategorization information into the decision making process.
E06-2026.txt,2,We present a first empirical evaluation that achieves competitive precision and recall rates.
E06-2026.txt,4,An often stressed point is that the most widely used classifiers such as Naive Bayes_comma_ HMM_comma_ and Memory based Learners are restricted to local decisions only.
E06-2026.txt,5,With grammatical role labeling_comma_ for example_comma_ there is no way to explicitly express global constraints that_comma_ say_comma_ the verb to give must have 3 arguments of a particular grammatical role.
E06-2026.txt,6,Among the approaches to overcome this restriction_comma_ i.e. that allow for global_comma_ theory based constraints_comma_ Integer Linear Programming ILP has been applied to NLP Punyakanok et al._comma_ 2004 .
E06-2026.txt,7,Weapply ILPto the problem of grammatical relation labeling_comma_ i.e. given two chunks.1 e.g. a verb and a np _comma_ what is the grammatical relation between them if there is any .
E06-2026.txt,8,We have trained a maximum entropy classifier on vectors with morphological_comma_ syntactic and positional information.
E06-2026.txt,9,Its output is utilized as weights to the ILP component which generates equations to solve the following problem Given subcategorization frames expressed in functional roles_comma_ e.g. subject _comma_ and given a sentence with verbs_comma_ a0 auxiliary_comma_ modal_comma_ finite_comma_ non finite_comma_ .. _comma_ and chunks_comma_ a1 a2a4a3 _comma_a3a5a3 _comma_ label all pairs a0a7a6 a1a9a8a11a10a7a12 a0a7a6 a1 withagrammatical role2.
E06-2026.txt,10,Inthispaper_comma_ wearepursuing twoempirical scenarios.
E06-2026.txt,11,The first is to collapse all subcategoriza1Currently_comma_ we use perfect chunks_comma_ that is_comma_ chunks stemming from automatically flattening a treebank.
E06-2026.txt,12,2Most of these pairs do not stand in a proper grammatical relation_comma_ they get a null class assignment. tion frames of a verb into a single one_comma_ comprising all subcategorized roles of the verb but not necessarily forming a valid subcategorization frame of that verb at all.
E06-2026.txt,13,For example_comma_ the verb to believe subcategorizes for a subject and a prepositional complement He believes in magic or for a subject and a clausal complement She believes that he is dreaming _comma_ but there is no frame that combines a subject_comma_ a prepositional object and a clausal object.
E06-2026.txt,14,Nevertheless_comma_ the set of valid grammatical roles of a verb can serve as a filter operating upon the output of a statistical classifier.
E06-2026.txt,15,The typical errors being made by classifiers with only local decisions are a constituent is assigned to a grammatical role more than once and a grammatical role e.g. of a verb is instantiated more than once.
E06-2026.txt,16,The worst example in our tests was a verb that receives from the maxent classifier two subjects and three clausal objects.
E06-2026.txt,17,Here_comma_ such a role filter will help to improve the results.
E06-2026.txt,18,The second setting is to provide ILP with the correct subcategorization frame of the verb.
E06-2026.txt,19,The results of such an oracle setting define the upper bound of the performance our ILP approach can achieve.
E06-2026.txt,20,Future work will be to let ILP find the optimal subcategorization frame given all frames of a verb.
E06-2026.txt,21,2 The ILP Specification Integer Linear Programming ILP is the name of a class of constraint satisfaction algorithms which are restricted to a numerical representation of the problem to be solved.
E06-2026.txt,22,The objective is to optimize minimize or maximize the numerical solution of linear equations see the objective function in Fig. 1 .
E06-2026.txt,23,The general form of an ILP specification is given in Fig. 1 here maximization .
E06-2026.txt,24,The goal is to maximize a a2 ary function a13 _comma_ which is defined as the sum of the variables a14a11a15a17a16a18a15 .
E06-2026.txt,25,Assignment decisions e.g. grammatical role labeling can be modeled in the following way a16a20a19 187 Objective Function a0a2a1a4a3 a13 a12 a16a6a5a8a7a8a9a8a9a8a9a10a7 a16 a19 a8a12a11a14a13 a14a15a5 a16a6a5a17a16a18a9a8a9a8a9a19a16 a14 a19 a16 a19 Constraints a20 a15a21a5 a16a6a5a17a16 a20 a15a23a22 a16a24a22a25a16a18a9a8a9a8a9a26a16 a20 a15 a19 a16 a19 a27a28 a29a31a30 a13a32 a33a35a34 a36a38a37 a15a39a7 a40 a13a31a41a26a7a8a9a8a9a8a9a42a7a44a43 a16a18a15 are variables_comma_ a14a5a15 _comma_ a37 a15 and a20 a15a46a45 are constants.
E06-2026.txt,26,Figure 1 ILP Specification are binary class variables that indicate the non assignment of a constituent a47 a15 to the grammatical function a48 a45 e.g. subject of a verb a49a51a50 .
E06-2026.txt,27,To represent this_comma_ three indices are needed.
E06-2026.txt,28,Thus_comma_ a16 is a complex variable name_comma_ e.g.
E06-2026.txt,29,a48 a15a46a45 a50 .
E06-2026.txt,30,For the sake of readability_comma_ weadd somemnemotechnical sugar and use a48 a15a52a49a53a45a10a47a54a50 instead or a55a25a49a10a45a8a47a56a50 for a constituent a47a56a50 being or not the subject a55 of verb a49a4a45 a55 thus is an instantiation of a48 a15 .
E06-2026.txt,31,If the value of such a class variable a48 a15 a49 a45 a47a56a50 is set to 1 in the course of the maximization task_comma_ the attachment was successful_comma_ otherwise a48 a15a57a49a53a45a8a47a56a50a58a13a60a59 it failed.
E06-2026.txt,32,a14a5a15 from Fig. 1 are weights that represent the impact of an assignment or a constraint they provide an empirically based numerical justification of the assignment we don t need the a20 a15a61a45 .
E06-2026.txt,33,For example_comma_ we represent the impact of a48a7a15a57a49a8a45a53a47a54a50 1 by a62a64a63a66a65a68a67a70a69a72a71a74a73 .
E06-2026.txt,34,These weights are derived from a maximum entropy model trained on a treebank see section 5 .
E06-2026.txt,35,a37 is used to set up numerical constraints.
E06-2026.txt,36,For example that a constituent can only be the filler of one grammatical role.
E06-2026.txt,37,The decision_comma_ which of the class variables are to be on or off is based on the weights and the constraints an overall solution must obey to.
E06-2026.txt,38,ILP seeks to optimize the solution.
E06-2026.txt,39,3 Formalization We restrict our formalization to the following set of grammatical functions subject a55 _comma_ direct i.e. accusative object a75 _comma_ indirect i.e. dative object a76 _comma_ clausal complement a1 _comma_ prepositional complement a77 _comma_ attributive np or pp attachment a78 and adjunct a79 .
E06-2026.txt,40,The set of grammatical relations of a verb verb complements is denoted with a48 _comma_ it comprises a55 _comma_ a75 _comma_a76 _comma_ a1 and a77 .
E06-2026.txt,41,The objective function is a43 a20a81a80 a11a82a79a83a16a84a78a60a16a86a85a87a16 a0 1 a79 represents the weighted sum of all adjunct attachments.
E06-2026.txt,42,a78 is the weighted sum of all attributive a88a89a88 the book in her hand .. and genitive a90 a88 attachments die Frau desa91a72a92 a19 Professorsa91a35a92 a19 the wife of the professor .
E06-2026.txt,43,a85 represents the weighted sum of all unassigned objects.3 a0 is the weighted sum of the case frame instantiations of all verbs in the sentence.
E06-2026.txt,44,It is defined as follows a0 a13a31a93 a67 a92a39a94a96a95a70a97 a93 a98 a15 a98 a63a100a99a26a101a103a102 a65 a93 a71a74a104 a19a19a97a74a105a106a97a74a107 a93 a98 a45 a108 a63a109a67a44a65a106a71a106a69a111a110a112a48a113a49 a15a114a47a96a45 2 This sums up over all verbs.
E06-2026.txt,45,For each verb_comma_ each grammatical role a115a116a67a96a65 is the set of such roles is instantiated from the stock of all constituents a47a56a117 a2a119a118a10a120a96a118a53a121 _comma_ which includes all np and pp constituents but also the verbs as potential heads of clausal objects .
E06-2026.txt,46,a48a113a49a5a15a114a47a44a45 is a variable that indicates the assignment of a constituent a47 a45 to the grammatical function a48 of verb a49 a15 .
E06-2026.txt,47,a108 a63a109a67a44a65a106a71a106a69 is the weight of such an assignment.
E06-2026.txt,48,The binary value of each a48a113a49 a15a114a47a44a45 is to be determined in the course of the constraint satisfaction process_comma_ the weight is taken from the maximum entropy model.
E06-2026.txt,49,a78 isthefunction for weighted attributive attachments a78a122a13a31a93 a71a70a104 a19a19a97a70a105a106a97 a93 a98 a15 a93 a71a70a104 a19a26a97a74a105a106a97 a93 a98 a45a12a123 a15a70a124 a125 a45a54a126 a62a100a127a119a71a70a65a128a71a106a69a112a110a112a78a129a47 a15a57a47a96a45 3 where a62a17a127a130a71a70a65a106a71a106a69 is the weight of an assignment of constituent a47a131a45 to constituent a47 a15 and a78a58a47 a15a114a47a44a45 is a binary variable indicating the classification decision whether a47a131a45 actually modifies a47 a15 .
E06-2026.txt,50,In contrast to a47a56a117 a2a119a118a53a120a44a118 a121 _comma_ a47a56a117 a2a119a118a53a120a44a118 does not include verbs.
E06-2026.txt,51,The function for weighted adjunct attachments_comma_ a79 _comma_ is a79a132a13a31a93 a71a70a104 a19a26a97a74a105a106a97a131a133 a93 a98 a45 a93 a67 a92a74a94a44a95a70a97 a93 a98 a15 a62a100a134 a67a44a65a128a71a106a69 a110a112a79a2a49 a15 a47 a45 4 where a47a56a117 a2a119a118a10a120a44a118a136a135 is the set of a88a89a88 constituents of the sentence.
E06-2026.txt,52,a62 a134 a67a44a65a106a71a52a69 is the weight given to a classification of a a88a89a88 as an adjunct of a clause with a49a11a15 as verbal head.
E06-2026.txt,53,The function for the weighted assignment to the null class_comma_ a85 _comma_ is a85a137a13a31a93 a71a74a104 a19a19a97a74a105a106a97 a107 a93 a98 a15 a108 a71a70a65a66a110a119a85a58a47 a15 5 This represents the impact of assigning a constituent neither to a verb as a complement nor 3Not every set ofchunks can form avalid dependency treea138 introduces robustness.
E06-2026.txt,54,188 to another constituent as an attributive modifier .
E06-2026.txt,55,a85a89a47 a15 a13 a41 means that the constituent a47 a15 has got no head e.g. a finite verb as part of a sentential coordination _comma_ although it might be the head of other a47a44a45 .
E06-2026.txt,56,The equations from 1 to 5 are devoted to the maximization task_comma_ i.e. which constituent is attached to which grammatical function and with which impact.
E06-2026.txt,57,Of course_comma_ without any further restrictions_comma_ every constituent would get assigned to every grammatical role because there are no cooccurrence restrictions.
E06-2026.txt,58,Exactly this would lead to a maximal sum.
E06-2026.txt,59,In order to assure a valid distribution_comma_ restrictions have to be formulated_comma_ e.g. that a grammatical role can have at most one filler object and that a constituent can be at most the filler of one grammatical role.
E06-2026.txt,60,4 Constraints A constituent a47 a45 must either be bound as an attribute_comma_ an adjunct_comma_ a verb complement or by the null class.
E06-2026.txt,61,This is to say that all class variables with a47a131a45 sum up to exactly 1 a47a131a45 then is consumed.
E06-2026.txt,62,a85a89a47a44a45a42a16 a98 a15 a98 a63 a48a113a49 a15a21a47a44a45a42a16 a98 a15 a78a129a47 a15a57a47a96a45a4a16 a98 a15 a79a24a49 a15a57a47a44a45a113a13a83a41a26a7 a1a3a2 6 Here_comma_a2 isan index over all constituents and a48 is one of the grammatical roles of verb a49 a15 a48a5a4 a115a113a67a44a65 .
E06-2026.txt,63,No two constituents can be attached to each other symmetrically being head and modifier of each other at the same time _comma_ i.e.
E06-2026.txt,64,a78 among others is defined to be asymmetric.
E06-2026.txt,65,a78a129a47 a15a57a47a44a45a112a16a84a78a58a47a44a45a53a47 a15 a30 a41a26a7 a1a6a2 a7 a40 7 Finally_comma_ we must restrict the number of filler objects a grammatical role can have.
E06-2026.txt,66,Here_comma_ we have to distinguish among our two settings.
E06-2026.txt,67,In setting one all case roles of all frames of a verb are collapsed into a single set of case roles _comma_ we can t require all grammatical roles to be instantiated since we have an artificial case frame_comma_ not necessarily aproper one .
E06-2026.txt,68,Thisis expressed as a30 a41 in equation 8.
E06-2026.txt,69,a71a70a104 a19a26a97a74a105a106a97 a107 a98 a45 a48a113a49 a15a21a47a44a45 a30 a41a26a7 a1 a40 a7a72a48a5a4 a115a113a67a44a65 8 In setting two the actual case frame is given _comma_ we require that every grammatical role a48 of the verb a49 a15 a48a7a4 a115a113a67a44a65 must be instantiated exactly once a71a70a104 a19a26a97a74a105a106a97 a107 a98 a45 a48a113a49 a15a21a47a44a45 a13a31a41a26a7 a1 a40 a7a72a48a5a4 a115a113a67a44a65 9 5 The Weighting Scheme Amaximum entropy model was used to fixa probability model that serves as the basis for the ILP weights.
E06-2026.txt,70,The model was trained on the Tiger treebank Brants et al._comma_ 2002 with feature vectors stemming from the following set of features the part of speech tags of the two candidate chunks_comma_ the distance between them in phrases_comma_ the number of verbs between them_comma_ the number of punctuation marks between them_comma_ the person_comma_ case and number of the candidates_comma_ their heads_comma_ the direction of the attachment left or right and a passive active voice flag.
E06-2026.txt,71,The output of the maxent model is for each pair of chunks represented by their feature vectors a probability vector.
E06-2026.txt,72,Each entry in this probability vector represents theprobability usedasaweight that the two chunks are in a particular grammatical relation including the non grammatical relation _comma_ a90a86a48a116a115 .
E06-2026.txt,73,For example_comma_ the weight for an adjunct assignment_comma_ a62 a134 a67a9a8a74a71a11a10 _comma_ of two chunks a49a103a41 a verb and a47a13a12 a a2a4a3 or a a3a5a3 is given by the corresponding entry in the probability vector of the maximum entropy model.
E06-2026.txt,74,The vector also provides values for a subject assignment of these two chunks etc. 6 Empirical Results The overall precision of the maximum entropy classifier is 87.46 .
E06-2026.txt,75,Since candidate pairs are generated almost without restrictions_comma_ most pairs do not realize a proper grammatical relation.
E06-2026.txt,76,In the training set these examples are labeled with the non grammatical relation label a90 a48 a115 which is the basis of ILPs null class a85 .
E06-2026.txt,77,Since maximum entropy modeling seeks to sharpen the classifier with respect to the most prominent class_comma_ a90 a48 a115 gets a strong bias.
E06-2026.txt,78,So things are getting worse_comma_ if wefocus on the proper grammatical relations.
E06-2026.txt,79,The precision then is low_comma_ namely 62.73 _comma_ the recall is 85.76 _comma_ the f measure is 72.46 .
E06-2026.txt,80,ILP improves the precision by almost 20 in the all frames in one setting the precision is 81.31 .
E06-2026.txt,81,We trained on 40_comma_000 sentences_comma_ which gives about 700_comma_000 vectors 90 training_comma_ 10 test_comma_ including negative and positive pairings .
E06-2026.txt,82,Our first experiment was devoted to fix an upper bound for the ILP approach we selected from the set of subcategorization frames of averbthecorrect one according to the gold standard .
E06-2026.txt,83,The set of licenced grammatical relations then is reduced to the cor189 rect subcategorized GR and the non governable GR a79 adjunct and a78 attribute .
E06-2026.txt,84,The results are given in Fig. 2 under Fa71a70a104 a94a96a94 cf. section 3 for GR shortcuts_comma_ e.g. a55 for subject .
E06-2026.txt,85,Fa71a70a104 a94a96a94 F a71a70a104a1a0a2a0 Prec Rec F Mea Prec Rec F Mea a55 91.4 86.1 88.7 89.8 85.7 87.7 a75 90.4 83.3 86.7 78.6 79.7 79.1 a76 88.5 76.9 82.3 73.5 62.1 67.3 a77 79.3 73.7 76.4 75.6 43.6 55.9 a1 98.6 94.1 96.3 82.9 96.6 89.3 a79 76.7 75.6 76.1 74.2 78.9 76.5 a78 75.7 76.9 76.3 73.6 79.9 76.7 Figure 2 Correct Frame and Collapsed Frames The results of the governable GR a55 down to a1 are quite good_comma_ only the results for prepositional complements a77 are low the f measure is 76.4 .
E06-2026.txt,86,From the 36509 grammatical relations_comma_ 37173 were found and 31680 were correct.
E06-2026.txt,87,Overall precision is 85.23 _comma_ recall is 86.77 and the f measure is 85.99 .
E06-2026.txt,88,The most dominant error being made here is the coherent but wrong assignment of constituents to grammatical roles e.g. the subject is taken to be object .
E06-2026.txt,89,This is not a problem with ILP or the subcategorization frames_comma_ but one of the statistical model and the feature vectors .
E06-2026.txt,90,It does not discriminate well among alternatives.
E06-2026.txt,91,Any improvement of the statistical model will push the precision of ILP.
E06-2026.txt,92,The results of the second setting_comma_ i.e. to collapse all grammatical roles of the verb frames to a single role set cf. Fig. 2_comma_ Fa71a70a104a1a0a3a0 _comma_ are astonishingly good.
E06-2026.txt,93,The f measures comes close to the results of Buchholz_comma_ 1999 .
E06-2026.txt,94,Overall precision is 79.99 _comma_ recall 82.67 and f measure is 81.31 .
E06-2026.txt,95,As expected_comma_ the values of the governable GR decrease e.g. recall for prepositional objects by 30.1 .
E06-2026.txt,96,The third setting will be to let ILP choose among all subcategorization frames of a verb there are up to 20 frames per verb .
E06-2026.txt,97,First experiments have shown that the results are between the a4 a71a70a104 a94 a94 and a4 a71a70a104a1a0a2a0 results.
E06-2026.txt,98,The question then is_comma_ how close can we come to the a4 a71a74a104 a94a96a94 upper bound.
E06-2026.txt,99,7 Related Work ILP has been applied to various NLP problems_comma_ including semantic role labeling Punyakanok et al._comma_ 2004 _comma_ extraction of predicates from parsetrees Klenner_comma_ 2005 and discourse ordering in generation Althaus et al._comma_ 2004 .
E06-2026.txt,100,Roth and Yih_comma_ 2005 discuss how to utilize ILP with Conditional Random Fields.
E06-2026.txt,101,Grammatical relation labeling has been coped with in a couple of articles_comma_ e.g.
E06-2026.txt,102,Buchholz_comma_ 1999 .
E06-2026.txt,103,There_comma_ a cascaded model of classifiers has been proposed using various tools around TIMBL .
E06-2026.txt,104,The f measure perfect test data was 83.5 .
E06-2026.txt,105,However_comma_ the set of grammatical relations differs from the one we use_comma_ which makes it difficult to compare the results.
E06-2026.txt,106,8 Conclusion and Future Work In this paper_comma_ we argue for the integration of top down theory based information into NLP.
E06-2026.txt,107,One kind of information that is well known but have been used only in a data driven manner within statistical approaches e.g. the Collins parser is subcategorization information or case frames .
E06-2026.txt,108,If subcategorization information turns out to be useful at all_comma_ it might become so only under the strict control of a global constraint mechanism.
E06-2026.txt,109,We are currently testing an ILP formalization where all subcategorization frames of a verb are competing witheachother.
E06-2026.txt,110,Thebenefits willbetohavetheinstantiation not only of licensed grammatical roles of a verb_comma_ but of a consistent and coherent instantiation of a single case frame.
E06-2026.txt,112,I would like to thank Markus Dreyer for fruitful long distance discussions and a number of steadily improved maximum entropy models.
E06-2026.txt,113,Also_comma_ the detailed comments of the reviewers have been very helpful. .
E06-1036.txt,1,of discourse structure is crucial in many text based applications. This paper presents an original framework for describing textual parallelism which allows us to generalize various discourse phenomena and to propose a unique method to recognize them.
E06-1036.txt,2,With this prospect_comma_ we discuss several methods in order to identify the most appropriate one for the problem_comma_ and evaluate them based on a manually annotated corpus.
E06-1036.txt,4,Detection of discourse structure is crucial in many text based applications such as Information Retrieval_comma_ Question Answering_comma_ Text Browsing_comma_ etc. Thanks to a discourse structure one can precisely point outaninformation_comma_ provide italocal context_comma_ situate it globally_comma_ link it to others.
E06-1036.txt,5,The context of our research is to improve automatic discourse analysis.
E06-1036.txt,6,A key feature of the most popular discourse theories RST Mann and Thompson_comma_ 1987 _comma_ SDRT Asher_comma_ 1993 _comma_ etc. is the distinction between two sorts of discourse relations or rhetorical functions the subordinating and the coordinating relations some parts of a text play a subordinate role relative to other parts_comma_ while some others have equal importance .
E06-1036.txt,7,In this paper_comma_ we focus our attention on a discourse feature we assume supporting coordination relations_comma_ namely the Textual Parallelism.
E06-1036.txt,8,Based on psycholinguistics studies Dubey et al._comma_ 2005 _comma_ our intuition isthat similarities concerning the surface_comma_ the content and the structure of textual units can be a way for authors to explicit their intention toconsider these units withthesamerhetorical importance.
E06-1036.txt,9,Parallelism can be encountered in many specific discourse structures such as continuity in information structure Kruijff Korbayov a and Kruijff_comma_ 1996 _comma_ frame structures Charolles_comma_ 1997 _comma_ VP ellipses Hobbs and Kehler_comma_ 1997 _comma_ headings Summers_comma_ 1998 _comma_ enumerations Luc et al._comma_ 1999 _comma_ etc. These phenomena are usually treated mostly independently within individual systems with ad hoc resource developments.
E06-1036.txt,10,In this work_comma_ we argue that_comma_ depending on description granularity we can proceed_comma_ computing syntagmatic succession axis of linguistic units and paradigmatic substitution axis similarities between units can allow us to generically handle such discourse structural phenomena.
E06-1036.txt,11,Section 2 introduces the discourse parallelism phenomenon.
E06-1036.txt,12,Section 3develops three methods weimplemented to detect it a similarity degree measure_comma_ a string editing distance Wagner and Fischer_comma_ 1974 and a tree editing distance1 Zhang and Shasha_comma_ 1989 .
E06-1036.txt,13,Section 4 discusses and evaluates these methods and their relevance.
E06-1036.txt,14,The final section reviews related work.
E06-1036.txt,15,2 Textual parallelism Our notion of parallelism is based on similarities between syntagmatic and paradigmatic representations of constituents of textual units.
E06-1036.txt,16,These similarities concern various dimensions from shallow to deeper description layout_comma_ typography_comma_ morphology_comma_ lexicon_comma_ syntax_comma_ and semantics.
E06-1036.txt,17,This account is not limited to the semantic dimension as defined by Hobbs and Kehler_comma_ 1997 who consider text fragments as parallel if the same predicate can be inferred from them with coreferential or similar pairs of arguments.
E06-1036.txt,18,1For all measures_comma_ elementary units considered are syntactic tags and word tokens.
E06-1036.txt,19,281 Weobserve parallelism atvarious structural levels of text among heading structures_comma_ VP ellipses and others_comma_ enumerations of noun phrases in a sentence_comma_ enumerations with or without markers such as frame introducers e.g. In France_comma_ ...In Italy_comma_ ... or typographical and layout markers.
E06-1036.txt,20,The underlying assumption is that parallelism between some textual units accounts for a rhetorical coordination relation.
E06-1036.txt,21,It means that these units can be regarded as equally important.
E06-1036.txt,22,By describing textual units in a two tier framework composed of a paradigmatic level and syntagmatic level_comma_ we argue that_comma_ depending on the description granularity we consider potentially at the character level for item numbering _comma_ we can detect a wide variety of parallelism phenomena.
E06-1036.txt,23,Among parallelism properties_comma_ we note that the parallelism of a given number of textual units is based on the parallelism of their constituents.
E06-1036.txt,24,We also note that certain semantic classes of constituents_comma_ such as item numbering_comma_ are more effective in marking parallelism than others.
E06-1036.txt,25,2.1 An example of parallelism The following example is extracted from our corpus see section 4.1 .
E06-1036.txt,26,In this case_comma_ we have an enumeration without explicit markers.
E06-1036.txt,27,For the purposes of chaining_comma_ each type of link between WordNet synsets is assigned a direction of up_comma_ down_comma_ or horizontal.
E06-1036.txt,28,Upward links correspond to generalization for example_comma_ an upward link from apple to fruit indicates that fruit is more general than apple.
E06-1036.txt,29,Downward links correspond to specialization forexample_comma_ alinkfromfruittoapplewould have a downward direction.
E06-1036.txt,30,Horizontal links are very specific specializations.
E06-1036.txt,31,The parallelism pattern of the first two items is described as follows JJ suff ward links correspond to NN suff alization for example _comma_ X link from Y to Z .
E06-1036.txt,32,This pattern indicates that several item constituents can be concerned by parallelism and that similarities can be observed at the typographic_comma_ lexical and syntactic description levels.
E06-1036.txt,33,Tokens words or punctuation marks having identical shallow descriptions are written in italics.
E06-1036.txt,34,The X_comma_ Y and Z variables stand for matching any nonparallel textareas between contiguous parallel textual units.
E06-1036.txt,35,Some words are parallel based on their syntactic category JJ adjectives_comma_ NN nouns or suffix specifications suff attribute .
E06-1036.txt,36,The third item is similar to the first two items but with a simpler pattern JJ links U NN suff alization W .
E06-1036.txt,37,Parallelism is distinguished by these types of similarities between sentences.
E06-1036.txt,38,3 Methods Three methods were used in this study.
E06-1036.txt,39,Given a pair of sentences_comma_ they all produce a score of similarity between these sentences.
E06-1036.txt,40,We first present the preprocessing to be performed on the texts.
E06-1036.txt,41,3.1 Prior processing applied on the texts The texts were automatically cut into sentences.
E06-1036.txt,42,The first two steps hereinafter have been applied for all the methods.
E06-1036.txt,43,The last third was not applied for the tree editing distance see 3.3 .
E06-1036.txt,44,Punctuation marks and syntactic labels were henceforward considered as words.
E06-1036.txt,46,Text homogenization lemmatization together with a semantic standardization.
E06-1036.txt,47,Lexical chains are built using WordNet relations_comma_ then words are replaced by their most representative synonym Horizontal links are specific specializations. horizontal connection be specific specialization .
E06-1036.txt,49,Syntactic analysis by Charniak_comma_ 1997 s parser S1 S NP JJ Horizontal NNS links  VP AUX are NP ADJP JJ specific  NNS specializations  SENT .  3.
E06-1036.txt,50,Syntactic structure flattening S1 S NP JJ Horizontal NNS links VP AUX are NP ADJP JJ specific NNS specializations SENT.
E06-1036.txt,51,3.2 Wagner Fischer s string edit distance This method is based on Wagner Fischer s string edit distance algorithm Wagner and Fischer_comma_ 1974 _comma_ applied to sentences viewed as strings of words.
E06-1036.txt,52,It computes a sentence edit distance_comma_ using edit operations on these elementary entities.
E06-1036.txt,53,The idea is to use edit operations to transform sentence S1 into S2.
E06-1036.txt,54,Similarly to Wagner and Fischer_comma_ 1974 _comma_ we considered three edit operations 1. replacing word x S1 by y S2 x y 2. deleting word x S1 x  3. inserting word y S2 into S1  y By definition_comma_ the cost of a sequence of edit operations is the sum of the costs2 of the elementary 2We used unitary costs in this study 282 operations_comma_ and the distance between S1 and S2 is the cost of the least cost transformation of S1 into S2.
E06-1036.txt,55,Wagner Fischer s method provides a simple and effective way O S1 S2 to compute it.
E06-1036.txt,56,To reduce size effects_comma_ we normalized by S1 S2 2 .
E06-1036.txt,57,3.3 Zhang Shasha s algorithm Zhang Shasha s method Zhang and Shasha_comma_ 1989 Dulucq and Tichit_comma_ 2003 generalizes Wagner Fischer s edit distance to trees given two trees T1 and T2_comma_ it computes the least cost sequence of edit operations that transforms T1 into T2.
E06-1036.txt,58,Elementary operations have unitary costs and apply to nodes labels and words in the syntactic trees .
E06-1036.txt,59,These operations are depicted below substitution of node c by node g top figure _comma_ insertion of node d middle fig. _comma_ and deletion of node d bottom fig. _comma_ each read from left to right.
E06-1036.txt,60,Tree edit distance d T1_comma_T2 is determined after aseries ofintermediate calculations involving special subtrees of T1 and T2_comma_ rooted in keyroots.
E06-1036.txt,61,3.3.1 Keyroots_comma_ special subtrees and forests Given a certain node x_comma_ L x denotes its leftmost leaf descendant.
E06-1036.txt,62,L is an equivalence relation overnodes and keyroots KR are bydefinition the equivalence relation representatives of highest postfix index.
E06-1036.txt,63,Special subtrees SST are the subtrees rooted in these keyroots.
E06-1036.txt,64,Consider a tree T postfix indexed left figure hereinafter and its three SSTs right figure .
E06-1036.txt,65,SST k1 rooted in k1 is denoted T L k1 _comma_L k1 1_comma_..._comma_k1 .
E06-1036.txt,66,E.g SST 3 T 1_comma_2_comma_3 is the subtree containing nodes a_comma_ b_comma_ d.
E06-1036.txt,67,A forest of SST k1 is defined as T L k1 _comma_L k1 1_comma_..._comma_x _comma_ where x is a node of SST k1 .
E06-1036.txt,68,E.g SST 3 has 3 forests T 1 node a _comma_ T 1_comma_2 nodes a and b and itself.
E06-1036.txt,69,Forests are ordered sequences of subtrees.
E06-1036.txt,70,3.3.2 An idea of how it works The algorithm computes the distance between all pairs of SSTs taken in T1 and T2_comma_ rooted in increasingly indexed keyroots.
E06-1036.txt,71,In the end_comma_ the last SSTs being the full trees_comma_ we have d T1_comma_T2 .
E06-1036.txt,72,In the main routine_comma_ an N1 N2 array called TREEDIST is progressively filled with values TREEDIST i_comma_j equal to the distance between the subtree rooted in T1 s ith node and the subtree rooted in T2 s jth node.
E06-1036.txt,73,The bottom right hand cell of TREEDIST is therefore equal to d T1_comma_T2 .
E06-1036.txt,74,Each step of the algorithm determines the edit distance between two SSTs rooted in keyroots k1_comma_k2  T1 T2 .
E06-1036.txt,75,An array FDIST is initialized for this step and contains as many lines and columns as the two given SSTs have nodes.
E06-1036.txt,76,The array is progressively filled with the distances between increasing forests of these SSTs_comma_ similarly to Wagner Fischer s method.
E06-1036.txt,77,The bottom right hand value of FDIST contains the distance between the SSTs_comma_ which is then stored in TREEDIST in the appropriate cell.
E06-1036.txt,78,Calculations in FDIST and TREEDIST rely on the double recurrence formula depicted below The first formula is used to compute the distance between two forests a white one and a black one _comma_ each of which is composed of several trees.
E06-1036.txt,79,The small circles stand for the nodes of highest postfix index.
E06-1036.txt,80,Distance between two forests is definedastheminimumcostoperation between three possibilities replacing the rightmost white tree by the rightmost black tree_comma_ deleting the white node_comma_ or inserting the black node.
E06-1036.txt,81,Thesecond formula isanalogous tothefirstone_comma_ in the special case where the forests are reduced to a single tree.
E06-1036.txt,82,The distance is defined as the minimum cost operation between replacing the white node with the black node_comma_ deleting the white node_comma_ or inserting the black node.
E06-1036.txt,83,283 It is important to notice that the first formula takes the left context of the considered subtrees into account3 ancestor and left sibling orders are preserved.
E06-1036.txt,84,It is not possible to replace the white node with the black node directly_comma_ the whole subtree rooted in the white node has to be replaced.
E06-1036.txt,85,The good thing is_comma_ the cost of this operation has already been computed and stored in TREEDIST.
E06-1036.txt,86,Let s see why all the computations required at a given step of the recurrence formula have already been calculated.
E06-1036.txt,87,Let two SSTs of T1 and T2 be rooted in pos1 and pos2.
E06-1036.txt,88,Considering the symmetry of the problem_comma_ let s only consider what happens with T1.
E06-1036.txt,89,When filling FDIST pos1_comma_pos2 _comma_ all nodes belonging to SST pos1 are run through_comma_ according to increasing postfix indexes.
E06-1036.txt,90,Consider x T L pos1 _comma_..._comma_pos1 If L x L pos1 _comma_ then x belongs to the leftmost branch of T L pos1 _comma_..._comma_pos1 and forest T L pos1 _comma_..._comma_x is reduced to a single tree.
E06-1036.txt,91,By construction_comma_ allFDIST T L pos1 _comma_..._comma_y _comma_ for y x 1 have already been computed.
E06-1036.txt,92,If things are the same for the current node in SST pos2 _comma_ then TREEDIST T L pos1 _comma_..._comma_x _comma_ can be calculated directly_comma_ using the appropriate FDIST values previously computed.
E06-1036.txt,93,If L x negationslash L pos1 _comma_ then x does not belong to the leftmost branch of T L pos1 _comma_..._comma_pos1 and therefore x has a non empty left context T L pos1 _comma_..._comma_L x 1 .
E06-1036.txt,94,Let s see why computing FDIST T L pos1 _comma_..._comma_x _comma_ requires values which have been previously obtained. If x is a keyroot_comma_ since the algorithm runs through keyroots by increasing order_comma_ TREEDIST T L x _comma_..._comma_x _comma_ has already been computed. If x is not a keyroot_comma_ then there exists a node z such that x z pos1_comma_ z is a keyroot and L z L x .
E06-1036.txt,95,Therefore x belongs to the leftmost branch of T L z _comma_..._comma_z _comma_ which means TREEDIST T L z _comma_..._comma_x _comma_ has already been computed.
E06-1036.txt,96,Complexity for this algorithm is O T1 T2 min p T1 _comma_f T1 min p T2 _comma_f T2 where d Ti is the depth Ti and f Ti is the number of terminal nodes of Ti.
E06-1036.txt,97,3The 2nd formula does too_comma_ since left context is empty.
E06-1036.txt,98,3.4 Our proposal a degree of similarity This final method computes a degree of similarity between two sentences_comma_ considered as lists of syntactic labels and lexical words constituents.
E06-1036.txt,99,Becausesomeconstituents aremorelikely toindicate parallelism than others e.g the list item marker is more pertinent than the determiner a _comma_ a crescent weight function p x  0_comma_1 w.r.t. pertinence is assigned to all lexical and syntactic constituents x.
E06-1036.txt,100,A set of special subsentences is then generated the greatest common divisor of S1 and S2_comma_ gcd S1_comma_S2 _comma_ is defined as the longest list of words common to S1 and S2.
E06-1036.txt,101,Then for each sentence Si_comma_ the set of special subsentences is computed using the words of gcd S1_comma_S2 according to their order of appearance in Si.
E06-1036.txt,102,For example_comma_ if S1 cabcad and S2 acbae_comma_ gcd S1_comma_S2  c_comma_a_comma_b_comma_a .
E06-1036.txt,103,The set of subsentences for S1 is caba_comma_abca and the set for S2 is reduced to acba .
E06-1036.txt,104,Note that any generated subsentence is exactly the size of gcd S1_comma_S2 .
E06-1036.txt,105,For any two subsentences s1 and s2_comma_ we define a degree of similarity D s1_comma_s2 _comma_ inspired from string edit distances D s1_comma_s2 nX i 1 d max d xi dmax p xi 8    n size of all subsentences xi ith constituent of s1 dmax max possible dist. between any xi s1 and its parallel constituent in s2_comma_ i.e. dmax n 1 d xi distance between current constituent xi in s1 and its parallel constituent in s2 p xi parallelism weight of xi The further a constituent from s1 is from its symmetric occurrence in s2_comma_ the more similar the compared subsentences are.
E06-1036.txt,106,Eventually_comma_ the degree of similarity between sentences S1 and S2 is defined as D S1_comma_S2 2 S 1 S2 max s1_comma_s2 D s1_comma_s2 Example Consider S1 cabcad and S2 acbae_comma_ along with their subsentences s1 caba and sprime1 abca for S1_comma_ and s2 acba for S2.
E06-1036.txt,107,The degrees of parallelism between s1 and s2_comma_ and between sprime1 and s2 are computed.
E06-1036.txt,108,The mapping between the parallel constituents is shown below.
E06-1036.txt,109,284 For example D s1_comma_s2 4X i 1 3 d x i 3 p xi  2 3p c 2 3p a p b p a Assume p b p c 12 and p a 1.
E06-1036.txt,110,Then D s1_comma_s2 2.5 and_comma_ similarly D sprime1_comma_s2 similarequal 2.67.
E06-1036.txt,111,Therefore the normalized degree of parallelism is D S1_comma_S2 25 6 2.67_comma_ which is about 0.48.
E06-1036.txt,112,4 Evaluation This section describes the methodology employed to evaluate performances.
E06-1036.txt,113,Then_comma_ after a preliminary study ofour corpus_comma_ results arepresented successively for each method.
E06-1036.txt,114,Finally_comma_ the behavior of the methods is analyzed at sentence level.
E06-1036.txt,115,4.1 Methodology Our parallelism detection is an unsupervised clustering application given a set of pairs of sentences_comma_ it automatically classifies them into the class of the parallelisms and the remainders class.
E06-1036.txt,116,Pairs were extracted from 5 scientific articles written in English_comma_ each containing about 200 sentences Green ACL 98 _comma_ Kan Kan et al. WVLC 98 _comma_ Mitkov Coling ACL 98 _comma_ Oakes IRSG 99 and Sand Sanderson et al. SIGIR 99 .
E06-1036.txt,117,The idea was to compute for each pair a parallelism score indicating the similarity between the sentences.
E06-1036.txt,118,Then the choice of a threshold determined which pairs showed a score high enough to be classified as parallel.
E06-1036.txt,119,Evaluation was based on a manual annotation we proceeded over the texts.
E06-1036.txt,120,In order to reduce computational complexity_comma_ we only considered the parallelism occurring between consecutive sentences.
E06-1036.txt,121,For each sentence_comma_ we indicated the index of its parallel sentence.
E06-1036.txt,122,We assumed transitivity of parallelism if S1 S2 and S2 S3_comma_ then S1 S3.
E06-1036.txt,123,It was thus considered sufficient to indicate the index of S1 for S2 and the index of S2 for S3 to account for a parallelism between S1_comma_ S2 and S3.
E06-1036.txt,124,We annotated pairs of sentences where textual parallelism led us to rhetorically coordinate them.
E06-1036.txt,125,The decision was sometimes hard to make.
E06-1036.txt,126,Yet we annotated it each time to get more data and to study the behavior of the methods on these examples_comma_ possibly penalizing our applications.
E06-1036.txt,127,In the end_comma_ 103 pairs were annotated.
E06-1036.txt,128,We used the notions of precision correctness and recall completeness .
E06-1036.txt,129,Because efforts in improving one often result in degrading the other_comma_ the F measure harmonic mean combines them into a unique parameter_comma_ which simplifies comparisons of results.
E06-1036.txt,130,Let P be the set of the annotated parallelisms and Q the set of the pairs automatically classified in the parallelisms after the use of athreshold.
E06-1036.txt,131,Then the associated precision p_comma_ recall r and F measure f are defined as p P Q Q r P Q P f 21 p 1 q As we said_comma_ the unique task of the implemented methods was to assign parallelism scores to pairs of sentences_comma_ which are collected in a list.
E06-1036.txt,132,We manually applied various thresholds to the list and computed their corresponding F measure.
E06-1036.txt,133,We keptasaperformance indicator thebestF measure found.
E06-1036.txt,134,This was performed for each method and on each text_comma_ as well as on the texts all gathered together.
E06-1036.txt,135,4.2 Preliminary corpus study This paragraph underlines some of the characteristics of the corpus_comma_ in particular the distribution of the annotated parallelisms in the texts for adjacent sentences.
E06-1036.txt,136,The following table gives the percentage of parallelisms for each text Parallelisms Nb of pairs Green 39 14.4 270 Kan 12 6 200 Mitkov 13 8.4 168 Oakes 22 13.7 161 Sand 17 7.7 239 All gathered 103 9.9 1038 Greenand Oakes show significantly moreparallelisms than the other texts.
E06-1036.txt,137,Therefore_comma_ if we consider a lazy method that would put all pairs in the class of parallelisms_comma_ Green and Oakes will yield a priori better results.
E06-1036.txt,138,Precision is indeed directly related tothepercentage ofparallelisms inthetext.
E06-1036.txt,139,In this case_comma_ it is exactly this percentage_comma_ and it gives us a minimum value of the F measure our methods should at least reach Precision Recall F measure Green 14.4 100 25.1 Kan 6 100 11.3 Mitkov 8.4 100 15.5 Oakes 13.7 100 24.1 Sand 7.7 100 14.3 All 9.9 100 18.0 4.3 A baseline counting words in common We first present the results of a very simple and thus very fast method.
E06-1036.txt,140,This baseline counts the 285 words sentences S1 and S2 have in common_comma_ and normalizes the result by S1 S2 2 in order to reduce size effects.
E06-1036.txt,141,No syntactic analysis nor lexical homogenization was performed on the texts.
E06-1036.txt,142,Results for this method are summarized in the following table.
E06-1036.txt,143,The last column shows the loss  in F measure after applying a generic threshold the optimal threshold found when all texts are gathered together on each text.
E06-1036.txt,147,Loss Green 45 34 67 0.4 2 Kan 24 40 17 0.9 10 Mitkov 22 13 77 0.0 8 Oakes 45 78 32 0.8 7 Sand 23 17 35 0.5 1 All 30 23 42 0.5 We first note that results are twice as good as with the lazy approach_comma_ with Green and Oakes far above the rest.
E06-1036.txt,148,Yet this is not sufficient for a real application.
E06-1036.txt,149,Furthermore_comma_ the optimal threshold is very different from one text to another_comma_ which makes the learning of a generic threshold able to detect parallelisms for any text impossible.
E06-1036.txt,150,The only advantage here is the simplicity of the method no prior treatment was performed on the texts before the search_comma_ and the counting itself was very fast.
E06-1036.txt,151,4.4 String edit distance We present the results for the 1st method below F meas.
E06-1036.txt,154,Loss Green 52 79 38 0.69 0 Kan 44 67 33 0.64 2 Mitkov 38 50 31 0.69 0 Oakes 82 94 73 0.68 0 Sand 47 54 42 0.72 9 All 54 73 43 0.69 Green and Oakes still yield the best results_comma_ but the other texts have almost doubled theirs.
E06-1036.txt,155,Results for Oakes are especially good an F measure of 82 guaranties high precision and recall.
E06-1036.txt,156,In addition_comma_ the use of a generic threshold on each text had little influence on the value of the F measure.
E06-1036.txt,157,The greatest loss is for Sand and only corresponds to the adjunction of four pairs of sentences intheclass ofparallelisms.
E06-1036.txt,158,Theselection of a unique generic threshold to predict parallelisms should therefore be possible.
E06-1036.txt,159,4.5 Tree edit distance The algorithm was applied using unitary edit costs.
E06-1036.txt,160,Since it did not seem natural to establish mappings between different levels of the sentence_comma_ edit operations between two constituents of different nature e.g substitution of a lexical by a syntactic element were forbidden by a prohibitive cost 1000 .
E06-1036.txt,161,However_comma_ this banning only improved the results shyly_comma_ unfortunately.
E06-1036.txt,165,Loss Green 46 92 31 0.72 3 Kan 44 67 33 0.75 0 Mitkov 43 40 46 0.87 11 Oakes 81 100 68 0.73 0 Sand 52 100 35 0.73 2 All 51 73 39 0.75 As illustrated in the table above_comma_ results are comparable to those previously found.
E06-1036.txt,166,We note an especially good F measure for Sand 52 _comma_ against 47 for the string edit distance.
E06-1036.txt,167,Optimal thresholds were quite similar from one text to another.
E06-1036.txt,168,4.6 Degree of similarity Because of the high complexity of this method_comma_ a heuristic was applied.
E06-1036.txt,169,The generation of the subsentences is indeed inproducttextCkini_comma_ ki being the number of occurrences of the constituent xi in gcd_comma_ and ni the number of xi in the sentence.
E06-1036.txt,170,We chose to limit the generation to a fixed amount of subsentences.
E06-1036.txt,171,The constituents that have a great Ckini bring too much complexity we chose to eliminate their ni ki last occurrences and to keep their ki firstoccurrences only togenerate subsequences.
E06-1036.txt,172,An experiment was conducted in order to determine the maximum amount of subsentences that could be generated in a reasonable amount of time without significant performance loss and 30 was a sufficient number.
E06-1036.txt,173,In another experiment_comma_ different parallelism weights were assigned to lexical constituents and syntactic labels.
E06-1036.txt,174,The aim was to understand their relative importance for parallelisms detection.
E06-1036.txt,175,Results show that lexical constituents have a significant role_comma_ but conclusions are more difficult to draw for syntactic labels.
E06-1036.txt,176,It was decided that_comma_ from now on_comma_ the lexicalweight should begiven the maximum value_comma_ 1.
E06-1036.txt,177,Finally_comma_ we assigned different weights to the syntactic labels.
E06-1036.txt,178,Weights were chosen after counting the occurrences of the labels in the corpus.
E06-1036.txt,179,In fact_comma_ we counted for each label the percentage of occurrences that appeared in the gcd of the parallelisms with respect to those appearing in the gcd of the other pairs.
E06-1036.txt,180,Percentages were then rescaled from 0 to 1_comma_ in order to emphasize differences 286 between labels.
E06-1036.txt,181,The obtained parallelism values measured the role of the labels in the detection of parallelism.
E06-1036.txt,182,Results for this experiment appear in the table below.
E06-1036.txt,186,Loss Green 55 59 51 0.329 2 Kan 47 80 33 0.354 5 Mitkov 35 40 31 0.355 0 Oakes 76 80 73 0.324 4 Sand 29 20 59 0.271 0 All 50 59 43 0.335 The optimal F measures were comparable to those obtained in 4.4 and the corresponding thresholds were similar from one text to another.
E06-1036.txt,187,This section showed how the three proposed methods outperformed the baseline.
E06-1036.txt,188,Each of them yielded comparable results.
E06-1036.txt,189,The next section presents the results at sentence level_comma_ together with a comparison of these three methods.
E06-1036.txt,190,4.7 Analysis at sentence level The different methods often agreed but sometimes reacted quite differently.
E06-1036.txt,191,Well retrieved parallelisms Some parallelisms were found by each method with no difficulty they were given a high degree of parallelism by each method.
E06-1036.txt,192,Typically_comma_ such sentences presented a strong lexical and syntactic similarity_comma_ as in the example in section 2.
E06-1036.txt,193,Parallelisms hard to find Other parallelisms received very low scores from each method.
E06-1036.txt,194,This happened when the annotated parallelism was lexically and syntactically poor and needed either contextual information or external semantic knowledge to find keywords e.g first _comma_ second _comma_ ... _comma_ paraphrases or patterns e.g X Y inthefollowing example Kan Rear a paragraph in which a link just stopped occurring the paragraph before.
E06-1036.txt,195,No link any remaining paragraphs.
E06-1036.txt,196,Different methods_comma_ different results Eventually_comma_ we present some parallelisms that obtained very different scores_comma_ depending on the method.
E06-1036.txt,197,First_comma_ it seems that a different ordering of the parallel constituents in the sentences alter the performances of the edit distance algorithms 3.2 3.3 .
E06-1036.txt,198,The following example Green received a low score with both methods When we consider AnsV as our dependent variable_comma_ the model for the High Web group is still not significant_comma_ and there is still a high probability that the coefficient of LI is 0.
E06-1036.txt,199,For our Low Web group_comma_ who followed significantly more intra article links than the High Web group_comma_ the model that results is significant and has the following equation EQN .
E06-1036.txt,200,This is due to the fact that both algorithms do not allow the inversion of two constituents and thus are unable to find all the links from the first sentence to the other.
E06-1036.txt,201,The parallelism measure is robust to inversion.
E06-1036.txt,202,Sometimes_comma_ the syntactic parser gave different analyses for the same expression_comma_ which made mapping between thesentences containing this expression more difficult_comma_ especially for the tree edit distance.
E06-1036.txt,203,The syntactic structure has less importance for the other methods_comma_ which are thus more insensitive to an incorrect analysis.
E06-1036.txt,204,Finally_comma_ the parallelism measure seems more adapted to a diffuse distribution of the parallel constituents in the sentences_comma_ whereas edit distances seem more appropriate when parallel constituents are concentrated in a certain part of the sentences_comma_ in similar syntactic structures.
E06-1036.txt,205,The following example Green obtained very high scores with the edit distances only Strong relations are also said to exist between words that have synsets connected by a single horizontal link or words that have synsets connected by a single IS A or INCLUDES relation.
E06-1036.txt,206,A regular relation is said to exist between two words when there is at least one allowable path between a synset containing the first word and a synset containing the second word in the WordNet database.
E06-1036.txt,207,5 Related work Experimental work in psycholinguistics has shown the importance of the parallelism effect in human language processing.
E06-1036.txt,208,Due to some kind of priming syntactic_comma_ phonetic_comma_ lexical_comma_ etc. _comma_ the comprehension and the production of a parallel utterance is made faster Dubey et al._comma_ 2005 .
E06-1036.txt,209,So far_comma_ most of the works were led in order to acquire resources and to build systems to retrieve specific parallelism phenomena.
E06-1036.txt,210,In the field of information structure theories_comma_ Kruijff Korbayov a and Kruijff_comma_ 1996 implemented an ad hoc system 287 to identify thematic continuity lexical relation between the subject parts of consecutive sentences .
E06-1036.txt,211,Luc et al._comma_ 1999 described and classified markers lexical clues_comma_ layout and typography occurring in enumeration structures.
E06-1036.txt,212,Summers_comma_ 1998 also described the markers required for retrieving heading structures.
E06-1036.txt,213,Charolles_comma_ 1997 was involved in the description of frame introducers.
E06-1036.txt,214,Integration of specialized resources dedicated to parallelism detection could be an improvement to our approach.
E06-1036.txt,215,Let us not forget that our final aim remains the detection of discourse structures.
E06-1036.txt,216,Parallelism should be considered as an additional feature which among other discourse features e.g. connectors .
E06-1036.txt,217,Regarding the use of parallelism_comma_ Hernandez and Grau_comma_ 2005 proposed an algorithm to parse the discourse structure and to select pairs of sentences to compare.
E06-1036.txt,218,Confronted to the problem of determining textual entailment4 the fact that the meaning of one expression can be inferred from another Kouylekov and Magnini_comma_ 2005 applied the Zhang and Shasha_comma_ 1989 s algorithm on the dependency trees of pairs of sentences they did not consider syntactic tags as nodes but only words .
E06-1036.txt,219,They encountered problems similar to ours due to pre treatment limits.
E06-1036.txt,220,Indeed_comma_ the syntactic parser sometimes represents in a different way occurrences of similar expressions_comma_ making it harder to apply edit transformations.
E06-1036.txt,221,A drawback concerning the tree edit distance approach is that it is not able to observe the whole tree_comma_ but only the subtree of the processed node.
E06-1036.txt,222,6 Conclusion Textual parallelism plays an important role among discourse features when detecting discourse structures.
E06-1036.txt,223,Sofar_comma_ onlyoccurrences ofthisphenomenon have been treated individually and often in an adhoc manner.
E06-1036.txt,224,Our contribution is a unifying framework which can be used for automatic processing with much less specific knowledge than dedicated techniques.
E06-1036.txt,225,In addition_comma_ we discussed and evaluated several methods to retrieve them generically.
E06-1036.txt,226,We showed that simple methods such as Wagner and Fischer_comma_ 1974 can compete with more complex approaches_comma_ such as our degree of similarity and the 4Compared to entailment_comma_ the parallelism relation is bidirectional and not restricted to semantic similarities.
E06-1036.txt,227,Zhang and Shasha_comma_ 1989 s algorithm.
E06-1036.txt,228,Among future works_comma_ it seems that variations such as the editing cost of transformation for edit distance methods and the weight of parallel units depending their semantic and syntactic characteristics can be implemented to enhance performances.
E06-1036.txt,229,Combining methods also seems an interesting track to follow. .
E06-1038.txt,1,present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase structure parsers. The parsers are trained out of domain and contain a significant amount of noise.
E06-1038.txt,2,We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.
E06-1038.txt,3,This differs from current state of the art models Knight and Marcu_comma_ 2000 that treat noisy parse trees_comma_ for both compressed and uncompressed sentences_comma_ as gold standard when calculating model parameters.
E06-1038.txt,5,The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.
E06-1038.txt,6,Most summarization systems are evaluated on the amount of relevant information retained aswellastheir compression rate.
E06-1038.txt,7,Thus_comma_ returning highly compressed_comma_ yet informative_comma_ sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted.
E06-1038.txt,8,We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original_comma_ which is the most common setting in the literature Knight and Marcu_comma_ 2000 Riezler et al._comma_ 2003 Turner and Charniak_comma_ 2005 .
E06-1038.txt,9,In this framework_comma_ the goal is to find the shortest substring oftheoriginal sentence thatconveys the most important aspects of the meaning.
E06-1038.txt,10,We will work in a supervised learning setting and assume as input a training set T xt_comma_yt T t 1 of original sentences xt and their compressions yt.
E06-1038.txt,11,We use the Ziff Davis corpus_comma_ which is a set of 1087 pairs of sentence compression pairs.
E06-1038.txt,12,Furthermore_comma_ we use the same 32 testing examples from Knight and Marcu 2000 and the rest for training_comma_ except that we hold out 20 sentences for the purpose of development.
E06-1038.txt,13,A handful of sentences occur twice but with different compressions.
E06-1038.txt,14,We randomly select a single compression for each unique sentence in order to create an unambiguous training set.
E06-1038.txt,15,Examples from this data set are given in Figure 1.
E06-1038.txt,16,Formally_comma_ sentence compression aims to shorten a sentence x x1 ...xn into a substring y y1 ...ym_comma_ where yi x1_comma_..._comma_xn .
E06-1038.txt,17,We define the function I yi  1_comma_..._comma_n that maps word yi in the compression to the index of the word in the original sentence.
E06-1038.txt,18,Finally we include the constraint I yi I yi 1 _comma_ which forces each word in x to occur at most once in the compression y.
E06-1038.txt,19,Compressions are evaluated on three criteria_comma_ 1.
E06-1038.txt,20,Grammaticality Compressed sentences should be grammatical.
E06-1038.txt,22,Importance How much of the important information is retained from the original.
E06-1038.txt,24,Compression rate How much compression took place.
E06-1038.txt,25,A compression rate of 65 means the compressed sentence is 65 the length of the original.
E06-1038.txt,26,Typically grammaticality and importance are traded off with compression rate.
E06-1038.txt,27,The longer our 297 The Reverse Engineer Tool is priced from 8_comma_000 for a single user to 90_comma_000 for a multiuser project site .
E06-1038.txt,28,The Reverse Engineer Tool is available now and is priced on a site licensing basis _comma_ ranging from 8_comma_000 for a single user to 90_comma_000 for a multiuser project site .
E06-1038.txt,29,Design recovery tools read existing code and translate it into definitions and structured diagrams .
E06-1038.txt,30,Essentially _comma_ design recovery tools read existing code and translate it into the language in which CASE is conversant definitions and structured diagrams .
E06-1038.txt,31,Figure 1 Two examples of compressed sentences from the Ziff Davis corpus.
E06-1038.txt,32,The compressed version and the original sentence are given. compressions_comma_ the less likely we are to remove important words or phrases crucial to maintaining grammaticality and the intended meaning.
E06-1038.txt,33,The paper is organized as follows Section 2 discusses previous approaches to sentence compression.
E06-1038.txt,34,In particular_comma_ we discuss the advantages and disadvantages of the models of Knight and Marcu 2000 .
E06-1038.txt,35,In Section 3 we present our discriminative large margin model for sentence compression_comma_ including the learning framework and an efficient decoding algorithm for searching the space of compressions.
E06-1038.txt,36,We also show how to extract a rich feature set that includes surfacelevel bigram features of the compressed sentence_comma_ dropped words and phrases from the original sentence_comma_ and features over noisy dependency and phrase structure trees for the original sentence.
E06-1038.txt,37,We argue that this rich feature set allows the model to learn which words and phrases should be dropped and which should remain in the compression.
E06-1038.txt,38,Section 4 presents an experimental evaluation of our model compared to the models of Knight and Marcu 2000 and finally Section 5 discusses some areas of future work.
E06-1038.txt,39,2 Previous Work Knight and Marcu 2000 first tackled this problem by presenting a generative noisy channel model and a discriminative tree to tree decision tree model.
E06-1038.txt,40,The noisy channel model defines the problem as finding the compressed sentence with maximum conditional probability y argmax y P y x argmax y P x y P y P y is the source model_comma_ which is a PCFG plus bigram language model.
E06-1038.txt,41,P x y is the channel model_comma_ the probability that the long sentence is an expansion of the compressed sentence.
E06-1038.txt,42,To calculate the channel model_comma_ both the original and compressed versions of every sentence in the training set are assigned a phrase structure tree.
E06-1038.txt,43,Given a tree for a long sentence x and compressed sentence y_comma_ the channel probability is the product of the probability for each transformation required if the tree for y is to expand to the tree for x.
E06-1038.txt,44,The tree to tree decision tree model looks to rewrite the tree for x into a tree for y.
E06-1038.txt,45,The model uses a shift reduce drop parsing algorithm that starts with the sequence of words in xand the corresponding tree.
E06-1038.txt,46,The algorithm then either shifts considers new words and subtrees for x _comma_ reduces combines subtrees from x into possibly new tree constructions or drops drops words and subtrees from x on each step of the algorithm.
E06-1038.txt,47,A decision tree model is trained on a set of indicative features for each type of action in the parser.
E06-1038.txt,48,These models are then combined in a greedy global search algorithm to find a single compression.
E06-1038.txt,49,Though both models of Knight and Marcu perform quite well_comma_ they do have their shortcomings.
E06-1038.txt,50,The noisy channel model uses a source model that is trained on uncompressed sentences_comma_ even though the source model is meant to represent the probability of compressed sentences.
E06-1038.txt,51,The channel model requires aligned parse trees for both compressed and uncompressed sentences in the training set in order to calculate probability estimates.
E06-1038.txt,52,These parses are provided from a parsing model trained on out of domain data the WSJ _comma_ which can result in parse trees with many mistakes for both the original and compressed versions.
E06-1038.txt,53,This makes alignment difficult and the channel probability estimates unreliable as a result.
E06-1038.txt,54,On the other hand_comma_ the decision tree model does not rely on the trees to align and instead simply learns a tree totree transformation model to compress sentences.
E06-1038.txt,55,The primary problem with this model is that most of the model features encode properties related to including or dropping constituents from the tree withno encoding of bigram or trigram surface features to promote grammaticality.
E06-1038.txt,56,As a result_comma_ the model will sometimes return very short and ungrammatical compressions.
E06-1038.txt,57,Both models rely heavily on the output of a noisy parser to calculate probability estimates for the compression.
E06-1038.txt,58,We argue in the next section that 298 ideally_comma_ parse trees should be treated solely as a source of evidence when making compression decisions to be balanced with other evidence such as that provided by the words themselves.
E06-1038.txt,59,Recently Turner and Charniak 2005 presented supervised and semi supervised versions of the Knight and Marcu noisy channel model.
E06-1038.txt,60,The resulting systems typically return informative and grammatical sentences_comma_ however_comma_ they do so at the cost of compression rate.
E06-1038.txt,61,Riezler et al. 2003 present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions.
E06-1038.txt,62,Though this model is highly likely to return grammatical compressions_comma_ it required the training data be human annotated with syntactic trees.
E06-1038.txt,63,3 Discriminative Sentence Compression For the rest of the paper we use x x1 ...xn to indicate an uncompressed sentence and y y1 ...ym a compressed version of x_comma_ i.e._comma_ each yj indicates the position in x of the jth word in the compression.
E06-1038.txt,64,We always pad the sentence with dummy start and end words_comma_ x1 START and xn END _comma_ which are always included in the compressed version i.e. y1 x1 and ym xn .
E06-1038.txt,65,Inthis section wedescribed adiscriminative online learning approach to sentence compression_comma_ the core of which is a decoding algorithm that searches the entire space of compressions.
E06-1038.txt,66,Let the score of a compression y for a sentence x as s x_comma_y In particular_comma_ we are going to factor this score using a first order Markov assumption on the words in the compressed sentence s x_comma_y  y summationdisplay j 2 s x_comma_I yj 1 _comma_I yj Finally_comma_ we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector s x_comma_y  y summationdisplay j 2 w f x_comma_I yj 1 _comma_I yj Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in between that were dropped from the original sentence to create the compression.
E06-1038.txt,67,We will show in Section 3.2 how this factorization also allows us to include features on dropped phrases and subtrees from both a dependency and a phrase structure parse of the original sentence.
E06-1038.txt,68,Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu 2000 .
E06-1038.txt,69,However_comma_ here they are merely treated as evidence for the discriminative learner_comma_ which will set the weight of each feature relative to the other possibly overlapping features to optimize the models accuracy on the observed data.
E06-1038.txt,70,3.1 Decoding We define a dynamic programming table C i which represents the highest score for any compression that ends at word xi for sentence x.
E06-1038.txt,71,We define a recurrence as follows C 1 0.0 C i maxj i C j s x_comma_j_comma_i for i 1 It is easy to show that C n represents the score of the best compression for sentence x whose length is n under the first order score factorization we made.
E06-1038.txt,72,We can show this by induction.
E06-1038.txt,73,If we assume that C j is the highest scoring compression that ends at word xj_comma_ for all j i_comma_ then C i must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word.
E06-1038.txt,74,Thus_comma_ since xn is by definition in every compressed version of x see above _comma_ then it must be the case that C n stores the score of the best compression.
E06-1038.txt,75,This table can be filled in O n2 .
E06-1038.txt,76,This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text Sarawagi and Cohen_comma_ 2004 McDonald et al._comma_ 2005a .
E06-1038.txt,77,As such_comma_ we can use back pointers to reconstruct the highest scoring compression as well as k best decoding algorithms.
E06-1038.txt,78,This decoding algorithm is dynamic with respect to compression rate.
E06-1038.txt,79,That is_comma_ the algorithm will return the highest scoring compression regardless of length.
E06-1038.txt,80,This may seem problematic since longer compressions might contribute more to the score since they contain more bigrams and thus be preferred.
E06-1038.txt,81,However_comma_ in Section 3.2 we define a rich feature set_comma_ including features on words dropped from the compression that will help disfavor compressions that drop very few words since 299 this is rarely seen in the training data.
E06-1038.txt,82,In fact_comma_ it turns out that our learned compressions have a compression rate very similar to the gold standard.
E06-1038.txt,83,Thatsaid_comma_ therearesomeinstances whenastatic compression rate is preferred.
E06-1038.txt,84,A user may specifically want a 25 compression rate for all sentences.
E06-1038.txt,85,This is not a problem for our decoding algorithm.
E06-1038.txt,86,We simply augment the dynamic programming table and calculate C i r _comma_ which is the score of the best compression of length r that ends at word xi.
E06-1038.txt,87,This table can be filled in as follows C 1 1 0.0 C 1 r  for r 1 C i r maxj i C j r 1 s x_comma_j_comma_i for i 1 Thus_comma_ if werequire aspecific compression rate_comma_ we simple determine the number of words r that satisfy this rate and calculate C n r .
E06-1038.txt,88,The new complexity is O n2r .
E06-1038.txt,89,3.2 Features So far we have defined the score of a compression as well as a decoding algorithm that searches the entire space of compressions to find the one with highest score.
E06-1038.txt,90,This all relies on a score factorization over adjacent words in the compression_comma_ s x_comma_I yj 1 _comma_I yj  w f x_comma_I yj 1 _comma_I yj .
E06-1038.txt,91,In Section 3.3 we describe an online large margin method for learning w.
E06-1038.txt,92,Here we present the feature representation f x_comma_I yj 1 _comma_I yj for a pair of adjacent words in the compression.
E06-1038.txt,93,These features were tuned on a development data set.
E06-1038.txt,94,3.2.1 Word POS Features The first set of features are over adjacent words yj 1 and yj in the compression.
E06-1038.txt,95,These include the part of speech POS bigrams for the pair_comma_ the POS of each word individually_comma_ and the POS context bigram and trigram of the most recent word being added to the compression_comma_ yj.
E06-1038.txt,96,These features are meant to indicate likely words to include in the compression as well as some level of grammaticality_comma_ e.g._comma_ the adjacent POS features JJ VB would get a low weight since we rarely see an adjective followed by a verb.
E06-1038.txt,97,We also add a feature indicating if yj 1 and yj were actually adjacent in the original sentence or not and we conjointhis feature withtheabovePOSfeatures.
E06-1038.txt,98,Note that we have not included any lexical features.
E06-1038.txt,99,We found during experiments onthedevelopment data that lexical information was too sparse and led to overfitting_comma_ so we rarely include such features.
E06-1038.txt,100,Instead we rely on the accuracy of POS tags to provide enough evidence.
E06-1038.txt,101,Next we added features over every dropped wordintheoriginal sentence between yj 1 and yj_comma_ if there were any.
E06-1038.txt,102,These include the POS of each dropped word_comma_ the POS of the dropped words conjoined with the POSof yj 1 and yj.
E06-1038.txt,103,If the dropped word is a verb_comma_ we add a feature indicating the actualverb thisisforcommonverbslike is _comma_ which are typically in compressions .
E06-1038.txt,104,Finally we add the POScontext bigram andtrigram ofeach dropped word.
E06-1038.txt,105,These features represent common characteristics of words that can or should be dropped from the original sentence in the compressed version e.g. adjectives and adverbs .
E06-1038.txt,106,We also add a feature indicating whether the dropped word is a negation e.g._comma_ not_comma_ never_comma_ etc. .
E06-1038.txt,107,We also have a set of features to represent brackets in the text_comma_ which are common in the data set.
E06-1038.txt,108,The first measures if all the dropped words between yj 1 and yj have a mismatched or inconsistent bracketing.
E06-1038.txt,109,The second measures if the left and right most dropped words are themselves both brackets.
E06-1038.txt,110,These features come in handy for examples like_comma_ The Associated Press AP reported the story_comma_ where the compressed version is The Associated Press reported the story.
E06-1038.txt,111,Information within brackets is often redundant.
E06-1038.txt,112,3.2.2 Deep Syntactic Features The previous set of features are meant to encodecommonPOScontexts thatarecommonly retained or dropped from the original sentence during compression.
E06-1038.txt,113,However_comma_ they do so without a larger picture of the function of each word in the sentence.
E06-1038.txt,114,For instance_comma_ dropping verbs is not that uncommon a relative clause for instance may be dropped during compression.
E06-1038.txt,115,However_comma_ dropping the main verb in the sentence is uncommon_comma_ since that verb and its arguments typically encode most of the information being conveyed.
E06-1038.txt,116,An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence.
E06-1038.txt,117,To do this we parse every sentence twice_comma_ once with a dependency parser McDonald et al._comma_ 2005b and once with a phrase structure parser Charniak_comma_ 2000 .
E06-1038.txt,118,These parsers have been trained out of domain on the Penn WSJ Treebank and as a result contain noise.
E06-1038.txt,119,However_comma_ we are merely going to use them as an additional source of features.
E06-1038.txt,120,We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard inourmodelbutjustasmoreevidence for 300 root0 saw2 on4 after6 Mary1 Ralph3 Tuesday5 lunch7 S VP PP PP NP NP NP NP NNP VBD NNP IN NNP IN NN Mary1 saw2 Ralph3 on4 Tuesday5 after6 lunch7 Figure 2 An example dependency tree from the McDonald et al. 2005b parser and phrase structure tree from the Charniak 2000 parser.
E06-1038.txt,121,In this example we want to add features from the trees for the case when Ralph and after become adjacent in the compression_comma_ i.e._comma_ we are dropping the phrase on Tuesday. or against particular compressions.
E06-1038.txt,122,The learning algorithm will set the feature weight accordingly depending on each features discriminative power.
E06-1038.txt,123,It is not unique to use soft syntactic features in this way_comma_ as it has been done for many problems in language processing.
E06-1038.txt,124,However_comma_ we stress this aspect of our model due to the history of compression systems using syntax to provide hard structural constraints on the output.
E06-1038.txt,125,Letsconsider thesentence x Mary saw Ralph on Tuesday after lunch_comma_ with corresponding parses given in Figure 2.
E06-1038.txt,126,In particular_comma_ lets consider the feature representation f x_comma_3_comma_6 .
E06-1038.txt,127,That is_comma_ the feature representation of making Ralph and after adjacent in the compression and dropping the prepositional phrase on Tuesday.
E06-1038.txt,128,Thefirstsetoffeatures we consider are over dependency trees.
E06-1038.txt,129,For every dropped word we add a feature indicating the POS of the words parent in the tree.
E06-1038.txt,130,For example_comma_ if the dropped words parent is root_comma_ then it typically means it is the main verb of the sentence and unlikely to be dropped.
E06-1038.txt,131,We also add a conjunction feature of the POS tag of the word being dropped and the POS of its parent as well as a feature indicating for each word being dropped whether it is a leaf node in the tree.
E06-1038.txt,132,We also add the same features for the two adjacent words_comma_ but indicating that they are part of the compression.
E06-1038.txt,133,For the phrase structure features we find every node in the tree that subsumes a piece of dropped textandisnotachild ofasimilarnode.
E06-1038.txt,134,Inthiscase the PP governing on Tuesday.
E06-1038.txt,135,We then add features indicating the context from which this node was dropped.
E06-1038.txt,136,For example we add a feature specifying that a PP was dropped which was the child of a VP.
E06-1038.txt,137,We also add a feature indicating that a PP was dropped which was the left sibling of another PP_comma_ etc. Ideally_comma_ for each production in the tree we would like to add a feature indicating every node that was dropped_comma_ e.g.
E06-1038.txt,138,VP VBD NP PP PP VP VBD NP PP .
E06-1038.txt,139,However_comma_ we cannot necessarily calculate this feature since the extent of the production might be well beyond the local context of first order feature factorization.
E06-1038.txt,140,Furthermore_comma_ since the training set is so small_comma_ these features are likely to be observed very few times.
E06-1038.txt,141,3.2.3 Feature Set Summary In this section we have described a rich feature set over adjacent words in the compressed sentence_comma_ dropped words and phrases from the original sentence_comma_ and properties of deep syntactic trees oftheoriginal sentence.
E06-1038.txt,142,Notethat these features in many ways mimic the information already present in the noisy channel and decision tree models of Knight and Marcu 2000 .
E06-1038.txt,143,Our bigram features encode properties that indicate both good and bad words to be adjacent in the compressed sentence.
E06-1038.txt,144,Thisissimilar inpurpose tothesource model from the noisy channel system.
E06-1038.txt,145,However_comma_ in that system_comma_ the source model is trained on uncompressed sentences and thus isnot asrepresentative oflikely bigram features for compressed sentences_comma_ which is really what we desire.
E06-1038.txt,146,Our feature set also encodes dropped words and phrases through the properties of the words themselves and through properties of their syntactic relation to the rest of the sentence in a parse tree.
E06-1038.txt,147,These features represent likely phrases to be dropped in the compression and are thus similar in nature to the channel model in the noisy channel system aswellasthefeatures inthetree to tree decision tree system.
E06-1038.txt,148,However_comma_ we use these syntactic constraints as soft evidence in our model.
E06-1038.txt,149,That is_comma_ they represent just another layer of evidence to be considered during training when setting parameters.
E06-1038.txt,150,Thus_comma_ if the parses have too much noise_comma_ the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data.
E06-1038.txt,151,This differs from the models of Knight and Marcu 2000 _comma_ whichtreat thenoisy parses asgold standard when 301 calculating probability estimates.
E06-1038.txt,152,An important distinction we should make is the notion of supported versus unsupported features Sha and Pereira_comma_ 2003 .
E06-1038.txt,153,Supported features are those that are on for the gold standard compressions in the training.
E06-1038.txt,154,For instance_comma_ the bigram feature NN VB will be supported since there is most likely a compression that contains a adjacent noun and verb.
E06-1038.txt,155,However_comma_ the feature JJ VB will not be supported since an adjacent adjective and verb most likely will not be observed in any valid compression.
E06-1038.txt,156,Our model includes all features_comma_ including those that are unsupported.
E06-1038.txt,157,The advantage of this is that the model can learn negative weights for features that are indicative of bad compressions.
E06-1038.txt,158,Thisisnotdifficult todosince most features are POS based and the feature set size even with all these features is only 78_comma_923.
E06-1038.txt,159,3.3 Learning Having defined a feature encoding and decoding algorithm_comma_ the last step is to learn the feature weights w.
E06-1038.txt,160,We do this using the Margin Infused Relaxed Algorithm MIRA _comma_ which is a discriminative large margin online learning technique shown in Figure 3 Crammer and Singer_comma_ 2003 .
E06-1038.txt,161,Oneach iteration_comma_ MIRA considers a single instance from the training set xt_comma_yt and updates the weights so that the score of the correct compression_comma_ yt_comma_ is greater than the score of all other compressions by a margin proportional to their loss.
E06-1038.txt,162,Many weight vectors will satisfy these constraints so we pick the one with minimum change from the previous setting.
E06-1038.txt,163,We define the loss to be the number of words falsely retained or dropped in the incorrect compression relative to the correct one.
E06-1038.txt,164,Forinstance_comma_ ifthecorrect compression ofthe sentence in Figure 2 is Mary saw Ralph_comma_ then the compression Mary saw after lunch would have a loss of 3 since it incorrectly left out one word and included two others.
E06-1038.txt,165,Ofcourse_comma_ forasentence thereareexponentially many possible compressions_comma_ which means that this optimization will have exponentially many constraints.
E06-1038.txt,166,We follow the method of McDonald et al. 2005b and create constraints only on the k compressions that currently have the highest score_comma_ bestk x w .
E06-1038.txt,167,This can easily be calculated by extending the decoding algorithm with standard Viterbi k best techniques.
E06-1038.txt,168,On the development data_comma_ we found that k 10 provided the Training data T  xt_comma_yt Tt 1 1.
E06-1038.txt,169,w0 0 v 0 i 0 2. for n 1..N 3. for t 1..T 4. min  w i 1 w i  s.t. s xt_comma_yt s xt_comma_yprime L yt_comma_yprime where yprime bestk x w i 5.
E06-1038.txt,170,v v w i 1 6.
E06-1038.txt,171,i i 1 7. w v N T Figure 3 MIRA learning algorithm as presented by McDonald et al. 2005b . best performance_comma_ though varying k did not have a major impact overall.
E06-1038.txt,172,Furthermore we found that after only 3 5 training epochs performance on the development data was maximized.
E06-1038.txt,173,The final weight vector is the average of all weight vectors throughout training.
E06-1038.txt,174,Averaging has been shown to reduce overfitting Collins_comma_ 2002 as well as reliance on the order of the examples during training.
E06-1038.txt,175,We found it to be particularly important for this data set.
E06-1038.txt,176,4 Experiments We use the same experimental methodology as Knight and Marcu 2000 .
E06-1038.txt,177,We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5.
E06-1038.txt,178,For each of the 32 sentences in our test set we ask the judges to evaluate three systems human annotated_comma_ the decision tree model of Knight and Marcu 2000 and our system.
E06-1038.txt,179,The judges were told all three compressions were automatically generated and the order in which they werepresented wasrandomly chosen foreach sentence.
E06-1038.txt,180,We compared our system to the decision tree model of Knight and Marcu instead of the noisy channel model since both performed nearly as well in their evaluation_comma_ and the compression rate of the decision tree model is nearer to our system around 57 58 .
E06-1038.txt,181,The noisy channel model typically returned longer compressions.
E06-1038.txt,183,Wepresent theaverage score over all judges as well as the standard deviation.
E06-1038.txt,184,The evaluation for the decision tree system of Knight and Marcu is strikingly similar to theoriginal evaluation intheir work.
E06-1038.txt,185,Thisprovides strong evidence that the evaluation criteria in both cases were very similar.
E06-1038.txt,186,Table 1 shows that all models had similar com302 Compression Rate Grammaticality Importance Human 53.3 4.96 0.2 3.91 1.0 Decision Tree K M2000 57.2 4.30 1.4 3.60 1.3 This work 58.1 4.61 0.8 4.03 1.0 Table 1 Compression results. pressions rates_comma_ with humans preferring to compress a little more aggressively.
E06-1038.txt,187,Not surprisingly_comma_ the human compressions are practically all grammatical.
E06-1038.txt,188,A quick scan of the evaluations shows that the few ungrammatical human compressions were for sentences that were not really grammatical in the first place.
E06-1038.txt,189,Of greater interest is that the compressions of our system are typically more grammatical than the decision tree model of Knight and Marcu.
E06-1038.txt,190,When looking at importance_comma_ we see that our system actually does the best even better than humans.
E06-1038.txt,191,The most likely reason for this is that our model returns longer sentences and is thus less likely to prune away important information.
E06-1038.txt,192,For example_comma_ consider the sentence The chemical etching process used for glare protection is effective and will help if your office has the fluorescent light overkill that s typical in offices The human compression was Glare protection is effective_comma_ whereas our model compressed the sentence to The chemical etching process used for glare protection is effective.
E06-1038.txt,193,A primary reason that our model does better than the decision tree model of Knight and Marcu is that on a handful of sentences_comma_ the decision tree compressions were a single word or noun phrase.
E06-1038.txt,194,For such sentences the evaluators typically rated the compression a 1 for both grammaticality and importance.
E06-1038.txt,195,In contrast_comma_ our model never failed in such drastic ways and always output something reasonable.
E06-1038.txt,196,This is quantified in the standard deviation of the two systems.
E06-1038.txt,197,Though these results are promising_comma_ more large scale experiments are required to really ascertain the significance of the performance increase.
E06-1038.txt,198,Ideally we could sample multiple training testing splits and use all sentences in the data set to evaluate the systems.
E06-1038.txt,199,However_comma_ since these systems require human evaluation we did not have the time or the resources to conduct these experiments.
E06-1038.txt,200,4.1 Some Examples Here we aim to give the reader a flavor of some common outputs from the different models.
E06-1038.txt,201,Three examples are given in Table 4.1.
E06-1038.txt,202,The first shows two properties.
E06-1038.txt,203,First of all_comma_ the decision tree model completely breaks and just returns a single noun phrase.
E06-1038.txt,204,Our system performs well_comma_ however it leaves out the complementizer of the relative clause.
E06-1038.txt,205,This actually occurred in a few examples and appears to be the most common problem of our model.
E06-1038.txt,206,A post processing rule should eliminate this.
E06-1038.txt,207,The second example displays a case in which our system and the human system are grammatical_comma_ but the removal of a prepositional phrase hurts the resulting meaning of the sentence.
E06-1038.txt,208,In fact_comma_ without the knowledge that the sentence is referring to broadband_comma_ the compressions are meaningless.
E06-1038.txt,209,This appears to be a harder problem determining which prepositional phrases can be dropped and which cannot.
E06-1038.txt,210,The final_comma_ and more interesting_comma_ example presents two very different compressions by the human and our automatic system.
E06-1038.txt,211,Here_comma_ the human kept the relative clause relating what languages the source code is available in_comma_ but dropped the main verb phrase of the sentence.
E06-1038.txt,212,Our model preferred to retain the main verb phrase and drop the relative clause.
E06-1038.txt,213,This is most likely due to the fact that dropping the main verb phrase of a sentence is much less likely in the training data than dropping a relative clause.
E06-1038.txt,214,Two out of four evaluators preferred the compression returned by our system and the other two rated them equal.
E06-1038.txt,215,5 Discussion In this paper we have described a new system for sentence compression.
E06-1038.txt,216,This system uses discriminative large margin learning techniques coupled with a decoding algorithm that searches the space of all compressions.
E06-1038.txt,217,In addition we defined a rich feature set of bigrams in the compression and dropped words and phrases from the original sentence.
E06-1038.txt,218,The model also incorporates soft syntactic evidence in the form of features over dependency and phrase structure trees for each sentence.
E06-1038.txt,219,This system has many advantages over previous approaches.
E06-1038.txt,220,First of all its discriminative nature allows us to use a rich dependent feature set and to optimize a function directly related to compres303 Full Sentence The first new product _comma_ ATF Protype _comma_ is a line of digital postscript typefaces that will be sold in packages of up to six fonts .
E06-1038.txt,221,Human ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .
E06-1038.txt,222,Decision Tree The first new product .
E06-1038.txt,223,This work ATF Protype is a line of digital postscript typefaces will be sold in packages of up to six fonts .
E06-1038.txt,224,Full Sentence Finally _comma_ another advantage of broadband is distance .
E06-1038.txt,225,Human Another advantage is distance .
E06-1038.txt,226,Decision Tree Another advantage of broadband is distance .
E06-1038.txt,227,This work Another advantage is distance .
E06-1038.txt,228,Full Sentence The source code _comma_ which is available for C _comma_ Fortran _comma_ ADA and VHDL _comma_ can be compiled and executed on the same system or ported to other target platforms .
E06-1038.txt,229,Human The source code is available for C _comma_ Fortran _comma_ ADA and VHDL .
E06-1038.txt,230,Decision Tree The source code is available for C .
E06-1038.txt,231,This work The source code can be compiled and executed on the same system or ported to other target platforms .
E06-1038.txt,232,Table 2 Example compressions for the evaluation data. sion accuracy during training_comma_ both of which have been shown to be beneficial for other problems.
E06-1038.txt,233,Furthermore_comma_ the system does not rely on the syntactic parses of the sentences to calculate probability estimates.
E06-1038.txt,234,Instead_comma_ this information is incorporated asjustanother formofevidence tobeconsidered during training.
E06-1038.txt,235,Thisisadvantageous because these parses are trained on out of domain data and often contain a significant amount of noise.
E06-1038.txt,236,A fundamental flaw with all sentence compression systems is that model parameters are set with the assumption that there isasingle correct answer for each sentence.
E06-1038.txt,237,Of course_comma_ like most compression andtranslation tasks_comma_ this isnot true_comma_ consider_comma_ TapeWare _comma_ which supports DOS and NetWare 286 _comma_ is a value added process that lets you directly connect the QA150 EXAT to a file server and issue a command from any workstation to back up the server The human annotated compression is_comma_ TapeWare supports DOS and NetWare 286.
E06-1038.txt,238,However_comma_ another completely valid compression might be_comma_ TapeWare lets you connect the QA150 EXAT to a fi le server.
E06-1038.txt,239,These two compressions overlap by a single word.
E06-1038.txt,240,Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator.
E06-1038.txt,241,A possible direction of research is to investigate multilabel learning techniques for structured data McDonald et al._comma_ 2005a that learn a scoring function separating a set of valid answers from all invalid answers.
E06-1038.txt,242,Thus if a sentence has multiple valid compressions we can learn to score each valid one higher than all invalid compressions during training to avoid this problem.
E06-1038.txt,243,Acknowledgments The author would like to thank Daniel Marcu for providing the data as well as the output of his and Kevin Knight s systems.
E06-1038.txt,244,Thanks also to Hal Daum e and Fernando Pereira for useful discussions.
E06-1038.txt,245,Finally_comma_ the author thanks the four reviewers for evaluating the compressed sentences.
E06-1038.txt,246,This work was supported by NSF ITR grants 0205448 and 0428193. .
E06-1051.txt,1,propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources i the whole sentence where the relation appears_comma_ and ii the local contexts around the interacting entities.
E06-1051.txt,2,We performed experiments on extracting gene and protein interactions from two different data sets.
E06-1051.txt,3,The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.
E06-1051.txt,5,Information Extraction IE is the process of finding relevant entities and their relationships within textual documents.
E06-1051.txt,6,Applications of IE range from Semantic Web to Bioinformatics.
E06-1051.txt,7,For example_comma_ there is an increasing interest in automatically extracting relevant information from biomedical literature.
E06-1051.txt,8,Recent evaluation campaigns on bio entity recognition_comma_ such as BioCreAtIvE and JNLPBA 2004 shared task_comma_ have shown that several systems are able to achieve good performance even if it is a bit worse than that reported on news articles .
E06-1051.txt,9,However_comma_ relation identification is more useful from an applicative perspective but it is still a considerable challenge for automatic tools.
E06-1051.txt,10,In this work_comma_ we propose a supervised machine learning approach to relation extraction which is applicable even when deep linguistic processing is not available or reliable.
E06-1051.txt,11,In particular_comma_ we explore a kernel based approach based solely on shallow linguistic processing_comma_ such as tokenization_comma_ sentence splitting_comma_ Part of Speech PoS tagging and lemmatization.
E06-1051.txt,12,Kernel methods Shawe Taylor and Cristianini_comma_ 2004 show their full potential when an explicit computation of the feature map becomes computationally infeasible_comma_ due to the high or even infinite dimension of the feature space.
E06-1051.txt,13,For this reason_comma_ kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information_comma_ in which the examples preserve their original representations i.e. parse trees and are compared by the kernel function Zelenko et al._comma_ 2003 Culotta and Sorensen_comma_ 2004 Zhao and Grishman_comma_ 2005 .
E06-1051.txt,14,Despite the positive results obtained exploiting syntactic information_comma_ we claim that there is still roomforimprovementrelyingexclusivelyonshallow linguistic information for two main reasons.
E06-1051.txt,15,First of all_comma_ previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information.
E06-1051.txt,16,A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real world sentences.
E06-1051.txt,17,This may prevent approaches based on syntactic features from producing any result.
E06-1051.txt,18,Another related issue concerns the fact that parsers are available only for few languages and may not produce reliable results when used on domain specific texts as is the case of the biomedical literature .
E06-1051.txt,19,For example_comma_ most of the participants at the Learning Language in Logic LLL challenge on Genic Interaction Extraction see Section 4.2 were unable to successfully exploit linguistic information provided by parsers.
E06-1051.txt,20,It is still an open issue whether the use of domainspecific treebanks such as the Genia treebank1 1http www tsujii.is.s.u tokyo.ac.jp 401 can be successfully exploited to overcome this problem.
E06-1051.txt,21,Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features.
E06-1051.txt,22,In our approach we use a combination of kernel functions to represent two distinct information sources the global context where entities appear and their local contexts.
E06-1051.txt,23,The whole sentence where the entities appear global context is used to discover the presence of a relation between two entities_comma_ similarly to what was done by Bunescu and Mooney 2005b .
E06-1051.txt,24,Windows of limited size around the entities local contexts provide useful clues to identify the roles of the entities within a relation.
E06-1051.txt,25,The approach has some resemblance with what was proposed by Roth and Yih 2002 .
E06-1051.txt,26,The main difference is that we perform the extraction task in a single step via a combined kernel_comma_ while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference.
E06-1051.txt,27,We evaluated our relation extraction algorithm on two biomedical data sets i.e. the AImed corpus and the LLL challenge data set see Section 4 .
E06-1051.txt,28,The motivations for using these benchmarks derive from the increasing applicative interest in tools able to extract relations between relevant entities in biomedical texts and_comma_ consequently_comma_ from the growing availability of annotated data sets.
E06-1051.txt,29,The experiments show clearly that our approach consistently improves previous results.
E06-1051.txt,30,Surprisingly_comma_ it outperforms most of the systems based on syntactic or semantic information_comma_ even when this information is manually annotated i.e. the LLL challenge .
E06-1051.txt,31,2 Problem Formalization The problem considered here is that of identifying interactions between genes and proteins from biomedical literature.
E06-1051.txt,32,More specifically_comma_ we performed experiments on two slightly different benchmark data sets see Section 4 for a detailed description .
E06-1051.txt,33,In the former AImed gene protein interactions are annotated without distinguishing the type and roles of the two interacting entities.
E06-1051.txt,34,The latter LLL challenge is more realistic and complex because it also aims at identifying the roles played by the interacting entities agent and target .
E06-1051.txt,35,For example_comma_ in Figure 1 three entities are mentioned and two of the six ordered pairs of GENIA topics Corpus GTB.html entities actually interact sigma K _comma_cwlH and gerE_comma_cwlH .
E06-1051.txt,36,Figure 1 A sentence with two relations_comma_ R12 and R32_comma_ between three entities_comma_ E1_comma_ E2 and E3.
E06-1051.txt,37,In our approach we cast relation extraction as a classificationproblem_comma_ inwhichexamplesaregenerated from sentences as follows.
E06-1051.txt,38,First of all_comma_ we describe the complex case_comma_ namely the protein gene interactions LLL challenge .
E06-1051.txt,39,For this data set entity recognition is performed using a dictionary of protein and gene namesinwhichthetypeoftheentitiesisunknown.
E06-1051.txt,40,We generate examples for all the sentences containing at least two entities.
E06-1051.txt,41,Thus the number of examples generated for each sentence is given by the combinations of distinct entities N selected two at a time_comma_ i.e.
E06-1051.txt,43,For example_comma_ as the sentence shown in Figure 1 contains three entities_comma_ the totalnumberofexamplesgeneratedis 3C2 3.
E06-1051.txt,44,In each example we assign the attribute CANDIDATE to each of the candidate interacting entities_comma_ while the other entities in the example are assigned the attribute OTHER_comma_ meaning that they do not participate in the relation.
E06-1051.txt,45,If a relation holds between the two candidate interacting entities the example is labeled 1 or 2 according to the roles of the interacting entities_comma_ agent and target_comma_ i.e. to the direction of the relation 0 otherwise.
E06-1051.txt,46,Figure 2 shows the examples generated from the sentence in Figure 1.
E06-1051.txt,47,Figure 2 The three protein gene examples generated from the sentence in Figure 1.
E06-1051.txt,48,Note that in generating the examples from the sentence in Figure 1 we did not create three neg402 ative examples there are six potential ordered relations between three entities _comma_ thereby implicitly under sampling the data set.
E06-1051.txt,49,This allows us to make the classification task simpler without loosing information.
E06-1051.txt,50,As a matter of fact_comma_ generating examples for each ordered pair of entities would produce two subsets of the same size containing similar examples differing only for the attributes CANDIDATE and OTHER _comma_ but with different classification labels.
E06-1051.txt,51,Furthermore_comma_ under sampling allows us to halve the data set size and reduce the data skewness.
E06-1051.txt,52,For the protein protein interaction task AImed we use the correct entities provided by the manual annotation.
E06-1051.txt,53,As said at the beginning of this section_comma_ this task is simpler than the LLL challenge because there is no distinction between types all entities are proteins and roles the relation is symmetric .
E06-1051.txt,54,As a consequence_comma_ the examples are generated as described above with the following difference an example is labeled 1 if a relation holds between the two candidate interacting entities 0 otherwise.
E06-1051.txt,55,3 Kernel Methods for Relation Extraction The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function  X F_comma_ and then use a linear algorithm for discovering nonlinear patterns.
E06-1051.txt,56,Instead of using the explicit mapping _comma_ we can use a kernel function K X X R_comma_ that corresponds to the inner product in a feature space whichis_comma_ingeneral_comma_differentfromtheinputspace.
E06-1051.txt,57,Kernel methods allow us to design a modular system_comma_ in which the kernel function acts as an interface between the data and the learning algorithm.
E06-1051.txt,58,Thus the kernel function is the only domain specific module of the system_comma_ while the learning algorithm is a general purpose component.
E06-1051.txt,59,Potentially any kernel function can work with any kernel based algorithm.
E06-1051.txt,60,In our approach we use Support Vector Machines Vapnik_comma_ 1998 .
E06-1051.txt,61,In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels.
E06-1051.txt,62,Different works Gliozzo et al._comma_ 2005 Zhao and Grishman_comma_ 2005 Culotta and Sorensen_comma_ 2004 empirically demonstrate the effectiveness of combining kernels in thisway_comma_ showingthatthecombinedkernelalways improves the performance of the individual ones.
E06-1051.txt,63,In addition_comma_ this formulation allows us to evaluate the individual contribution of each information source.
E06-1051.txt,64,We designed two families of kernels Global Context kernels and Local Context kernels_comma_ in which each single kernel is explicitly calculated as follows K x1_comma_x2  x1 _comma_ x2 bardbl x 1 bardblbardbl x2 bardbl _comma_ 1 where  is the embedding vector and bardbl bardbl is the 2 norm.
E06-1051.txt,65,The kernel is normalized divided by the product of the norms of embedding vectors.
E06-1051.txt,66,The normalization factor plays an important role in allowing us to integrate information from heterogeneous feature spaces.
E06-1051.txt,67,Even though the resulting feature space has high dimensionality_comma_ an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse.
E06-1051.txt,68,3.1 Global Context Kernel In Bunescu and Mooney_comma_ 2005b _comma_ the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns Fore Between tokens before and between the two candidate interacting entities.
E06-1051.txt,69,For instance binding of P1 to P2 _comma_ interaction involving P1 and P2 _comma_ association of P1 by P2 .
E06-1051.txt,70,Between only tokens between the two candidate interacting entities.
E06-1051.txt,71,For instance P1 associates with P2 _comma_ P1 binding to P2 _comma_ P1 _comma_ inhibitor of P2 .
E06-1051.txt,72,Between After tokens between and after the two candidate interacting entities.
E06-1051.txt,73,For instance P1  P2 association_comma_ P1 and P2 interact_comma_ P1 has influence on P2 binding.
E06-1051.txt,74,Our global context kernels operate on the patterns above_comma_ where each pattern is represented using a bag of words instead of sparse subsequences of words_comma_ PoS tags_comma_ entity and chunk types_comma_ or WordNet synsets as in Bunescu and Mooney_comma_ 2005b .
E06-1051.txt,75,More formally_comma_ given a relation example R_comma_ we represent a pattern P as a row vector P R  tf t1_comma_P _comma_tf t2_comma_P _comma_..._comma_tf tl_comma_P  Rl_comma_ 2 where the function tf ti_comma_P records how many times a particular token ti is used in P.
E06-1051.txt,76,Note that_comma_ 403 this approach differs from the standard bag ofwords as punctuation and stop words are included in P_comma_ while the entities with attribute CANDIDATE and OTHER are not.
E06-1051.txt,77,To improve the classification performance_comma_ we have further extended P to embed n grams of contiguous tokens up to n 3 .
E06-1051.txt,78,By substituting P into Equation 1_comma_ we obtain the n gram kernel Kn_comma_ which counts common uni grams_comma_ bi grams_comma_ ..._comma_ n grams that two patterns have in common2.
E06-1051.txt,79,The Global Context kernel KGC R1_comma_R2 is then defined as KFB R1_comma_R2 KB R1_comma_R2 KBA R1_comma_R2 _comma_ 3 where KFB_comma_ KB and KBA are n gram kernels that operate on the Fore Between_comma_ Between and Between After patterns respectively.
E06-1051.txt,80,3.2 Local Context Kernel The type of the candidate interacting entities can provide useful clues for detecting the agent and target of the relation_comma_ as well as the presence of the relation itself.
E06-1051.txt,81,As the type is not known_comma_ we use the information provided by the two local contexts of the candidate interacting entities_comma_ called left and right local context respectively.
E06-1051.txt,82,As typically done in entity recognition_comma_ we represent each local context by using the following basic features Token The token itself.
E06-1051.txt,83,Lemma The lemma of the token.
E06-1051.txt,84,PoS The PoS tag of the token.
E06-1051.txt,85,Orthographic This feature maps each token into equivalence classes that encode attributes such as capitalization_comma_ punctuation_comma_ numerals and so on.
E06-1051.txt,86,Formally_comma_ given a relation example R_comma_ a local context L t w_comma_..._comma_t 1_comma_t0_comma_t 1_comma_..._comma_t w is represented as a row vector L R  f1 L _comma_f2 L _comma_..._comma_fm L  0_comma_1 m_comma_ 4 where fi is a feature function that returns 1 if it is active in the specified position of L_comma_ 0 otherwise3.
E06-1051.txt,87,The Local Context kernel KLC R1_comma_R2 is defined as Kleft R1_comma_R2 Kright R1_comma_R2 _comma_ 5 whereKleft andKright aredefinedbysubstituting the embedding of the left and right local context into Equation 1 respectively.
E06-1051.txt,88,2In the literature_comma_ it is also called n spectrum kernel.
E06-1051.txt,89,3In the reported experiments_comma_ we used a context window of 2 tokens around the candidate entity.
E06-1051.txt,90,Notice that KLC differs substantially from KGC as it considers the ordering of the tokens and the feature space is enriched with PoS_comma_ lemma and orthographic features.
E06-1051.txt,91,3.3 Shallow Linguistic Kernel Finally_comma_ the Shallow Linguistic kernel KSL R1_comma_R2 is defined as KGC R1_comma_R2 KLC R1_comma_R2 .
E06-1051.txt,92,6 It follows directly from the explicit construction of the feature space and from closure properties of kernels that KSL is a valid kernel.
E06-1051.txt,93,4 Data sets Thetwodatasetsusedfortheexperimentsconcern the same domain i.e. gene protein interactions .
E06-1051.txt,94,However_comma_ they present a crucial difference which makes it worthwhile to show the experimental results on both of them.
E06-1051.txt,95,In one case AImed interactions are considered symmetric_comma_ while in the other LLL challenge agents and targets of genic interactions have to be identified.
E06-1051.txt,96,4.1 AImed corpus The first data set used in the experiments is the AImed corpus4_comma_ previously used for training protein interaction extraction systems in Bunescu et al._comma_ 2005 Bunescu and Mooney_comma_ 2005b .
E06-1051.txt,97,It consists of 225 Medline .
E06-1004.txt,1,this paper we study a set of problems that are of considerable importance to Statistical Machine Translation SMT but which have not been addressed satisfactorily by the SMT research community. Over the last decade_comma_ a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT.
E06-1004.txt,2,Our work aimsat providing useful insights into the the computational complexity of those problems.
E06-1004.txt,3,We prove that while IBM Models 1 2 are conceptually and computationally simple_comma_ computations involving the higher and more useful models are hard.
E06-1004.txt,4,Since it is unlikely that there exists a polynomial time solution for any of these hard problems unless P NP and P P P _comma_ our results highlight and justify the need for developing polynomial time approximations for these computations.
E06-1004.txt,5,We also discuss some practical ways of dealing with complexity.
E06-1004.txt,7,Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation Brown et al._comma_ 1993 _comma_ Al Onaizan et al._comma_ 1999 .
E06-1004.txt,8,The parameters of the models are estimated by iterative maximum likelihood training on a large parallel corpus of natural language texts using the EM algorithm Brown et al._comma_ 1993 .
E06-1004.txt,9,The models are then used to decode_comma_ i.e. translate texts from the source language to the target language 1 Tillman_comma_ 2001 _comma_ Wang_comma_ 1997 _comma_ Germann et al._comma_ 2003 _comma_ Udupa et al._comma_ 2004 .
E06-1004.txt,10,The models are independent of the language pair and therefore_comma_ can be used to build a translation system for any language pair as long as a parallel corpus of texts is available for training.
E06-1004.txt,11,Increasingly_comma_ parallel corpora are becoming available for many language pairs and SMT systems have been built for French English_comma_ German English_comma_ Arabic English_comma_ Chinese English_comma_ Hindi English and other language pairs Brown et al._comma_ 1993 _comma_ AlOnaizan et al._comma_ 1999 _comma_ Udupa_comma_ 2004 .
E06-1004.txt,12,In SMT_comma_ every English sentence e is considered as a translation of a given French sentence f with probability Pr f e .
E06-1004.txt,13,Therefore_comma_ the problem of translatingf can beviewed as aproblem of finding the most probable translation of f e argmax e Pr e f argmax e Pr f e P e .
E06-1004.txt,14,1 The probability distributions Pr f e and Pr e are known as translation model and language model respectively.
E06-1004.txt,15,In the classic work on SMT_comma_Brownandhiscolleagues atIBMintroduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models Brown et al._comma_ 1993 .
E06-1004.txt,16,An alignment between f f1 ...fm and e e1 ...el is a many to one mapping a 1_comma_..._comma_m  0_comma_..._comma_l .
E06-1004.txt,17,Thus_comma_ an alignment a between f and e associates the french word fj to the English word eaj 2.
E06-1004.txt,18,The number of words of f mapped to ei by a is called the fertility of ei and is denoted by i.
E06-1004.txt,19,Since Pr f e summationtexta Pr f_comma_a e _comma_ equation 1 can 1In this paper_comma_ we use French and English as the prototypical examples of source and target languages respectively.
E06-1004.txt,20,2e0 is a special word called the null word and is used to account for those words in f that are not connected by a to any of the words of e.
E06-1004.txt,21,25 be rewritten as follows e argmax e summationdisplay a Pr f_comma_a e Pr e .
E06-1004.txt,22,2 Brown and his colleagues developed a series of 5 translation models which have become to be known in the field of machine translation as IBM models.
E06-1004.txt,23,For a detailed .
E06-1007.txt,1,present an implemented machine learning system for the automatic detection of nonreferential it in spoken dialog. The system builds on shallow features extracted from dialog transcripts.
E06-1007.txt,2,Our experiments indicate a level of performance that makes the system usable as a preprocessing filter for a coreference resolution system.
E06-1007.txt,3,We also report results of an annotationstudydealingwiththeclassificationof it by naive subjects.
E06-1007.txt,5,This paper describes an implemented system for the detection of nonreferential it in spoken multiparty dialog.
E06-1007.txt,6,The system has been developed on the basis of meeting transcriptions from the ICSI Meeting Corpus Janin et al._comma_ 2003 _comma_ and it is intended as a preprocessing component for a coreference resolution system in the DIANA Summ dialog summarization project.
E06-1007.txt,7,Consider the following utterance MN059 Yeah.
E06-1007.txt,10,I m sure I could learn a lot about um_comma_ yeah_comma_ just how to how to come up with these structures_comma_ cuz it s it s very easy to whip up something quickly_comma_ but it maybe then makes sense to to me_comma_ but not to anybody else_comma_ and and if we want to share and integrate things_comma_ they must well_comma_ they must be well designed really.
E06-1007.txt,11,Bed017 In this example_comma_ only one of the three instances of it is a referential pronoun The first it appears in the reparandum part of a speech repair Heeman Allen_comma_ 1999 .
E06-1007.txt,12,It is replaced by a subsequent alteration and is thus not part of the final utterance.
E06-1007.txt,13,The second it is the subject of an extraposition construction and serves as the placeholder for the postposed infinitive phrase to whip up something quickly.
E06-1007.txt,14,Only the third it is a referential pronoun which anaphorically refers to something.
E06-1007.txt,15,The task of the system described in the following is to identify and filter out nonreferential instances of it_comma_ like the first and second one in the example.
E06-1007.txt,16,By preventing these instances from triggering the search for an antecedent_comma_ the precision of a coreference resolution system is improved.
E06-1007.txt,17,Up to the present_comma_ coreference resolution has mostly been done on written text.
E06-1007.txt,18,In this domain_comma_ the detection of nonreferential it has by now become a standard preprocessing step e.g. Ng Cardie 2002 .
E06-1007.txt,19,In the few works that exist on coreference resolution in spoken language_comma_ on the other hand_comma_ the problem could be ignored_comma_ because almost none of these aimed at developing a system that could handle unrestricted input.
E06-1007.txt,20,Eckert Strube 2000 focus on an unimplemented algorithm for determining the type of antecedent mostly NP vs. non NP _comma_ given an anaphorical pronoun or demonstrative.
E06-1007.txt,21,The system of Byron 2002 is implemented_comma_ but deals mainly with how referents for already identified discourse deictic anaphors can be created.
E06-1007.txt,22,Finally_comma_ Strube M uller 2003 describe an implemented system for resolving 3rd person pronouns in spoken dialog_comma_ but they also exclude nonreferential it from consideration.
E06-1007.txt,23,In contrast_comma_ the present work is part of a project to develop a coreference resolution system that_comma_ in its final implementation_comma_ can handle unrestricted multi party dialog.
E06-1007.txt,24,In such a system_comma_ no a priori knowledge is available about whether an instance of it is referential or not.
E06-1007.txt,25,The remainder of this paper is structured as follows Section 2 describes the current state of the art for the detection of nonreferential it in written text.
E06-1007.txt,26,Section 3 describes our corpus of transcribed spoken dialog.
E06-1007.txt,27,It also reports on the annotation that we performed in order to collect training and test data for our machine learning experiments.
E06-1007.txt,28,The annotation also offered interesting insights into how reliably humans can identify nonreferential it in spoken language_comma_ a question that_comma_ 49 to our knowledge_comma_ has not been adressed before.
E06-1007.txt,29,Section 4 describes the setup and results of our machine learning experiments_comma_ Section 5 contains conclusion and future work.
E06-1007.txt,30,2 Detecting Nonreferential It In Text Nonreferential it is a rather frequent phenomenon in written text_comma_ though it still only constitutes a minority of all instances of it.
E06-1007.txt,31,Evans 2001 reports that his corpus of approx.
E06-1007.txt,32,370.000 words from the SUSANNE corpus and the BNC contains 3.171 examples of it_comma_ approx.
E06-1007.txt,33,29 of which are nonreferential.
E06-1007.txt,34,Dimitrov et al. 2002 work on the ACE corpus and give the following figures the newspaper part of the corpus ca. 61.000 words contains 381 instances of it_comma_ with 20.7 being nonreferential_comma_ and the news wire part ca. 66.000 words contains 425 instances of it_comma_ 16.5 of which are nonreferential.
E06-1007.txt,35,Boyd et al. 2005 use a 350.000 word corpus from a variety of genres.
E06-1007.txt,36,They count 2.337 instances of it_comma_ 646 of which 28 are nonreferential.
E06-1007.txt,37,Finally_comma_ Clemente et al. 2004 report that in the GENIA corpus of medical .
E06-1037.txt,1,the growing complexity of tasks that spoken dialogue systems are trying to handle_comma_ Reinforcement Learning RL has been increasingly used as a way of automatically learning the best policy for a system to make. While most work has focused on generating better policies for a dialogue manager_comma_ very little work has been done in using RLto construct abetter dialogue state.
E06-1037.txt,2,This paper presents a RL approach for determining what dialogue features are important to a spoken dialogue tutoring system.
E06-1037.txt,3,Our experiments show that incorporating dialogue factors such as dialogue acts_comma_ emotion_comma_ repeated concepts and performance play a significant role in tutoring and should be taken into account when designing dialogue systems.
E06-1037.txt,5,This paper presents initial research toward the long term goal of designing a tutoring system that can effectively adapt to the student.
E06-1037.txt,6,While most work in Markov Decision Processes MDPs and spoken dialogue have focused on building better policies Walker_comma_ 2000 Henderson et al._comma_ 2005 _comma_ to date very little empirical workhas tested the utility of adding specialized features to construct a better dialogue state.
E06-1037.txt,7,We wish to show that adding more complex factors toarepresentation ofstudent state is a worthwhile pursuit_comma_ since it alters what action the tutor should make.
E06-1037.txt,8,The five dialogue factors we explore are dialogue acts_comma_ certainty level_comma_ frustration level_comma_ concept repetition_comma_ and student performance.
E06-1037.txt,9,All five are factors that are not just unique to the tutoring domain but are important to dialogue systems in general.
E06-1037.txt,10,Our results show that using these features_comma_ combined with the common baseline of student correctness_comma_ leads to asignificant change in the policies produced_comma_ and thus should be taken into account when designing a system.
E06-1037.txt,11,2 Background We follow past lines of research such as Singh et al._comma_ 1999 for describing a dialogue a0 as a trajectory within a Markov Decision Process Sutton and Barto_comma_ 1998 .
E06-1037.txt,12,A MDP has four main components states_comma_ actions_comma_ a policy_comma_ which specifies what is the best action to take in a state_comma_ and a reward function which specifies the utility of each state and the process as a whole.
E06-1037.txt,13,Dialogue management is easily described using a MDP because one can consider the actions as actions made by the system_comma_ the state as the dialogue context_comma_ and a reward which for many dialogue systems tends to be task completion success or dialogue length.
E06-1037.txt,14,Typically the state is viewed as a vector of features such as dialogue history_comma_ speech recognition confidence_comma_ etc. Thegoal of using MDPsis to determine the best policy a1 for a certain state and action space.
E06-1037.txt,15,That is_comma_ we wish to find the best combination of states and actions to maximize the reward at the end of the dialogue.
E06-1037.txt,16,In most dialogues_comma_ the exact reward for each state is not known immediately_comma_ in fact_comma_ usually only the final reward is known at the end of the dialogue.
E06-1037.txt,17,As long as wehave a reward function_comma_ Reinforcement Learning allows one to automatically compute the best policy.
E06-1037.txt,18,The following recursive equation gives us a way of calculating the expected cumulative value V value of a state a2 value 289 a0a2a1 a2a4a3a6a5a8a7a10a9a10a11a13a12a13a14a16a15 a9a18a17 a9a18a9a10a11a20a19a21 a14a16a15 a9a18a17 a9a22a9a10a11a24a23a26a25 a0a2a1 a2a28a27a29a3a10a30 Here a1 a1 a2a31a3 is the best action for state a2 at this time_comma_ a12 is the probability of getting from state a2 to a2 a27 via a1 a1 a2a4a3 .
E06-1037.txt,19,This ismultiplied by the sum ofthe reward a21 for that traversal plus the value of the new state multiplied by a discount factor a25 .
E06-1037.txt,20,a25 ranges between 0 and 1 and discounts the value of past states.
E06-1037.txt,21,The policy iteration algorithm Sutton and Barto_comma_ 1998 iteratively updates the value of each state V s based on the values of its neighboring states.
E06-1037.txt,22,Theiteration stops when each update yields an epsilon difference implying that V s has converged and we select the action that produces the highest V value for that state.
E06-1037.txt,23,Normally one would want a dialogue system to interact with users thousands of times to explore the entire traversal space of the MDP_comma_ however in practice that is very time consuming.
E06-1037.txt,24,Instead_comma_ the next best tactic is to train the MDP that is_comma_ calculate transition probabilities for getting from one state to another_comma_ and the reward for each state on already collected data.
E06-1037.txt,25,Of course_comma_ the whole space will not be considered_comma_ but if one reduces the size of the state vector effectively_comma_ data size becomes less of an issue Singh et al._comma_ 2002 .
E06-1037.txt,26,3 Corpus For our study_comma_ we used an annotated corpus of 20 human computer spoken dialogue tutoring sessions.
E06-1037.txt,27,Each session consists of an interaction with one student over 5 different college level physics problems_comma_ for a total of 100 dialogues.
E06-1037.txt,28,Before the 5 problems_comma_ the student is asked to read physics material for 30 minutes and then take a pre test based on that material.
E06-1037.txt,29,Each problem begins with the student writing out a short essay response to the question posed by the computer tutor.
E06-1037.txt,30,The system reads the essay and detects the problem areas and then starts a dialogue with the student asking questions regarding the confused concepts.
E06-1037.txt,31,Informally_comma_ the dialogue follows a question answer format.
E06-1037.txt,32,Each of the dialogues has been manually authored in advance meaning that the system has a response based on the correctness of the student s last answer.
E06-1037.txt,33,Once the student has successfully answered all the questions_comma_ he or she is asked to correct the initial essay.
E06-1037.txt,34,On average_comma_ each of the dialogues takes 20 minutes and contains 25 student turns.
E06-1037.txt,35,Finally_comma_ the student is given a post test similar to the pre test_comma_ from which we can calculate their normalized learning gain a32a34a33a36a35 a5a38a37a40a39 a2a28a41a43a42 a37a45a44a47a46 a48 a42 a37a45a44a47a46 Prior to our study_comma_ the corpus was then annotated for Student and Tutor Moves see Tables 1 and 2 which can be viewed as Dialogue Acts Forbes Riley etal._comma_ 2005 .
E06-1037.txt,36,Notethattutor andstudent turns can consist of multiple utterances and can thus be labeled with multiple moves.
E06-1037.txt,37,For example_comma_ a tutor can give feedback and then ask a question in the same turn.
E06-1037.txt,38,Whether to include feedback will be the action choice addressed in this paper since it is an interesting open questionintheIntelligent TutoringSystems ITS community.
E06-1037.txt,39,Student Moves refer to the type of answer a student gives.
E06-1037.txt,40,Answers that involve a concept already introduced in the dialogue are called Shallow_comma_ answers that involve a novel concept are called Novel_comma_ I don t know type answers are called Assertions As _comma_ and Deep answers refer to answers that involve linking two concepts through reasoning.
E06-1037.txt,41,In our study_comma_ we merge all non Shallow moves into a new move Other. In addition to Student Moves_comma_ we annotated five other features to include in our representation of the student state.
E06-1037.txt,42,Two emotion related features were annotated manually Forbes Riley and Litman_comma_ 2005 certainty and frustration.
E06-1037.txt,43,Certainty describes how confident a student seemed to be in his answer_comma_ while frustration describes how frustrated thestudent seemed tobe inhislast response.
E06-1037.txt,44,We include three other features for the Student state that were extracted automatically.
E06-1037.txt,45,Correctness says if the last student answer was correct or incorrect.
E06-1037.txt,46,As noted above_comma_ this is what most current tutoring systems use as their state.
E06-1037.txt,47,Percent Correct is the percentage of questions in the current problem the student has answered correctly so far.
E06-1037.txt,48,Finally_comma_ if a student performs poorly when it comes to a certain topic_comma_ the system may be forced to repeat a description of that concept again concept repetition .
E06-1037.txt,49,It should be noted that all the dialogues were authored beforehand by physics experts.
E06-1037.txt,50,For every turn there is a list of possible correct_comma_ incorrect and partially correct answers the student can make_comma_ and then for each one of these student responses a link to the next turn.
E06-1037.txt,51,In addition to 290 State Parameters Student Move Shallow S Novel As Deep O Certainty Certain_comma_ Uncertain_comma_ Neutral Frustration Frustrated F _comma_ Neutral N _comma_ Correctness Correct C _comma_ Incorrect I Partially Correct PC Percent Correct 50 100 High _comma_ 0 50 Low Concept Repetition Concept is not repeated 0 _comma_ Concept is repeated R Table 1 Student Features in Tutoring Corpus Action Parameters Tutor Feedback Act Positive_comma_ Negative Tutor Question Act Short Answer Question SAQ Complex Answer Question CAQ Tutor State Act Restatement_comma_ Recap_comma_ Hint Expansion_comma_ Bottom Out Table 2 Tutor Acts in Tutoring Corpus explaining physics concepts_comma_ the authors also include feedback and other types of helpful measures such as hints or restatements to help the student along.
E06-1037.txt,52,These were not written with the goal of how best to influence student state.
E06-1037.txt,53,Our goal in this study is to automatically learn from this corpus which state action patterns evoke the highest learning gain.
E06-1037.txt,54,4 Infrastructure To test different hypotheses of what features best approximate thestudent state andwhatare thebest actions for a tutor to consider_comma_ one must have a flexible system that allows one to easily test different configurations of states and actions.
E06-1037.txt,55,To accomplish this_comma_ we designed a system similar to the Reinforcement Learning for Dialogue Systems RLDS Singh et al._comma_ 1999 .
E06-1037.txt,56,The system allows a system designer to specify what features willcompose the state and actions as well as perform operations on each individual feature.
E06-1037.txt,57,For instance_comma_ the tool allows the user to collapse features together such as collapsing all Question Acts together into one or quantize features that have continuous values such as the number of utterances in the dialogue so far .
E06-1037.txt,58,These collapsing functions allow the user to easily constrain the trajectory space.
E06-1037.txt,59,To further reduce the search space for the MDP_comma_ our tool allows the user to specify a threshold to combine states that occur less than the threshold into a single threshold state. In addition_comma_ the user can specify a reward function and a discount factor_comma_ For this study_comma_ we use a threshold of 50 and a discount factor of 0.9_comma_ which is also what is commonly used in other RL models_comma_ such as Frampton and Lemon_comma_ 2005 .
E06-1037.txt,60,For the dialogue reward function_comma_ we did a median split on the 20 students based on their normalized learning gain_comma_ which is a standard evaluation metric in the Intelligent Tutoring Systems community.
E06-1037.txt,61,So 10 students and their respective 5 dialogues were assigned a positive reward of 100 high learners _comma_ and the other 10 students and their respective dialogues were assigned a negative reward of 100 low learners .
E06-1037.txt,62,It should be noted that a student s 5 dialogues were assigned the same reward since there was no way to approximate their learning gain in the middle of a session.
E06-1037.txt,63,The output of the tool is a probability matrix over the user specified states and actions.
E06-1037.txt,64,This matrix isthen passed toan MDPtoolkit Chades et al._comma_ 2005 written in Matlab.1 The toolkit performs policy iteration and generates a policy as well as a list of V values for each state.
E06-1037.txt,65,5 Experimental Method With the infrastructure created and the MDP parameters set_comma_ we can then move on to the goal of this experiment to see what sources of information impact a tutoring dialogue system.
E06-1037.txt,66,First_comma_ we need to develop a baseline to compare the effects of adding more information.
E06-1037.txt,67,Second_comma_ we generate a new policy by adding the new information source to the baseline state.
E06-1037.txt,68,However_comma_ since we are currently not running any new experiments to testourpolicy_comma_ orevaluating overuser simulations_comma_ we evaluate the reliability of our policies by looking at how well they converge over time_comma_ that is_comma_ if you incrementally add more data ie. a student s 5 dialogues does the policy generated tend to stabilize over time
E06-1037.txt,69,And also_comma_ do the V values for each state stabilize over time as well
E06-1037.txt,70,The intuition is that if both the policies and V values tend to converge then wecan besure that the policy generated is reasonable.
E06-1037.txt,71,The first step in our experiment is to determine a baseline.
E06-1037.txt,72,We use feedback as our system action in our MDP.
E06-1037.txt,73,The action size is 3 tutor can give feedback Feed _comma_ give feedback with another tutor act Mix _comma_ or give no feedback at all NonFeed .
E06-1037.txt,74,Examples from our corpus can be seen in Table 3.
E06-1037.txt,75,It should be noted that NonFeed does not mean that the student s answer is not acknowledged_comma_ it 1MDP toolkit can be downloaded from http www.inra.fr bia T MDPtoolbox 291 Case Tutor Moves Example Turn Feed Pos Super. Mix Pos_comma_ SAQ Good.
E06-1037.txt,76,What is the direction of that force relative to your fist NonFeed Hint_comma_ CAQ To analyze the pumpkin s acceleration we will use Newton s Second Law.
E06-1037.txt,77,What is the definition of the law Table 3 Tutor Action Examples means that something more complex than a simple positive or negative phrase is given such as a Hint or Restatement .
E06-1037.txt,78,Currently_comma_ the system s response toastudent depends only onwhether ornot the student answered the last question correctly_comma_ so we use correctness as the sole feature in our dialogue state.
E06-1037.txt,79,Recall that a student can either be correct_comma_ partially correct_comma_ or incorrect.
E06-1037.txt,80,Since partially correct occurs infrequently compared to the other two_comma_ we reduced the state size to two by combining Incorrect and Partially Correct into one state IPC and keeping correct C .
E06-1037.txt,81,The third column of Table 4 has the resulting learned MDP policy as well as the frequencies of both states in the data.
E06-1037.txt,82,So for both states_comma_ the best action for the tutor to make is to give feedback_comma_ without knowing anything else about the student state.
E06-1037.txt,83,The second step in our experiment is to test whether the policies generated are indeed reliable.
E06-1037.txt,84,Normally_comma_ thebestwaytoverify apolicy isbyconducting experiments and seeing if the new policy leads to a higher reward for the new dialogues.
E06-1037.txt,85,In our context_comma_ this would entail running more subjects with the augmented dialogue manager and checking if the students had a higher learning gain with the new policies.
E06-1037.txt,86,However_comma_ collecting data in this fashion can take months.
E06-1037.txt,87,So_comma_ we take a different tact of checking if the polices and values for each state are indeed converging as we add data to our MDP model.
E06-1037.txt,88,The intuition here is that if both of those parameters were varying between a corpus of 19 students to 20 students_comma_ then we can t assume that our policy is stable_comma_ and hence not reliable.
E06-1037.txt,89,However_comma_ if these parameters converged as more data was added_comma_ this would indicate that the MDP is reliable.
E06-1037.txt,90,To test this out_comma_ we conducted a 20 fold crossaveraging test over our corpus of 20 students.
E06-1037.txt,91,Specifically_comma_ we made 20 random orderings of our students to prevent any one ordering from giving a false convergence.
E06-1037.txt,92,Each ordering was then chunkedinto20cutsranging fromasizeof1student_comma_ to the entire corpus of 20 students.
E06-1037.txt,93,We then passed each cut to our MDP infrastructure such that we started with a corpus of just the first student of the ordering and then determined a MDP policy for that cut_comma_ then added another student to that original corpus and reran our MDP system.
E06-1037.txt,94,We continue thisincremental addition ofastudent 5dialogues until we completed all 20 students.
E06-1037.txt,95,So at the end_comma_ we have 20 random orderings with 20 cuts each_comma_ so 400 MDP trials were run.
E06-1037.txt,96,Finally_comma_ we average the V values of same size cuts together to produce an average V value for that cut size.
E06-1037.txt,97,The left hand graph in Figure 1 shows a plot of the average Vvalues for each state against a cut.
E06-1037.txt,98,The state with the plusses is the positive final state_comma_ and the one at the bottom is the negative final state.
E06-1037.txt,99,However_comma_ we are most concerned with how the non final states converge_comma_ which are the states in the middle.
E06-1037.txt,100,The plot shows that for early cuts_comma_ there is a lot of instability but then each state tends to stabilize after cut 10.
E06-1037.txt,101,So this tells us that the V values are fairly stable and thus reliable when we derive policies from the entire corpus of 20 students.
E06-1037.txt,102,As a further test_comma_ we also check that the policies generated for each cut tend to stabilize over time.
E06-1037.txt,103,That is_comma_ the differences between a policy at a smaller cut and the final cut converge to zero as more data is added.
E06-1037.txt,104,This diffs test is discussed in more detail in Section 6.
E06-1037.txt,105,6 Results In this section_comma_ we investigate whether adding more information to our student state will lead to interesting policy changes.
E06-1037.txt,106,First_comma_ we add certainty to our baseline of correctness_comma_ and then compare this new baseline s policy henceforth Baseline 2 with the policies generated when student moves_comma_ frustration_comma_ concept repetition_comma_ and percent correctness are included.
E06-1037.txt,107,For each test_comma_ we employed the same methodology as with the baseline case of doing a 20 fold cross averaging and examining if the states V values converge.
E06-1037.txt,108,We first add certainty to correctness because prior work such as Bhatt et al._comma_ 2004 has shown the importance of considering certainty in tutoring 292 0 2 4 6 8 10 12 14 16 18 20 100 80 60 40 20 0 20 40 60 80 100 of students V value Correctness 0 2 4 6 8 10 12 14 16 18 20 100 80 60 40 20 0 20 40 60 80 100 of students V value Correctness Certainty Figure 1 Baseline 1 and Baseline 2 Convergence Plots systems.
E06-1037.txt,109,For example_comma_ a student who is correct and certain probably does not need a lot of feedback.
E06-1037.txt,110,But one that is correct but uncertain could signal that the student is becoming doubtful or at least confused about a concept.
E06-1037.txt,111,There are three types of certainty certain cer _comma_ uncertain unc _comma_ and neutral neu .
E06-1037.txt,112,Adding these to our state representation increases state size from 2 to 6.
E06-1037.txt,113,The new policy is shown in Table 4.
E06-1037.txt,114,The second and third columns show theoriginal baseline states and their policies.
E06-1037.txt,115,The next column shows the new policy when splitting the original state into the new three states based on certainty_comma_ as well as the frequency of the new state.
E06-1037.txt,116,Sothe firstrow can be interpreted as if the student is correct and certain_comma_ one should give no feedback if the student is correct and neutral_comma_ givefeedback andifthestudent iscorrect and uncertain_comma_ give non feedback. State Baseline Certainty 1 C Feed 1308 cer NonFeed 663 neu Feed 480 unc NonFeed 165 2 IPC Feed 872 cer NonFeed 251 neu Mix 377 unc NonFeed 244 Table 4 Baseline Policies Our reasoning is that if a feature is important to include in a state representation it should change the policies of the old states.
E06-1037.txt,117,For example_comma_ if certainty did not impact how well students learned as deemed by the MDP then the policies for certainty_comma_ uncertainty_comma_ and neutral would be the same as the original policy for Correct or Incorrect Partially Correct_comma_ in this case they would be Feed.
E06-1037.txt,118,However_comma_ the figures show otherwise as when you add certainty to the state_comma_ only one new state C while being neutral retains the old policy of having the tutor give feedback.
E06-1037.txt,119,The policies which differ with the original are shown in bold.
E06-1037.txt,120,So in general_comma_ the learned policy is that one should not give feedback if the student is certain or uncertain_comma_ but rather give some other form of feedback such as a Hint or a Restatement perhaps.
E06-1037.txt,121,Butwhen the student is neutral with respect to certainty_comma_ one should give feedback.
E06-1037.txt,122,One way of interpreting these results is that given our domain_comma_ for students who are confident or not confident at all in their last answer_comma_ there are better things to say to improve their learning down the road than Great Job But if the student does not display a lot of emotion_comma_ than one should use explicit positive or negative feedback to perhaps bolster their confidence level.
E06-1037.txt,123,The right hand graph in Figure 1 shows the convergence plot for the baseline state with certainty.
E06-1037.txt,124,It shows that as we add more data_comma_ the values for each state converge.
E06-1037.txt,125,So in general_comma_ we can say that the values for our Baseline 2 case are fairly stable.
E06-1037.txt,126,Next_comma_ we add Student Moves_comma_ Frustration_comma_ Concept Repetition_comma_ and Percent Correct features individually to Baseline 2.
E06-1037.txt,127,The first graph in Figure 2 shows a plot of the convergence values for the Percent Correct feature.
E06-1037.txt,128,We only show one convergence plot since the other three are similar.
E06-1037.txt,129,The result is that the V values for all four converge after 14 15 students.
E06-1037.txt,130,The second graph shows the differences in policies between the final cut of 20 students and all smaller cuts.
E06-1037.txt,131,This check is necessary because some states may exhibit stable V values but actually be oscillating between two different policies of equal values.
E06-1037.txt,132,So each point on the graph tells us how many differences in policies there are between the cut in question and the final cut.
E06-1037.txt,133,For 293 example_comma_ if the policy generated at cut 15 was to give feedback for all states_comma_ and the policy at the final cut was to give feedback for all but two states_comma_ the diff for cut 15 would be two.
E06-1037.txt,134,So in the best case_comma_ zero differences mean that the policies generated for both cuts are exactly the same.
E06-1037.txt,135,The diff plots shows the differences decrease as data is added and they exhibited very similar plots to both Baseline cases.
E06-1037.txt,136,Forcuts greater than 15_comma_ there are still some differences but these are usually due to low frequency states.
E06-1037.txt,137,So we can conclude that since our policies are fairly stable they are worth investigating in more detail.
E06-1037.txt,138,In the remainder of this section_comma_ we look at the differences between the Baseline 2 policies and the policies generated by adding a new feature to the Baseline 2 state.
E06-1037.txt,139,If adding a new feature actually does not really change what the tutor should do that is_comma_ the tutor will do the baseline policy regardless of the new information _comma_ one can conclude that the feature is not worth including in a student state.
E06-1037.txt,140,Onthe other hand_comma_ ifadding the state results in a much different policy_comma_ then the feature is important to student modeling.
E06-1037.txt,141,Student Move Feature The results of adding Student Moves to Baseline 2 are shown in Table 5.
E06-1037.txt,142,Out of the 12 new states created_comma_ 7 deviate from the original policy.
E06-1037.txt,143,The main trend is for the neutral and uncertain states to give mixed feedback after a student shallow move_comma_ and a non feed response when the student says something deep or novel.
E06-1037.txt,144,When the student is certain_comma_ always give a mixed response except in the case where he said something Shallow and Correct. State Baseline New Policy 1 certain C NonFeed S NonFeed O Mix 2 certain IPC NonFeed S Mix O Mix 3 neutral C Feed S Feed O NonFeed 4 neutral IPC Mix S Mix O NonFeed 5 uncertain C NonFeed S Mix O NonFeed 6 uncertain IPC NonFeed S Mix O NonFeed Table 5 Student Move Policies Concept Repetition Feature Table 6 shows the new policy generated.
E06-1037.txt,145,Unlike the Student Move policies which impacted all 6 of the baseline states_comma_ Concept Repetition changes the policies for the first three baseline states resulting in 4 out of 12 new states differing from the baseline.
E06-1037.txt,146,For states 1 through 4_comma_ the trend is that if the concept has been repeated_comma_ the tutor should give feedback or a combination of feedback with another Tutor Act.
E06-1037.txt,147,Intuitively_comma_ this seems clear because if a concept were repeated it shows the student is not understanding the concept completely and it is neccessary to give them a little more feedback than when they first see the concept.
E06-1037.txt,148,So_comma_ this test indicates that keeping track of repeated concepts has a significant impact on the policy generated. State Baseline New Policy 1 certain C NonFeed 0 NonFeed R Feed 2 certain IPC NonFeed 0 Mix R Mix 3 neutral C Feed 0 Mix R Feed 4 neutral IPC Mix 0 Mix R Mix 5 uncertain C NonFeed 0 NonFeed R NonFeed 6 uncertain IPC NonFeed 0 NonFeed R NonFeed Table 6 Concept Repetition Policies Frustration Feature Table 7 shows the new policy generated.
E06-1037.txt,149,Comparing the baseline policy with the new policy which includes categories for when the original state is either neutral or frustration _comma_ shows that adding frustration changes the policy for state 1_comma_ when the student is certain or correct.
E06-1037.txt,150,In that case_comma_ the better option is to give them positive feedback.
E06-1037.txt,151,For all other states_comma_ frustration occurs with each of them so infrequently 2 that the resulting states appeared less than the our threshold of 50 instances.
E06-1037.txt,152,As a result_comma_ these 5 frustration states are grouped together in the threshold state and our MDP found that the best policy when in that state is to give no feedback.
E06-1037.txt,153,So the two neutral states change when the student is frustrated.
E06-1037.txt,154,Interestingly_comma_ for students that are uncertain_comma_ the policy does not change if they are frustrated or neutral.
E06-1037.txt,155,The trend is to always give NonFeedback.
E06-1037.txt,156,Percent Correctness Feature Table 8 shows the new policy generated for incorporating a simple model of current student performance within the dialog.
E06-1037.txt,157,This feature_comma_ along with Frustration_comma_ seems to impact the baseline the state least since both only alter the policies for 3 of the 12 new 2Only 225 out of 2180 student turns are marked as frustration_comma_ while all the others are neutral 294 0 2 4 6 8 10 12 14 16 18 20 100 80 60 40 20 0 20 40 60 80 100 of students V value Percent Correctness Convergence 0 2 4 6 8 10 12 14 16 18 200 2 4 6 8 10 12 14 Diffs for All 4 Features Smoves Concept Percent Correct Emotion Figure 2 Percent Correct Convergence_comma_ and Diff Plots for all 4 Features State Baseline New Policy 1 certain C NonFeed N NonFeed F Feed 2 certain IPC NonFeed N NonFeed F NonFeed 3 neutral C Feed N Feed F NonFeed 4 neutral IPC Mix N Mix F NonFeed 5 uncertain C NonFeed N NonFeed F NonFeed 6 uncertain IPC NonFeed N NonFeed F NonFeed Table 7 Frustration Policies states.
E06-1037.txt,158,States 3_comma_ 4_comma_ and 5 show a change in policy for different parameters of correctness.
E06-1037.txt,159,One trend seems to be that when a student has not been performing well L _comma_ to give a NonFeedback response such as a hint or restatement. State Baseline New Policy 1 certain C NonFeed H NonFeed L NonFeed 2 certain IPC NonFeed H NonFeed L NonFeed 3 neutral C Feed H Feed L NonFeed 4 neutral IPC Mix H Mix L NonFeed 5 uncertain C NonFeed H Mix L NonFeed 6 uncertain IPC NonFeed H NonFeed L NonFeed Table 8 Correctness Policies 7 Related Work RL has been applied to improve dialogue systems in past work but very few approaches have looked at which features are important to include in the dialogue state.
E06-1037.txt,160,Paek and Chickering_comma_ 2005 showed how the state space can be learned from data along with the policy.
E06-1037.txt,161,One result is that a state space can be constrained by only using features that are relevant to receiving a reward.
E06-1037.txt,162,Singh et al. 1999 found an optimal dialogue length in their domain_comma_ and showed that the number of information and distress attributes impact the state.
E06-1037.txt,163,They take a different approach than the work here in that they compare which feature values are optimal for different points in the dialogue.
E06-1037.txt,164,Frampton et al. 2005 is similar to ours in that they experiment on including another dialogue feature into their baseline system the user s last dialogue act_comma_ which was found to produce a 52 increase in average reward.
E06-1037.txt,165,Williams et al. 2003 used Supervised Learning to select good state and action features as an initial policy to bootstrap a RL based dialoge system.
E06-1037.txt,166,They found that their automatically created state and action seeds outperformed hand crafted polices inadriving directions corpus.
E06-1037.txt,167,In addition_comma_ there has been extensive work on creating new corpora via user simulations such as Georgila et al._comma_ 2005 to get around the possible issue of not having enough data to train on.
E06-1037.txt,168,Our results here indicate that a small training corpus is actually acceptable to use in a MDP framework as long as the state and action features are pruned effectively.
E06-1037.txt,169,The use of features such as context and student moves is nothing new to the ITS community however_comma_ such as the BEETLE system Zinn et al._comma_ 2005 _comma_ but very little work has been done using RL in developing tutoring systems.
E06-1037.txt,170,8 Discussion In this paper we showed that incorporating more information into a representation of the student state has an impact on what actions the tutor should take.
E06-1037.txt,171,We first showed that despite not be295 ing able to test on real users or simulated users just yet_comma_ that our generated policies were indeed reliable since they converged in terms of the V values of each state and the policy for each state.
E06-1037.txt,172,Next_comma_ we showed that all five features investigated in this study were indeed important to include when constructing an estimation of the student state.
E06-1037.txt,173,Student Moves_comma_ Certainty and Concept Repetition were the most compelling since adding them to their respective baseline states resulted in major policy changes.
E06-1037.txt,174,Tracking the student s frustration levels and how correct the student had been in the dialogue had the least impact on policies.
E06-1037.txt,175,While these features and their resulting policies may appear unique to tutoring systems they also generalize to dialogue systems as a whole.
E06-1037.txt,176,Repeating a concept whether it be a physics term or travel information is important because it is an implicit signal that there might be some confusion and a different action is needed when the concept is repeated.
E06-1037.txt,177,Whether a student or user gives a short answer or a good explanation can indicate to the system how well the user is understanding system questions.
E06-1037.txt,178,Emotion detection and adaptation is a key issue for any spoken dialogue systems as designers try to make the system as easy to use for a student or trip planner_comma_ etc. Frustration can come from difficulty in questions or in the more frequent problem for any dialogue system_comma_ speech recognition errors_comma_ so the manner in dealing with it will always be important.
E06-1037.txt,179,Percent Correctness can be viewed as a specific instance of tracking user performance such as if they are continously answering questions properly or are confused by what the system wants from them.
E06-1037.txt,180,In terms of future work_comma_ we are currently annotating more human computer dialogue data and will triple the size of our test corpus allowing us to 1. create more complicated states since more states will have been explored and 2. test out more complex tutor actions such as when to give Hints and Restatements.
E06-1037.txt,181,Finally_comma_ wearein theprocess of running this same experiment on a corpus of human human tutoring dialogues to compare if human tutors have different policies.
E06-1037.txt,182,9 Acknowledgments We would like to thank the ITSPOKE group and the three anonymous reviewers for their insight and comments.
E06-1037.txt,183,Support for the research reported in this paper was provided by NSF grants 0325054 and 0328431. .
E06-1034.txt,1,with the problem of annotation errors in part of speech POS annotated corpora_comma_ we develop a method for automatically correcting such errors. Building on top of a successful error detection method_comma_ wefirsttry correcting acorpus using two off the shelf POS taggers_comma_ based on the idea that they enforce consistency with this_comma_ we find some improvement.
E06-1034.txt,2,After some discussion of the tagging process_comma_ we alter the tagging model to better account for problematic tagging distinctions.
E06-1034.txt,3,This modification results in significantly improved performance_comma_ reducing the error rate of the corpus.
E06-1034.txt,5,Annotated corpora serve as training material and as gold standard testing material for the development of tools in computational linguistics_comma_ and as a source of data for theoretical linguists searching for relevant language patterns.
E06-1034.txt,6,However_comma_ they contain annotation errors_comma_ and such errors provide unreliable training andevaluation data_comma_ ashasbeen previously shown see ch. 1 of Dickinson 2005 and .
E06-1033.txt,1,present an adaptive technique that enables users to produce a high quality dictionary parsed into its lexicographic components headwords_comma_ pronunciations_comma_ parts of speech_comma_ translations_comma_ etc. using an extremely small amount of user provided training data. We use transformationbased learning TBL as a postprocessor at two points in our system to improve performance.
E06-1033.txt,2,The results using two dictionaries show that the tagging accuracy increases from 83 and 91 to 93 and 94 for individual words or tokens _comma_ and from 64 and 83 to 90 and 93 for contiguous phrases such as definitions or examples of usage.
E06-1033.txt,4,The availability and use of electronic resources such as electronic dictionaries has increased tremendously in recent years and their use in Natural Language Processing NLP systems is widespread.
E06-1033.txt,5,For languages with limited electronic resources_comma_ i.e. low density languages_comma_ however_comma_ we cannot use automated techniques based on parallel corpora Gale and Church_comma_ 1991 Melamed_comma_ 2000 Resnik_comma_ 1999 Utsuro et al._comma_ 2002 _comma_ comparable corpora Fung and Yee_comma_ 1998 _comma_ or multilingual thesauri Vossen_comma_ 1998 .
E06-1033.txt,6,Yet for these lowdensity languages_comma_ printed bilingual dictionaries oftenoffereffectivemappingfromthelow density language to a high density language_comma_ such as English.
E06-1033.txt,7,Dictionaries can have different formats and can provide a variety of information.
E06-1033.txt,8,However_comma_ they typically have a consistent layout of entries and a 1 Headword 5 Translation 2 POS 6 Example of usage 3 Sense number 7 Example of usage translation 4 Synonym 8 Subcategorization Figure 1 Sample tagged dictionary entries.
E06-1033.txt,9,Eight tags are identified and tagged in the given entries. consistent structure within entries.
E06-1033.txt,10,Publishers of dictionaries often use a combination of features to impose this structure including 1 changes in font style_comma_ font size_comma_ etc. that make implicit the lexicographic information1_comma_ such as headwords_comma_ pronunciations_comma_ parts of speech POS _comma_ and translations_comma_ 2 keywords that provide an explicit interpretation of the lexicographic information_comma_ and 3 various separators that impose an overall structure on the entry.
E06-1033.txt,11,For example_comma_ a boldface font may indicate a headword_comma_ italics may indicate an example of usage_comma_ keywords may designate the POS_comma_ commas may separate different translations_comma_ and a numbering system may identify different senses of a word.
E06-1033.txt,12,We developed an entry tagging system that recognizes_comma_ parses_comma_ and tags the entries of a printed dictionary to reproduce the representation electronically Karagol Ayan et al._comma_ 2003 .
E06-1033.txt,13,The system aims to use features as described above and the consistent layout and structure of the dictio1For the purposes of this paper_comma_ we will refer to the lexicographic information as tag when necessary.
E06-1033.txt,14,257 naries to capture and recover the lexicographic information in the entries.
E06-1033.txt,15,Each token2 or group of tokens phrase 3 in an entry associates with a tag indicating its lexicographic information in the entry.
E06-1033.txt,16,Figure1showssampletaggedentriesinwhich eight different types of lexicographic information are identified and marked.
E06-1033.txt,17,The system gets format and style information from a document image analyzer module Ma and Doermann_comma_ 2003 and is retargeted at many levels with minimal human assistance.
E06-1033.txt,18,A major requirement for a human aided dictionary tagging application is the need to minimize human generated training data.4 This requirement limits the effectiveness of data driven methods for initial training.
E06-1033.txt,19,We chose rule based tagging that uses the structure to analyze and tag tokens as our baseline_comma_ because it outperformed the baseline results of an HMM tagger.
E06-1033.txt,20,The approachhasdemonstratedpromisingresults_comma_ butwe willshowitsshortcomingscanbeimprovedbyapplyingatransformation basedlearning TBL post processing technique.
E06-1033.txt,21,TBL Brill_comma_1995 isarule basedmachinelearning method with some attractive qualities that make it suitable for language related tasks.
E06-1033.txt,22,First_comma_ the resulting rules are easily reviewed and understood.
E06-1033.txt,23,Second_comma_ itiserror driven_comma_ thusdirectlyminimizes the error rate Florian and Ngai_comma_ 2001 .
E06-1033.txt,24,Furthermore_comma_ TBL can be applied to other annotation systems output to improve performance.
E06-1033.txt,25,Finally_comma_ it makes use of the features of the token and those in the neighborhood surrounding it.
E06-1033.txt,26,In this paper_comma_ we describe an adaptive TBL basedtechniquetoimprovetheperformanceofthe rule based entry tagger_comma_ especially targeting certain shortcomings.
E06-1033.txt,27,We first investigate how using TBL to improve the accurate rendering of tokens font style affects the rule based tagging accuracy.
E06-1033.txt,28,We then apply TBL on tags of the tokens.
E06-1033.txt,29,In our experiments with two dictionaries_comma_ the range of font style accuracies is increased from 84 94 to 97 98 _comma_ and the range of tagging accuracies is increased from 83 90 to 93 94 for tokens_comma_andfrom64 83 to90 93 forphrases.
E06-1033.txt,30,Section 2 discusses the rule based entry tagging 2Token is a set of glyphs i.e._comma_ a visual representation of a set of characters in the OCRed output.
E06-1033.txt,31,Each punctuation is counted as a token as well.
E06-1033.txt,32,3InFigure1_comma_ not on timeisaphraseconsistingof3tokens.
E06-1033.txt,33,4For our experiments we required hand tagging of no more than eight pages that took around three hours of human effort. method.
E06-1033.txt,34,In Section 3_comma_ we briefly describe TBL_comma_ and Section 4 recounts how we apply TBL to improve the performance of the rule based method.
E06-1033.txt,35,Section5explainstheexperimentsandresults_comma_and we conclude with future work.
E06-1033.txt,36,2 A Rule based Dictionary Entry Tagger The rule based entry tagger Karagol Ayan et al._comma_ 2003 utilizes the repeating structure of the dictionaries to identify and tag the linguistic role of tokens or sets of tokens.
E06-1033.txt,37,Rule based tagging usesthreedifferenttypesofclues fontstyle_comma_keywords and separators to tag the entries in a systematic way.
E06-1033.txt,38,The method accommodates noise introduced by the document analyzer by allowing for a relaxed matching of OCRed output to tags.
E06-1033.txt,39,For each dictionary_comma_ a human operator must specify the lexicographic information used in that particular dictionary_comma_ along with the clues for each tag.
E06-1033.txt,40,This process can be performed in a few hours.
E06-1033.txt,41,Therule basedmethodaloneachievedtokenaccuracy between 73 87 and phrase accuracy between 75 89 in experiments conducted using three different dictionaries5.
E06-1033.txt,42,The rule based method has demonstrated promising results_comma_ but has two shortcomings.
E06-1033.txt,43,First_comma_ the method does not consider the relations between different tags in the entries.
E06-1033.txt,44,While not a problem for some dictionaries_comma_ for others ordering the relations between tags may be the only information that will tag a token correctly.
E06-1033.txt,45,Consider the dictionary entries in Figure 1.
E06-1033.txt,46,In this dictionary_comma_ the word a represents POS when in italic font_comma_ and part of a translation if in normal font.
E06-1033.txt,47,However if the font is incorrect font errors are more likely to happen with short tokens _comma_ the only way to mark correctly the tag involves checking the neighboring tokens and tags to determine its relative position within the entry.
E06-1033.txt,48,When the token has an incorrect font or OCR errors exist_comma_ and the other clues are ambiguous or inconclusive_comma_ the rule based method may yield incorrect results.
E06-1033.txt,49,Second_comma_ the rule based method can produce incorrect splitting and or merging of phrases.
E06-1033.txt,50,An erroneous merge of two tokens as a phrase may take place either because of a font error in one of the tokens or the lack of a separator_comma_ such as a punctuation mark.
E06-1033.txt,51,A phrase may split erroneously either 5Using HMMs for entry tagging on the same set of dictionaries produced slightly lower performance_comma_ resulting in token accuracy between 73 88 and phrase accuracy between 57 85 .
E06-1033.txt,52,258 as a result of a font error or an ambiguous separator.
E06-1033.txt,53,For instance_comma_ a comma may be used after an example of usage to separate it from its translation or within it as a normal punctuation mark.
E06-1033.txt,54,3 TBL TBL Brill_comma_ 1995 _comma_ a rule based machine learning algorithm_comma_ has been applied to various NLP tasks.
E06-1033.txt,55,TBL starts with an initial state_comma_ and it requires a correctly annotated training corpus_comma_ or truth_comma_ for the learning or training process.
E06-1033.txt,56,The iterative learning process acquires an ordered list of rules or transformations that correct the errors in this initial state.
E06-1033.txt,57,At each iteration_comma_ the transformation which achieved the largest benefit during application is selected.
E06-1033.txt,58,During the learning process_comma_ the templates of allowable transformations limit the search space for possible transformation rules.
E06-1033.txt,59,The proposed transformations are formed by instantiation of the transformation templates in the context of erroneous tags.
E06-1033.txt,60,The learning algorithm stops when no improvement can be made to the current state of the training data or when a prespecified threshold is reached.
E06-1033.txt,61,A transformation modifies a tag when its context such as neighboring tags or tokens matches the context described by the transformation.
E06-1033.txt,62,Two parts comprise a transformation a rewrite rule what to replace and a triggering environment when to replace.
E06-1033.txt,63,A typical rewrite rule is Change the annotation from aa to ab_comma_ and a typical triggering environment is The preceding word is wa.
E06-1033.txt,64,The system s output is the final state of this data after applying all transformations in the order they are produced.
E06-1033.txt,65,To overcome the lengthy training time associated with this approach_comma_ we used fnTBL_comma_ a fast version of TBL that preserves the performance of the algorithm Ngai and Florian_comma_ 2001 .
E06-1033.txt,66,Our research contribution shows this method is effective when applied to a miniscule set of training data.
E06-1033.txt,67,4 Application of TBL to Entry Tagging In this section_comma_ we describe how we used TBL in the context of tagging dictionary entries.
E06-1033.txt,68,We apply TBL at two points to render correctly the font style of the tokens and to label correctly thetagsofthetokens6.
E06-1033.txt,69,Althoughourultimategoal 6In reality_comma_ TBL improves the accuracy of tags and phrase boundary flags.
E06-1033.txt,70,In this paper_comma_ whenever we say application of TBL to tagging _comma_ we mean tags and phrase boundary flags a0a2a1a4a3a6a5a8a7a10a9a12a11 a13a15a14a8a16a18a17a20a19a22a21a24a23 a9a26a25 a13a15a14a8a16a18a17a20a19a22a21a24a23 a9a26a25 a13a15a14a8a16a18a17 a9a28a27a30a29a24a25 a13a15a14a8a16a18a17 a9a28a27a30a29a24a25 a13a15a14a8a16a18a17 a9a28a27a30a29a24a25 a31 a13a32a17a20a19a22a21a24a23 a9a26a25 a31 a13a32a17a20a19a22a21a24a23 a9a26a25 a0a33a1a4a3a6a5a34a7a35a9a37a36 a0a33a1a4a3a6a5a34a7a35a9a39a38 a0a33a1a4a3a6a5a34a7a35a9a39a40 a0a33a1a4a3a6a5a34a7a35a9a39a41 a0a33a1a4a3a6a5a34a7a35a9a39a42 Figure 2 Phases of TBL application isimprovingtaggingresults_comma_ fontstyleplaysacrucial role in identifying tags.
E06-1033.txt,71,The rule based entry taggerreliesonfontstyle_comma_ whichcanbealsoincorrect.
E06-1033.txt,72,Therefore we also investigate whether improving font style accuracy will further improve tagging results.
E06-1033.txt,73,We apply TBL in three configurations 1 to improve font style_comma_ 2 to improve tagging and 3 to improve both_comma_ one after another.
E06-1033.txt,74,Figure 2 shows the phases of TBL application.
E06-1033.txt,75,First we have the rule based entry tagging results with the font style assigned by document image analysis Result1 _comma_ then we apply TBL to tagging using this result Result2 .
E06-1033.txt,76,We also apply TBL to improve the font style accuracy_comma_ and we feed these changed font styles to the rule based method Result3 .
E06-1033.txt,77,We then apply TBL to tagging using this result Result4 .
E06-1033.txt,78,Finally_comma_ in order to find the upper bound when we use the manually corrected font stylesinthegroundtruthdata_comma_ wefeedcorrectfont stylestotherule basedmethod Result5 _comma_ andthen apply TBL to tagging using this result Result6 .
E06-1033.txt,79,In the transformation templates_comma_ we use the tokens themselves as features_comma_ i.e. the items in the triggering environment_comma_ because the token s content is useful in indicating the role.
E06-1033.txt,80,For instance a comma and a period may have different functionalities when tagging the dictionary.
E06-1033.txt,81,However_comma_ when transformations are allowed to make reference to tokens_comma_ i.e._comma_ when lexicalized transformations are allowed_comma_ some relevant information may be lost because of sparsity.
E06-1033.txt,82,To overcome the data sparseness problem_comma_ we also assign a type to each token that classifies the token s content.
E06-1033.txt,83,We use eight types punctuation_comma_ symbol_comma_ numeric_comma_ uppercase_comma_ capitalized_comma_ lowercase_comma_ non Latin_comma_ and other.
E06-1033.txt,84,For TBL on font style_comma_ the transformation templatescontainthreefeatures thetoken_comma_ thetoken s type_comma_ andthetoken sfont.
E06-1033.txt,85,ForTBLontagging_comma_ we together.
E06-1033.txt,86,259 use four features the token_comma_ the token s type_comma_ the token s font style_comma_ and the token s tag.
E06-1033.txt,87,The initial state annotations for font style are assigned by document image analysis.
E06-1033.txt,88,The rulebasedentrytaggingmethodassignstheinitialstate ofthetokens tags.
E06-1033.txt,89,Thetemplatesforfontstyleaccuracy improvement consist of those from studying the data and all templates using all features within a window of five tokens i.e._comma_ two preceding tokens_comma_ the current token_comma_ and two following tokens .
E06-1033.txt,90,For tagging accuracy improvement_comma_ we prepared the transformation templates by studying dictionaries and errors in the entry tagging results.
E06-1033.txt,91,The objective function for evaluating transformations in both cases is the classification accuracy_comma_ and the objective is to minimize the number of errors.
E06-1033.txt,92,5 Experiments We performed our experiments on a CebuanoEnglish dictionary Wolff_comma_ 1972 consisting of 1163 pages_comma_ 4 font styles_comma_ and 18 tags_comma_ and on anIraqiArabic Englishdictionary Woodheadand Beene_comma_ 2003 consisting of 507 pages_comma_ 3 font styles_comma_ and 26 tags.
E06-1033.txt,93,For our experiments_comma_ we used a publicly available implementation of TBL s fast version_comma_ fnTBL7_comma_ described in Section 3.
E06-1033.txt,94,Weusedeightrandomlyselectedpagesfromthe dictionaries to train TBL_comma_ and six additional randomly selected pages for testing.
E06-1033.txt,95,The font style and tag of each token on these pages are manually corrected from an initial run.
E06-1033.txt,96,Our goal is to measure the effect of TBL on font style and tagging that have the same noisy input.
E06-1033.txt,97,For the Cebuano dictionary_comma_ the training data contains 156 entries_comma_ 8370 tokens_comma_ and 6691 non punctuation tokens_comma_ and the test data contains 137 entries_comma_ 6251 tokens_comma_ and 4940 non punctuation tokens.
E06-1033.txt,98,For the Iraqi Arabic dictionary_comma_ the training data contains 232 entries_comma_ 6130 tokens_comma_ and 4621 non punctuation tokens_comma_ and the testdata contains 175 entries_comma_ 4708 tokens_comma_ 3467 non punctuation tokens.
E06-1033.txt,99,For evaluation_comma_ we used the percentage of accuracy for non punctuation tokens_comma_ i.e._comma_ the number of correctly identified tags divided by total number of tokens phrases.
E06-1033.txt,100,The learning phase of TBL took less than one minute for each run_comma_ and application of learned transformations to the whole dictionary less than two minutes.
E06-1033.txt,101,We report how TBL affects accuracy of tagging 7http nlp.cs.jhu.edu rflorian fntbl when applied to font styles_comma_ tags_comma_ and font styles and tags together.
E06-1033.txt,102,To find the upper bound tagging results with correct font styles_comma_ we also ran rule based entry tagger using manually corrected font styles_comma_ and applied TBL for tagging accuracy improvement to these results.
E06-1033.txt,103,We should note that feedingthecorrectfonttotherule basedentrytagger does not necessarily mean the data is totally correct_comma_ it may still contain noise from document image analysis or ambiguity in the entry.
E06-1033.txt,104,We conducted three sets of experiments to observe the effects of TBL Section 5.1 _comma_ the effects of different training data Section 5.2 _comma_ and the effects of training data size Section 5.3 .
E06-1033.txt,105,5.1 TBL on Font Styles and Tags Cebuano Iraqi Arabic Original 84.43 94.15 TBL font 97.07 98.13 Table 1 Font style accuracy results for nonpunctuation tokens We report the accuracy of font styles on the test data before and after applying TBL to the font styleofthenon punctuationtokensinTable1.
E06-1033.txt,106,The initial font style accuracy of Cebuano dictionary was much less than the Iraqi Arabic dictionary_comma_ but applying TBL resulted in similar font style accuracy for both dictionaries 97 and 98 .
E06-1033.txt,107,Cebuano Iraqi Arabic Token Phrase Token Phrase RB 83.25 64.08 90.89 82.72 RB TBL tag 91.44 87.37 94.05 92.33 TBL font RB 87.99 72.44 91.46 83.48 TBL font RB TBL tag 93.06 90.19 94.30 92.58 GT font RB 90.76 74.71 91.74 83.90 GT font RB TBL tag 95.74 92.29 94.54 93.11 Table 2 Tagging accuracy results for non punctuation tokens and phrases for two dictionaries The results of tagging accuracy experiments are presented in Table 2.
E06-1033.txt,108,In the tables_comma_ RB is rule based method_comma_ TBL tag is the TBL run on tags_comma_ TBL font is the TBL run on font style_comma_ and GT font is the ground truth font style.
E06-1033.txt,109,In each case_comma_ we begin with font style information provided by document image analysis.
E06-1033.txt,110,We tabulate percentagesoftaggingaccuracyofindividualnonpunctuation tokens and phrases8.
E06-1033.txt,111,The results for 8In phrase accuracy_comma_ if a group of consequent tokens is assigned one tag as a phrase in the ground truth_comma_ the tagging of the phrase is considered correct only if the same group of 260 token and phrase accuracy are presented for three different sets The entry tagger using the font style 1 provided by document image analysis_comma_ 2 after TBL is applied to font style_comma_ and 3 corrected manually_comma_ i.e. the ground truth.
E06-1033.txt,112,All results reported_comma_ except the token accuracies for two cases for the Iraqi Arabic dictionary_comma_ namely using TBL font vs. GT font and using TBL font and TBL tag together vs. using GT font and TBL tag _comma_ are statistically significant within the 95 confidence interval with two tailed paired ttests9.
E06-1033.txt,113,Using TBL font instead of initial font styles improved initial accuracy as much as 4.74 for tokens_comma_ and 8.36 for phrases in the Cebuano dictionary which has a much lower initial font style accuracy than the Iraqi Arabic dictionary.
E06-1033.txt,114,Using the GT font further increased the tagging accuracy by 2.77 for tokens and 2.27 for phrases for the Cebuano dictionary.
E06-1033.txt,115,As for the Iraqi Arabic dictionary_comma_ using TBL font and GT font resulted in an improvement of 0.57 and 0.85 for tokens and 0.74 and 1.18 for phrases respectively.
E06-1033.txt,116,The improvements in these two dictionaries differ because the initial font style accuracy for the Iraqi Arabic dictionary is very high while for the Cebuano dictionary potentially very useful font style information namely_comma_ the font style for POS tokens is often incorrect in the initial run.
E06-1033.txt,117,Using TBL tag alone improved rule based method results by 8.19 and 3.16 for tokens and by 23.25 and 9.61 for phrases in Cebuano and Iraqi Arabic dictionaries respectively.
E06-1033.txt,118,The last two rows in Table 2 show the upper bound.
E06-1033.txt,119,For the two dictionaries_comma_ our results using TBL font and TBL tag together is 2.68 and 0.24 for token accuracy and 2.10 and 0.53 for phrase accuracy less than the upper bound of using the GT font and TBL tag together.
E06-1033.txt,120,Applying TBL to font styles resulted in a higher accuracy than applying TBL to tagging.
E06-1033.txt,121,Since the number of tag types 18 and 26 is much larger than that of font style types 4 and 3 _comma_ TBL application on tags requires more training data than the font style to perform as well as TBL application on font style.
E06-1033.txt,122,In summary_comma_ applying TBL using the same templates to two different dictionaries using very limitedtrainingdataresultedinperformanceincrease_comma_ tokens was assigned the same tag as a phrase in the result.
E06-1033.txt,123,9We did the t tests on the results of individual entries. and the greatest increases we observed are in phrase accuracy.
E06-1033.txt,124,Applying TBL to font style first increased the accuracy even further.
E06-1033.txt,125,5.2 Effect of Training Data We conducted experiments to measure the robustness of our method with different training data.
E06-1033.txt,126,For this purpose_comma_ we trained TBL on eight pages randomly selected from the 14 pages for which we have ground truth_comma_ for each dictionary.
E06-1033.txt,127,We used theremainingsixpagesfortesting.
E06-1033.txt,128,Wedidthisten times_comma_ and calculated the average accuracy and the standard deviation.
E06-1033.txt,129,Table 3 presents the average accuracy and standard deviation.
E06-1033.txt,130,The accuracy results are consistent with the results we presented in Table 2_comma_ and the standard deviation is between 0.56 2.28.
E06-1033.txt,131,These results suggest that using different training data does not affect the performance dramatically.
E06-1033.txt,132,5.3 Effect of Training Data Size The problem to which we apply TBL has one important challenge and differs from other tasks in which TBL has been applied.
E06-1033.txt,133,Each dictionary has a different structure and different noise patterns_comma_ hence_comma_ TBL must be trained for each dictionary.
E06-1033.txt,134,This requires preparing ground truth manually for each dictionary before applying TBL.
E06-1033.txt,135,Moreover_comma_ although each dictionary has hundreds of pages_comma_ it is not feasible to use a significant portion of the dictionary for training.
E06-1033.txt,136,Therefore the training data should be small enough for someone to annotate ground truth in a short amount of time.
E06-1033.txt,137,One of our goals is to calculate the quantity of training data necessaryforareasonableimprovementintagging accuracy.
E06-1033.txt,138,For this purpose_comma_ we investigated the effect of the training data size by increasing the training data size for TBL one entry at a time.
E06-1033.txt,139,The entries are added in the order of the number of errorstheycontain_comma_ startingwiththeentrywithmaximum errors.
E06-1033.txt,140,We then tested the system trained with these entries on two test pages10.
E06-1033.txt,141,Figure3showsthenumberoffontstyleandtagging errors for non punctuation tokens on two test pages as a function of the number of entries in the training data.
E06-1033.txt,142,The tagging results are presented when using font style from document image analysis and font style after TBL.
E06-1033.txt,143,In these graphs_comma_ the 10We used two test data pages because if such a method will determine the minimum training data required to obtain a reasonable performance_comma_ the test data should be extremely limited to reduce human provided data.
E06-1033.txt,144,261 Cebuano Iraqi Arabic Token Phrase Token Phrase RB 81.46 1.14 62.38 1.09 92.10 0.69 85.05 1.64 RB TBL tag 89.34 0.96 85.17 1.55 94.94 0.56 93.25 0.87 TBL font RB 87.40 1.69 71.97 1.26 93.20 1.02 85.49 1.13 TBL font RB TBL tag 93.13 1.58 90.48 0.80 94.88 0.56 93.03 0.70 GT font RB 89.25 1.57 73.13 1.02 93.02 0.58 85.03 2.28 GT font RB TBL tag 95.31 1.43 91.89 1.80 95.32 0.65 93.36 0.81 Table 3 Average tagging accuracy results with standard deviation for ten runs using different eight pages for training_comma_ and six pages for testing 0 50 100 150 200 250 300 0 20 40 60 80 100 120 140 Number of Errors Number of Entries in Training Data for Cebuano Dictionary of Errors in Font Style of Errors in Tagging with TBL tag of Errors in Tagging with TBL font TBL tag 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 Number of Errors in Test Data Number of Entries in Training Data for Iraqi Arabic Dictionary of Errors in Font Style of Errors in Tagging with TBL tag of Errors in Tagging with TBL font TBL tag Figure 3 The number of errors in two test pages as a function of the number of entries in the training data for two dictionaries number of errors declines dramatically with the addition of the first entries.
E06-1033.txt,145,For the tags_comma_ the declineisnotassteepasthedeclineinfontstyle.
E06-1033.txt,146,The main reason involves the number of tags 18 and 26 _comma_ which are more than the number of font styles 4 and 3 .
E06-1033.txt,147,The method of adding entries to training data one by one_comma_ and finding the point when the number of errors on selected entries stabilizes_comma_ can determine minimum training data size to get a reasonable performance increase. lexicalized 5.4 Example Results Table 4 presents some learned transformations for Cebuano dictionary.
E06-1033.txt,148,Table 5 shows how these transformations change the font style and tags of tokens from Figure 4.
E06-1033.txt,149,The first column gives the tagging results before applying TBL.
E06-1033.txt,150,The consecutive columns shows how different TBL runs changes these results.
E06-1033.txt,151,The tags with indicate incorrect tags_comma_ the tags with indicate corrected tags_comma_ and the tags with indicate introduced errors.
E06-1033.txt,152,The font style of tokens is also represented.
E06-1033.txt,153,The No column in Tables 4 and 5 gives the applied transformation number.
E06-1033.txt,154,For these entries_comma_ using TBL on font styles and tagging together gives correct results in all cases.
E06-1033.txt,155,Using TBL only on tagging gives the correct tagging only for the last entry.
E06-1033.txt,156,TBL introduces new errors in some cases.
E06-1033.txt,157,One error we observed occurs when an example of usage translation is assigned a tag before any example of usage tag in an entry.
E06-1033.txt,158,This case is illustrated when applying transformation 9 to the token Abaa because of a misrecognized comma before the token.
E06-1033.txt,159,6 Conclusion In this paper_comma_ we introduced a new dictionary entry tagging system in which TBL improves tagging accuracy.
E06-1033.txt,160,TBL is applied at two points_comma_ on font style and tagging and yields high performance even with limited user provided training data.
E06-1033.txt,161,For two different dictionaries_comma_ we achieved an increase from 84 and 94 to 97 and 98 in font style accuracy_comma_ from 83 and 91 to 93 and 94 in tagging accuracy of tokens_comma_ and from 64 and 83 to 90 and 93 in tagging accuracy of phrases.
E06-1033.txt,162,If the initial font style is not accurate_comma_ first improving font style with TBL further assisted the tagging accuracy as much as 2.62 for tokens and 2.82 for phrases compared to using TBL only for tagging.
E06-1033.txt,163,This result cannot be 262 No Triggering Environment Change To 10 typen 2 lowercase and typen 1 punctuation and typen capitalized and normal fontn 1 normal and fontn 2 normal 15 fontn 1 italic and typen lowercase and typen 1 lowercase and fontn 2 italic italic 18 tokenn the first token in the entry bold 1 tokenn a and tagn 1 translation and tagn 1 translation translation 4 tag n 7_comma_n 1 example and tokenn 1 _comma_ and fontn bold example translation 2 typen lowercase and fontn normal and tagn 1 translation and fontn 1 normal translation 9 tokenn 1 _comma_ and fontn italic and typen capitalized example translation 8 tagn 2 example translation and tagn 1 separator and continuation tagn example translation and typen capitalized of a phrase 11 tagn 2 example and tagn 1 separator and tagn example and typen capitalized continuation of a phrase Table 4 Some sample transformations used for Cebuano dictionary entries in Figure 4.
E06-1033.txt,164,Here_comma_ continuation of a phrase indicates this token merges with the previous one to form a phrase. attributed to a low rule based baseline as a similar_comma_ even a slightly lower baseline is obtained from an HMM trained system.
E06-1033.txt,165,Results came from a method used to compensate for extremely limited training data.
E06-1033.txt,166,The similarity of performance acrosstwodifferentdictionariesshowsthemethod as adaptive and able to be applied genericly.
E06-1033.txt,167,In the future_comma_ we plan to investigate the sources of errors introduced by TBL and whether these can be avoided by post processing TBL results using heuristics.
E06-1033.txt,168,We will also examine the effects of using TBL to increase the training data size in a bootstrapped manner.
E06-1033.txt,169,We will apply TBL to a few pages_comma_ then correct these and use them as new training data in another run.
E06-1033.txt,170,Since TBL improves accuracy_comma_ manually preparing training data will take less time.
E06-1033.txt,171,Acknowledgements The partial support of this research under contract MDA 9040 2C 0406 is gratefully acknowledged. .
E06-1052.txt,1,paraphrase acquisition has been an active research field in recent years_comma_ but its effective coverage and performance have rarely been evaluated. We propose a generic paraphrase based approach for Relation Extraction RE _comma_ aiming at a dual goal obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervisedconfigurationforRE.Weanalyze the potential of our approach and evaluate an implemented prototype of it using an RE dataset.
E06-1052.txt,2,Our findings reveal a high potential for unsupervised paraphrase acquisition.
E06-1052.txt,3,We also identify the need for novel robust models for matching paraphrasesintexts_comma_whichshouldaddresssyntactic complexity and variability.
E06-1052.txt,5,A crucial challenge for semantic NLP applications is recognizing the many different ways for expressing the same information.
E06-1052.txt,6,This semantic variability phenomenon was addressed within specific applications_comma_ such as question answering_comma_ information extraction and information retrieval.
E06-1052.txt,7,Recently_comma_ the problem was investigated within generic application independent paradigms_comma_ such as paraphrasing and textual entailment.
E06-1052.txt,8,Eventually_comma_ it would be most appealing to apply genericmodelsforsemanticvariabilitytoconcrete applications.
E06-1052.txt,9,This paper investigates the applicability of a generic paraphrase based approach to the Relation Extraction RE task_comma_ using an available RE dataset of protein interactions.
E06-1052.txt,10,RE is highly suitable for such investigation since its goal is to exactly identify all the different variations in which a target semantic relation can be expressed.
E06-1052.txt,11,Taking this route sets up a dual goal a from the generic paraphrasing perspective an objective evaluation of paraphrase acquisition performance on a concrete application dataset_comma_ as well as identifying the additional mechanisms needed to match paraphrases in texts b from the RE perspective investigating the feasibility and performance of a generic paraphrase based approach for RE.
E06-1052.txt,12,Our configuration assumes a set of entailing templates non symmetric paraphrases for the target relation.
E06-1052.txt,13,For example_comma_ for the target relation X interact with Y we would assume a set of entailing templates as in Tables 3 and 7.
E06-1052.txt,14,In addition_comma_ we require a syntactic matching module that identifies template instances in text.
E06-1052.txt,15,First_comma_ we manually analyzed the proteininteractiondatasetandidentifiedallcasesinwhich protein interaction is expressed by an entailing template.
E06-1052.txt,16,This set a very high idealized upper bound for the recall of the paraphrase based approach for this dataset.
E06-1052.txt,17,Yet_comma_ obtaining high coverage in practice would require effective paraphrase acquisition and lexical syntactic template matching.
E06-1052.txt,18,Next_comma_ we implemented a prototype that utilizes a state of the art method for learning entailment relations from the web Szpektor et al._comma_ 2004 _comma_ the Minipar dependency parser Lin_comma_ 1998 and a syntactic matching module.
E06-1052.txt,19,As expected_comma_ the performance of the implemented system was much lower than the ideal upper bound_comma_ yet obtaining quite reasonable practical results given its unsupervised nature.
E06-1052.txt,20,The contributions of our investigation follow 409 the dual goal set above.
E06-1052.txt,21,To the best of our knowledge_comma_ this is the first comprehensive evaluation thatmeasuresdirectlytheperformanceofunsupervised paraphrase acquisition relative to a standard application dataset.
E06-1052.txt,22,It is also the first evaluation of a generic paraphrase based approach for the standard RE setting.
E06-1052.txt,23,Our findings are encouraging for both goals_comma_ particularly relative to their early maturity level_comma_ and reveal constructive evidence for the remaining room for improvement.
E06-1052.txt,24,2 Background 2.1 Unsupervised Information Extraction Information Extraction IE and its subfield Relation Extraction RE are traditionally performed in a supervised manner_comma_ identifying the different ways to express a specific information or relation.
E06-1052.txt,25,Given that annotated data is expensive to produce_comma_ unsupervised or weakly supervised methods have been proposed for IE and RE.
E06-1052.txt,26,Yangarber et al. 2000 and Stevenson and Greenwood 2005 define methods for automatic acquisition of predicate argument structures that are similar to a set of seed relations_comma_ which represent a specific scenario.
E06-1052.txt,27,Yangarber et al. 2000 approachwasevaluatedintwoways 1 manually mapping the discovered patterns into an IE system andrunningafullMUC styleevaluation 2 using the learned patterns to perform document filtering at the scenario level.
E06-1052.txt,28,Stevenson and Greenwood 2005 evaluated their method through document and sentence filtering at the scenario level.
E06-1052.txt,29,Sudo et al. 2003 extract dependency subtrees withinrelevantdocumentsasIEpatterns.
E06-1052.txt,30,Thegoal of the algorithm is event extraction_comma_ though performance is measured by counting argument entities rather than counting events directly.
E06-1052.txt,31,Hasegawa et al. 2004 performs unsupervised hierarchical clustering over a simple set of features.
E06-1052.txt,32,The algorithm does not extract entity pairs for a given relation from a set of documents but ratherclassifiesallrelationsinalargecorpus.
E06-1052.txt,33,This approach is more similar to text mining tasks than to classic IE problems.
E06-1052.txt,34,To conclude_comma_ several unsupervised approaches learn relevant IE templates for a complete scenario_comma_ but without identifying their relevance to each specific relation within the scenario.
E06-1052.txt,35,Accordingly_comma_ the evaluations of these works either did not addressthedirectapplicabilityforREorevaluated it only after further manual postprocessing.
E06-1052.txt,36,2.2 Paraphrases and Entailment Rules A generic model for language variability is usingparaphrases_comma_ textexpressionsthatroughlyconvey the same meaning.
E06-1052.txt,37,Various methods for automatic paraphrase acquisition have been suggested recently_comma_ ranging from finding equivalent lexical elements to learning rather complex paraphrases at the sentence level1.
E06-1052.txt,38,More relevant for RE are atomic paraphrases betweentemplates_comma_ textfragmentscontainingvariables_comma_ e.g.
E06-1052.txt,39,X buy Y X purchase Y .
E06-1052.txt,40,Under a syntacticrepresentation_comma_ atemplateisaparsedtext fragment_comma_ e.g.
E06-1052.txt,41,X subj interact mod with pcomp n Y based on the syntactic dependency relations of the Minipar parser .
E06-1052.txt,42,The parses include part ofspeech tags_comma_ which we omit for clarity.
E06-1052.txt,43,Dagan and Glickman 2004 suggested that a somewhat more general notion than paraphrasing is that of entailment relations.
E06-1052.txt,44,These are directional relations between two templates_comma_ where the meaning of one can be entailed from the meaning oftheother_comma_e.g.
E06-1052.txt,45,X bindtoY X interactwithY .
E06-1052.txt,46,For RE_comma_ when searching for a target relation_comma_ it is sufficient to identify an entailing template since it implies that the target relation holds as well.
E06-1052.txt,47,Under this notion_comma_ paraphrases are bidirectional entailment relations.
E06-1052.txt,48,Several methods extract atomic paraphrases by exhaustively processing local corpora Lin and Pantel_comma_ 2001 Shinyama et al._comma_ 2002 .
E06-1052.txt,49,Learning from a local corpus is bounded by the corpus scope_comma_ which is usually domain specific both works above processed news domain corpora .
E06-1052.txt,50,To cover a broader range of domains several works utilized the Web_comma_ while requiring several manually provided examples for each input relation_comma_ e.g.
E06-1052.txt,51,Ravichandran and Hovy_comma_ 2002 .
E06-1052.txt,52,Taking a stepfurther_comma_ theTEASEalgorithm Szpektoretal._comma_ 2004 provides a completely unsupervised method for acquiring entailment relations from the Web for a given input relation see Section 5.1 .
E06-1052.txt,53,Most of these works did not evaluate their results in terms of application coverage.
E06-1052.txt,54,Lin and Pantel 2001 compared their results to humangenerated paraphrases.
E06-1052.txt,55,Shinyama et al. 2002 measured the coverage of their learning algorithm relative to the paraphrases present in a given corpus.
E06-1052.txt,56,Szpektor et al. 2004 measured yield _comma_ the number of correct rules learned for an input re1See the 3rd IWP workshop for a sample of recent works on paraphrasing http nlp.nagaokaut.ac.jp IWP2005 .
E06-1052.txt,58,Ravichandran and Hovy 2002 evaluated the performance of a QA system that is based solely on paraphrases_comma_ an approach resembling ours.
E06-1052.txt,59,However_comma_ they measured performance using Mean Reciprocal Rank_comma_ which does not reveal the actual coverage of the learned paraphrases.
E06-1052.txt,60,3 Assumed Configuration for RE Phenomenon Example Passive form Y is activated by X Apposition X activates its companion_comma_ Y Conjunction X activates prot3 and Y Set X activates two proteins_comma_ Y and Z Relative clause X_comma_ which activates Y Coordination X binds and activates Y Transparent head X activates a fragment of Y Co reference X is a kinase_comma_ though it activates Y Table 1 Syntactic variability phenomena_comma_ demonstrated for the normalized template X activate Y .
E06-1052.txt,61,The general configuration assumed in this paper for RE is based on two main elements a list of lexical syntactic templates which entail the relation of interest and a syntactic matcher which identifies the template occurrences in sentences.
E06-1052.txt,62,The set of entailing templates may be collected either manually or automatically.
E06-1052.txt,63,We propose this configuration both as an algorithm for RE and as an evaluation scheme for paraphrase acquisition.
E06-1052.txt,64,The role of the syntactic matcher is to identifythedifferentsyntacticvariationsinwhichtemplates occur in sentences.
E06-1052.txt,65,Table 1 presents a list of generic syntactic phenomena that are known in the literature to relate to linguistic variability.
E06-1052.txt,66,A phenomenon which deserves a few words of explanation is the transparent head noun Grishman et al._comma_ 1986 Fillmore et al._comma_ 2002 .
E06-1052.txt,67,A transparent noun N1 typically occurs in constructs of the form N1 preposition N2 for which the syntactic relation involving N1_comma_ which is the head of the NP_comma_ applies to N2_comma_ the modifier.
E06-1052.txt,68,In the example in Table 1_comma_ fragment is the transparent head noun while the relation activate applies to Y as object.
E06-1052.txt,69,4 Manual Data Analysis 4.1 Protein Interaction Dataset Bunescu et al. 2005 proposed a set of tasks regarding protein name and protein interaction extraction_comma_ for which they manually tagged about 200 Medline .
E06-1032.txt,1,argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality_comma_ and give two significant counterexamples to Bleu s correlation with human judgments of quality.
E06-1032.txt,2,This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
E06-1032.txt,4,Over the past five years progress in machine translation_comma_ and to a lesser extent progress in natural language generation tasks such as summarization_comma_ has been driven by optimizing against n grambased evaluation metrics such as Bleu Papineni et al._comma_ 2002 .
E06-1032.txt,5,The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training Och_comma_ 2003 .
E06-1032.txt,6,Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores_comma_ while neglecting to show any actual example translations.
E06-1032.txt,7,Workshops commonly compare systems using Bleu scores_comma_ often without confirming these rankings through manual evaluation.
E06-1032.txt,8,All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality_comma_ which has been shown to hold in many cases Doddington_comma_ 2002 Coughlin_comma_ 2003 .
E06-1032.txt,9,However_comma_ there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements.
E06-1032.txt,10,If Bleu s correlation with human judgments has been overestimated_comma_ then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is.
E06-1032.txt,11,In this paper we give a number of counterexamples for Bleu s correlation with human judgments.
E06-1032.txt,12,We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality_comma_ and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality.
E06-1032.txt,13,We argue that Bleu is insufficient by showing that Bleu admits a huge amount of variation for identically scored hypotheses.
E06-1032.txt,14,Typically there are millions of variations on a hypothesis translation that receive the same Bleu score.
E06-1032.txt,15,Because not all these variations are equally grammatically or semantically plausible there are translations which have the same Bleu score but a worse human evaluation.
E06-1032.txt,16,We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examplesofBleuvastlyunderestimatingthetranslation quality of systems.
E06-1032.txt,17,Finally_comma_ we discuss appropriate uses for Bleu and suggest that for some research projects it may be preferable to use a focused_comma_ manual evaluation instead.
E06-1032.txt,18,2 BLEU Detailed The rationale behind the development of Bleu Papinenietal._comma_ 2002 isthathumanevaluationofmachine translation can be time consuming and expensive.
E06-1032.txt,19,An automatic evaluation metric_comma_ on the other hand_comma_ can be used for frequent tasks like monitoringincrementalsystemchangesduringdevelopment_comma_ which are seemingly infeasible in a manual evaluation setting.
E06-1032.txt,20,The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations.
E06-1032.txt,21,Machine translation evaluation metrics differ from other metrics that use a reference_comma_ like the word error rate metric that is used 249 Orejuela appeared calm as he was led to the American plane which will take him to Miami_comma_ Florida.
E06-1032.txt,22,Orejuela appeared calm while being escorted to the plane that would take him to Miami_comma_ Florida.
E06-1032.txt,23,Orejuela appeared calm as he was being led to the American plane that was to carry him to Miami in Florida.
E06-1032.txt,24,Orejuela seemed quite calm as he was being led to the American plane that would take him to Miami in Florida.
E06-1032.txt,25,Appeared calm when he was taken to the American plane_comma_ which will to Miami_comma_ Florida.
E06-1032.txt,26,Table 1 A set of four reference translations_comma_ and a hypothesis translation from the 2005 NIST MT Evaluation in speech recognition_comma_ because translations have a degree of variation in terms of word choice and in terms of variant ordering of some phrases.
E06-1032.txt,27,Bleu attempts to capture allowable variation in word choice through the use of multiple reference translations as proposed in Thompson 1991 .
E06-1032.txt,28,In order to overcome the problem of variation in phrase order_comma_ Bleu uses modified n gram precision instead of WER s more strict string edit distance.
E06-1032.txt,29,Bleu s n gram precision is modified to eliminate repetitions that occur across sentences.
E06-1032.txt,30,For example_comma_ even though the bigram to Miami is repeated across all four reference translations in Table 1_comma_ it is counted only once in a hypothesis translation.
E06-1032.txt,31,Table 2 shows the n gram sets created from the reference translations.
E06-1032.txt,32,Papineni et al. 2002 calculate their modified precision score_comma_ pn_comma_ for each n gram length by summing over the matches for every hypothesis sentence S in the complete corpus C as pn summationtext S C summationtext ngram S Countmatched ngram summationtext S C summationtext ngram S Count ngram Counting punctuation marks as separate tokens_comma_ the hypothesis translation given in Table 1 has 15 unigram matches_comma_ 10 bigram matches_comma_ 5 trigram matches these are shown in bold in Table 2 _comma_ and three 4 gram matches not shown .
E06-1032.txt,33,The hypothesis translation contains a total of 18 unigrams_comma_ 17 bigrams_comma_ 16 trigrams_comma_ and 15 4 grams.
E06-1032.txt,34,If the complete corpus consisted of this single sentence 1 grams American_comma_ Florida_comma_ Miami_comma_ Orejuela_comma_ appeared_comma_ as_comma_ being_comma_ calm_comma_ carry_comma_ escorted_comma_ he_comma_ him_comma_ in_comma_ led_comma_ plane_comma_ quite_comma_ seemed_comma_ take_comma_ that_comma_ the_comma_ to_comma_ to_comma_ to_comma_ was _comma_ was_comma_ which_comma_ while_comma_ will_comma_ would_comma_ _comma__comma_ .
E06-1032.txt,35,2 grams American plane_comma_ Florida ._comma_ Miami _comma__comma_ Miami in_comma_ Orejuela appeared_comma_ Orejuela seemed_comma_ appeared calm_comma_ ashe_comma_ beingescorted_comma_ beingled_comma_ calmas_comma_ calmwhile_comma_ carry him_comma_ escorted to_comma_ he was_comma_ him to_comma_ in Florida_comma_ led to_comma_ plane that_comma_ plane which_comma_ quite calm_comma_ seemed quite_comma_ take him_comma_ that was_comma_ that would_comma_ the American_comma_ the plane_comma_ to Miami_comma_ to carry_comma_tothe_comma_ wasbeing_comma_ wasled_comma_ wasto_comma_whichwill_comma_ while being_comma_ will take_comma_ would take_comma_ _comma_ Florida 3 grams American plane that_comma_ American plane which_comma_ Miami _comma_ Florida_comma_ Miami in Florida_comma_ Orejuela appeared calm_comma_ Orejuela seemed quite_comma_ appeared calm as_comma_ appeared calmwhile_comma_ashewas_comma_beingescortedto_comma_beingledto_comma_calm as he_comma_ calm while being_comma_ carry him to_comma_ escorted to the_comma_ he was being_comma_ he was led_comma_ him to Miami_comma_ in Florida ._comma_ led to the_comma_ plane that was_comma_ plane that would_comma_ plane which will_comma_ quite calm as_comma_ seemed quite calm_comma_ take him to_comma_ that was to_comma_ that would take_comma_ the American plane_comma_ the plane that_comma_ to Miami _comma__comma_ to Miami in_comma_ to carry him_comma_ to the American_comma_ to the plane_comma_ was being led_comma_ was led to_comma_ was to carry_comma_ which will take_comma_ while being escorted_comma_ will take him_comma_ would take him_comma_ _comma_ Florida .
E06-1032.txt,36,Table 2 The n grams extracted from the reference translations_comma_ with matches from the hypothesis translation in bold then the modified precisions would be p1 .83_comma_ p2 .59_comma_ p3 .31_comma_ and p4 .2.
E06-1032.txt,37,Each pn is combined and can be weighted by specifying a weight wn.
E06-1032.txt,38,In practice each pn is generally assigned an equal weight.
E06-1032.txt,39,Because Bleu is precision based_comma_ and because recall is difficult to formulate over multiple reference translations_comma_ a brevity penalty is introduced to compensate for the possibility of proposing highprecision hypothesis translations which are too short.
E06-1032.txt,40,The brevity penalty is calculated as BP braceleftBigg 1 if c r e1 r c if c r where c is the length of the corpus of hypothesis translations_comma_ and r is the effective reference corpus length.1 Thus_comma_ the Bleu score is calculated as Bleu BP exp Nsummationdisplay n 1 wn logpn A Bleu score can range from 0 to 1_comma_ where higher scores indicate closer matches to the reference translations_comma_ and where a score of 1 is assigned to a hypothesis translation which exactly 1The effective reference corpus length is calculated as the sum of the single reference translation from each set which is closest to the hypothesis translation.
E06-1032.txt,41,250 matches one of the reference translations.
E06-1032.txt,42,A score of 1 is also assigned to a hypothesis translation which has matches for all its n grams up to the maximum n measured by Bleu in the clipped reference n grams_comma_ and which has no brevity penalty.
E06-1032.txt,43,TheprimaryreasonthatBleuisviewedasauseful stand in for manual evaluation is that it has been shown to correlate with human judgments of translation quality.
E06-1032.txt,44,Papineni et al. 2002 showed that Bleu correlated with human judgments in its rankings of five Chinese to English machine translation systems_comma_ and in its ability to distinguish between human and machine translations.
E06-1032.txt,45,Bleu s correlation with human judgments has been further tested in the annual NIST Machine Translation Evaluation exercise wherein Bleu s rankings of Arabic to English and Chinese to English systems is verified by manual evaluation.
E06-1032.txt,46,In the next section we discuss theoretical reasons why Bleu may not always correlate with human judgments.
E06-1032.txt,47,3 Variations Allowed By BLEU WhileBleuattemptstocaptureallowablevariation in translation_comma_ it goes much further than it should.
E06-1032.txt,48,In order to allow some amount of variant order in phrases_comma_ Bleu places no explicit constraints on the order that matching n grams occur in.
E06-1032.txt,49,To allow variation in word choice in translation Bleu uses multiple reference translations_comma_ but puts very few constraints on how n gram matches can be drawn from the multiple reference translations.
E06-1032.txt,50,Because Bleu is underconstrained in these ways_comma_ it allows a tremendous amount of variation far beyond what could reasonably be considered acceptable variation in translation.
E06-1032.txt,51,Inthissectionweexaminevariouspermutations and substitutions allowed by Bleu.
E06-1032.txt,52,We show that foranaveragehypothesistranslationtherearemillions of possible variants that would each receive a similar Bleu score.
E06-1032.txt,53,We argue that because the number of translations that score the same is so large_comma_ it is unlikely that all of them will be judged to be identical in quality by human annotators.
E06-1032.txt,54,This means that it is possible to have items which receive identical Bleu scores but are judged by humans to be worse.
E06-1032.txt,55,It is also therefore possible to have a higher Bleu score without any genuine improvement in translation quality.
E06-1032.txt,56,In Sections 3.1 and 3.2 we examine ways of synthetically producing such variant translations.
E06-1032.txt,57,3.1 Permuting phrases One way in which variation can be introduced is by permuting phrases within a hypothesis translation.
E06-1032.txt,58,A simple way of estimating a lower bound on thenumber ofways thatphrases ina hypothesis translation can be reordered is to examine bigram mismatches.
E06-1032.txt,59,Phrases that are bracketed by these bigram mismatch sites can be freely permuted because reordering a hypothesis translation at these points will not reduce the number of matching ngrams and thus will not reduce the overall Bleu score.
E06-1032.txt,60,Here we denote bigram mismatches for the hypothesis translation given in Table 1 with vertical bars Appeared calm when he was taken to the American plane _comma_ which will to Miami _comma_ Florida .
E06-1032.txt,61,We can randomly produce other hypothesis translations that have the same Bleu score but are radically different from each other.
E06-1032.txt,62,Because Bleu only takes order into account through rewarding matches of higher order n grams_comma_ a hypothesis sentence may be freely permuted around these bigram mismatch sites and without reducing the Bleu score.
E06-1032.txt,63,Thus which will he was _comma_ when taken Appeared calm to the American plane to Miami _comma_ Florida . receives an identical score to the hypothesis translation in Table 1.
E06-1032.txt,64,If b is the number of bigram matches in a hypothesis translation_comma_ and k is its length_comma_ then there are k b
E06-1032.txt,65,1 possible ways to generate similarly scored items using only the words in the hypothesis translation.2 Thus for the example hypothesis translation there are at least 40_comma_320 different ways of permuting the sentence and receiving a similar Bleu score.
E06-1032.txt,66,The number of permutations varies with respect to sentence length and number of bigram mismatches.
E06-1032.txt,67,Therefore as a hypothesis translation approaches being an identical match to one of the reference translations_comma_ the amount of variance decreases significantly.
E06-1032.txt,68,So_comma_ as translations improve 2Note that in some cases randomly permuting the sentence in this way may actually result in a greater number of n gram matches however_comma_ one would not expect random permutation to increase the human evaluation.
E06-1032.txt,69,251 0 20 40 60 80 100 120 1 1e 10 1e 20 1e 30 1e 40 1e 50 1e 60 1e 70 1e 80 Sentence Length Number of Permutations Figure 1 Scatterplot of the length of each translation against its number of possible permutations due to bigram mismatches for an entry in the 2005 NIST MT Eval spurious variation goes down.
E06-1032.txt,70,However_comma_ at today s levels the amount of variation that Bleu admits is unacceptably high.
E06-1032.txt,71,Figure 1 gives a scatterplot of each of the hypothesis translations produced by the second best Bleu system from the 2005 NIST MT Evaluation.
E06-1032.txt,72,The number of possible permutations for some translations is greater than 1073.
E06-1032.txt,73,3.2 Drawing different items from the reference set In addition to the factorial number of ways that similarly scored Bleu items can be generated by permuting phrases around bigram mismatch points_comma_ additional variation may be synthesized by drawing different items from the reference ngrams.
E06-1032.txt,74,For example_comma_ since the hypothesis translation from Table 1 has a length of 18 with 15 unigram matches_comma_ 10 bigram matches_comma_ 5 trigram matches_comma_ and three 4 gram matches_comma_ we can artificially construct an identically scored hypothesis by drawing an identical number of matching ngrams from the reference translations.
E06-1032.txt,75,Therefore the far less plausible was being led to the calm as he was would take carry him seemed quite when taken would receive the same Bleu score as the hypothesis translation from Table 1_comma_ even though human judges would assign it a much lower score.
E06-1032.txt,76,This problem is made worse by the fact that Bleu equally weights all items in the reference sentences Babych and Hartley_comma_ 2004 .
E06-1032.txt,77,Therefore omitting content bearing lexical items does not carry a greater penalty than omitting function words.
E06-1032.txt,78,The problem is further exacerbated by Bleu not having any facilities for matching synonyms or lexicalvariants.
E06-1032.txt,79,Thereforewordsinthehypothesis that did not appear in the .
E06-2011.txt,1,is a general domain Portuguese question answering system. It tries to take advantage of the great amount of information existent in the World Wide Web.
E06-2011.txt,2,Since Portuguese is one of the most used languages in the web and the web itself is a constantly growing source of updated information_comma_ this kind of techniques are quite interesting and promising.
E06-2011.txt,4,There are some question answering systems for Portuguese like the ones developed by the University of vora Quaresma and Rodrigues_comma_ 2005 and Priberam Amaral et al_comma_ 2005 _comma_ but these systems rely heavily on the pre processing of document collections.
E06-2011.txt,5,Esfinge explores a different approach instead of investing in preprocessing corpora_comma_ it tries to use the redundancy existent in the web to find its answers.
E06-2011.txt,6,In addition it has an interface on the web where everyone can pose questions to the system http www.linguateca.pt Esfinge .
E06-2011.txt,7,Esfinge is based on the architecture proposed in Brill_comma_ 2003 .
E06-2011.txt,8,Brill suggests that it is possible to obtain interesting results_comma_ applying simple techniques to large quantities of data.
E06-2011.txt,9,The Portuguese web can be an interesting resource for such architecture.
E06-2011.txt,10,Nuno Cardoso p.c. is compiling a collection of pages from the Portuguese web and this collection will amount to 8.000.000 pages.
E06-2011.txt,11,Using the techniques described in Aires and Santos_comma_ 2002 one can estimate that Google and Altavista index 34_comma_900_comma_000 and 60_comma_500_comma_000 pages in Portuguese respectively.
E06-2011.txt,12,The system is described in detail in Costa_comma_ 2005a_comma_ 2005b .
E06-2011.txt,13,2 System Architecture The inputs to the system are questions in natural language.
E06-2011.txt,14,Esfinge begins by transforming these questions into patterns of plausible answers.
E06-2011.txt,15,As an example_comma_ take the question Onde fica Braga
E06-2011.txt,16,Where is Braga located .
E06-2011.txt,17,This generates the pattern Braga fica Braga is located with a score of 20_comma_ that can be used to search for documents that might contain an answer to the question.
E06-2011.txt,18,The patterns used by the system have the same syntax as the one commonly used in search engines_comma_ quoted text meaning a phrase pattern.
E06-2011.txt,19,Then_comma_ these patterns are searched in the Web using Google at the moment and the system extracts the first 100 document snippets created by the search engine.
E06-2011.txt,20,Some tests performed with Esfinge showed that certain types of sites may compromise the quality of the returned answers.
E06-2011.txt,21,With that in mind_comma_ the system uses a list of address patterns which are not to be considered it does not consider documents stored in addresses that match these patterns .
E06-2011.txt,22,The patterns in this list such as blog_comma_ humor_comma_ piadas were created manually based on the fore mentioned tests.
E06-2011.txt,23,The next step involves the extraction of word n grams length 1 to 3 from the document passages obtained previously.
E06-2011.txt,24,The system uses the Ngram Statistic Package Banerjee and Pedersen_comma_ 2003 for that purpose.
E06-2011.txt,25,These n grams are scored using the formula N gram score  F S L _comma_ through the first 100 snippets resulting from the web search where F is the n gram frequency_comma_ S is the score of the search pattern that recovered the document and L is the n gram length.
E06-2011.txt,26,Identifying the type of question can be quite useful in the task of searching for an answer.
E06-2011.txt,27,For 127 example a question beginning with When suggests that most likely the answer will be a date.
E06-2011.txt,28,Esfinge has a module that uses the named entity recognition NER system SIEMES to detect specific types of answers.
E06-2011.txt,29,This NER system detects and classifies named entities in a wide range of categories Sarmento_comma_ submitted .
E06-2011.txt,30,Esfinge used a sub set of these categories_comma_ namely Human_comma_ Country_comma_ Settlement including cities_comma_ villages_comma_ etc _comma_ Geographical Locations locations with no political entailment_comma_ like for example Africa _comma_ Date and Quantity.
E06-2011.txt,31,When the type of question leads to one or more of those named entity categories_comma_ the 200 best scored word ngrams from the previous modules are submitted to SIEMES.
E06-2011.txt,32,The results from the NER system are then analysed in order to check whether it recognizes named entities classified as one of the desired categories.
E06-2011.txt,33,If such named entities are recognized_comma_ their position in the ranking of possible answers is pushed to the top and they will skip the filter Interesting PoS described ahead .
E06-2011.txt,34,In the next module the list of possible answers by ranking order is submitted to several filters xA filter that discards words contained in the questions.
E06-2011.txt,35,Ex the answer Eslov quia is not desired for the question Qual a capital da Eslov quia
E06-2011.txt,36,What is the capital of Slovakia and should be discarded.
E06-2011.txt,37,xA filter that rejects answers included in a list of undesired answers .
E06-2011.txt,38,This list includes very frequent words that do not answer questions alone like pessoas persons_comma_ nova new_comma_ lugar place_comma_ grandes big_comma_ exemplo example .
E06-2011.txt,39,It was built with the help of Esfinge log which records all the answers analysed by the system .
E06-2011.txt,40,Later some other answers were added to this list_comma_ as a result of tests performed with the system.
E06-2011.txt,41,The list includes now 92 entries.
E06-2011.txt,42,xA filter that uses the morphological analyzer jspell Sim es and Almeida_comma_ 2002 to check the PoS of the various tokens in each answer.
E06-2011.txt,43,This filter rejects the answers whose first and last answer are not common or proper nouns_comma_ adjectives or numbers.
E06-2011.txt,44,Using this simple technique it is possible to discard incomplete answers beginning or ending with prepositions or interjections for example.
E06-2011.txt,45,Figure 1 describes the algorithm steps related to named entity recognition classification in the ngrams and n gram filtering.
E06-2011.txt,47,Named entity recognition classification and filtering in the n grams The final answers of the system are the best scored candidate answers that manage to go through all the previously described filters.
E06-2011.txt,48,There is a final step in the algorithm where the system searches for longer answers.
E06-2011.txt,49,These are answers that include one of the best candidate answers and also pass all the filters.
E06-2011.txt,50,For example_comma_ the best scored answer for the question Who is the British prime minister might be just Tony.
E06-2011.txt,51,However_comma_ if the system manages to recover the n gram Tony Blair and this n gram also passes all the filters_comma_ it will be the returned answer.
E06-2011.txt,52,Figure 2 gives an overview of the several steps of the question answering algorithm.
E06-2011.txt,54,The architecture of Esfinge 128 Figure 3 shows how the system returns the results.
E06-2011.txt,55,Each answer is followed by some passages of documents from where the answers were extracted.
E06-2011.txt,56,Clicking on a passage_comma_ the user navigates to the document from which the passage was extracted.
E06-2011.txt,57,This enables the user to check whether the answer is appropriate or to find more information related to the formulated question.
E06-2011.txt,59,Esfinge answers to the question Who is the Russian president At the moment_comma_ Esfinge is installed in a Pentium 4 2.4 GHz machine running Red Hat Linux 9_comma_ with 1 GB of RAM memory and it can take from one to two minutes to answer a question.
E06-2011.txt,60,Figure 4 shows the modules and data flow in the QA system.
E06-2011.txt,61,The external modules are represented as white boxes_comma_ while the modules specifically developed for the QA system are represented as grey boxes.
E06-2011.txt,63,Modules and data flow 3 Results In order to measure the evolution and the performance of the different techniques used_comma_ Esfinge participated in the QA task at CLEF in 2004 and 2005 Vallin et al_comma_ 2005 .
E06-2011.txt,64,In this task the participants receive 200 questions prepared by the organization and a document collection.
E06-2011.txt,65,The systems are then supposed to return the answers to each question_comma_ indicating also the documents that support each of the answers.
E06-2011.txt,66,The questions are mainly factoid ex Who is the president of South Africa _comma_ but there are also some definitions ex Who is Angelina Jolie .
E06-2011.txt,67,Esfinge needed some extra features to participate in the QA task at CLEF.
E06-2011.txt,68,While in its original version_comma_ the document retrieval task was left to Google_comma_ in CLEF it is necessary to search in the CLEF document collection in order to return the documents supporting the answers.
E06-2011.txt,69,For that purpose this document collection was encoded with CQP Christ et al_comma_ 1999 and a document retrieval module was added to the system.
E06-2011.txt,70,Two different strategies were tested.
E06-2011.txt,71,In the first one_comma_ the system searched the answers in the Web and used the CLEF document collection to confirm these answers.
E06-2011.txt,72,In the second experiment_comma_ Esfinge searched the answers in the CLEF document collection only.
E06-2011.txt,73,Table 1 presents the results obtained by Esfinge at CLEF 2004 and 2005.
E06-2011.txt,74,Due to these participations some errors were detected and corrected.
E06-2011.txt,75,The table also includes the results obtained by the current version of the system with the CLEF questions in 2004 and 2005_comma_ as well as the results of the best system U. Amsterdam and the best system for Portuguese University of vora in 2004 and 2005 where Priberam s system for Portuguese got the best results among all the systems .
E06-2011.txt,76,System Number of questions Number  of exact answers Esfinge 199 30 15 Esfinge current version 199 55 28 Best system for Portuguese 199 56 28 CLEF2004 Best system 200 91 46 Esfinge 200 48 24 Esfinge current version 200 61 31 CLEF 2005 Best system 200 129 65 Table 1.
E06-2011.txt,77,Results at CLEF 2004 and 2005 We tried to investigate whether CLEF questions are the most appropriate to evaluate a system like Esfinge.
E06-2011.txt,78,With that intention 20 questions were picked randomly and Google was queried to check whether it was possible to find answers in the first 100 returned snippets.
E06-2011.txt,79,For 5 of the questions no answers were found_comma_ there were few occurrences of the right answer 3 or less for 8 of the questions and for only 7 of the questions there was some redundancy 4 or more right an129 swers .
E06-2011.txt,80,There are more details about the evaluation of the system in Costa_comma_ 2006 .
E06-2011.txt,81,4 Conclusions Even though the results in CLEF 2005 improved compared to CLEF 2004_comma_ they are still far from the results obtained by the best systems.
E06-2011.txt,82,However_comma_ there are not many question answering systems developed for Portuguese and the existing ones rely heavily on the pre processing of document collections.
E06-2011.txt,83,Esfinge tries to explore a different angle_comma_ namely the use of the web as a corpus where information can be found and extracted.
E06-2011.txt,84,It is not proved that CLEF questions are the most appropriate to evaluate the system.
E06-2011.txt,85,In the experiment described in the previous section_comma_ it was possible to get some answer redundancy in the web for less than half of the analyzed questions.
E06-2011.txt,86,We plan to study search engine logs_comma_ in order to find whether it is possible to build a question collection with real users questions.
E06-2011.txt,87,Since Esfinge is a project in the scope of Linguateca http www.linguateca.pt _comma_ it follows Linguateca s main assumptions.
E06-2011.txt,88,For example_comma_ the one stating that all research results should be made public.
E06-2011.txt,89,The web interface_comma_ where everyone can freely test the system was the first step in that direction_comma_ and now the source code of modules used in the system is freely available to make it more useful for other researchers in this area.
E06-2011.txt,90,Acknowledgements I thank Diana Santos for reviewing previous versions of this paper_comma_ Alberto Sim es for the hints on using the perl module jspell .
E06-2011.txt,91,Lu s Sarmento_comma_ Lu s Cabral and Ana Sofia Pinto for supporting the use of the NER system SIEMES.
E06-2011.txt,92,This work is financed by the Portuguese Funda o para a Ci ncia e Tecnologia through grant POSI PLP 43931 2001_comma_ co financed by POSI. .
E06-2008.txt,1,current sentence generators lack the ability to compute elliptical versions of coordinated clauses in accordance with the rules for Gapping_comma_ Forward and Backward Conjunction Reduction_comma_ and SGF Subject Gap in clauses with Finite Fronted verb . We describe a module implemented in JAVA_comma_ with German and Dutch as target languages that takes non elliptical coordinated clauses as input and returns all reduced versions licensed by coordinative ellipsis.
E06-2008.txt,2,It is loosely based on a new psycholinguistic theory of coordinative ellipsis proposed by Kempen.
E06-2008.txt,3,In this theory_comma_ coordinative ellipsis is not supposed to result from the application of declarative grammar rules for clause formation but from a procedural component that interacts with the sentence generator and may block the overt expression of certain constituents.
E06-2008.txt,5,Coordination and coordinative ellipsis are essential tools for the sentence aggregation component of any language generator.
E06-2008.txt,6,Very often_comma_ when the aggregator chooses to combine several clauses into a single coordinate structure_comma_ the need arises to eliminate unnatural reduplications of coreferential constituents.
E06-2008.txt,7,In the literature_comma_ one often distinguishes four major types of clause level coordinative ellipsis Gapping as in 1 _comma_ with a special variant called Long Distance Gapping LDG .
E06-2008.txt,8,In LDG_comma_ the second conjunct consists of constituents stemming from different clauses in 2 _comma_ the main clause and the complement. Forward Conjunction Reduction FCR cf. 3 and the relative clause in 4 . SGF Subject Gap in clauses with Finite Fronted verb as in 5 _comma_ and Backward Conjunction reduction BCR_comma_ also termed Right Node Raising see 6 .
E06-2008.txt,9,1 Henk lives in Leiden and Chris livesg in Delft 2 My wife wants to buy a car_comma_ my son wantsg to buy gl a motorcycle.
E06-2008.txt,10,3 My sister lives in Utrecht and my sister f works in Amsterdam 4 Amsterdam is the city S where Jan lives and wheref Piet works 5 Why did you leave but didn t yous warn me
E06-2008.txt,11,6 Anne arrived before three o clock b_comma_ and Susi left after three o clock The subscripts denote the elliptical mechanism at work g Gapping_comma_ gl LDG_comma_ f FCR_comma_ s SGF_comma_ b BCR.
E06-2008.txt,12,We will not deal with VP Ellipsis and VP Anaphora because they generate pro forms rather than elisions and are not restricted to coordination cf. the title of the paper .
E06-2008.txt,13,In current sentence generators_comma_ the coordinative ellipsis rules are often inextricably intertwined with the rules for generating nonelliptical coordinate structures_comma_ so that they cannot easily be ported to other grammar formalisms e.g._comma_ Sarkar Joshi 1996 for Tree Adjoining Grammar Steedman 2000 for Combinatory Categorial Grammar Bateman_comma_ Matthiessen Zeng 1999 for Functional Grammar.
E06-2008.txt,14,Generators that do include an autonomous component for coordinative ellipsis Dalianis_comma_ 1999 Shaw_comma_ 2002 Hielkema_comma_ 2005 _comma_ use incomplete rule sets_comma_ thus risking over or undergeneration_comma_ and incorrect or unnatural output.
E06-2008.txt,15,The module dubbed ELLEIPO_comma_ from Greek uni1F18 uni1F77pi I leave out we present here_comma_ is less 115 formalism dependent and_comma_ in principle_comma_ less liable to over or undergeneration than its competitors.
E06-2008.txt,16,In Section 2_comma_ we sketch the theoretical background.
E06-2008.txt,17,Section 3 and the Appendix describe our implementation_comma_ with examples from German.
E06-2008.txt,18,Finally_comma_ in Section 4_comma_ we discuss the prospects of extending the module to additional constructions.
E06-2008.txt,19,2 Some theoretical background ELLEIPO is loosely based on Kempen s subm. psycholinguistically motivated syntactic theory of clausal coordination and coordinative ellipsis.
E06-2008.txt,20,It departs from the assumption that the generator s strategic conceptual_comma_ pragmatic component is responsible for selecting the concepts and conceptual structures that enable identification of discourse referents except in case of syntactically conditioned pronominalization .
E06-2008.txt,21,The strategic component may conjoin two or more clauses into a coordination and deliver as output a non reduced sequence of conjuncts.1 The concepts in these conjuncts are adorned with reference tags_comma_ and identical tags express coreferentiality.2 Structures of this kind serve as input to the syn tactical component of the generator_comma_ where they are grammatically encoded lexicalized and given syntactic form without any form of coordinative ellipsis.
E06-2008.txt,22,The resulting non elliptical structures are input to ELLEIPO_comma_ which computes and executes options for coordinative ellipsis.
E06-2008.txt,23,ELLEIPO s functioning is based on the assumption that coordinative ellipsis does not result from the application of declarative grammar rules for clause formation but from a procedural component that interacts with the sentence generator and may block the overt expression of certain constituents.
E06-2008.txt,24,Due to this feature_comma_ ELLEIPO can be combined_comma_ at least in principle_comma_ with various grammar formalisms.
E06-2008.txt,25,However_comma_ this advantage is not entirely gratis The module needs a formalism dependent interface that converts gen1The strategic component is also supposed to apply rules of logical inference yielding the conceptual structures that underlie respectively coordinations. Hence_comma_ the conversion of clausal into NP coordination such as Anne likes biking and Susi likes skating into Anne and Susi like biking and skating_comma_ respectively is supposed to arise in the strategic_comma_ not the syn tactical component of the generator.
E06-2008.txt,26,This also applies to simpler cases without respectively_comma_ such as John is skating and Peter is skating versus John and Peter are skating.
E06-2008.txt,27,The module presented here does not handle these conversions see Reiter Dale 2000_comma_ pp. 133 139 for examples and possible solutions. 2Coordinative ellipsis is insensitive to the distinction between strict and sloppy token vs. type identity. erator output to a simple canonical form.
E06-2008.txt,28,3 A sketch of the algorithm This sketch presupposes and coordinations of only n 2 conjuncts.
E06-2008.txt,29,Actually_comma_ ELLEIPO handles and coordinations with ng12 conjuncts if_comma_ in every pair of conjuncts_comma_ the major constituents embody the same pattern of co.
E06-1005.txt,1,paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation MT systems. The outputs are combined and a possibly new translation hypothesis can be generated.
E06-1005.txt,2,Similarly to the well established ROVER approach of Fiscus_comma_ 1997 for combining speech recognition hypotheses_comma_ the consensus translation is computed by voting on a confusion network.
E06-1005.txt,3,To create the confusion network_comma_ we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
E06-1005.txt,4,The context of a wholedocumentoftranslationsratherthan a single sentence is taken into account to produce the alignment.
E06-1005.txt,5,The proposed alignment and voting approach was evaluated on several machine translation tasks_comma_ including a large vocabulary task.
E06-1005.txt,6,The method was also tested in the framework of multi source and speech translation.
E06-1005.txt,7,On all tasks and conditions_comma_ we achieved significant improvements in translation quality_comma_ increasing e.g. the BLEU score by as much as 15 relative.
E06-1005.txt,9,In this work we describe a novel technique for computing a consensus translation from the outputs of multiple machine translation systems.
E06-1005.txt,10,Combining outputs from different systems was shown to be quite successful in automatic speech recognition ASR .
E06-1005.txt,11,Voting schemes like the ROVER approach of Fiscus_comma_ 1997 use edit distance alignment and time information to create confusion networks from the output of several ASR systems.
E06-1005.txt,12,Some research on multi engine machine translation has also been performed in recent years.
E06-1005.txt,13,The most straightforward approaches simply select_comma_ for each sentence_comma_ one of the provided hypotheses.
E06-1005.txt,14,The selection is made based on the scores of translation_comma_ language_comma_ and other models Nomoto_comma_ 2004 Paul et al._comma_ 2005 .
E06-1005.txt,15,Other approaches combine lattices or N best lists from several different MT systems Frederking and Nirenburg_comma_ 1994 .
E06-1005.txt,16,To be successful_comma_ such approaches require compatible lattices and comparable scores of the word hypotheses in the lattices.
E06-1005.txt,17,However_comma_ the scores of most statistical machine translation SMT systems are not normalized and therefore not directly comparable.
E06-1005.txt,18,For some other MT systems e.g. knowledge based systems _comma_ the lattices and or scores of hypotheses may not be even available.
E06-1005.txt,19,Bangalore et al._comma_ 2001 used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses.
E06-1005.txt,20,This algorithm produces monotone alignments only i.e. allows insertion_comma_ deletion_comma_ and substitution of words it is not able to align translation hypotheses with significantly different word order.
E06-1005.txt,21,Jayaraman and Lavie_comma_ 2005 try to overcome this problem.
E06-1005.txt,22,They introduce a method that allows non monotone alignments of words in different translation hypotheses for the same sentence.
E06-1005.txt,23,However_comma_ this approach uses many heuristicsandisbasedonthealignmentthatisperformed to calculate a specific MT error measure the performance improvements are reported only in terms of this measure.
E06-1005.txt,24,33 Here_comma_ we propose an alignment procedure that explicitly models reordering of words in the hypotheses.
E06-1005.txt,25,In contrast to existing approaches_comma_ the context of the whole document rather than a single sentence is considered in this iterative_comma_ unsupervised procedure_comma_ yielding a more reliable alignment.
E06-1005.txt,26,Based on the alignment_comma_ we construct a confusion network from the possibly reordered translation hypotheses_comma_ similarly to the approach of Bangalore et al._comma_ 2001 .
E06-1005.txt,27,Using global system probabilities and other statistical models_comma_ the voting procedure selects the best consensus hypothesis from the confusion network.
E06-1005.txt,28,This consensus translation may be different from the original translations.
E06-1005.txt,29,This paper is organized as follows.
E06-1005.txt,30,In Section 2_comma_ we will describe the computation of consensus translations with our approach.
E06-1005.txt,31,In particular_comma_ we will present details of the enhanced alignment and reordering procedure.
E06-1005.txt,32,A large set of experimental results on several machine translation tasks is presented in Section 3_comma_ which is followed by a summary.
E06-1005.txt,33,2 Description of the Algorithm The proposed approach takes advantage of multiple translations for a whole test corpus to compute a consensus translation for each sentence in this corpus.
E06-1005.txt,34,Given a single source sentence in the test corpus_comma_ we combine M translation hypotheses E1_comma_..._comma_EM from M MT engines.
E06-1005.txt,35,We first choose one of the hypotheses Em as the primary one.
E06-1005.txt,36,We consider this primary hypothesis to have the correct word order.
E06-1005.txt,37,We then align and reorder the other_comma_ secondary hypotheses En n 1_comma_..._comma_M n negationslash m to match this word order.
E06-1005.txt,38,Since each hypothesis may have an acceptable word order_comma_ we let every hypothesis play the role of the primary translation once_comma_ and thus align all pairs of hypotheses En_comma_Em n negationslash m.
E06-1005.txt,39,In the following subsections_comma_ we will explain the word alignment procedure_comma_ the reordering approach_comma_ and the construction of confusion networks.
E06-1005.txt,40,2.1 Statistical Alignment Thewordalignmentisperformedinanalogytothe training procedure in SMT.
E06-1005.txt,41,The difference is that the two sentences that have to be aligned are in the same language.
E06-1005.txt,42,We consider the conditional probability Pr En Em of the event that_comma_ given Em_comma_ another hypothesis En is generated from the Em.
E06-1005.txt,43,Then_comma_ the alignment between the two hypotheses is introduced as a hidden variable Pr En Em summationdisplay A Pr En_comma_A Em Thisprobabilityisthendecomposedintothealignment probabilityPr A Em and the lexicon probability Pr En A_comma_Em Pr En_comma_A Em Pr A Em Pr En A_comma_Em As in statistical machine translation_comma_ we make modelling assumptions.
E06-1005.txt,44,We use the IBM Model 1 Brown et al._comma_ 1993 uniform distribution and the Hidden Markov Model HMM_comma_ first order dependency_comma_ Vogel et al._comma_ 1996 to estimate the alignment model.
E06-1005.txt,45,The lexicon probability of a sentence pair is modelled as a product of single word based probabilities of the aligned words.
E06-1005.txt,46,The training corpus for alignment is created from a test corpus of N sentences usually a few hundred translated by all of the involved MT engines.
E06-1005.txt,47,However_comma_ the effective size of the training corpus is larger than N_comma_ since all pairs of different hypotheses have to be aligned.
E06-1005.txt,48,Thus_comma_ the effective size of the training corpus is M M 1 N.
E06-1005.txt,49,The single word based lexicon probabilities p en em are initialized with normalized lexicon counts collected over the sentence pairs En_comma_Em on this corpus.
E06-1005.txt,50,Since all of the hypotheses are in the same language_comma_ we count co occurring equal words_comma_ i.e. if en is the same word as em.
E06-1005.txt,51,In addition_comma_ we add a fraction of a count for words with identical prefixes.
E06-1005.txt,52,The initialization could be furthermore improved by using word classes_comma_ part of speech tags_comma_ or a list of synonyms.
E06-1005.txt,53,The model parameters are trained iteratively in an unsupervised manner with the EM algorithm using the GIZA toolkit Och and Ney_comma_ 2003 .
E06-1005.txt,54,The training is performed in the directions En Em and Em En.
E06-1005.txt,55,The updated lexicon tables from the two directions are interpolated after each iteration.
E06-1005.txt,56,The final alignments are determined using cost matrices defined by the state occupation probabilities of the trained HMM Matusov et al._comma_ 2004 .
E06-1005.txt,57,The alignments are used for reordering each secondary translation En and for computing the confusion network.
E06-1005.txt,58,34 Figure1 Exampleofcreatingaconfusionnetworkfrommonotoneone to onewordalignments denoted with symbol .
E06-1005.txt,59,The words of the primary hypothesis are printed in bold.
E06-1005.txt,60,The symbol denotes a null alignment or an arc in the corresponding part of the confusion network.
E06-1005.txt,61,1. would you like coffee or tea original 2. would you have tea or coffee hypotheses 3. would you like your coffee or 4.
E06-1005.txt,62,I have some coffee tea would you like alignment would would you you have like coffee coffee or or tea tea and would would you you like like your coffee coffee or or tea reordering I would would you you like like have some coffee coffee or tea tea would you like  coffee or tea confusion would you have  coffee or tea network would you like your coffee or I would you like have some coffee tea 2.2 Word Reordering The alignment between En and the primary hypothesis Em used for reordering is computed as a function of words in the secondary translation En with minimal costs_comma_ with an additional constraint that identical words in En can not be all aligned to the same word in Em.
E06-1005.txt,63,This constraint is necessary toavoidthatreorderedhypotheseswithe.g. multipleconsecutivearticles the wouldbeproducedif fewerarticleswereusedintheprimaryhypothesis.
E06-1005.txt,64,The new word order for En is obtained through sortingthewordsinEn bytheindicesofthewords in Em to which they are aligned.
E06-1005.txt,65,Two words in En which are aligned to the same word in Em are kept in the original order.
E06-1005.txt,66,After reordering each secondary hypothesis En_comma_ we determine M 1 monotone one to one alignments betweenEm and En_comma_n 1_comma_..._comma_M n negationslash m.
E06-1005.txt,67,In case of many toone connections of words inEn to a single wordin Em_comma_ we only keep the connection with the lowest alignment costs.
E06-1005.txt,68,The one to one alignments are convenient for constructing a confusion network in the next step of the algorithm.
E06-1005.txt,69,2.3 Building Confusion Networks GiventheM 1monotoneone to onealignments_comma_ the transformation to a confusion network as described by Bangalore et al._comma_ 2001 is straightforward.
E06-1005.txt,70,It is explained by the example in Figure 1.
E06-1005.txt,71,Here_comma_ the original 4 hypotheses are shown_comma_ followedbythealignmentofthereorderedsecondary hypotheses2 4withtheprimaryhypothesis1.
E06-1005.txt,72,The alignment is shown with the symbol_comma_ and the words of the primary hypothesis are to the right of this symbol.
E06-1005.txt,73,The symbol denotes a null alignment or an arc in the corresponding part of the confusion network_comma_ which is shown at the bottom of the figure.
E06-1005.txt,74,Note that the word have in translation 2 is aligned to the word like in translation 1.
E06-1005.txt,75,This alignment is acceptable considering the two translations alone.
E06-1005.txt,76,However_comma_ given the presence of the word have in translation 4_comma_ this is not the best alignment.
E06-1005.txt,77,Yet the problems of this type can in part be solved by the proposed approach_comma_ since every translation once plays the role of the primary translation.
E06-1005.txt,78,For each sentence_comma_ we obtain a total of M confusion networks and unite them in a single lattice.
E06-1005.txt,79,The consensus translation can be chosen among different alignment and reordering paths in this lattice.
E06-1005.txt,80,The voting on the union of confusion networks is straightforward and analogous to the ROVER system.
E06-1005.txt,81,We sum up the probabilities of the arcs which are labeled with the same word and have the same start and the same end state.
E06-1005.txt,82,These probabilities are the global probabilities assigned to the different MT systems.
E06-1005.txt,83,They are manually adjusted based on the performance of the involvedMTsystemsonaheld outdevelopmentset.
E06-1005.txt,84,In general_comma_ a better consensus translation can be produced if the words hypothesized by a betterperforming system get a higher probability.
E06-1005.txt,85,Additional scores like word confidence measures can be used to score the arcs in the lattice.
E06-1005.txt,86,2.4 Extracting Consensus Translation In the final step_comma_ the consensus translation is extracted as the best path from the union of confu35 Table 1 Corpus statistics of the test corpora.
E06-1005.txt,87,BTEC IWSLT04 BTEC CSTAR03 EPPS TC STAR Chinese Japanese English Italian English Spanish English Sentences 500 506 1073 Running Words 3681 4131 3092 3176 2942 2889 18896 18289 Distinct Words 893 979 1125 1134 1028 942 3302 3742 sion networks.
E06-1005.txt,88,Note that the extracted consensus translation can be different from the original M translations.
E06-1005.txt,89,Alternatively_comma_ the N best hypotheses can be extracted for rescoring by additional models.
E06-1005.txt,90,We performed experiments with both approaches.
E06-1005.txt,91,Since M confusion networks are used_comma_ the lattice may contain two best paths with the same probability_comma_ the same words_comma_ but different word order.
E06-1005.txt,92,We extended the algorithm to favor more well formed word sequences.
E06-1005.txt,93,We assign a higher probability to each arc of the primary unreordered translation in each of the M confusion networks.
E06-1005.txt,94,Experimentally_comma_ this extension improved translation fluency on some tasks.
E06-1005.txt,95,3 Experimental Results 3.1 Corpus Statistics The alignment and voting algorithm was evaluated on both small and large vocabulary tasks.
E06-1005.txt,96,Initial experiments were performed on the IWSLT 2004 Chinese English and Japanese English tasks Akiba et al._comma_ 2004 .
E06-1005.txt,97,The data for these tasks come from the Basic Travel Expression corpus BTEC _comma_ consisting of tourism related sentences.
E06-1005.txt,98,We combined the outputs of several MT systems that had officially been submitted to the IWSLT 2004 evaluation.
E06-1005.txt,99,Each system had used 20K sentence pairs 180K running words from the BTEC corpus for training.
E06-1005.txt,100,Experiments with translations of automatically recognized speech were performed on the BTEC Italian English task Federico_comma_ 2003 .
E06-1005.txt,101,Here_comma_ the involved MT systems had used about 60K sentence pairs 420K running words for training.
E06-1005.txt,102,Finally_comma_ wealsocomputedconsensustranslation from some of the submissions to the TC STAR 2005 evaluation campaign TC STAR_comma_ 2005 .
E06-1005.txt,103,The TC STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions EPPS .
E06-1005.txt,104,In our experiments_comma_ we used the translations from SpanTable 2 Improved translation results for the consensus translation computed from 5 translation outputs on the Chinese English IWSLT04 task.
E06-1005.txt,105,BTEC WER PER BLEU Chinese English    worst single system 04 58.3 46.6 34.6 best single system 04 54.6 42.6 40.3 consensus of 5 systems from 2004 47.8 38.0 46.2 system  in 2005 50.3 40.5 45.1 ish to English.
E06-1005.txt,106,The MT engines for this task had been trained on 1.2M sentence pairs 32M running words .
E06-1005.txt,107,Table 1 gives an overview of the test corpora_comma_ on which the enhanced hypotheses alignment was computed_comma_ and for which the consensus translations were determined.
E06-1005.txt,108,The official IWSLT04 test corpus was used for the IWSLT 04 tasks the CSTAR03 test corpus was used for the speech translation task.
E06-1005.txt,109,The March 2005 test corpus of the TC STAR evaluation verbatim condition was used for the EPPS task.
E06-1005.txt,110,In Table 1_comma_ the number of running words in English is the average number of running words in the hypotheses_comma_ from which the consensus translation was computed the vocabulary of English is the merged vocabulary of these hypotheses.
E06-1005.txt,111,For the BTEC IWSLT04 corpus_comma_ the statistics for English is given for the experiments described in Sections 3.3 and 3.5_comma_ respectively.
E06-1005.txt,112,3.2 Evaluation Criteria Well established objective evaluation measures like the word error rate WER _comma_ positionindependent word error rate PER _comma_ and the BLEU score Papineni et al._comma_ 2002 were used to assess the translation quality.
E06-1005.txt,113,All measures were computed with respect to multiple reference translations.
E06-1005.txt,114,The evaluation as well as the alignment training was case insensitive_comma_ without considering the punctuation marks.
E06-1005.txt,115,36 3.3 Chinese English Translation Different applications of the proposed combination method have been evaluated.
E06-1005.txt,116,First_comma_ we focused on combining different MT systems which have the same source and target language.
E06-1005.txt,117,The initial experiments were performed on the BTEC Chinese English task.
E06-1005.txt,118,We combined translations produced by 5 different MT systems.
E06-1005.txt,119,Table 2 shows the performance of the best and the worst of these systems in terms of the BLEU score.
E06-1005.txt,120,The results for the consensus translation show a dramatic improvement in translation quality.
E06-1005.txt,121,The word error rate is reduced e.g. from 54.6 to 47.8 .
E06-1005.txt,122,The researchgroupwhichhadsubmittedthebesttranslation in 2004 translated the same test set a year later with an improved system.
E06-1005.txt,123,We compared the consensus translation with this new translation last line of Table 2 .
E06-1005.txt,124,It can be observed that the consensus translation based on the MT systems developed in 2004 is still superior to this 2005 single system translation in terms of all error measures.
E06-1005.txt,125,We also checked how many sentences in the consensus translation of the test corpus are different from the 5 original translations.
E06-1005.txt,126,185 out of 500 sentences 37 had new translations.
E06-1005.txt,127,Computing the error measures on these sentences only_comma_ we observed significant improvements in WER and PER and a small improvement in BLEU with respect to the original translations.
E06-1005.txt,128,Thus_comma_ the quality of previously unseen consensus translations as generated from the original translations is acceptable.
E06-1005.txt,129,In this experiment_comma_ the global system probabilities for scoring the confusion networks were tuned manually on a development set.
E06-1005.txt,130,The distribution was 0.35_comma_0.25_comma_0.2_comma_0.1_comma_0.1_comma_ with 0.35 for the words of the best single system and 0.1 for the words of the worst single system.
E06-1005.txt,131,We observed that the consensus translation did not change significantly with small perturbations of these values.
E06-1005.txt,132,However_comma_ the relation between the probabilities is very important for good performance.
E06-1005.txt,133,No improvement can be achieved with a uniform probability distribution it is necessary to penalize translations of low quality.
E06-1005.txt,134,3.4 Spanish English Translation The improvements in translation quality are also significant on the TC STAR EPPS SpanishEnglish task.
E06-1005.txt,135,Here_comma_ we combined four different systems which performed best in the TC STAR Table 3 Improved translation results for the consensus translation computed from 4 translation outputs on the Spanish English TC STAR task.
E06-1005.txt,136,EPPS WER PER BLEU Spanish English    worst single system 49.1 38.2 39.6 best single system 41.0 30.2 47.7 consensus of 4 systems 39.1 29.1 49.3 rescoring 38.8 29.0 50.7 2005 evaluation_comma_ see Table 3.
E06-1005.txt,137,Compared to the best performing single system_comma_ the consensus hypothesis reduces the WER from 41.0 to 39.1 .
E06-1005.txt,138,This result is further improved by rescoring the N best lists derived from the confusion networks N 1000 .
E06-1005.txt,139,For rescoring_comma_ a word penalty feature_comma_ the IBM Model 1_comma_ and a 4 gram target language model were included.
E06-1005.txt,140,The linear interpolation weights of these models and the score from the confusion network were optimized on a separate development set with respect to word error rate.
E06-1005.txt,141,Table 4 gives examples of improved translation quality by using the consensus translation as derived from the rescored N best lists.
E06-1005.txt,142,3.5 Multi source Translation In the IWSLT 2004 evaluation_comma_ the English reference translations for the Chinese English and Japanese English test corpora were the same_comma_ except for a permutation of the sentences.
E06-1005.txt,143,Thus_comma_ we could combine MT systems which have different source and the same target language_comma_ performing multi source machine translation described e.g. by Och and Ney_comma_ 2001 .
E06-1005.txt,144,We combined two Japanese English and two Chinese English systems.
E06-1005.txt,145,The best performing system was a JapaneseEnglish system with a BLEU score of 44.7 _comma_ see Table 5.
E06-1005.txt,146,By computing the consensus translation_comma_ we improved this score to 49.6 _comma_ and also significantly reduced the error rates.
E06-1005.txt,147,To investigate the potential of the proposed approach_comma_ we generated the N best lists N 1000 ofconsensustranslations.
E06-1005.txt,148,Then_comma_foreachsentence_comma_ we selected the hypothesis in the N best list with the lowest word error rate with respect to the multiple reference translations for the sentence.
E06-1005.txt,149,We then evaluated the quality of these oracle translations with all error measures.
E06-1005.txt,150,In a contrastive experiment_comma_ for each sentence we simply selected 37 Table4 ExamplesofimprovedtranslationqualitywiththeconsensustranslationsontheSpanish English TC STAR EPPS task case insensitive output . best system I also authorised to committees to certain reports consensus I also authorised to certain committees to draw up reports reference I have also authorised certain committees to prepare reports best system human rights which therefore has fought the european union consensus human rights which the european union has fought reference human rights for which the european union has fought so hard best system we of the following the agenda consensus moving on to the next point on the agenda reference we go on to the next point of the agenda Table 5 Multi source translation improvements in translation quality when computing consensus translation using the output of two ChineseEnglish and two Japanese English systems on the IWSLT04 task.
E06-1005.txt,151,BTEC Chinese English WER PER BLEU Japanese English    worst single system 58.0 41.8 39.5 best single system 51.3 38.6 44.7 consensus of 4 systems 44.9 33.9 49.6 Table 6 Consensus based combination vs. selection potential for improvement multi source translation_comma_ selection combination of 4 translation outputs .
E06-1005.txt,152,BTEC Chinese English WER PER BLEU Japanese English    best single system 51.3 38.6 44.7 oracle selection 33.3 29.3 59.2 oracle consensus 1000 best list 27.0 22.8 64.2 the translation with the lowestWER from the original 4 MT system outputs.
E06-1005.txt,153,Table 6 shows that the potential for improvement is significantly larger for the consensus based combination of translation outputs than for simple selection of the best translation1.
E06-1005.txt,154,In our future work_comma_ we plan to improve the scoring of hypotheses in the confusion networks to explore this large potential.
E06-1005.txt,155,3.6 Speech Translation Some state of the art speech translation systems can translate either the first best recognition hy1Similar oracle results were observed on other tasks. potheses or the word lattices of an ASR system.
E06-1005.txt,156,It has been previously shown that word lattice input generally improves translation quality.
E06-1005.txt,157,In practice_comma_ however_comma_ the translation system may choose_comma_ for some sentences_comma_ the paths in the lattice with many recognition errors and thus produce inferior translations.
E06-1005.txt,158,These translations can be improved if we compute a consensus translation from the output ofatleasttwodifferentspeechtranslationsystems.
E06-1005.txt,159,From each system_comma_ we take the translation of the single best ASR output_comma_ and the translation of the ASR word lattice.
E06-1005.txt,160,Two different statistical MT systems capable of translating ASR word lattices have been compared by Matusov and Ney_comma_ 2005 .
E06-1005.txt,161,Both systems produced translations of better quality on the BTEC Italian English speech translation task when using lattices instead of single best ASR output.
E06-1005.txt,162,We obtained the output of each of the two systems under each of these translation scenarios on the CSTAR03 test corpus.
E06-1005.txt,163,The first best recognition worderrorrateonthiscorpusis22.3 .
E06-1005.txt,164,Theobjective error measures for the 4 translation hypotheses are given in Table 7.
E06-1005.txt,165,We then computed a consensus translation of the 4 outputs with the proposed method.
E06-1005.txt,166,The better performing word lattice translations were given higher system probabilities.
E06-1005.txt,167,With the consensus hypothesis_comma_ the word error rate went down from 29.5 to 28.5 .
E06-1005.txt,168,Thus_comma_ the negative effect of recognition errors on the translation quality was further reduced.
E06-1005.txt,169,4 Conclusions In this work_comma_ we proposed a novel_comma_ theoretically well founded procedure for computing a possibly new consensus translation from the outputs of multiple MT systems.
E06-1005.txt,170,In summary_comma_ the main con38 Table 7 Improvements in translation quality on the BTEC Italian English task through computing consensus translations from the output of two speech translation systems with different types of source language input. system input WER PER BLEU    2 correct text 23.3 19.3 65.6 1 a single best 32.8 28.6 53.9 b lattice 30.7 26.7 55.9 2 c single best 31.6 27.5 54.7 d lattice 29.5 26.1 58.2 consensus a d 28.5 25.0 58.9 tributions of this work compared to previous approaches are as follows The words of the original translation hypotheses are aligned in order to create a confusion network.
E06-1005.txt,171,The alignment procedure explicitly models word reordering. A test corpus of translations generated by each of the systems is used for the unsupervised statistical alignment training.
E06-1005.txt,172,Thus_comma_ the decision on how to align two translations of a sentence takes the whole document context into account. Large and significant gains in translation quality were obtained on various translation tasks and conditions. A significant improvement of translation quality was achieved in a multi source translation scenario.
E06-1005.txt,173,Here_comma_ we combined the output of MT systems which have different source and the same target language. The proposed method can be effectively applied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.
E06-1005.txt,174,An important feature of a real life application of the proposed alignment technique is that the lexicon and alignment probabilities can be updated with each translated sentence and or text.
E06-1005.txt,175,Thus_comma_ thecorrespondencebetweenwordsindifferenthypotheses and_comma_ consequently_comma_ the consensus translation can be improved overtime.
E06-1005.txt,176,5 Acknowledgement This paper is based upon work supported by the Defense Advanced Research Projects Agency DARPA under Contract No. HR0011 06 C0023.
E06-1005.txt,177,This work was also in part funded by the European Union under the integrated project TCSTAR Technology and Corpora for Speech to Speech Translation IST 2002 FP6 506738 . .
E06-1003.txt,1,present a weakly supervised approach to automatic Ontology Population from text and compare it with other two unsupervised approaches. In our experiments we populate a part of our ontology of Named Entities.
E06-1003.txt,2,We considered two high level categories geographical locations and person names and ten sub classes for each category.
E06-1003.txt,3,For each sub class_comma_ from a list of training examples and a syntactically parsed corpus_comma_ we automatically learn a syntactic model a set of weighted syntactic features_comma_ i.e. words which typically co occur in certain syntactic positions with the members of that class.
E06-1003.txt,4,The modelisthenusedtoclassifytheunknown Named Entities in the test set.
E06-1003.txt,5,The method is weakly supervised_comma_ since no annotated corpus is used in the learning process.
E06-1003.txt,6,We achieved promising results_comma_ i.e.
E06-1003.txt,7,65 accuracy_comma_ outperforming significantly previous unsupervised approaches.
E06-1003.txt,9,Automatic Ontology Population OP from texts has recently emerged as a new field of application for knowledge acquisition techniques see_comma_ among others_comma_ Buitelaar et al._comma_ 2005 .
E06-1003.txt,10,Although there is no a univocally accepted definition for the OP task_comma_ a useful approximation has been suggested Bontcheva and Cunningham_comma_ 2003 as Ontology Driven Information Extraction_comma_ where_comma_ in place of atemplatetobefilled_comma_thegoalofthetaskistheextraction and classification of instances of concepts and relations defined in a Ontology.
E06-1003.txt,11,The task has been approached in a variety of similar perspectives_comma_ including term clustering e.g. Lin_comma_ 1998a and Almuhareb and Poesio_comma_ 2004 and term categorization e.g. Avancini et al._comma_ 2003 .
E06-1003.txt,12,A rather different task is Ontology Learning OL _comma_ where new concepts and relations are supposed to be acquired_comma_ with the consequence of changing the definition of the Ontology itself see_comma_ for instance_comma_ Velardi et al._comma_ 2005 .
E06-1003.txt,13,In this paper OP is defined in the following scenario.
E06-1003.txt,14,Given a set of terms T t1_comma_t2_comma_..._comma_tn_comma_ a document collection D_comma_ where terms in T are supposed to appear_comma_ and a set of predefined classes C c1_comma_c2_comma_..._comma_cm denoting concepts in an Ontology_comma_ each term ti has to be assigned to the proper class in C.
E06-1003.txt,15,For the purposes of the experiments presented in this paper we assume that i classes in C are mutually disjoint and ii each term is assigned to just one class.
E06-1003.txt,16,As we have defined it_comma_ OP shows a strong similarity with Named Entity Recognition and Classification NERC .
E06-1003.txt,17,However_comma_ a major difference is that in NERC each occurrences of a recognized term has to be classified separately_comma_ while in OP it is the term_comma_ independently of the context in which it appears_comma_ that has to be classified.
E06-1003.txt,18,While Information Extraction_comma_ and NERC in particular_comma_ have been addressed prevalently by means of supervised approaches_comma_ Ontology Populationistypicallyattackedinanunsupervisedway.
E06-1003.txt,19,As many authors have pointed out e.g. Cimiano and V olker_comma_ 2005 _comma_ the main motivation is the fact that in OP the set of classes is usually larger and more fine grained than in NERC where the typical set includes Person_comma_ Location_comma_ Organization_comma_ GPE_comma_ and a Miscellanea class for all other kind of entities .
E06-1003.txt,20,In addition_comma_ by definition_comma_ the set of classes in C changes as a new ontology is considered_comma_ making the creation of annotated data almost impossible practically.
E06-1003.txt,21,17 According with the demand for weakly supervised approaches to OP_comma_ we propose a method_comma_ called Class Example_comma_ which learns a classification model from a set of classified terms_comma_ exploiting lexico syntactic features.
E06-1003.txt,22,Unlike most of theapproacheswhichconsiderpairwisesimilarity between terms Cimiano and V olker_comma_ 2005 Lin_comma_ 1998a _comma_ the Class Example method considers the similarity between a term ti and a set of training examples which represent a certain class.
E06-1003.txt,23,This results in a great number of class features and opens the possibility to exploit more statistical data_comma_ such asthefrequencyofappearance ofaclassfeature in different training terms.
E06-1003.txt,24,In order to show the effectiveness of the ClassExample approach_comma_ it has been compared against twodifferentapproaches i aClass Patternunsupervised approach_comma_ in the style of Hearst_comma_ 1998 ii an unsupervised approach that considers the word of the class as a pivot word for acquiring relevant contexts for the class we refer to this methodasClass Word .
E06-1003.txt,25,Resultsofthecomparison show that the Class Example method outperforms significantly the other two methods_comma_ making it appealing even considering the need of supervision.
E06-1003.txt,26,Although the Class Example method we propose is applicable in general_comma_ in this paper we show its usefulness when applied to terms denoting Named Entities.
E06-1003.txt,27,The motivation behind this choice is the practical value of Named Entity classifications_comma_ as_comma_ for instance_comma_ in applications such as Questions Answering and Information Extraction.
E06-1003.txt,28,Moreover_comma_ some Named Entity classes_comma_ including names of writers_comma_ athletes and organizations_comma_ dynamically change over the time_comma_ which makes it impossible to capture them in a static Ontology.
E06-1003.txt,29,The rest of the paper is structured as follows.
E06-1003.txt,30,Section 2 describes the state of the art methods in Ontology Population.
E06-1003.txt,31,Section 3 presents the three approaches to the task we have compared.
E06-1003.txt,32,Section 4 introduces Syntactic Network_comma_ a formalism used for the representation of syntactic information and exploited in both the Class Word and the ClassExample approaches.
E06-1003.txt,33,Section 5 reports on the experimental settings_comma_ results obtained_comma_ and discusses the three approaches.
E06-1003.txt,34,Section 6 concludes the paper and suggests directions for future work.
E06-1003.txt,35,2 Related Work There are two main paradigms distinguishing Ontology Population approaches.
E06-1003.txt,36,In the first one Ontology Population is performed using patterns Hearst_comma_ 1998 or relying on the structure of terms Velardi et al._comma_ 2005 .
E06-1003.txt,37,In the second paradigm the task is addressed using contextual features Cimiano and V olker_comma_ 2005 .
E06-1003.txt,38,Pattern based approaches search for phrases which explicitly show that there is an is a relation between two words_comma_ e.g. the ant is an insect or ants and other insects .
E06-1003.txt,39,However_comma_ such phrases do not appear frequently in a text corpus.
E06-1003.txt,40,Forthisreason_comma_ someapproachesusetheWeb Schlobach et al._comma_ 2004 .
E06-1003.txt,41,Velardi et al._comma_ 2005 experimented several head matching heuristics according to which if a term1 is in the head of term2_comma_ then there is an is a relation between them For example Christmas tree is a kind of tree .
E06-1003.txt,42,Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear.
E06-1003.txt,43,Contextual features may be superficial Fleischman and Hovy_comma_ 2002 or syntactic Lin_comma_ 1998a _comma_ Almuhareb and Poesio_comma_ 2004 .
E06-1003.txt,44,Comparative evaluation in Cimiano and V olker_comma_ 2005 shows that syntactic features lead to better performance.
E06-1003.txt,45,Feature weights can be calculated either by Machine Learning algorithms Fleischman and Hovy_comma_ 2002 or by statistical measures_comma_ like Point Wise Mutual Information or the Jaccard coefficient Lin_comma_ 1998a .
E06-1003.txt,46,A hybrid approach using both pattern based_comma_ term structure_comma_ and contextual feature methods is presented in Cimiano et al._comma_ 2005 .
E06-1003.txt,47,State of the art approaches may be divided in two classes_comma_ according to different use of training data Unsupervised approaches see Cimiano et al._comma_ 2005 for details and supervised approaches which use manually tagged training data_comma_ e.g.
E06-1003.txt,48,Fleischman and Hovy_comma_ 2002 .
E06-1003.txt,49,While stateof the art unsupervised methods have low performance_comma_ supervised approaches reach higher accuracy_comma_ but require the manual construction of a training set_comma_ which impedes them from large scale applications.
E06-1003.txt,50,3 Weakly supervised approaches for Ontology Population In this Section we present three Ontology Population approaches.
E06-1003.txt,51,Two of them are unsupervised 18 i a pattern based approach described in Hearst_comma_ 1998 _comma_ which we refer to as Class Pattern and ii a feature similarity method reported in Cimiano and V olker_comma_ 2005 to which we will refer as ClassWord.
E06-1003.txt,52,Finally_comma_ we describe a new weakly supervised approach for ontology population which accepts as a training data lists of instances for each class under consideration.
E06-1003.txt,53,This method we call Class Example.
E06-1003.txt,54,3.1 Class Pattern approach This approach was described first in Hearst_comma_ 1998 .
E06-1003.txt,55,The main idea is that if a term t belongs to a class c_comma_ then in a text corpus we may expect the occurrence of phrases like such c as t_comma_.... In our experiments for ontology population we used the patterns described in the Hearst s paper plus the pattern t is a the c 1. t is a the c 2. such c as t 3. such c as NP_comma_ _comma_ and or t 4. t _comma_NP and or other c 5.
E06-1003.txt,56,c_comma_ especially including NP_comma_ t Foreachinstancefromthetestsettandforeach conceptcweinstantiatedthepatternsandsearched with them in the corpus.
E06-1003.txt,57,If a pattern which is instantiated with a concept c and a term t appears in the corpus_comma_ then we assume the t belongs to c.
E06-1003.txt,58,For example_comma_ if the term to be classified is Etna and the concept is mountain _comma_ one of the instantiated patterns will be mountains such as Etna if this pattern is found in the text_comma_ then Etna is considered to be a mountain .
E06-1003.txt,59,If the algorithm assigns a term to several categories_comma_ we choose the one which co occurs most often with the term.
E06-1003.txt,60,3.2 Class Word approach Cimiano and V olker_comma_ 2005 describes an unsupervised approach for ontology population based on vector feature similarity between each concept c and a term to be classified t.
E06-1003.txt,61,For example_comma_ in order to conclude how much Etna is an appropriate instance of the class mountain _comma_ this method finds the feature vector similarity between the word Etna and the word mountain .
E06-1003.txt,62,Each instance from the test set T is assigned to one of the classes in the set C.
E06-1003.txt,63,Features are collected from Corpus and the classification algorithm on classify T_comma_ C_comma_ Corpus foreach t in T do vt getContextV ector t_comma_Corpus foreach c in C do vc getContextV ector c_comma_Corpus foreach t in T do classes t argmaxc Csim vt_comma_vc return classes end classify Figure 1 Unsupervised algorithm for Ontology Population. figure 1 is applied.
E06-1003.txt,64,The problem with this approach is that the context distribution of a name e.g. Etna is sometimes different than the context distribution of the class word e.g. mountain .
E06-1003.txt,65,Moreover_comma_ a single word provides a limited quantity of contextual data.
E06-1003.txt,66,In this algorithm the context vectors vt and vc are feature vectors whose elements represent weighted context features from Corpus of the term t e.g. Etna or the concept word c e.g. mountain .
E06-1003.txt,67,Cimiano and V olker evaluate different context features and prove that syntactic features work best.
E06-1003.txt,68,Therefore_comma_ in our experimental settings we considered only such features extracted from a corpus parsed with a dependency parser.
E06-1003.txt,69,Unlike the original approach which relies on pseudo syntactic features_comma_ we used features extracted from dependency parse trees.
E06-1003.txt,70,Moreover_comma_ we used virtually all the words connected syntactically to a term_comma_ not only the modifiers.
E06-1003.txt,71,A syntactic feature is a pair word_comma_ syntactic relation Lin_comma_ 1998a .
E06-1003.txt,72,We use two feature types First order features_comma_ which are directly connected to the training or test examples in the dependency parse trees of Corpus second order features_comma_ which are connected to the training or test instances indirectly byskipping one word the verb in the dependency tree.
E06-1003.txt,73,As an example_comma_ let s consider two sentences Edison invented the phonograph and Edison created the phonograph .
E06-1003.txt,74,If Edison is a name to be classified_comma_ then two first order features of this name exist  invent _comma_ subject of and create _comma_ subject of .
E06-1003.txt,75,One second order feature can be extracted  phonograph _comma_ object of subject it cooccurs two times with the word Edison .
E06-1003.txt,76,In our experiments second order features are considered only those words which are governed by the same verb whose subject is the name which is a training 19 or test instance in this example Edison .
E06-1003.txt,77,3.3 Weakly Supervised Class Example Approach The approach we put forward here uses the same processing stages as the one presented in Figure 1 and relies on syntactic features extracted from a corpus.
E06-1003.txt,78,However_comma_ the Class Example algorithm receives as an additional input parameter the sets of training examples for each class c C.
E06-1003.txt,79,These training sets are simple lists of instances i.e. terms denoting Named Entities _comma_ without context_comma_ and can be acquired automatically or semi automatically from an existing ontology or gazetteer.
E06-1003.txt,80,To facilitate their acquisition_comma_ the Class Example approach imposes no restrictions to the training examples they can be ambiguous and have different frequencies.
E06-1003.txt,81,However_comma_ they have to appear in Corpus in our experimental settings at least twice .
E06-1003.txt,82,For example_comma_ for the class mountain training examples are Everest _comma_ Mauna Loa _comma_ etc. The algorithm learns from each training set Train c asinglefeaturevectorvc_comma_ calledthesyntactic model of the class.
E06-1003.txt,83,Therefore_comma_ in our algorithm_comma_ the statement vc getContextVector c_comma_Corpus in Figure 1 is substituted with vc getSyntacticModel Train c _comma_Corpus .
E06-1003.txt,84,For each class c_comma_ a set of syntactic features F c are collected by finding the union of the features extracted from each occurrence in the corpus of each training instance in Train c .
E06-1003.txt,85,Next_comma_ the feature vector vc is constructed If a feature is not present in F c _comma_ then its corresponding coordinate in vc has value 0 otherwise_comma_ it has a value equal to the feature weight.
E06-1003.txt,86,The weight of a feature fc F c is calculated in three steps 1.
E06-1003.txt,87,First_comma_ the co occurrence of fc with the training set is calculated weight1 fc summationdisplay t Train c .log P fc_comma_t P f c .P t where P fc_comma_t is the probability that feature fc co occurs with t_comma_ P fc and P t are the probabilities that fc and t appear in the corpus_comma_  14 for syntactic features with lexical element noun and  1 for all the other syntactic features.
E06-1003.txt,88,The parameter reflects thelinguisticintuitionthatnounsaremoreinformative than verbs and adjectives which in most cases represent generic predicates.
E06-1003.txt,89,The values of were automatically learned from the training data.
E06-1003.txt,91,We normalize the feature weights_comma_ since we observed that they vary a lot between different classes for each class c we find the feature with maximal weight and denote its weight with mxW c _comma_ mxW c maxfc F c weight1 fc Next_comma_ the weight of each feature fc F c is normalized by dividing it with mxW c weightN fc weight1 fc mxW c 3.
E06-1003.txt,92,To obtain the final weight of fc_comma_ we divide weightN fc by the number of classes in which this feature appears.
E06-1003.txt,93,This is motivated by the intuition that a feature which appears in the syntactic models of many classes is not a good class predictor. weight fc weightN fc Classes f c where Classes fc is the number of classes for whichfc is present in the syntactic model.
E06-1003.txt,94,As shown in Figure 1_comma_ the classification uses a similarity function sim vt_comma_vc whose arguments are the feature vector of a term vt and the feature vector vc for a class c.
E06-1003.txt,95,We defined the similarity function as the dot product of the two feature vectors sim vt_comma_vc vc.vt.
E06-1003.txt,96,Vectors vt are binary i.e. the feature value is 1 if the feature is present and_comma_ 0 otherwise _comma_ while the features in the syntactic model vectors vc receive weights according to the approach described in this Section.
E06-1003.txt,97,4 Representing Syntactic Information Since both the Class Word and the Class Example methods work with syntactic features_comma_ the main source of information is a syntactically parsed corpus.
E06-1003.txt,98,We parsed about half a gigabyte of a news corpus with MiniPar Lin_comma_ 1998b .
E06-1003.txt,99,It is a statistically based dependency parser which is reported to reach 89 precision and 82 recall on press reportage texts.
E06-1003.txt,100,MiniPar generates syntactic dependency structures directed labeled graphs whose 20 g1 g2 SyntNet g1_comma_g2 loves 1 s d15d15 o d37d37d74d74d74 d74d74d74d74 d74d74d74d74 loves 4 o d47d47 s d15d15 Jane 6 loves 1_comma_4 1_comma_2 4_comma_5 d15d15 4_comma_6 o d47d47 1_comma_3 o d42d42d84d84d84d84d84d84d84d84d84 d84d84d84d84d84d84 d84d84 Jane 6 John 2 Mary 3 John 5 John 2_comma_5 Mary 3 Figure 2 Two syntactic graphs and their Syntactic Network. vertices represent words and the edges between them represent syntactic relations like subject_comma_ object_comma_ modifier_comma_ etc. Examples for two dependency structures g1 and g2_comma_ are shown in Figure 2 They represent the sentences John loves Mary and John loves Jane labels s and o on their edges stand for subject and object respectively.
E06-1003.txt,101,The syntactic structures generated by MiniPar are dendroid tree like _comma_ butstillcyclesappearinsome cases.
E06-1003.txt,102,In order to extract information from the parsed corpus_comma_ we had to choose a model for representing dependency trees which allows to search efficiently for syntactic structures and to calculate their frequencies.
E06-1003.txt,103,Building a classic index at word levelwasnotanoption_comma_sincewehavetosearchfor syntactic structures_comma_ not words.
E06-1003.txt,104,On the other hand_comma_ indexing syntactic relations i.e. word pair and the relation between the words would be useful_comma_ but still does not resolve the problem_comma_ since in many cases we search for more complex structures than a relation between two words for example_comma_ when we have to find which words are syntactically related to a Named Entity composed by two words_comma_ we have to search for syntactic structures which consists of three vertices and two edges.
E06-1003.txt,105,In order to trace efficiently more complex structures in the corpus_comma_ we put forward a model for representation of a set of labeled graphs_comma_ called Syntactic Network SyntNet for short .
E06-1003.txt,106,The model is inspired by a model presented earlier in Szpektor et al._comma_ 2004 _comma_ however our model allows more efficient construction of the representation.
E06-1003.txt,107,The scope of SyntNet is to represent a set of labeled graphs through one aggregate structure in which the isomorphic sub structures overlap.
E06-1003.txt,108,When SyntNet represents a syntactically parsed text corpus_comma_ its vertices are labeled with words from the text while edges represent syntactic relations from the corpus and are labeled accordingly.
E06-1003.txt,109,An example is shown in Figure 2_comma_ where two syntactic graphs_comma_ g1 and g2_comma_ are merged into one aggregate representation SyntNet g1_comma_g2 .
E06-1003.txt,110,Vertices labeled with equal words in g1 and g2 are merged into one generalizing vertex in SyntNet g1_comma_g2 .
E06-1003.txt,111,For example_comma_ the vertices with labelJohning1 andg2 aremergedintoonevertex John in SyntNet g1_comma_g2 .
E06-1003.txt,112,Edges are merged in a similar way loves_comma_John g1 and loves_comma_John g2 are represented through one edge loves_comma_John in SyntNet g1_comma_g2 .
E06-1003.txt,113,Each vertex in g1 and g2 is labeled additionally with a numerical index which is unique for the graph set.
E06-1003.txt,114,Numbers on vertices in SyntNet g1_comma_g2 show which vertices from g1 and g2 are merged in the corresponding SyntNet vertices.
E06-1003.txt,115,For example_comma_ vertex loves SyntNet g1_comma_g2 has a set 1_comma_4 which means that vertices 1 and 4 are merged in it.
E06-1003.txt,116,In a similar way the edge loves_comma_John SyntNet g1_comma_g2 is labeled with two pairs of indices 4_comma_5 and 1_comma_2 _comma_ which shows that it represents two edges the edge between vertices 4 and 5 and the edge between 1 and 2.
E06-1003.txt,117,Two properties of SyntNet are important first isomorphic sub structures from all the graphs represented via a SyntNet are mapped into one structure.
E06-1003.txt,118,This allows us to easily find all the occurrences of multiword terms and named entities.
E06-1003.txt,119,Second_comma_ using the numerical indices on vertices and edges_comma_ we can efficiently calculate which structures are connected syntactically to the training and test terms.
E06-1003.txt,120,As an example_comma_ let s try to calculate in which constructions the word Mary appears considering SyntNet in Figure 2.
E06-1003.txt,121,First_comma_ in SyntNet we can directly observe that there is the relation loves Mary labeled with the pair 1 3 therefore this relation appears once in the corpus.
E06-1003.txt,122,Next_comma_ tracing the numerical indices on the vertices and edges we can find a path from Mary to John through loves .
E06-1003.txt,123,The path passes through the following numerical indices 3 1 2 this means that there is one appearance of the structure 21 JohnlovesMary inthecorpus_comma_spanningthrough vertices 1_comma_2_comma_ and 3.
E06-1003.txt,124,Such a path through the numerical indices cannot be found between Mary and Jane which means that they do not appear in the same syntactic construction in the corpus.
E06-1003.txt,125,SyntNet is built incrementally in a straightforward manner Each new vertex or edge added to the network is merged with the identical vertex or edge_comma_ if such already existsin SyntNet.
E06-1003.txt,126,Otherwise_comma_ a new vertex or edge is added to the network.
E06-1003.txt,127,The time necessary for building a SyntNet is proportional to the number of the vertices and the edges in the represented graphs and does not otherwise depend on their complexity .
E06-1003.txt,128,The efficiency of the SyntNet model when representing and searching for labeled structures makes it very appropriate for the representation of a syntactically parsed corpus.
E06-1003.txt,129,We used the properties of SyntNet in order to trace efficiently the occurrences of Named Entities in the parsed corpus_comma_ to calculate their frequencies_comma_ to find the syntactic features which co occur with these Named Entities_comma_ as well as the frequencies of these cooccurrences.
E06-1003.txt,130,Moreover_comma_ the SyntNet model allowed us to extract more complex_comma_ second order syntactic features which are connected indirectly to the terms in the training and the test set.
E06-1003.txt,131,5 Experimental settings and results We have evaluated all the three approaches described in Section 3.
E06-1003.txt,132,The same evaluation settings were used for the three experiments.
E06-1003.txt,133,The source of features was a news corpus of about half a gigabyte.
E06-1003.txt,134,The corpus was parsed with MiniPar and a Syntactic Network representation was built from thedependencyparsetreesproducedbytheparser.
E06-1003.txt,135,Syntactic features were extracted from this SyntNet.
E06-1003.txt,136,We considered two high level Named Entity categories Locations and Persons.
E06-1003.txt,137,For each of them five fine grained sub classes were taken into consideration.
E06-1003.txt,138,For locations mountain_comma_ lake_comma_ river_comma_ city_comma_ and country for persons statesman_comma_ writer_comma_ athlete_comma_ actor_comma_ and inventor.
E06-1003.txt,139,For each class under consideration we created a test set of Named Entities using WordNet 2.0 and Internet sites like Wikipedia.
E06-1003.txt,140,For the ClassExample approach we also provided training data using the same resources.
E06-1003.txt,141,WordNet was the primarydatasourcefortrainingandtestdata.
E06-1003.txt,142,Theexamples from it were extracted automatically.
E06-1003.txt,143,We P  R  F  mountain 58 78 67 lake 75 50 60 river 69 55 61 city 56 76 65 country 86 93 89 locations macro 69 70 68 locations micro 78 78 78 statesman 42 72 53 writer 93 55 69 athlete 90 47 62 actor 90 73 80 inventor 12 33 18 persons macro 65 56 57 persons micro 57 57 57 total macro 67 63 62 total micro 65 65 65 category location 83 91 87 category person 95 89 92 Table 1 Performance of the Class Example approach. used Internet to get additional examples for some classes.
E06-1003.txt,144,To do this_comma_ we created automatic text extraction scripts for Web pages and manually filtered their output when it was necessary.
E06-1003.txt,145,Totally_comma_ the test data comprised 280 Named Entities which were not ambiguous and appeared at least twice in the corpus.
E06-1003.txt,146,For the Class Example approach we provided a training set of 1194 names.
E06-1003.txt,147,The only requirement to the names in the training set was that they appear at least twice in the parsed corpus.
E06-1003.txt,148,They were allowed to be ambiguous and no manual post processing or filtering was carried out on this data.
E06-1003.txt,149,For both context feature approaches i.e. ClassWord and Class Example _comma_ we used the same type of syntactic features and the same classification function_comma_ namely the one described in Section 3.3.
E06-1003.txt,150,This was done in order to compare better the approaches.
E06-1003.txt,151,Results from the comparative evaluation are shown in Table 2.
E06-1003.txt,152,For each approach we measured macro average precision_comma_ macro average recall_comma_ macro average F measure and micro average F for Class Word and Class Example micro F is equaltotheoverallaccuracy_comma_ i.e. thepercentofthe instances classified correctly.
E06-1003.txt,153,The first row shows 22 macro P  macro R  macro F  micro F Class Patterns 18 6 9 10 Class Word 32 41 33 42 Class Example 67 63 62 65 Class Example sec. ord. 65 61 62 68 Table 2 Comparison of different approaches. the results obtained with superficial patterns.
E06-1003.txt,154,The second row presents the results from the ClassWord approach.
E06-1003.txt,155,The third row shows the results of our Class Example method.
E06-1003.txt,156,The fourth line presents the results for the same approach but using second order features for the person category.
E06-1003.txt,157,The Class Pattern approach showed low performance_comma_ similar to the random classification_comma_ for which macro and micro F 10 .
E06-1003.txt,158,Patterns succeeded to classify correctly only instances of the classes river and city .
E06-1003.txt,159,For the class city the patterns reached precision of 100 and recall 65 for the class river precision was high i.e. 75 _comma_ but recall was 15 .
E06-1003.txt,160,The Class Word approach showed significantly betterperformance macroF 33 _comma_ microF 42 than the Class Pattern approach.
E06-1003.txt,161,The performance of the Class Example 62 macro F and 65 68 micro F is much higher than the performance of Class Word 29 increase in macro F and 23 in micro F .
E06-1003.txt,162,The last row of the table shows that second order syntactic features augment further the performance of the Class Example method in terms of micro average F 68 vs. 65 .
E06-1003.txt,163,A more detailed evaluation of the ClassExample approach is shown in Table 1.
E06-1003.txt,164,In this table we show the performance of the approach without the second order features.
E06-1003.txt,165,Results vary between different classes The highest F is measured for the class country 89 and the lowest is for the class inventor 18 .
E06-1003.txt,166,However_comma_ the class inventor is an exception for all the other classes the F measure is over 50 .
E06-1003.txt,167,Another difference may be observed between the Location and Person classes Our approach performs significantly better for the locations 68 vs. 57 macro F and 78 vs. 57 micro F .
E06-1003.txt,168,Although different classes had different number of training examples_comma_ we observed that the performance for a class does not depend on the size of its training set.
E06-1003.txt,169,Wethink_comma_thatthevariationinperformancebetween categories is due to the different specificity of their textual contexts.
E06-1003.txt,170,As a consequence_comma_ some classes tend to co occur with more specific syntactic features_comma_ while for other classes this is not true.
E06-1003.txt,171,Additionally_comma_ we measured the performance of our approach considering only the macrocategories Location and Person .
E06-1003.txt,172,For this purpose we did not run another experiment_comma_ we rather used the results from the fine grained classification and grouped the already obtained classes.
E06-1003.txt,173,Results are shown in the last two rows of table 1 It turns out that the Class Example method makes very well the difference between location and person 90 of the test instances were classified correctly between these categories.
E06-1003.txt,174,6 Conclusions and future work In this paper we presented a new weakly supervised approach for Ontology Population_comma_ called Class Example_comma_ and confronted it with two other methods.
E06-1003.txt,175,Experimental results show that the Class Example approach has best performance.
E06-1003.txt,176,In particular_comma_ it reached 65 of accuracy_comma_ outperforming in our experimental framework the stateof the art Class Word method by 42 .
E06-1003.txt,177,Moreover_comma_ for location names the method reached accuracy of 78 .
E06-1003.txt,178,Although the experiments are not comparable_comma_ we would like to state that some supervised approaches for fine grained Named Entity classification_comma_ e.g.
E06-1003.txt,179,Fleischman_comma_ 2001 _comma_ have similar accuracy.
E06-1003.txt,180,On the other hand_comma_ the presented weakly supervised Class Example approach requires as a training data only a list of terms for each class under consideration.
E06-1003.txt,181,Training examples can be automatically acquired from existing ontologies or other sources_comma_ since the approach imposes virtually no restrictions on them.
E06-1003.txt,182,This makes our weakly supervised methodology applicable on larger scale than supervised approaches_comma_ still having significantly better performance than the unsupervised ones.
E06-1003.txt,183,In our experimental framework we used syntactic features extracted from dependency parse trees 23 and we put forward a novel model for the representation of a syntactically parsed corpus.
E06-1003.txt,184,This model allows for performing a comprehensive extraction of syntactic features from a corpus including more complex second order ones_comma_ which resulted in an improvement of performance.
E06-1003.txt,185,This and other empirical observations not described in this paper lead us to the conclusion that the performance of an Ontology Population system improves with the increase of the types of syntactic features under consideration.
E06-1003.txt,186,In our future work we consider applying our Ontology Population methodology to more semantic categories and to experiment with other types of syntactic features_comma_ as well as other types of feature weighting formulae and learning algorithms.
E06-1003.txt,187,We consider also the integration of the approach in a Question Answering or Information Extractionsystem_comma_ whereitcanbeusedtoperform fine grained type checking. .
E06-2031.txt,1,describe a method for discovering irregularities in temporal mood patterns appearing in a large corpus of blog posts_comma_ and labeling them with a natural language explanation. Simple techniques based on comparing corpus frequencies_comma_ coupled with large quantities of data_comma_ are shown to be effective for identifying the events underlying changes in global moods.
E06-2031.txt,3,Blogs_comma_ diary like web pages containing highly opinionated personal commentary_comma_ are becoming increasingly popular.
E06-2031.txt,4,This new type of media offers a unique look into people s reactions and feelings towards current events_comma_ for a number of reasons.
E06-2031.txt,5,First_comma_ blogs are frequently updated_comma_ and like other forms of diaries are typically closely linked toongoingeventsintheblogger slife.
E06-2031.txt,6,Second_comma_ the blog contents tend to be unmoderated and subjective_comma_ more so than mainstream media expressing opinions_comma_ thoughts_comma_ and feeling.
E06-2031.txt,7,Finally_comma_ the large amount of blogs enables aggregation of thousands of opinions expressed every minute this aggregation allows .
E06-1018.txt,1,this paper a novel solution to automatic and unsupervised word sense induction WSI is introduced. It represents an instantiation of the one sense per collocation observation Gale et al._comma_ 1992 .
E06-1018.txt,2,Like most existing approaches it utilizes clustering of word co occurrences.
E06-1018.txt,3,This approach differs from other approaches to WSI in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs.
E06-1018.txt,4,The combination with a two step clustering process using sentence co occurrences as features allows for accurate results.
E06-1018.txt,5,Additionally_comma_ a novel and likewise automatic and unsupervised evaluation method inspired by Sch utze s 1992 idea of evaluation of word sense disambiguation algorithms is employed.
E06-1018.txt,6,Offering advantages like reproducability and independency of a given biased gold standard it also enables automatic parameter optimization of the WSI algorithm.
E06-1018.txt,8,The aim of word sense induction1 WSI is to find senses of a given target word Yarowski_comma_ 1995 automatically and if possible in an unsupervised manner.
E06-1018.txt,9,WSIisakintowordsensedisambiguation WSD both in methods employed and in problems encountered_comma_ such as vagueness of sense distinctions Kilgarriff_comma_ 1997 .
E06-1018.txt,10,The input to a WSI algorithm is a target word to be disambiguated_comma_ e.g.
E06-1018.txt,11,1Sometimes called word sense discovery Dorow and Widdows_comma_ 2003 or word sense discrimination Purandare_comma_ 2004 Velldal_comma_ 2005 space_comma_ and the output is a number of word sets representing the various senses_comma_ e.g.
E06-1018.txt,12,3 dimensional_comma_ expanse_comma_ locate and office_comma_ building_comma_ square .
E06-1018.txt,13,Such results can be at the very least used as empirically grounded suggestions for lexicographers or as input for WSD algorithms.
E06-1018.txt,14,Other possible uses include automatic thesaurus or ontology construction_comma_ machine translation or information retrieval.
E06-1018.txt,15,But the usefulness of WSI in real world applications has yet to be tested and proved.
E06-1018.txt,16,2 Related work A substantial number of different approaches to WSI has been proposed so far.
E06-1018.txt,17,They are all based on co occurrence statistics_comma_ albeit using different context representations such as co occurrence of words within phrases Pantel and Lin_comma_ 2002 Dorow and Widdows_comma_ 2003 Velldal_comma_ 2005 _comma_ bigrams Sch utze_comma_ 1998 Neill_comma_ 2002 Udani et al._comma_ 2005 _comma_ small windows around a word Gauch and Futrelle_comma_ 1993 _comma_ or larger contexts such as sentences Bordag_comma_ 2003 Rapp_comma_ 2004 or large windows of up to 20 words Ferret_comma_ 2004 .
E06-1018.txt,18,Moreover they all employ clustering methods to partition the co occurring words into sets describing concepts or senses.
E06-1018.txt,19,Some algorithms aim for a global clustering of words into concepts Yarowski_comma_ 1995 Pantel and Lin_comma_ 2002 Velldal_comma_ 2005 .
E06-1018.txt,20,But the majority of algorithms are based on a local clustering Words co occurring with the target word are grouped into the various senses the target word has.
E06-1018.txt,21,It is not immediately clear which approach to favor_comma_ however aiming at global senses has the inherent property to produce a uniform granularity of distinctions between senses that might not be desired Rapp_comma_ 2004 .
E06-1018.txt,22,Graph based algorithms differ from the majority of algorithms in several aspects.
E06-1018.txt,23,Words 137 can be taken as nodes and co occurrence of two words defines an edge between the two respective nodes.
E06-1018.txt,24,Activation spreading on the resulting graph can be employed Barth_comma_ 2004 in order to obtain most distinctly activated areas in the vicinity of the target word.
E06-1018.txt,25,It is also possible to use graph based clustering techniques to obtain sense representations based on sub graph density measures Dorow and Widdows_comma_ 2003 Bordag_comma_ 2003 .
E06-1018.txt,26,However_comma_ it is not yet clear_comma_ whether this kind of approach differs qualitatively from the standard clustering approaches.
E06-1018.txt,27,Generally though_comma_ the notion of sub graph density seems to be more intuitive compared to the more .
E06-1002.txt,1,present a new method for detecting and disambiguating named entities in open domain text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia.
E06-1002.txt,2,The resulting model significantly outperforms a less informed baseline.
E06-1002.txt,4,1.1 Motivation The de facto web search paradigm defines the result to a user s query as roughly a set of links to the best matching documents selected out of billions of items available.
E06-1002.txt,5,Whenever the queries search for pinpointed_comma_ factual information_comma_ the burden of filling the gap between the output granularity whole documents and the targeted information a set of sentences or relevant phrases stays with the users_comma_ by browsing the returned documents in order to find the actually relevant bits of information.
E06-1002.txt,6,A frequent case are queries about named entities_comma_ which constitute a significant fraction of popular web queries according to search engine logs.
E06-1002.txt,7,When submitting queries such as John Williams or Python_comma_ search engine users could also be presented with a compilation of facts and specific attributes about those named entities_comma_ rather than a set of best matching web pages.
E06-1002.txt,8,One of the challenges in creating such an alternative search result page is the inherent ambiguity of the queries_comma_ as several instances of the same class e.g._comma_ different people or different classes e.g._comma_ a type of snake_comma_ a programming language_comma_ or a movie may share the same name in the query.
E06-1002.txt,9,As an example_comma_ the A3Work done during a summer internship at Google. contexts below are part of web documents referring to different people who share the same name John Williams 1.
E06-1002.txt,10,John Williams and the Boston Pops conducted a summer Star Wars concert at Tanglewood. 2.
E06-1002.txt,11,John Williams lost a Taipei death match against his brother_comma_ Axl Rotten. 3.
E06-1002.txt,12,John Williams won a Victoria Cross for his actions at the battle of Rorke s Drift. The effectiveness of the search could be greatly improved if the search results were grouped together according to the corresponding sense_comma_ rather than presented as a flat_comma_ sense mixed list of items whether links to full length documents_comma_ or extracted facts .
E06-1002.txt,13,As an added benefit_comma_ users would have easier access to a wider variety of results_comma_ whenever the top 10 or so results returned by the largest search engines happen to refer to only one particular arguably the most popular sense of the query e.g._comma_ the programming language in the case of Python _comma_ thus submerging or hiding documents that refer to other senses of the query.
E06-1002.txt,14,In various natural language applications_comma_ significant performance gains are achieved as a function of data size rather than algorithm complexity_comma_ as illustrated by the increasingly popular use of the web as a very large corpus Dale_comma_ 2003 .
E06-1002.txt,15,It seems therefore natural to try to exploit the web in order to also improve the performance of relation extraction_comma_ i.e. the discovery of useful relationships between named entities mentioned in text documents.
E06-1002.txt,16,However_comma_ if one wants to combine evidence from multiple web pages_comma_ then one needs again to solve the name disambiguation problem.
E06-1002.txt,17,9 Without solving it_comma_ a relation extraction system analyzing the sentences in the above example could mistakenly consider the third as evidence that John Williams the composer fought at Rorke s Drift.
E06-1002.txt,18,1.2 Approach The main goal of the research reported in this paper is to develop a named entity disambiguation method that is intrinsically linked to a dictionary mapping proper names to their possible named entitiy denotations.
E06-1002.txt,19,More exactly_comma_ the method 1.
E06-1002.txt,20,Detects whether a proper name refers to a named entity included in the dictionary detection .
E06-1002.txt,22,Disambiguates between multiple named entities that can be denoted by the same proper name disambiguation .
E06-1002.txt,23,As a departure from the methodology of previous approaches_comma_ the paper exploits a non traditional web based resource.
E06-1002.txt,24,Concretely_comma_ it takes advantage of some of the human knowledge available in Wikipedia_comma_ a free online encyclopedia created through decentralized_comma_ collective efforts of thousands of users Remy_comma_ 2002 .
E06-1002.txt,25,We show that the structure of Wikipedia lends itself to a set of useful features for the detection and disambiguation of named entities.
E06-1002.txt,26,The remainder of the paper is organized as follows.
E06-1002.txt,27,Section 2 describes Wikipedia_comma_ with an emphasis on the features that are most important to the entity disambiguation task.
E06-1002.txt,28,Section 3 describes the extraction of named entity entries versus other types of entries from Wikipedia.
E06-1002.txt,29,Section 4 introduces two disambiguation methods_comma_ which are evaluated experimentally in Section 5.
E06-1002.txt,30,We conclude with future work and conclusions.
E06-1002.txt,31,2 Wikipedia A Wiki Encyclopedia Wikipedia is a free online encyclopedia written collaboratively by volunteers_comma_ using a wiki software that allows almost anyone to add and change articles.
E06-1002.txt,32,It is a multilingual resource there are about 200 language editions with varying levels of coverage.
E06-1002.txt,33,Wikipedia is a very dynamic and quickly growing resource articles about newsworthy events are often added within days of their occurrence.
E06-1002.txt,34,As an example_comma_ the September 2005 version contains 751_comma_666 articles_comma_ around 180_comma_000 more articles than four months earlier.
E06-1002.txt,35,The work in this paper is based on the English version from May 2005_comma_ which contains 577_comma_860 articles.
E06-1002.txt,36,Each article in Wikipedia is uniquely identified by its title a sequence of words separated by underscores_comma_ with the first word always capitalized.
E06-1002.txt,37,Typically_comma_ the title is the most common name for the entity described in the article.
E06-1002.txt,38,When the name is ambiguous_comma_ it is further qualified with a parenthetical expression.
E06-1002.txt,39,For instance_comma_ the article on John Williams the composer has the title John Williams composer .
E06-1002.txt,40,Because each article describes a specific entity or concept_comma_ the remainder of the paper sometimes uses the term entity interchangeably to refer to both the article and the corresponding entity.
E06-1002.txt,41,Also_comma_ let BX denote the entire set of entities from Wikipedia.
E06-1002.txt,42,For any entity CTBEBX_comma_ CTBMD8CXD8D0CT is the title name of the corresponding article_comma_ and CTBMCC is the text of the article.
E06-1002.txt,43,In general_comma_ there is a many to many correspondence between names and entities.
E06-1002.txt,44,This relation is captured in Wikipedia through redirect and disambiguation pages_comma_ as described in the next two sections.
E06-1002.txt,45,2.1 Redirect Pages A redirect page exists for each alternative name that can be used to refer to an entity in Wikipedia.
E06-1002.txt,46,The name is transformed using underscores for spaces into a title whose article contains a redirect link to the actual article for that entity.
E06-1002.txt,47,For example_comma_ John Towner Williams is the full name of the composer John Williams.
E06-1002.txt,48,It is therefore an alternative name for the composer_comma_ and consequently the article with the title John Towner Williams is just a pointer to the article for John Williams composer .
E06-1002.txt,49,An example entry with a considerably higher number of redirect pages is United States.
E06-1002.txt,50,Its redirect pages correspond to acronyms U.S.A._comma_ U.S._comma_ USA_comma_ US _comma_ Spanish translations Los Estados Unidos_comma_ Estados Unidos _comma_ misspellings Untied States or synonyms Yankee land .
E06-1002.txt,51,For any given Wikipedia entity CTBEBX_comma_ let CTBMCA be the set of all names that redirect to CT.
E06-1002.txt,52,2.2 Disambiguation Pages Another useful structure is that of disambiguation pages_comma_ which are created for ambiguous names_comma_ i.e. names that denote two or more entities in Wikipedia.
E06-1002.txt,53,For example_comma_ the disambiguation page for the name John Williams lists 22 associated 10 TITLE REDIRECT DISAMBIG CATEGORIES Star Wars music_comma_ ... John Williams composer John Towner Williams John Williams Film score composers_comma_ 20th century classical composers John Williams wrestler Ian Rotten John Williams Professional wrestlers_comma_ People living in Baltimore John Williams VC none John Williams British Army soldiers_comma_ British Victoria Cross recipients Boston Pops Orchestra Boston Pops_comma_ Pops American orchestras_comma_ The Boston Pops Orchestra Massachusetts musicians United States US_comma_ USA_comma_ ... US_comma_ USA_comma_ North American countries_comma_ United States of America United States Republics_comma_ United States Venus_comma_ Venus Venus planet Planet Venus Morning Star_comma_ Planets of the Solar System_comma_ Evening Star Planets_comma_ Solar System_comma_ ... Table 1 Examples of Wikipedia titles_comma_ aliases and categories entities.
E06-1002.txt,54,Therefore_comma_ besides the non ambiguous names that come from redirect pages_comma_ additional aliases can be found by looking for all disambiguation pages that list a particular Wikipedia entity.
E06-1002.txt,55,In his philosophical article On Sense and Reference Frege_comma_ 1999 _comma_ Gottlob Frege gave a famous argument to show that sense and reference are distinct.
E06-1002.txt,56,In his example_comma_ the planet Venus may be referred to using the phrases morning star and evening star .
E06-1002.txt,57,This theoretical example is nicely captured in practice in Wikipedia by two disambiguation pages_comma_ Morning Star and Evening Star_comma_ both listing Venus as a potential referent.
E06-1002.txt,58,For any given Wikipedia entity CT BE BX_comma_ let CTBMBW be the set of names whose disambiguation pages contain a link to CT.
E06-1002.txt,59,2.3 Categories Every article in Wikipedia is required to have at least one category.
E06-1002.txt,60,As shown in Table 1_comma_ John Williams composer is associated with a set of categories_comma_ among them Star Wars music_comma_ Film score composers_comma_ and 20th century classical composers.
E06-1002.txt,61,Categories allow articles to be placed into one or more topics.
E06-1002.txt,62,These topics can be further categorized by associating them with one or more parent categories.
E06-1002.txt,63,In Table 1 Venus is shown as both an article title and a category.
E06-1002.txt,64,As a category_comma_ it has one direct parent Planets of the Solar System_comma_ which in turn belongs to two more general categories_comma_ Planets and Solar System.
E06-1002.txt,65,Thus_comma_ categories form a directed acyclic graph_comma_ allowing multiple categorization schemes to co exist simultaneously.
E06-1002.txt,66,There are in total 59_comma_759 categories in Wikipedia.
E06-1002.txt,67,For a given Wikipedia entity CTBEBX_comma_ let CTBMBV be the set of categories to which CT belongs i.e. CT s immediate categories and all their ancestors in the Wikipedia taxonomy .
E06-1002.txt,68,2.4 Hyperlinks Articles in Wikipedia often contain mentions of entities that already have a corresponding article.
E06-1002.txt,69,When contributing authors mention an existing Wikipedia entity inside an article_comma_ they are required to link at least its first mention to the corresponding article_comma_ by using links or piped links.
E06-1002.txt,70,Both types of links are exemplified in the following wiki source code of a sentence from the article on Italy The Vatican City Vatican is now an independent enclave surrounded by Rome .
E06-1002.txt,71,The string from the second link Rome denotes the title of the referenced article.
E06-1002.txt,72,The same string is also used in the display version.
E06-1002.txt,73,If the author wants another string displayed e.g._comma_ Vatican instead of VaticanCity _comma_ then the alternative string is included in a piped link_comma_ after the title string.
E06-1002.txt,74,Consequently_comma_ the display string for the aforementioned example is The Vatican is now an independent enclave surrounded by Rome .
E06-1002.txt,75,As described later in Section 4_comma_ the hyperlinks can provide useful training examples for a named entity disambiguator.
E06-1002.txt,76,3 A Dictionary of Named Entities We organize all named entities from Wikipedia into a dictionary structure BW_comma_ where each string entry CS BE BW is mapped to the set of entities CSBMBX that can be denoted by CS in Wikipedia.
E06-1002.txt,77,The first step is to identify named entities_comma_ i.e. entities with a proper name title.
E06-1002.txt,78,Because every title in Wikipedia must begin with a capital letter_comma_ the decision whether a title is a proper name relies on the following sequence of heuristic steps 11 1.
E06-1002.txt,79,If CTBMD8CXD8D0CT is a multiword title_comma_ check the capitalization of all content words_comma_ i.e. words other than prepositions_comma_ determiners_comma_ conjunctions_comma_ relative pronouns or negations.
E06-1002.txt,80,Consider CT a named entity if and only if all content words are capitalized.
E06-1002.txt,82,If CTBMD8CXD8D0CT is a one word title that contains at least two capital letters_comma_ then CT is a named entity.
E06-1002.txt,83,Otherwise_comma_ go to step 3.
E06-1002.txt,85,Count how many times CTBMD8CXD8D0CT occurs in the text of the article_comma_ in positions other than at the beginning of sentences.
E06-1002.txt,86,If at least BJBHB1 of these occurrences are capitalized_comma_ then CT is a named entity.
E06-1002.txt,87,The combined heuristics extract close to half a million named entities from Wikipedia.
E06-1002.txt,88,The second step constructs the actual dictionary BW as follows AF The set of entries in BW consists of all strings that may denote a named entity_comma_ i.e. if CTBEBX is a named entity_comma_ then its title name CTBMD8CXD8D0CT_comma_ its redirect names CTBMCA_comma_ and its disambiguation names CTBMBW are all added as entries in BW.
E06-1002.txt,89,AF Each entry string CSBEBW is mapped to CSBMBX_comma_ the set of entities that CS may denote in Wikipedia.
E06-1002.txt,90,Consequently_comma_ a named entity CT is included in CSBMBX if and only if CS BP CTBMD8CXD8D0CT_comma_ CS BE CTBMCA_comma_ or CSBECTBMBW.
E06-1002.txt,91,4 Named Entity Disambiguation As illustrated in Section 1_comma_ the same proper name may refer to more than one named entity.
E06-1002.txt,92,The named entity dictionary from Section 3 and the hyperlinks from Wikipedia articles provide a dataset of disambiguated occurrences of proper names_comma_ as described in the following.
E06-1002.txt,93,As shown in Section 2.4_comma_ each link contains the title name of an entity_comma_ and the proper name the display string used to refer to it.
E06-1002.txt,94,We use the term query to denote the occurrence of a proper name inside a Wikipedia article.
E06-1002.txt,95,If there is a dictionary entry matching the proper name in the query D5 such that the set of denoted entities D5BMBX contains at least two entities_comma_ one of them the true answer entity D5BMCT_comma_ then the query D5 is included in the dataset.
E06-1002.txt,96,More exactly_comma_ if D5BMBX contains D2 named entities CT BD _comma_ CT BE _comma_ ..._comma_ CT D2 _comma_ then the dataset will be augmented with D2 pairs CWD5BNCT CZ CX represented as follows CWD5BNCT CZ CX BP CJ B4CT CZ BND5BMCTB5 CY D5BMCC CY CT CZ BMD8CXD8D0CTCL The field D5BMCC contains all words occurring in a limit length window centered on the proper name.
E06-1002.txt,97,The window size is set to 55_comma_ which is the value that was observed to give optimum performance in the related task of cross document coreference Gooi and Allan_comma_ 2004 .
E06-1002.txt,98,The Kronecker delta function B4CT CZ BND5BMCTB5 is 1 when CT CZ is the same as the entity D5BMCT referred in the link.
E06-1002.txt,99,Table 2 lists the query pairs created for the three John Williams queries from Section 1.1_comma_ assuming only three entities in Wikipedia correspond to this name.
E06-1002.txt,100,Query Text Entity Title 1 Boston Pops conduct ... John Williams composer 0 Boston Pops conduct ... John Williams wrestler 0 Boston Pops conduct ... John Williams VC 1 lost Taipei match ... John Williams wrestler 0 lost Taipei match ... John Williams composer 0 lost Taipei match ... John Williams VC 1 won Victoria Cross ... John Williams VC 0 won Victoria Cross ... John Williams composer 0 won Victoria Cross ... John Williams wrestler Table 2 Disambiguation dataset.
E06-1002.txt,101,The application of this procedure on Wikipedia results into a dataset of 1_comma_783_comma_868 disambiguated queries.
E06-1002.txt,102,4.1 Context Article Similarity Using the representation from the previous section_comma_ the name entity disambiguation problem can be cast as a ranking problem.
E06-1002.txt,103,Assuming that an appropriate scoring function D7CRD3D6CTB4D5BNCT CZ B5 is available_comma_ the named entity corresponding to query D5 is defined to be the one with the highest score CMCT BP CPD6CVD1CPDC CT CZ D7CRD3D6CTB4D5BNCT CZ B5 1 If CMCT BP D5BMCT then CMCT represents a hit_comma_ otherwise CMCT is a miss.
E06-1002.txt,104,Disambiguation methods will then differ based on the way they define the scoring function.
E06-1002.txt,105,One ranking function that is evaluated experimentally in this paper is based on the cosine similarity between the context of the query and the text of the article D7CRD3D6CTB4D5BNCT CZ B5 BP CRD3D7B4D5BMCCBNCT CZ BMCCB5 BP D5BMCC CZD5BMCCCZ CT CZ BMCC CZCT CZ BMCCCZ The factors D5BMCC and CT CZ BMCC are represented in the standard vector space model_comma_ where each component corresponds to a term in the vocabulary_comma_ and the term weight is the standard D8CU A2 CXCSCU score Baeza Yates and Ribeiro Neto_comma_ 1999 .
E06-1002.txt,106,The vocabulary CE is created by reading all Wikipedia 12 articles and recording_comma_ for each word stem DB_comma_ its document frequency CSCUB4DBB5 in Wikipedia.
E06-1002.txt,107,Stopwords and words that are too frequent or too rare are discarded.
E06-1002.txt,108,A generic document CS is then represented as a vector of length CYCE CY_comma_ with a position for each vocabulary word.
E06-1002.txt,109,If CUB4DBB5 is the frequency of word DB in document CS_comma_ and C6 is the total number of Wikipedia articles_comma_ then the weight of word DBBECE in the D8CU A2 CXCSCU representation of CS is CS DB BP CUB4DBB5D0D2 C6 CSCUB4DBB5 2 4.2 Taxonomy Kernel An error analysis of the cosine based ranking method reveals that_comma_ in many cases_comma_ the pair CWD5BNCTCX fails to rank first_comma_ even though words from the query context unambiguously indicate CT as the actual denoted entity.
E06-1002.txt,110,In these cases_comma_ cue words from the context do not appear in CT s article due to two main reasons 1.
E06-1002.txt,111,The article may be too short or incomplete.
E06-1002.txt,113,Even though the article captures most of the relevant concepts expressed in the query context_comma_ it does this by employing synonymous words or phrases.
E06-1002.txt,114,The cosine similarity between D5 and CT CZ can be seen as an expression of the total degree of correlation between words from the context of query D5 and a given named entity CT CZ .
E06-1002.txt,115,When the correlation is too low because the Wikipedia article for named entity CT CZ does not contain all words that are relevant to CT CZ _comma_ it is worth considering the correlation between context words and the categories to which CT CZ belongs.
E06-1002.txt,116,For illustration_comma_ consider the two queries for the name John Williams from Figure 1.
E06-1002.txt,117,To avoid clutter_comma_ Figure 1 depicts only two entities with the name JohnWilliams in Wikipedia the composer and the wrestler.
E06-1002.txt,118,On top of each entity_comma_ the figure shows one of their Wikipedia categories Film score composers and Professional wrestlers respectively _comma_ together with some of their ancestor categories in the Wikipedia taxonomy.
E06-1002.txt,119,The two query contexts are shown at the bottom of the figure.
E06-1002.txt,120,In the context on the left_comma_ words such as conducted and concert denote concepts that are highly correlated with the Musicians_comma_ Composers and Filmscore composers categories.
E06-1002.txt,121,On the other hand_comma_ their correlation with other categories in Figure 1 is considerably lower.
E06-1002.txt,122,Consequently_comma_ a Musicians Composers Film score composers People by occupation People People known in connection with sports and hobbies Wrestlers Professional wrestlers high correlationshigh correlations  conducted a summer Star Wars John Williams John Williams a Taipei death lost concert match ... ... John Williams composer John Williams wrestler Figure 1 Word Category correlations. goal of this paper is to design a disambiguation method that 1 learns the magnitude of these correlations_comma_ and 2 uses these correlations in a scoring function_comma_ together with the cosine similarity.
E06-1002.txt,123,Our intuition is that_comma_ given the query context on the left_comma_ such a ranking function has a better chance of ranking the composer entity higher than the wrestler entity_comma_ when compared with the simple cosine similarity baseline.
E06-1002.txt,124,We consider using a linear ranking function as follows CMCT BP CPD6CVD1CPDC CT CZ DB A8B4D5BNCT CZ B5 3 The feature vector A8B4D5BNCT CZ B5 contains a dedicated feature AU CRD3D7 for cosine similarity_comma_ and CYCE CY A2 CYBVCY features AU DBBNCR corresponding to combinations of words DB from the Wikipedia vocabulary CE and categories CR from the Wikipedia taxonomy BV AU CRD3D7 B4D5BNCT CZ B5 BP CRD3D7B4D5BMCCBNCT CZ BMCCB5 4 AU DBBNCR B4D5BNCT CZ B5 BP AQ BD if DBBED5BMCC and CRBECT CZ BMBVBN BC otherwiseBM The weight vector DB models the magnitude of each word category correlation_comma_ and can be learned by training on the query dataset described at the beginning of Section 4.
E06-1002.txt,125,We used the kernel version of the large margin ranking approach from Joachims_comma_ 2002 which solves the optimization 13 problem in Figure 2.
E06-1002.txt,126,The aim of this formulation is to find a weight vector DB such that 1 the number of ranking constraints DB A8B4D5BND5BMCTB5 AL DB A8B4D5BNCT CZ B5 from the training data that are violated is minimized_comma_ and 2 the ranking function DB A8B4D5BNCT CZ B5 generalizes well beyond the training data. minimize CEB4DBBNAOB5 BP BD BE DBA1DBB7BV C8 AO D5BNCZ subject to DBB4A8B4D5BND5BMCTB5 A0A8B4D5BNCT CZ B5B5 AL BDA0 AO D5BNCZ AO D5BNCZ AL BC BKD5BNBKCT CZ BE D5BMBX A0 CUD5BMCTCV Figure 2 Optimization problem.
E06-1002.txt,127,BV is a parameter that allows trading off margin size against training error.
E06-1002.txt,128,The number of linear ranking constraints is C8 D5 B4CYD5BMBXCY A0BDB5.
E06-1002.txt,129,As an example_comma_ each of the three queries from Table 2 generates two constraints.
E06-1002.txt,130,The learned DB is a linear combination of the feature vectors A8B4D5BNCT CZ B5_comma_ which makes it possible to use kernels Vapnik_comma_ 1998 .
E06-1002.txt,131,It is straightforward to show that the dot product between two feature vectors A8B4D5BNCT CZ B5 and A8B4D5 BC BNCT BC CZ B5 is equal with the product between the number of common words in the contexts of the two queries and the number of categories common to the two named entities_comma_ plus the product of the two cosine similarities.
E06-1002.txt,132,The corresponding ranking kernel is C3 A0 CWD5BNCT CZ CXBNCWD5 BC BNCT BC CZ CX A1 BP AC AC D5BMCC CK D5 BC BMCC AC AC A1 AC AC CT CZ BMBV CK CT BC CZ BMBV AC AC B7 CRD3D7B4D5BMCCBNCT CZ BMCCB5 A1 CRD3D7B4D5 BC BMCCBNCT BC CZ BMCCB5 To avoid numerical problems_comma_ the first term of the kernel is normalized and the second term is multiplied with a constant AB BP BDBCBK C3 A0 CWD5BNCT CZ CXBNCWD5 BC BNCT BC CZ CX A1 BP CYD5BMCC CK D5 BC BMCCCY D4 CYD5BMCCCY A1 CYD5 BC BMCCCY A1 CYCT CZ BMBV CK CT BC CZ BMBVCY D4 CYCT CZ BMBVCY A1 CYCT BC CZ BMBVCY B7 AB A1 CRD3D7B4D5BMCCBNCT CZ BMCCB5 A1 CRD3D7B4D5 BC BMCCBNCT BC CZ BMCCB5 In McCallum et al._comma_ 1998 _comma_ a statistical technique called shrinkage is used in order to improve the accuracy of a naive Bayes text classifier.
E06-1002.txt,133,Accordingly_comma_ one can take advantage of a hierarchy of classes by combining parameter estimates of parent categories into the parameter estimates of a child category.
E06-1002.txt,134,The taxonomy kernel is very related to the same technique one can actually regard it as a distribution free analogue of shrinkage.
E06-1002.txt,135,4.3 Detecting Out of Wikipedia Entities The two disambiguation methods discussed above Sections 4.1 and 4.2 implicitly assume that Wikipedia contains all entities that may be denoted by entries from the named entity dictionary.
E06-1002.txt,136,Taking for example the name John Williams_comma_ both methods assume that in any context_comma_ the referred entity is among the 22 entities listed on the disambiguation page in Wikipedia.
E06-1002.txt,137,In practice_comma_ there may be contexts where John Williams refers to an entity CT D3D9D8 that is not covered in Wikipedia_comma_ especially when CT D3D9D8 is not a popular entity.
E06-1002.txt,138,These out of Wikipedia entities are accommodated in the ranking approach to disambiguation as follows.
E06-1002.txt,139,A special entity CT D3D9D8 is introduced to denote any entity not covered by Wikipedia.
E06-1002.txt,140,Its attributes are set to null values e.g._comma_ the article text CT D3D9D8 BMCC BP BN_comma_ and the set of categories CT D3D9D8 BMBV BP BN .
E06-1002.txt,141,The ranking in Equation 1 is then updated so that it returns the Wikipedia entity with the highest score_comma_ if this score is greater then a fix threshold AS_comma_ otherwise it returns CT D3D9D8 CT D1CPDC BP CPD6CVD1CPDC CT CZ D7CRD3D6CTB4D5BNCT CZ B5 CMCT BP AQ CT D1CPDC if D7CRD3D6CTB4D5BNCT D1CPDC B5 BQ ASBN CT D3D9D8 otherwiseBM If the scoring function is implemented as a weighted combination of feature functions_comma_ as in Equation 3_comma_ then the modification shown above results into a new feature AU D3D9D8 AU D3D9D8 B4D5BNCT CZ B5 BP B4CT CZ BNCT D3D9D8 B5 5 The associated weight AS is learned along with the weights for the other features as defined in Equation 5 .
E06-1002.txt,142,5 Experimental Evaluation The taxonomy kernel was trained using the SVMD0CXCVCWD8 package Joachims_comma_ 1999 .
E06-1002.txt,143,As described in Section 4_comma_ through its hyperlinks_comma_ Wikipedia provides a dataset of 1_comma_783_comma_868 ambiguous queries that can be used for training a named entity disambiguator.
E06-1002.txt,144,The apparently high number of queries actually corresponds to a moderate size dataset_comma_ given that the space of parameters includes one parameter for each word category combination.
E06-1002.txt,145,However_comma_ assuming SVMD0CXCVCWD8 does not run out of memory_comma_ using the entire dataset for training and testing is extremely 14 TRAINING DATASET TEST DATASET CAT.
E06-1002.txt,146,QUERIES PAIRS CWD5BNCT CZ CX CONSTR.
E06-1002.txt,147,QUERIES PAIRS CWD5BNCT CZ CX SV TK A Cos A CB BD 110 12_comma_288 39_comma_880 27_comma_592 48_comma_661 147_comma_165 19_comma_693 77.2 61.5 CB BE 540 17_comma_970 55_comma_452 37_comma_482 70_comma_468 235_comma_290 29_comma_148 68.4 55.8 CB BF 2_comma_847 21_comma_185 64_comma_560 43_comma_375 75_comma_190 261_comma_723 36_comma_383 68.0 55.4 CB BG 540 38_comma_726 102_comma_553 63_comma_827 80_comma_386 191_comma_227 35_comma_494 84.8 82.3 Table 3 Scenario statistics and comparative evaluation. time consuming.
E06-1002.txt,148,Therefore_comma_ we decided to evaluate the taxonomy kernel under the following scenarios A4 CB BD The working set of Wikipedia categories BV BD is restricted to only the 110 top level categories under People by occupation.
E06-1002.txt,149,The query dataset used for training and testing is reduced to contain only ambiguous queries CWD5BNCT CZ CX for which any potential matching entity CT CZ belongs to at least one of the 110 categories i.e. CT CZ BMBV CK BV BD BIBP BN .
E06-1002.txt,150,The set of negative matching entities CT CZ is also reduced to those that differ from the true answer CT in terms of their categories from BV BD i.e. CT CZ BMBV CK BV BD BIBP CTBMBV CKBV BD .
E06-1002.txt,151,In other words_comma_ this scenario addresses the task of disambiguating between entities with different top level categories under People by occupation.
E06-1002.txt,152,A4 CB BE In a slight generalization of CB BD _comma_ the set of categories BV BE is restricted to all categories under People by occupation.
E06-1002.txt,153,Each category must have at least 200 articles to be retained_comma_which results in a total of 540 categories out of the 8202 categories under People by occupation.
E06-1002.txt,154,The query dataset is generated as in the first scenario by replacing BV BD with BV BE .
E06-1002.txt,155,A4 CB BF This scenario is similar with CB BE _comma_ except that each category has to contain at least 20 articles to be retained_comma_ leading to 2847 out of 8202 categories.
E06-1002.txt,156,A4 CB BG This scenario uses the same categories as CB BE i.e. BV BG BPBV BE .
E06-1002.txt,157,In order to make the task more realistic_comma_ all queries from the initial Wikipedia dataset are considered as follows.
E06-1002.txt,158,For each query D5_comma_ out of all matching entities that do not have a category under People by occupation_comma_ one is randomly selected as an out of Wikipedia entity.
E06-1002.txt,159,Then_comma_ out of all queries for which the true answer is an out of Wikipedia entity_comma_ a subset is randomly selected such that_comma_ in the end_comma_ the number of queries with out of Wikipedia true answers is BDBCB1 of the total number of queries.
E06-1002.txt,160,In other words_comma_ the scenario assumes the task is to detect if a name denotes an entity belonging to the People by occupation taxonomy and_comma_ in the positive cases_comma_ to disambiguate between multiple entities under People by occupation that have the same name.
E06-1002.txt,161,The dataset for each scenario is split into a training dataset and a testing dataset which are disjoint in terms of the query names used in their examples.
E06-1002.txt,162,For instance_comma_ if a query for the name John Williams is included in the training dataset_comma_ then all other queries with this name are allocated for learning and consequently excluded from testing .
E06-1002.txt,163,Using a disjoint split is motivated by the fact that many Wikipedia queries that have the same true answer also have similar contexts_comma_ containing rare words that are highly correlated_comma_ almost exclusively_comma_ with the answer.
E06-1002.txt,164,For example_comma_ query names that refer to singers often contain album or song names_comma_ query names that refer to writers often contain book names_comma_ etc. The taxonomy kernel can easily memorize these associations_comma_ especially when the categories are very fine grained.
E06-1002.txt,165,In the current framework_comma_ the unsupervised method of context article similarity does not utilize the correlations present in the training data.
E06-1002.txt,166,Therefore_comma_ for the sake of comparison_comma_ we decided to prohibit the taxonomy kernel from using these correlations by training and testing on a disjoint split.
E06-1002.txt,167,Section 6 describes how the training queries could be used in the computation of the context article similarity_comma_ which has the potential of boosting the accuracy for both disambiguation methods.
E06-1002.txt,168,Table 3 shows a number of relevant statistics for each scenario CAT represents the number of Wikipedia categories_comma_ SV is the number of support vectors_comma_ TK A and Cos A are the accuracy of the Taxonomy Kernel and the Cosine similarity respectively.
E06-1002.txt,169,The training and testing datasets are characterized in terms of the number of queries and query answer pairs.
E06-1002.txt,170,The number of ranking contraints as specified in Figure 2 is also included for the training data in column CONSTR.
E06-1002.txt,171,The size of the training data is limited so that learning in each scenario takes within three days on a Pentium 4 CPU at 2.6 GHz.
E06-1002.txt,172,Furthermore_comma_ 15 in CB BG _comma_ the termination error criterion AF is changed from its default value of BCBMBCBCBD to BCBMBCBD.
E06-1002.txt,173,Also_comma_ the threshold AS for detecting out of Wikipedia entities when ranking with cosine similarity is set to the value that gives highest accuracy on training data.
E06-1002.txt,174,As can be seen in the last two columns_comma_ the Taxonomy Kernel significantly outperforms the Cosine similarity in the first three scenarios_comma_ confirming our intuition that correlations between words from the query context and categories from Wikipedia taxonomy provide useful information for disambiguating named entities.
E06-1002.txt,175,In the last scenario_comma_ which combines detection and disambiguation_comma_ the gain is not that substantial.
E06-1002.txt,176,Most queries in the corresponding dataset have only two possible answers_comma_ one of them an out of Wikipedia answer_comma_ and for these cases the cosine is already doing well at disambiguation.
E06-1002.txt,177,We conjecture that a more significant impact would be observed if the dataset queries were more ambiguous.
E06-1002.txt,178,6 Future Work The high number of support vectors half the number of query answer pairs in training data suggests that all scenarios can benefit from more training data.
E06-1002.txt,179,One method for making this feasible is to use the weight vector DB explicitely in a linear SVM.
E06-1002.txt,180,Because much of the computation time is spent on evaluating the decision function_comma_ using DB explicitely may result in a significant speed up.
E06-1002.txt,181,The dimensionality of DB by default CYCE CY A2 CYBVCY can be reduced significantly by considering only word category pairs whose frequency in the training data is above a predefined threshold.
E06-1002.txt,182,A complementary way of using the training data is to augment the article of each named entity with the contexts from all queries for which this entity is the true answer.
E06-1002.txt,183,This method has the potential of improving the accuracy of both methods when the training and testing datasets are not disjoint in terms of the proper names used in their queries.
E06-1002.txt,184,Word category correlations have been used in Ciaramita et al._comma_ 2003 to improve word sense disambiguation WSD _comma_ although with less substantial gains.
E06-1002.txt,185,There_comma_ a separate model was learned for each of the 29 ambiguous nouns from the Senseval 2 lexical sample task.
E06-1002.txt,186,While creating a separate model for each named entity is not feasible there are 94_comma_875 titles under People by occupation named entity disambiguation can nevertheless benefit from correlations between Wikipedia categories and features traditionally used in WSD such as bigrams and trigrams centered on the proper name occurrence_comma_ and syntactic information.
E06-1002.txt,187,7 Conclusion We have presented a novel approach to named entity detection and disambiguation that exploited the untapped potential of an online encyclopedia.
E06-1002.txt,188,Experimental results show that using the Wikipedia taxonomy leads to a substantial improvement in accuracy.
E06-1002.txt,189,The application of the new named entity disambiguation method holds the promise of better results to popular web searches.
E06-1002.txt,190,8 Acknowledgments We would like to thank Peter Dienes_comma_ Thorsten Joachims_comma_ and the anonymous reviewers for their helpful comments. .
E06-2004.txt,1,demonstrate a new development environment1 Information State Update dialogue systems which allows non expert developers to produce complete spoken dialogue systems based only on a Business Process Model BPM describing their application e.g. banking_comma_ cinema booking_comma_ shopping_comma_ restaurant information . The environment includes automatic generation of Grammatical Framework GF grammars for robust interpretation of spontaneous speech_comma_ and uses application databases to generatelexicalentries and grammar rules.
E06-2004.txt,2,The GF grammar is compiled to an ATK or Nuance language model for speech recognition.
E06-2004.txt,3,The demonstration system allows users to create and modify spoken dialogue systems_comma_ starting with a definition of a BusinessProcessModelandendingwithaworking system.
E06-2004.txt,4,This paper describes the environment_comma_ its main components_comma_and someof the research issues involved in its development.
E06-2004.txt,5,1 . Business Process Modelling and Contact Centres Many companies use business process models BPMs tospecifycommunicative andmanyother actions that must be performed in order to complete various tasks e.g. verify customer identity_comma_ pay a bill .
E06-2004.txt,6,See for example BPEL4WS 2 Andrews_comma_ 2003 .
E06-2004.txt,7,These representations specify states of processes or tasks_comma_ transitions between the states_comma_ and conditions on transitions see e.g. thecinemabookingexamplein figure1 .
E06-2004.txt,8,Typically_comma_ a human telephone operator using a presentation of a BPM on a GUI will step through these states with a customer_comma_ during a telephone interaction e.g. in a contact centre _comma_ in order to complete a business process.
E06-2004.txt,9,Note_comma_ however_comma_ that BPM representations do not 1This research is supported by Scottish Enterprise under the Edinburgh Stanford Link programme.
E06-2004.txt,10,We thank Graham Technology for their collaboration.
E06-2004.txt,11,2Business Process Execution Language forWeb Services. traditionallymodeldialoguecontext_comma_ so that as well as speech recognition_comma_ interpretation_comma_ and production the human operator is responsible for a2 contextual interpretation of incoming speech a2 maintaining and updating dialogue context a2 dialogue strategy e.g. implicit explicit confirmation_comma_ initiative management .
E06-2004.txt,12,Figure 1 Part of an example Business Process Model cinema booking in the GT X7 system Graham Technology plc_comma_ 2005 version 1.8.0 .
E06-2004.txt,13,A major advantage of current BPM systems as well as their support for database access and enterprise system integration etc. is their graphical development and authoring environments.
E06-2004.txt,14,See for example figure 1 from the GT X7 system Graham Technology plc_comma_ 2005 _comma_ version 1.8.0.
E06-2004.txt,15,This shows part of a BPM for a cinema booking process.
E06-2004.txt,16,First top left .
E06-1046.txt,1,grammars provide an expressive formalism for multimodal integration and understanding. However_comma_ handcrafted multimodal grammars can be brittle with respect to unexpected_comma_ erroneous_comma_ or disfluent inputs.
E06-1046.txt,2,Spoken language speech only understanding systems have addressed this issue of lack of robustness of hand crafted grammars by exploiting classification techniques to extract fillers of a frame representation.
E06-1046.txt,3,In this paper_comma_ we illustrate the limitations of such classification approaches for multimodal integration and understanding and present an approach based on edit machines that combine the expressiveness of multimodal grammars with the robustness of stochasticlanguage models ofspeech recognition.
E06-1046.txt,4,We also present an approach where the edit operations are trained from data using a noisy channel model paradigm.
E06-1046.txt,5,We evaluate and compare the performance of the hand crafted and learned edit machines in the context of a multimodal conversational system MATCH .
E06-1046.txt,7,Over the years_comma_ there have been several multimodal systems that allow input and or output to be conveyed over multiple channels such as speech_comma_ graphics_comma_ and gesture_comma_ for example_comma_ put that there Bolt_comma_ 1980 _comma_ CUBRICON Neal and Shapiro_comma_ 1991 _comma_ QuickSet Cohen et al._comma_ 1998 _comma_ SmartKom Wahlster_comma_ 2002 _comma_ Match Johnston et al._comma_ 2002 .
E06-1046.txt,8,Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars Johnston and Bangalore_comma_ 2000 .
E06-1046.txt,9,These grammars support composite multimodal inputs by aligning speech input words and gesture input represented as sequences of gesture symbols while expressing the relation between the speech and gesture input and their combined semantic representation.
E06-1046.txt,10,In Bangalore and Johnston_comma_ 2000 Johnston and Bangalore_comma_ 2005 _comma_ we have shown that such grammars can be compiled into finite state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities.
E06-1046.txt,11,However_comma_ like other approaches based on handcrafted grammars_comma_ multimodal grammars can be brittle with respect to extra grammatical_comma_ erroneous and disfluent input.
E06-1046.txt,12,For speech recognition_comma_ a corpus driven stochastic language model SLM with smoothing or a combination of grammarbased and a0 gram model Bangalore and Johnston_comma_ 2004 Wang et al._comma_ 2002 can be built in order to overcome the brittleness of a grammar based language model.
E06-1046.txt,13,Although the corpus driven language model might recognize a user s utterance correctly_comma_ the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar.
E06-1046.txt,14,There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature.
E06-1046.txt,15,First_comma_ a parsing based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise_comma_ in order to construct a partial semantic representation Dowding et al._comma_ 1993 Allen et al._comma_ 2001 Ward_comma_ 1991 .
E06-1046.txt,16,Second_comma_ a classification based approach views the problem of understanding as extracting certain bits of information from the input.
E06-1046.txt,17,It attempts to classify the utterance and identifies substrings of the input as slot filler values to construct a frame like semantic representation.
E06-1046.txt,18,Both approaches have shortcomings.
E06-1046.txt,19,Although in the first approach_comma_ the grammar can encode richer semantic representations_comma_ the method for combining the fragmented parses is quite ad hoc.
E06-1046.txt,20,In the second approach_comma_ the robustness is derived from training classifiers on annotated data_comma_ this data is very expensive to collect and annotate_comma_ and the semantic representation is fairly limited.
E06-1046.txt,21,Furthermore_comma_ it is not clear how to extend this approach to apply on lattice input an important requirement for multimodal processing.
E06-1046.txt,22,361 An alternative to these approaches is to edit the recognized string to match the closest string that can be accepted by the grammar.
E06-1046.txt,23,Essentially the idea is that_comma_ if the recognized string cannot be parsed_comma_ then we determine which in grammar string it is most like.
E06-1046.txt,24,For example_comma_ in Figure 1_comma_ the recognized string is mapped to the closest string in the grammar by deletion of the words restaurants and in.
E06-1046.txt,25,ASR show cheap restaurants thai places in in chelsea Edits show cheap a1 thai places in a1 chelsea Grammar show cheap thai places in chelsea Figure 1 Editing Example In this paper_comma_ we develop further this edit based approach to finite state multimodal language understanding and show how when appropriately tuned it can provide a substantial improvement in concept accuracy.
E06-1046.txt,26,We also explore learning edits from data and present an approach of modeling this process as a machine translation problem.
E06-1046.txt,27,Welearn amodel to translate from out of grammar ormisrecognized language such as ASR above to the closest language the system can understand Grammar above .
E06-1046.txt,28,To this end_comma_ we adopt techniques from statistical machine translation Brown et al._comma_ 1993 Och and Ney_comma_ 2003 and use statistical alignment to learn the edit patterns.
E06-1046.txt,29,Here we evaluate these different techniques on data from the MATCHmultimodal conversational system Johnston et al._comma_ 2002 but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal.
E06-1046.txt,30,The layout of the paper is as follows.
E06-1046.txt,31,In Sections 2 and 3_comma_ we briefly describe the MATCH application and the finite state approach to multimodal language understanding.
E06-1046.txt,32,In Section 4_comma_ we discuss the limitations of the methods used for robust understanding in spoken language understanding literature.
E06-1046.txt,33,In Section 5 we present our approach to building hand crafted edit machines.
E06-1046.txt,34,In Section 6_comma_ we describe our approach to learning the edit operations using a noisy channel paradigm.
E06-1046.txt,35,In Section 7_comma_ we describe our experimental evaluation.
E06-1046.txt,36,2 MATCH A Multimodal Application MATCH Multimodal Access To City Help is a working city guide and navigation system that enables mobile users to access restaurant and subway information for New York City and Washington_comma_ D.C.
E06-1046.txt,37,Johnston et al._comma_ 2002 .
E06-1046.txt,38,The user interacts with an interface displaying restaurant listings and a dynamic map showing locations and street information.
E06-1046.txt,39,The inputs can be speech_comma_ drawing pointing on the display with a stylus_comma_ or synchronous multimodal combinations of the two modes.
E06-1046.txt,40,The user can ask for the review_comma_ cuisine_comma_ phone number_comma_ address_comma_ or other information about restaurants and subway directions to locations.
E06-1046.txt,41,The system responds with graphical labels on the display_comma_ synchronized with synthetic speech output.
E06-1046.txt,42,Forexample_comma_ if the user says phone numbers for these two restaurants and circles two restaurants asinFigure2 A _comma_thesystem willdraw a callout with the restaurant name and number and say_comma_ for example Time Cafe can be reached at 212533 7000_comma_ for each restaurant in turn Figure 2 B .
E06-1046.txt,43,Figure 2 MATCH Example 3 Finite state Multimodal Understanding Our approach to integrating and interpreting multimodal inputs Johnston et al._comma_ 2002 is an extension of the finite state approach previously proposed in Bangalore and Johnston_comma_ 2000 Johnston and Bangalore_comma_ 2005 .
E06-1046.txt,44,In this approach_comma_ a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands.
E06-1046.txt,45,The grammar consists of a set of context free rules.
E06-1046.txt,46,The multimodal aspects of the grammar become apparent in the terminals_comma_ each of which is a triple W G M_comma_ consisting of speech words_comma_ W _comma_ gesture gesture symbols_comma_ G _comma_ and meaning meaning symbols_comma_ M .
E06-1046.txt,47,The multimodal grammar encodes not just multimodal integration patterns but also the syntax of speech and gesture_comma_ and the assignment of meaning_comma_ here represented in XML.
E06-1046.txt,48,The symbol SEM is used to .
E06-2027.txt,1,paper describes a study in which a corpus of spoken Danish annotated with focus and topic tags was used to investigate the relation between information structure and pauses. The results show that intra clausal pauses in the focus domain_comma_ tend to precede those words that express the property or semantic type whereby the object in focus is distinguished from other ones in the domain.
E06-2027.txt,3,The interest for corpora annotated with information structure has been raised recently by several authors.
E06-2027.txt,4,Kruijff Korbayov a and Kruijff 2004 describe a method where a rich discourselevel annotation is used to investigate information structure_comma_ while both Postolache 2005 and Diderichsen and Elming 2005 study the application of machine learning to the problem of automatic identification of topic and focus.
E06-2027.txt,5,In this study_comma_ on the contrary_comma_ information structure is annotated manually_comma_ and the annotation is used to investigate the correlation between information structure tags and intra clausal pauses.
E06-2027.txt,6,2 Annotating information structure The starting point for this study was the corpus of spoken Danish DanPass Gr nnum_comma_ 2005 _comma_ a collection of 54 monologues produced by 18 different subjects dealing with three well defined tasks_comma_ following the methodology established in Terken 1985 .
E06-2027.txt,7,In the first task_comma_ the subjects describe a geometrical network_comma_ in the second the process of assembling the drawing of a house out of existing pieces_comma_ and in the third they solve a map task.
E06-2027.txt,8,The corpus has been annotated with several annotation tiers_comma_ including orthography_comma_ phonetic transcription_comma_ pauses and PoS tags.
E06-2027.txt,9,Two independent annotators added then tags for focus and topic based on a set of simple guidelines_comma_ and using the Praat tool to carry out the annotation.
E06-2027.txt,10,The annotation reflects the assumption that a sentence can be divided into an obligatory focus part_comma_ which expresses the non presupposed information_comma_ and a presupposed background part.
E06-2027.txt,11,A referent in the background part may function as the sentence topic in the sense of Lambrecht 1994 .
E06-2027.txt,12,For each sentence in the corpus_comma_ the annotators were asked to identify what they intuitively considered non presupposed information and annotate it as belonging to the focus.
E06-2027.txt,13,Technically_comma_ each word belonging to the focus is added a focus tag.
E06-2027.txt,14,The annotators were also asked to test whether they could single out a sentence referent by means of the What about X test Reinhart_comma_ 1981 .
E06-2027.txt,15,If they could_comma_ they were asked to add topic tags to all the words making up the corresponding expression.
E06-2027.txt,16,Words not bearing any tag are considered part of the background.
E06-2027.txt,17,The guidelines did not contain any reference to pausing_comma_ nor did the annotators know that their work would be used to study the correlation be191 Focus Topic No tag Total Network C1 1608 268 2526 4402 Network C2 1889 287 2226 4402 House C1 4025 386 4151 8562 House C2 4193 377 3992 8562 Table 1 Tags in two corpus sections tween pauses and information structure.
E06-2027.txt,18,In fact_comma_ that was not the purpose of the annotation work_comma_ which is of more general interest.
E06-2027.txt,19,It should also be noted that the annotators were not explicitly instructed to code phrases_comma_ since we did not want to make the assumption that topic or focus necessarily correspond to syntactic phrases.
E06-2027.txt,20,Approximately two person months were spent annotating two sections of the corpus.
E06-2027.txt,21,The kappa score varied between 0.7 to 0.8 depending on the corpus section_comma_ showing an acceptable interannotator agreement.
E06-2027.txt,22,Most disagreements relate to the identification of the focus left hand boundary_comma_ where one of the annotators sometimes identified wider focus domains than the other.
E06-2027.txt,23,These differences have not been inspected yet_comma_ but will be used to revise the guidelines to produce a unique consistent annotation.
E06-2027.txt,24,Table 1 shows the number of tags assigned by the two coders C1 and C2 in the two sections of the corpus coded so far.
E06-2027.txt,25,Below_comma_ an example of an annotated tier is shown in a linearised format the textgrids output by Praat also contain time intervals that link the transcription to the sound file 1 ovenover er der en F gr n F cirkel F og oven over den T gr nne T cirkel T er der en F lilla F trekant F PAUSEabove PAUSE there is F a PAUSE green circle PAUSEand above T the green circle there is F a PAUSEpurple triangle The example consists of two sentences.
E06-2027.txt,26,In the first_comma_ the annotator has tagged en gr n cirkel a green circle as the focus in the second_comma_ den gr nne cirkel the green circle has been tagged as the topic_comma_ while en lilla trekant a purple triangle is tagged as the focus.
E06-2027.txt,27,Pauses are indicated by  and .
E06-2027.txt,28,Theformer is asilent pause_comma_ and the latter a pause accompanied by a sound_comma_ like hmm .
E06-2027.txt,29,Pauses were already available in the orthographic transcription of the corpus_comma_ which was produced earlier by different annotators.
E06-2027.txt,30,3 Pauses in earlier studies The material annotated so far already gives us the possibility to investigate whether there is a significant relation between pauses and information structure.
E06-2027.txt,31,Earlier studies Jensen_comma_ 2005 Hansen et al._comma_ 1993 investigated the effect of syntactic boundaries clausal as well as phrasal on the placing of pauses in spoken Danish.
E06-2027.txt,32,In the first study_comma_ it is found that more than 55 of the pauses co occur with clause boundaries_comma_ 12 with phrase boundaries_comma_ and the remaining 33 occur within phrases or in conjunction with repairs_comma_ interjections and enumerations.
E06-2027.txt,33,It is also noted that pauses falling within a syntactic phrase tend to be placed in the final part of the sentence.
E06-2027.txt,34,The second study confirms this observation by showing that 60 of the pauses that do not co occur with syntactic boundaries occur within the last 40 of the sentence measured in number of syllables .
E06-2027.txt,35,The authors of both investigations make the hypothesis that information structure may have an effect on the occurrence of pauses within clauses.
E06-2027.txt,36,However_comma_ the empirical material used in those works is not annotated with respect to information structure_comma_ and therefore_comma_ no conclusive claim could be made.
E06-2027.txt,37,In addition_comma_ the data used in Hansen et al 1993 come from news reading_comma_ and are thus essentially written language although delivered orally.
E06-2027.txt,38,4 Pauses and focusing in Danish The purpose of this pilot study is_comma_ on the basis of the annotated DanPass corpus_comma_ to verify i. to what degree pauses tend to be associated with focus and topic_comma_ and ii. where in the focus domain pauses tend to occur_comma_ particularly whether pauses are used to mark the left hand focus boundary.
E06-2027.txt,39,Since we already know from the studies cited above that there is a strong tendency for pauses to coincide with clause boundaries_comma_ we decided 192 F word T word No tag Total Pause 20.29 7.59 39.70 28.34 No pause 79.71 92.41 60.30 71.66 Total 100 100 100 100 Table 2 Distribution of pauses over information structure categories  to exclude those from the study_comma_ and only look at pauses that occur within clauses.
E06-2027.txt,40,So far_comma_ the investigation has been carried out for the network description part of the corpus_comma_ and only for the data produced by one of the coders.
E06-2027.txt,41,The first question whether pauses relate to words coded as either focus or topic was investigated by counting_comma_ out of a total 3659 words_comma_ how many words tagged as either F or T_comma_ or bearing no tag_comma_ are preceded by a pause silent or non silent .
E06-2027.txt,42,The results_comma_ shown in Tables 2 _comma_ seem to disconfirm the hypothesis that there should be a correlation between pauses and information structure categories_comma_ or at least that a correlation_comma_ if it exists_comma_ can be expressed by looking at the frequency with which pauses precede focus or topic words.
E06-2027.txt,43,In fact_comma_ over 65 of the intra clausal pauses in the material precede untagged words_comma_ and the observed frequency of a pause before a focus or a topic word is lower than the average 28.34 baseline .
E06-2027.txt,44,Since we know that topics often occur sentence initially_comma_ the results in the tables are misleading in that only intra clausal pauses are taken into consideration.
E06-2027.txt,45,Therefore we also looked at what percentage of topic words are succeeded rather than preceded by a pause_comma_ and found that 33.50 are.
E06-2027.txt,46,This figure is interesting_comma_ but needs further investigations.
E06-2027.txt,47,Now we zoom in on the focus domain.
E06-2027.txt,48,First of all_comma_ we look at pause distribution across different part of speech categories_comma_ again by inspecting the pauses preceding words.
E06-2027.txt,49,Table 3 shows the frequency with which different partof speech categories occurring in the focus domain i.e. tagged F are preceded by a pause.
E06-2027.txt,50,The total no. of words considered is 1661.
E06-2027.txt,51,The interesting fact that emerges is that adjectives have a remarkably higher probability to be preceded by a pause than any of the other category_comma_ and also a clearly higher probability than the average 28.34 .
E06-2027.txt,52,We then looked at the first pause in the focus domain.
E06-2027.txt,53,The first pause falls before the first focus word in only 30 of the cases.
E06-2027.txt,54,In other words_comma_ it does not seem to mark the left hand boundary of the focus domain.
E06-2027.txt,55,By running a decision tree generator Witten and Eibe_comma_ 2005 on the data_comma_ we found that the strongest rule learnt by the system was one that places the first pause in the focus domain between a determiner and an adjective 2 .
E06-2027.txt,56,Another rule predicts that a pause will fall between an adjective and a noun 3 .
E06-2027.txt,57,2 tilbage er der... en F r d F firkant F left there is... F a PAUSE red square 3 til venstre... l gger du en F r d F firkant F to the left... you put F a red PAUSE square The two rules reflect a strong characteristic of the monologues under investigation_comma_ where the speakers have to draw the listener s attention to the various geometrical figures in the network they are describing.
E06-2027.txt,58,To tell them apart from each other_comma_ they either use the colour of the figure or its shape.
E06-2027.txt,59,In other words_comma_ the pauses occurring in the focus domain tend to precede the word that expresses what Dik 1989 calls selecting focus_comma_ here an adjective that_comma_ by defining aselecting property or type_comma_ helps distinguishing the object in focus from other similar ones.
E06-2027.txt,60,From the point of view of accentuation_comma_ however_comma_ the adjective is not more prominent than the noun_comma_ and is therefore not annotated as the only word in focus.
E06-2027.txt,61,5 Conclusions and further research In conclusion_comma_ the pilot study shows that words making up the topic or the focus of a sentence do not show a general tendency to be preceded by pauses.
E06-2027.txt,62,However_comma_ preliminary results indicate that topics tend to be followed by pauses.
E06-2027.txt,63,Furthermore_comma_ words belonging to specific syntatic 193 Adj Adv Conj Det N Prep Part Pro Verb Other Total Pause 36.34 6.94 16.67 18.97 17.11 19.83 25.00 4.76 6.33 20.00 20.29 No pause 63.66 93.06 83.33 81.03 82.89 80.17 75.00 95.24 93.67 80.00 79.71 Total 100 100 100 100 100 100 100 100 100 100 100 Table 3 Distribution of pauses over part of speech categories in the focus domain  categories may have a significantly higher probability to be preceded by a pause than a randomly chosen word.
E06-2027.txt,64,In the corpus we have worked with_comma_ these words express the property or semantic type whereby the object in focus can be distinguished from other similar objects.
E06-2027.txt,65,In other words_comma_ the system by which Danish speakers use pauses seems sensitive to information structure in a subtle way that_comma_ at least as far as focus is concerned_comma_ creates boundaries that do not necessarily correspond to those between syntactic constituents.
E06-2027.txt,66,An interesting issue we haven t yet addressed is whether intra clausal pauses relate to prosodic phrases_comma_ which according to Steedman 2001 correspond to information structural constituents.
E06-2027.txt,67,Since the DanPass annotation also foresees a tier for prosodic phrases_comma_ this investigation is possible.
E06-2027.txt,68,Furthermore_comma_ we want to test whether there are differences in the way in which different users relate pauses to topic establishment and focusing.
E06-2027.txt,69,We know already now that the percentage of pauses per word varies across speakers_comma_ and that speakers individual pause rates do not vary much depending on the task.
E06-2027.txt,70,The corpus provides a very nice means of studying whether they use pauses for different purposes.
E06-2027.txt,71,Acknowledgements This work was supported by the Carlsberg Foundation. .
E06-1023.txt,1,paper proposes a method for dealing with repairs in action control dialogue to resolve participants misunderstanding. The proposed method identifies the repair target based on common grounding ratherthansurfaceexpressions.
E06-1023.txt,2,Weextend Traum s grounding act model by introducing degree of groundedness_comma_ and partial and mid discourse unit grounding.
E06-1023.txt,3,This paper contributes to achieving more natural human machine dialogue and instantaneous and flexible control of agents.
E06-1023.txt,5,In natural language dialogue_comma_ misunderstanding and its resolution is inevitable for the natural course of dialogue.
E06-1023.txt,6,The past research dealing with misunderstanding has been focused on the dialogue involving only utterances.
E06-1023.txt,7,In this paper_comma_ we discuss misunderstanding problem in the dialogue involving participant s actions as well as utterances.
E06-1023.txt,8,In particular_comma_ we focus on misunderstanding in action control dialogue.
E06-1023.txt,9,Action control dialogue is a kind of taskoriented dialogue in which a commander controls the actions1 of other agents called followers through verbal interaction.
E06-1023.txt,10,This paper deals with disagreement repair initiation utterances2 DRIUs which are used by commanders to resolve followers misunderstandings3_comma_ or to correct commanders previous erroneous utterances.
E06-1023.txt,11,These are so called third turn 1We use the term action for the physical behavior of agents except for speaking.
E06-1023.txt,12,2This denomination is lengthy and may be still controversial.
E06-1023.txt,13,However we think this is most descriptively adequate for the moment.
E06-1023.txt,14,3Misunderstanding is a state where miscommunication has occurred but participants are not aware of this_comma_ at least initially Hirst et al._comma_ 1994 . repair Schegloff_comma_ 1992 .
E06-1023.txt,15,Unlike in ordinary dialogue consisting of only utterances_comma_ in action control dialogue_comma_ followers misunderstanding could be manifested as their inappropriate actions in response to a given command.
E06-1023.txt,16,Let us look at a sample dialogue 1.1 1.3 .
E06-1023.txt,17,Utterance 1.3 is a DRIU for repairing V s misunderstanding of command 1.1 which is manifested by his action performed after saying OK in 1.2 .
E06-1023.txt,18,1.1 U Put the red book on the shelf to the right.
E06-1023.txt,19,1.2 V OK. V performs the action 1.3 U Not that.
E06-1023.txt,20,It is not easy for machine agents to understand DRIUs because they can sometimes be so elliptical and context dependent that it is difficult to apply traditional interpretation methodology to DRIUs.
E06-1023.txt,21,In the rest of this paper_comma_ we describe the difficulty of understanding DRIUs and propose a method to identify repair targets.
E06-1023.txt,22,The identification of repair targets plays a key role in understanding DRIUs and this paper is intensively focused on this issue.
E06-1023.txt,23,2 Difficulty of Understanding DRIUs Understanding a DRIU consists of repair target identification and repair content interpretation.
E06-1023.txt,24,Repair target identification identifies a target to be repaired by the speaker s utterance.
E06-1023.txt,25,Repair content interpretation recovers the speaker s intention by replacing the identified repair target with the correct one.
E06-1023.txt,26,One of the major source of difficulties in understanding DRIUs is that they are often elliptical.
E06-1023.txt,27,Repair content interpretation depends heavily on repair targets but the information to identify repair targets is not always mentioned explicitly in DRIUs.
E06-1023.txt,28,Let us look at dialogue 1.1 1.3 again.
E06-1023.txt,29,The DRIU 1.3 indicates that V failed to identify U s intended object in utterance 1.1 .
E06-1023.txt,30,However_comma_ 1.3 does not explicitly mention the repair target_comma_ i.e._comma_ either book or shelf in this case.
E06-1023.txt,31,The interpretation of 1.3 changes depending on when it is uttered.
E06-1023.txt,32,More specifically_comma_ the interpretation depends on the local context and the situation when the DRIU is uttered.
E06-1023.txt,33,If 1.3 is uttered when V is reaching for a book_comma_ it would be natural to consider that 1.3 is aimed at repairing V s interpretation of the book .
E06-1023.txt,34,On the other hand_comma_ if 1.3 is uttered when V is putting the book on a shelf_comma_ it would be natural to consider that 1.3 is aimed at repairing V s interpretation of the shelf to the right .
E06-1023.txt,35,AssumethatUuttered 1.3 whenVwasputting a book in his hand on a shelf_comma_ how can V identify the repair target as shelf instead of book
E06-1023.txt,36,This paper explains this problem on the basis of common grounding Traum_comma_ 1994 Clark_comma_ 1996 .
E06-1023.txt,37,Common grounding or shortly grounding is the process of building mutual belief among a speaker and hearers through dialogue.
E06-1023.txt,38,Note that in action control dialogue_comma_ we need to take into account not only utterances but also followers actions.
E06-1023.txt,39,To identify repair targets_comma_ we keep track of states of grounding by treating followers actions as grounding acts see Section 3 .
E06-1023.txt,40,Suppose V is placing a book in his hand on a shelf.
E06-1023.txt,41,At this moment_comma_ V s interpretation of the book in 1.1 has been already grounded_comma_ since U did not utter any DRIU when V was taking the book.
E06-1023.txt,42,This leads to the interpretation that the repair target of 1.1 is shelf rather than already grounded book.
E06-1023.txt,43,3 Grounding This section briefly reviews the grounding acts model Traum_comma_ 1994 which we adopted in our framework.
E06-1023.txt,44,We will extend the grounding act model by introducing degree of groundedness that have a quaternary distinction instead of the original binary distinction.
E06-1023.txt,45,The notions of partial grounding and mid discourse unit grounding are also introduced for dealing with action control dialogue.
E06-1023.txt,46,3.1 Grounding Acts Model The grounding acts model is a finite state transition model to dynamically compute the state of grounding in a dialogue from the viewpoint of each participant.
E06-1023.txt,47,This theory models the process of grounding with a theoretical construct_comma_ namely the discourse unit DU .
E06-1023.txt,48,A DU is a sequence of utterance units UUs assigned grounding acts GAs .
E06-1023.txt,49,Each UU in a dialogue has at least one GA_comma_ except fillers or several cue phrases_comma_ which are considered useful for turn taking but not for grounding.
E06-1023.txt,50,Each DU has an initiator I who opened it_comma_ and other participants of that DU are called responders R .
E06-1023.txt,51,Each DU is in one of seven states listed in Table1atatime.
E06-1023.txt,52,GivenoneofGAsshowninTable2 as an input_comma_ the state of DU changes according to the current state and the input.
E06-1023.txt,53,A DU starts with a transition from initial state S to state 1_comma_ and finishes at state F or D.
E06-1023.txt,54,DUs in state F are regarded as grounded.
E06-1023.txt,55,Analysis of the grounding process for a sample dialogue is illustrated in Figure 1.
E06-1023.txt,56,Speaker B can not understand the first utterance by speaker A and requests a repair ReqRep R with his utterance.
E06-1023.txt,57,Responding to this request_comma_ A makes a repair Repair I .
E06-1023.txt,58,Finally_comma_ B acknowledges to show he has understood the first utterance and the discourse unit reaches the final state_comma_ i.e._comma_ state F.
E06-1023.txt,59,State Description S Initial state 1 Ongoing 2 Requested a repair by a responder 3 Repaired by a responder 4 Requested a repair by the initiator F Finished D Canceled Table 1 DU states Grounding act Description Initiate Begin a new DU Continue Add related content Ack Present evidences of understanding Repair Correct misunderstanding ReqRepair Request a repair act ReqAck Request an acknowledge act Cancel Abandon the DU Table 2 Grounding acts UU DU1 A Can I speak to Jim Johnstone please
E06-1023.txt,60,Init I 1 B Senior
E06-1023.txt,61,ReqRep R 2 A Yes Repair I 1 B Yes Ack R F Figure 1 An example of grounding Ishizaki and Den_comma_ 2001 178 3.2 Degree of Groundedness and Evidence Intensity AsTraumadmitted_comma_thebinarydistinctionbetween grounded and ungrounded in the grounding acts model is an oversimplification Traum_comma_ 1999 .
E06-1023.txt,62,Repair target identification requires more finely defined degree of groundedness.
E06-1023.txt,63,The reason for this will be elucidated in Section 5.
E06-1023.txt,64,Here_comma_ we will define the four levels of evidence intensity and equate these with degrees of groundedness_comma_ i.e._comma_ if an utterance is grounded with evidence of level N intensity_comma_ the degree of groundedness of the utterance is regarded as levelN.
E06-1023.txt,65,2 Levels of evidence intensity Level 0 No evidence i.e._comma_ not grounded .
E06-1023.txt,66,Level 1 The evidence shows that the responder thinks he understood the utterance.
E06-1023.txt,67,However_comma_ it does not necessarily mean that the responder understood it correctly.
E06-1023.txt,68,E.g._comma_ the acknowledgment OK in response to the request turn to the right. Level 2 The evidence shows that the responder partially succeeded in transferringsurfacelevelinformation.
E06-1023.txt,69,Itdoes not yet ensure that the interpretation of the surface information is correct.
E06-1023.txt,70,E.g._comma_ the repetition to the right in response to the request turn to the right. Level 3 The evidence shows that the responder succeeded in interpretation.
E06-1023.txt,71,E.g._comma_ turning to the right as the speaker intended in response to the request turn to the right. 3.3 Partial and mid DU Grounding In Traum s grounding model_comma_ the content of a DU is uniformly grounded.
E06-1023.txt,72,However_comma_ things in the same DU should be more finely grounded at various levels individually.
E06-1023.txt,73,For example_comma_ if one acknowledged by saying to the right in response to the command put the red chair to the right of the table _comma_ to the right of should be regarded as grounded at Level 2 even though other parts of the request are grounded at Level 1.
E06-1023.txt,74,In addition_comma_ in Traum s model_comma_ the content of a DU is grounded all at once when the DU reaches the final state_comma_ F.
E06-1023.txt,75,However_comma_ some elements in a DU can be grounded even though the DU has not yet reached state F.
E06-1023.txt,76,For example_comma_ if one requested a repair as to the right of what in response to the command put the red chair to the right of the table _comma_ to the right of should be regarded as grounded at level 2 even though table has not yet been grounded.
E06-1023.txt,77,Although Traum admitted these problems existed in his model_comma_ he retained it for the sake of simplicity.
E06-1023.txt,78,However_comma_ such partial and mid DU grounding is necessary to identify repair targets.
E06-1023.txt,79,We will describe the usage of these devices to identify repair targets in Section 5.
E06-1023.txt,80,In brief_comma_ when a level 3 evidence is presented by the follower and negative feedback i.e._comma_ DRIUs is not provided by the commander_comma_ only propositions supported by the evidence are considered to be grounded even though the DU has not yet reached state F.
E06-1023.txt,81,4 Treatment of Actions in Dialogue In general_comma_ past work on discourse has targeted dialogue consisting of only utterances_comma_ or has consideredactionsassubsidiaryelements.
E06-1023.txt,82,Incontrast_comma_ this paper targets action control dialogue_comma_ where actions are considered to be primary elements of dialogue as well as utterances.
E06-1023.txt,83,Two issues have to be mentioned for handling action control dialogue in the conventional sequential representation as in Figure 1.
E06-1023.txt,84,We will introduce assumptions 3 and 4 as shown below.
E06-1023.txt,85,Overlap between utterances and actions Actions in dialogue do not generally obey turn allocationrulesasClarkpointedout Clark_comma_ 1996 .
E06-1023.txt,86,In human human action control dialogue_comma_ followers often start actions in the middle of a commander s utterance.
E06-1023.txt,87,This makes it difficult to analyze discourse in sequential representation.
E06-1023.txt,88,Given this fact_comma_ we impose the three assumptions on followers as shown in 3 so that followers actions will not overlap the utterances of commanders.
E06-1023.txt,89,These requirements are not unreasonable as long as followers are machine agents.
E06-1023.txt,90,3 Assumptions on follower s actions a The follower will not commence action until turn taking is allowed. b The follower immediately stops the action when the commander interrupts him.
E06-1023.txt,91,c Thefollowerwillnotmakeactionasprimary elements while speaking.
E06-1023.txt,92,4 4We regard gestures such as pointing as secondary ele179 Hierarchy of actions An action can be composed of several subactions_comma_ thus has a hierarchical structure.
E06-1023.txt,93,For example_comma_ making tea is composed of boiling the water_comma_ preparing the tea pot_comma_ putting tea leaves in the pot_comma_ and pouring the boiled water into it_comma_ and so on.
E06-1023.txt,94,To analyze actions in dialogue as well as utterances in the traditional way_comma_ a unit of analysis should be determined.
E06-1023.txt,95,We assume that there is a certain granularity of action that human can recognize as primitive.
E06-1023.txt,96,These actions would correspond to basic verbs common to humans such as walk _comma_ grasp _comma_ look _comma_ etc.We call these actions fundamental actions and consider them as UUs in action control dialogue.
E06-1023.txt,97,4 Assumptions on fundamental actions In the hierarchy of actions_comma_ there is a certain level consisting of fundamental actions thathumancancommonlyrecognizeasprimitives.
E06-1023.txt,98,Fundamental actions can be treated as units of primary presentations in an analogy with utterance units .
E06-1023.txt,99,5 Repair Target Identification In this section_comma_ we will discuss how to identify the repair target of a DRIU based on the notion of grounding.
E06-1023.txt,100,The following discussion is from the viewpoint of the follower.
E06-1023.txt,101,Let us look at a sample dialogue 5.1 5.5 _comma_ where U is the commander and V is the follower.
E06-1023.txt,102,The annotation Ack1 R F in 5.2 means that 5.2 has grounding act Ack by the responder R for DU1 and the grounding act made DU1 enter state F.
E06-1023.txt,103,The angle bracketed descriptions in 5.3 and 5.4 indicate the fundamental actions by V.
E06-1023.txt,104,Note that thanks to assumption 4 in Section 4_comma_ a fundamental action itself can be considered as a UU even though the action is performed without any utterances.
E06-1023.txt,105,5.1 U Puttheredballontheleftbox.
E06-1023.txt,106,Init1 I 1 5.2 V Sure.
E06-1023.txt,107,Ack1 R F 5.3 V V grasps the ball Init2 I 1 5.4 V V moves the ball Cont2 I 1 5.5 U Not that.
E06-1023.txt,108,Repair1 R 3 The semantic content of 5.1 can be represented as a set of propositions as shown in 6 . mentswhentheyarepresentedinparallelwithspeech.
E06-1023.txt,109,Therefore_comma_ this constraint does not apply to them.
E06-1023.txt,110,6 Request U_comma_V_comma_Put Agt1_comma_ Obj1_comma_ Dst1 a speechActType Request b presenter U c addressee V d actionType content  Put e agent content  Agt1_comma_ referent Agt1 V f object content  Obj1_comma_ referent Obj1 Ball1 g destination content  Dst1_comma_ referent Dst1 Box1 represents the entire content of 5.1 .
E06-1023.txt,111,Symbols beginning with a lower case letter are function symbols.
E06-1023.txt,112,For example_comma_ 6a means the speech act type for is Request .
E06-1023.txt,113,Symbols beginning with an upper case letter are constants.
E06-1023.txt,114,Request is the name of a speech act type and Move is that of fundamental action respectively.
E06-1023.txt,115,U and V represents dialogue participants and Ball1 represents an entity in the world.
E06-1023.txt,116,Symbols beginning with are notional entities introduced in the discourse and are called discourse referents.
E06-1023.txt,117,A discourse referent represents something referred to linguistically.
E06-1023.txt,118,During a dialogue_comma_ we need to connectdiscoursereferentstoentitiesintheworld_comma_but in the middle of the dialogue_comma_ some discourse referents might be left unconnected.
E06-1023.txt,119,As a result we can talk about entities that we do not know.
E06-1023.txt,120,However_comma_ when one takes some actions on a discourse referent_comma_ he must identify the entity in the world e.g._comma_ an object or a location corresponding to the discourse referent.
E06-1023.txt,121,Many problems in action control dialogue are caused by misidentifying entities in the world.
E06-1023.txt,122,Follower V interprets 5.1 to obtain 6 _comma_ and prepares an action plan 7 to achieve Put Agt1_comma_ Obj1_comma_ Dst1 .
E06-1023.txt,123,Plan 7 is executed downward from the top.
E06-1023.txt,124,7 Plan for Put Agt1_comma_ Obj1_comma_ Dst1 Grasp Agt1_comma_ Obj1 _comma_ Move Agt1_comma_ Obj1_comma_ Dst1 _comma_ Release Agt1_comma_ Obj1 Here_comma_ 5.1 5.5 are reformulated as in 8.1 8.5 .
E06-1023.txt,125,Perform represents performing the action.
E06-1023.txt,126,8.1 U Request U_comma_V_comma_Put Agt1_comma_ Obj1_comma_ Dst1 8.2 V Accept V_comma_ U_comma_ 8.3 V Perform V_comma_ U_comma_ Grasp Agt1_comma_ Obj1 180 8.4 V Perform V_comma_U_comma_Move Agt1_comma_ Obj1_comma_ Dst1 8.5 U Inform U_comma_ V_comma_ incorrect X To understand DRIU 5.5 _comma_ i.e._comma_ 8.5 _comma_ follower V has to identify repair target X in 8.5 referred to as that in 5.5 .
E06-1023.txt,127,In this case_comma_ the repair target of 5.5 X is the left box _comma_ i.e._comma_ Dst1.5 However_comma_ thepronoun that cannotberesolvedbyanaphora resolution only using textual information.
E06-1023.txt,128,We treat propositions_comma_ or bindings of variables and values_comma_ such as 6a 6g _comma_ as the minimum granularityofgroundingbecausetheidentification of repair targets requires that granularity.
E06-1023.txt,129,We then make the following assumptions concerning repair target identification.
E06-1023.txt,130,9 Assumptions on repair target identification a Locality of elliptical DRIUs The target ofanellipticalDRIUthatinterruptedthe follower s action is a proposition that is given an evidence of understanding by the interrupted action. b Instancy of error detection A dialogue participant observes his dialogue constantly and actions presenting strong evidence Level 3 .
E06-1023.txt,131,Thus_comma_ when there is an error_comma_ the commander detects it immediately once an action related to that error occurs.
E06-1023.txt,132,c Instancy of repairs If an error is found_comma_ the commander immediately interrupts the dialogue and initiates a repair against it. d Lack of negative evidence as positive evidence The follower can determine that his interpretation is correct if the commander does not initiates a repair against the follower s action related to the interpretation. e Priority of repair targets If there are several possible repair targets_comma_ the least grounded one is chosen.
E06-1023.txt,133,9a assumes that a DRIU can only be elliptical when it presupposes the use of local context to identify its target.
E06-1023.txt,134,It also predicts that if the target of a repair is neither local nor accessible within local information_comma_ the DRIU will not be elliptical dependingonlocalcontextbutcontainexplicitand 5We assume that there is a sufficiently long interval between the initiations of 5.4 and 5.5 . sufficient information to identify the target.
E06-1023.txt,135,9b and 9c enable 9a .
E06-1023.txt,136,Nakano et al. 2003 experimentally confirmed that we observe negative responses as well as positive responses in the process of grounding.
E06-1023.txt,137,According to their observations_comma_ speakers continue dialogues if negative responses are not found even when positive responses are not found.
E06-1023.txt,138,This evidence supports 9d .
E06-1023.txt,139,An intuitive rationale for 9e is that an issue with less proof would more probably be wrong than one with more proof.
E06-1023.txt,140,Now let us go through 8.2 to 8.5 again according to the assumptions in 9 .
E06-1023.txt,141,First_comma_ is grounded at intensity level 1 by 8.2 .
E06-1023.txt,142,Second_comma_ V executes Grasp Agt1_comma_ Obj1 at 8.3 .
E06-1023.txt,143,Because V does not observe any negative response from U even after this action is completed_comma_ V considers that the interpretations of Agt1 and Obj1 have been confirmed and grounded at intensity level 3 according to 9d this is the partial and mid DU groundingmentionedinSection3.3 .
E06-1023.txt,144,Afterinitiating Move Agt1_comma_ Obj1_comma_ Dst1 _comma_ V is interrupted by commander U with 8.5 in the middle of the action.
E06-1023.txt,145,V interprets elliptical DRIU 5.5 as Inform S_comma_ T_comma_ incorrect X _comma_ but he cannot identify repair target X.
E06-1023.txt,146,He tries to identify this from the discourse stateorcontext.
E06-1023.txt,147,Accordingto 9a _comma_ Vassumes that the repair target is a proposition that its interpretation is demonstrated by interrupted action 8.4 .
E06-1023.txt,148,Due to the nature of the word that _comma_ V knows that possible candidates are not types of action or the speech act but discourse referents Agt1_comma_ Obj1 and Dst16.
E06-1023.txt,149,Here_comma_ Agt1 and Obj1 have been grounded at intensity level 3 by the completion of 8.3 .
E06-1023.txt,150,Now_comma_ 9e tells V that the repair target is Dst1_comma_ which has only been grounded at intensity level 1 7.
E06-1023.txt,151,10 belowsummarizesthemethodofrepairtarget identification based on the assumptions in 9 .
E06-1023.txt,152,10 Repair target identification 6We have consistently assumed Japanese dialogues in this paper although examples have been translated into English.
E06-1023.txt,153,That is originally the pronoun sotti in Japanese_comma_ which can only refer to objects_comma_ locations_comma_ or directions_comma_ but cannot refer to actions.
E06-1023.txt,154,7There are two propositions concerned with Dst1 destination content  Dst1 and referent Dst1 Box1.
E06-1023.txt,155,However if dest content  Dst1 is not correct_comma_ this means that V grammatically misinterpreted 8.1 .
E06-1023.txt,156,It seems hard to imagine for participants speaking in their mother tongue and thus one can exclude dest content  Dst1 from the candidates of the repair target.
E06-1023.txt,157,181 a Specify the possible types of the repair target from the linguistic expression. b List the candidates matching the types determined in 10a from the latest presented content.
E06-1023.txt,158,c Rank candidates based on groundedness according to 9e and choose the top ranking one.
E06-1023.txt,159,Dependencies between Parameters The follower prepares an action plan to achieve the commander s command as in plan 7 .
E06-1023.txt,160,Here_comma_ the planned actions can contain parameters not directly corresponding to the propositions given by the commander.
E06-1023.txt,161,Sometimes a selected parameter by using 10 is not the true target but the dependent of the target.
E06-1023.txt,162,Agents must retrieve the true target by recognizing dependencies of parameters.
E06-1023.txt,163,For example_comma_ assume a situation where objects are not within the follower s reach as shown in Figure 2.
E06-1023.txt,164,Then_comma_ the commander issues command 6 to the follower Agent1 in Figure 2 and he prepares an action plan 11 .
E06-1023.txt,165,11 Agent1 s plan partial for 6 in Figure 2.
E06-1023.txt,166,Walk Agt1_comma_ Dst1 _comma_ Grasp Agt1_comma_ Obj1 _comma_ ... The first Walk is a prerequisite action for Grasp and Dst1 depends on Obj1.
E06-1023.txt,167,In this case_comma_ if referent Obj1 is Object1 then referent Dst1 is Position1_comma_ or if referent Obj1 is Object2 then referent Dst1 is Position2.
E06-1023.txt,168,Now_comma_ assume that the commander intends referent Obj1 to be Object2 with 6 _comma_ but the follower interprets this as referent Obj1 Object1 i.e._comma_ referent Dst1 Position1 and performs Walk Agt1_comma_ Dst1 .
E06-1023.txt,169,The commander then observes the follower moving towarda direction differentfrom his expectation and infers the follower has misunderstood the target object.
E06-1023.txt,170,He_comma_ then_comma_ interrupts the follower with the utterance not that at the timing illustrated in Figure 3.
E06-1023.txt,171,Because 10c chooses Dst2 as the repair target_comma_ the follower must be aware of the dependencies between parameters Dst1 and Obj1 to notice his misidentification of Obj1.
E06-1023.txt,172,6 Implementation and Some Problems We implemented the repair target identification method described in Section 5 into our prototype 1 P T J U J P O  H F O U 0 C K F D U  X S P O H 0 C K F D U  D P S S F D U 1 P T J U J P O Figure 2 Situation with dependent parameters 5 J N F 8 B M L  H U T U S B T Q  H U 0 C K P U U I B U Figure 3 Dependency between parameters dialogue system Figure 4 .
E06-1023.txt,173,The dialogue system has animated humanoid agents in its visualized 3D virtual world.
E06-1023.txt,174,Users can command the agent by speech to move around and relocate objects.
E06-1023.txt,175,Figure 4 Snapshot of the dialogue system Becauseourdomainisrathersmall_comma_currentpossible repair targets are agents_comma_ objects and goals of actions.
E06-1023.txt,176,According to the qualitative evaluation of the system through interaction with several subjects_comma_ most of the repair targets were correctlyidentifiedbytheproposedmethoddescribed in Section 5.
E06-1023.txt,177,However_comma_ through the evaluation_comma_ we found several important problems to be solved as below.
E06-1023.txt,178,6.1 Feedback Delay In a dialogue where participants are paying attention to each other_comma_ the lack of negative feedback can be considered as positive evidence see 9d .
E06-1023.txt,179,However_comma_ it is not clear how long the system needs towaitto consider thelack of negativefeedback as positive evidence.
E06-1023.txt,180,In some cases_comma_ it will be not appropriate to consider the lack of negative feedback 182 as positive evidence immediately after an action has been completed.
E06-1023.txt,181,Non linguistic information such as nodding and gazing should be taken into consideration to resolve this problem as Nakano et al._comma_ 2003 proposed.
E06-1023.txt,182,Positive feedback is also affected by delay.
E06-1023.txt,183,Whenonereceivesfeedbackshortlyafteranaction is completed and begins the next action_comma_ it may be difficult to determine whether the feedback is directed to the completed action or to the just started action.
E06-1023.txt,184,6.2 Visibility of Actions The visibility of followers actions must be considered.
E06-1023.txt,185,If the commander cannot observe the follower s action due to environmental conditions_comma_ the lack of negative feedback cannot be positive evidence for grounding.
E06-1023.txt,186,For example_comma_ assume the command bring me a big red cup from the next room is given and assume that the commander cannot see the inside of the next room.
E06-1023.txt,187,Because the follower s fundamental action of taking a cup in the next room is invisible to the commander_comma_ it cannot be grounded at that time.
E06-1023.txt,188,They have to wait for the return of the follower with a cup.
E06-1023.txt,189,6.3 Time dependency of Grounding Utterances are generally regarded as points on the time line in dialogue processing.
E06-1023.txt,190,However_comma_ this approximation cannot be applied to actions.
E06-1023.txt,191,One action can present evidences for multiple propositions but it will present these evidences at considerably different time.
E06-1023.txt,192,This affects repair target identification.
E06-1023.txt,193,Let us look at an action Walk Agt_comma_ Dst _comma_ where agent Agt walks to destination Dst.
E06-1023.txt,194,This action will present evidence for who is the intended agent Agt at the beginning.
E06-1023.txt,195,However_comma_ the evidence for where is the intended position Dst will require the action to be completed.
E06-1023.txt,196,However_comma_ if the position intended by the follower is in a completely different direction from the one intended by the commander_comma_ his misunderstanding will be evident at a fairly early stage of the action.
E06-1023.txt,197,6.4 Differences in Evidence Intensities between Actions Evidence intensities vary depending on the characteristics of actions.
E06-1023.txt,198,Although the symbolic description of actions such as 12 and 13 does not explicitly represent differences in intensity_comma_ there is a significant difference between 12 where Agent looks at Object at a distance_comma_ and 13 where Agent directly contacts Object.
E06-1023.txt,199,Agents must recognize these differences to conform with human recognition and share the same state of grounding with participants.
E06-1023.txt,200,12 LookAt Agent_comma_ Object 13 Grasp Agent_comma_ Object 6.5 Other Factors of Confidence in Understanding Performing action can provide strong evidence of understanding and such evidence enables participants to have strong confidence in understanding.
E06-1023.txt,201,However_comma_ other factors such as linguistic constraints not limited to surface information and plan goal inference can provide confidence in understanding without grounding.
E06-1023.txt,202,Such factors of confidence also must be incorporated to explain some repairs.
E06-1023.txt,203,Let us see a sample dialogue below_comma_ and assume that follower V missed the word red in 14.3 .
E06-1023.txt,204,14.1 U Get the white ball in front of the table.
E06-1023.txt,205,14.2 V OK. V takes a white ball 14.3 U Put it on the red table.
E06-1023.txt,206,14.4 V Sure. V puts the white ball holding in his hand on a non red table 14.5 U I said red.
E06-1023.txt,207,When commander U repairs V s misunderstanding by 14.5 _comma_ V cannot correctly decide that the repair target is not it but the red table in 14.3 by using the proposed method_comma_ because the referent of it had already been in V s hand and no explicit action choosing a ball was performed after 14.3 .
E06-1023.txt,208,However_comma_ in such a situation we seem to readily doubt misunderstanding of the table because of strong confidence in understanding of it that comes from outside of grounding process.
E06-1023.txt,209,Hence_comma_ we need a unified model of confidence in understanding that can map different sources of confidence into one dimension.
E06-1023.txt,210,Such a model is also useful for clarification management of dialogue systems.
E06-1023.txt,211,7 Discussion 7.1 Advantage of Proposed Method The method of repair target identification proposed in this paper less relies on surface information to identify targets.
E06-1023.txt,212,This is advantageous 183 against some sort of misrecognitions by automatic speech recognizers and contributes to the robustness of spoken dialogue systems.
E06-1023.txt,213,Only surface information is generally insufficient to identify repair targets.
E06-1023.txt,214,For example_comma_ assume that there is an agent acting in response to 15 and his commander interrupts him with 16 .
E06-1023.txt,215,15 Put the red ball on the table 16 Sorry_comma_ I meant blue If one tries to identify the repair target with surface information_comma_ the most likely candidate will be the red ball because of the lexical similarity.
E06-1023.txt,216,Such methods easily break down.
E06-1023.txt,217,They cannot deal with 16 after 17 .
E06-1023.txt,218,If_comma_ however_comma_ one pays attention to the state of grounding as our proposed method_comma_ he can decide which one is likely to be repaired the red ball or the green table depending on the timing of the DRIU.
E06-1023.txt,219,17 Put the red ball on the green table 7.2 Related Work McRoy and Hirst 1995 addressed the detection and resolution of misunderstandings on speech acts using abduction.
E06-1023.txt,220,Their model only dealt with speech acts and did not achieve our goals.
E06-1023.txt,221,Ardissono et al. 1998 also addressed the same problem but with a different approach.
E06-1023.txt,222,Their modelcouldalsohandlemisunderstanding regarding domain level actions.
E06-1023.txt,223,However_comma_ we think that their model using coherence to detect and resolve misunderstandings cannot handle DRIUs such as 8.5 _comma_ since both possible repairs for Obj1 and Dst1 have the same degree of coherence in their model.
E06-1023.txt,224,Although we did not adopt this_comma_ the notion of QUD questions under discussion proposed by Ginzburg Ginzburg_comma_ 1996 would be another possible approach to explaining the problems addressed in this paper.
E06-1023.txt,225,It is not yet clear whether QUD would be better or not.
E06-1023.txt,226,8 Conclusion Identifying repair targets is a prerequisite to understand disagreement repair initiation utterances DRIUs .
E06-1023.txt,227,This paper proposed a method to identify the target of a DRIU for conversational agents in action control dialogue.
E06-1023.txt,228,We explained how a repair target is identified by using the notion of common grounding.
E06-1023.txt,229,The proposed method has been implemented in our prototype system and evaluated qualitatively.
E06-1023.txt,230,We described the problems found in the evaluation and looked at the future directions to solve these problems.
E06-1023.txt,231,Acknowledgment This workwassupported in part by the Ministry of Education_comma_ Science_comma_ SportsandCultureofJapanas the Grant in Aid for Creative Basic Research No. 13NP0301. .
E06-1009.txt,1,tackle the problem of presenting a large number of options in spoken dialogue systems_comma_ we identify compelling optionsbasedonamodelofuserpreferences_comma_ and present tradeoffs between alternative options explicitly. Multiple attractive options are structured such that the user can gradually refine her request to find the optimal tradeoff.
E06-1009.txt,2,We show that our approach presents complex tradeoffs understandably_comma_ increases overall user satisfaction_comma_ and significantly improves the user s overview of the available options.
E06-1009.txt,3,Moreover_comma_ our results suggest that presenting users with a brief summary of the irrelevant options increases users confidence in having heard about all relevant options.
E06-1009.txt,5,The goal of spoken dialogue systems SDS is to offer efficient and natural access to applications and services_comma_ such as email and calendars_comma_ travelandentertainment booking_comma_ andproductrecommendation.
E06-1009.txt,6,In evaluating nine SDS in the DARPA Communicator domain flight_comma_ hotel_comma_ and rental car hire _comma_ Walker et al. 2001 found that 1 shorter task duration correlates with higher user satisfaction_comma_ and 2 the information presentation phase of dialogues is the primary contributor to dialogue duration.
E06-1009.txt,7,During this phase_comma_ the typical system enumerates the set of options that match the user s constraints_comma_ as shown in Figure 1.
E06-1009.txt,8,The user can then refine these options by offering new constraints.
E06-1009.txt,9,When the number of options to be presented is large_comma_ this process can be painstaking_comma_ leading to reduced user satisfaction.
E06-1009.txt,10,Moreover_comma_ as S I found 28 round trips from Edinburgh to Brussels on October 31st and returning Edinburgh on November 6th.
E06-1009.txt,11,For option 1_comma_ I have an outbound flight with KLM departing at 7 am and arriving at 8 49 am non stop.
E06-1009.txt,12,A return flight is with KLM departing at 10 30 am and arriving at 11 28 am with 1 stopover.
E06-1009.txt,13,Ticket price is 165.
E06-1009.txt,14,Please say next option _comma_ or_comma_ flight details _comma_ or_comma_ I ll take it .
E06-1009.txt,15,U Next option S For option 2_comma_ I have an outbound flight with KLM departing at 3 30 pm and arriving at 5 19 pm nonstop.
E06-1009.txt,16,A return flight is with KLM departing at 8 25 pm and arriving at 12 28 pm with 1 stopover.
E06-1009.txt,17,Ticket price is 165 please say next option _comma_ or_comma_ flight details _comma_ or_comma_ I ll take it. U Next option S ... Figure 1 Typical Information Presentation Phase of a Communicator Dialogue Walker et al. 2004 observe_comma_ having to access the set of available options sequentially makes it difficultfortheusertorememberthevariousaspectsof multiple options and to compare them in memory.
E06-1009.txt,18,Clearly_comma_ alternative strategies to sequential presentation of information in SDS are needed.
E06-1009.txt,19,Recently_comma_ two approaches have been proposed.
E06-1009.txt,20,In the user model UM based approach_comma_ the system identifies a small number of options that best match the user s p.
E06-1019.txt,1,work is concerned with the space of alignments searched by word alignment systems. We focus on situations where word re ordering is limited by syntax.
E06-1019.txt,2,We present two new alignment spaces that limit an ITG according to a given dependency parse.
E06-1019.txt,3,Weprovide D ITGgrammars to search these spaces completely and without redundancy.
E06-1019.txt,4,We conduct a careful comparison of five alignment spaces_comma_ and show that limiting search with an ITG reduces error rate by 10 _comma_ while a D ITG produces a 31 reduction.
E06-1019.txt,6,Bilingual word alignment finds word level correspondences between parallel sentences.
E06-1019.txt,7,The task originally emerged as an intermediate result of training the IBM translation models Brown et al._comma_ 1993 .
E06-1019.txt,8,These models use minimal linguistic intuitions they essentially treat sentences as flat strings.
E06-1019.txt,9,They remain the dominant method for word alignment Och and Ney_comma_ 2003 .
E06-1019.txt,10,There have been several proposals to introduce syntax into wordalignment.
E06-1019.txt,11,Someworkwithintheframework of synchronous grammars Wu_comma_ 1997 Melamed_comma_ 2003 _comma_ while others create a generative story that includes a parse tree provided for one of the sentences Yamada and Knight_comma_ 2001 .
E06-1019.txt,12,There are three primary reasons toadd syntax to word alignment.
E06-1019.txt,13,First_comma_ one can incorporate syntactic features_comma_ such as grammar productions_comma_ into the models that guide the alignment search.
E06-1019.txt,14,Second_comma_ movement can be modeled more naturally when a three word noun phrase moves during translation_comma_ it can be modeled as one movement operation instead of three.
E06-1019.txt,15,Finally_comma_ one can restrict the type of movement that is considered_comma_ shrinking the number of alignments that are attempted.
E06-1019.txt,16,We investigate this last advantage of syntactic alignment.
E06-1019.txt,17,We fix an alignment scoring model that works equally well on flat strings as on parse trees_comma_ but we vary the space of alignments evaluated with that model.
E06-1019.txt,18,These spaces become smaller as more linguistic guidance is added.
E06-1019.txt,19,We measure the benefits and detriments of these constrained searches.
E06-1019.txt,20,Several of the spaces we investigate draw guidance from a dependency tree for one of the sentences.
E06-1019.txt,21,We will refer to the parsed language as English and the other as Foreign.
E06-1019.txt,22,Lin and Cherry 2003 have shown that adding a dependency based cohesion constraint to an alignment search can improve alignment quality.
E06-1019.txt,23,Unfortunately_comma_ the usefulness of their beam search solution is limited potential alignments are constructed explicitly_comma_ which prevents a perfect search of alignment space and the use of algorithms like EM.
E06-1019.txt,24,However_comma_ the cohesion constraint is based on a tree_comma_ which should make it amenable to dynamic programming solutions.
E06-1019.txt,25,To enable such techniques_comma_ we bring the cohesion constraint inside the ITG framework Wu_comma_ 1997 .
E06-1019.txt,26,Zhang and Gildea 2004 compared Yamada and Knight s 2001 tree to string alignment model to ITGs.
E06-1019.txt,27,They concluded that methods like ITGs_comma_ which create a tree during alignment_comma_ perform better than methods with a fixed tree established before alignment begins.
E06-1019.txt,28,However_comma_ the use of a fixed tree is not the only difference between Yamada and Knight_comma_ 2001 and ITGs the probability models are also very different.
E06-1019.txt,29,By using a fixed dependency tree inside an ITG_comma_ we can revisit the question of whether using a fixed tree is harmful_comma_ but in a controlled environment.
E06-1019.txt,30,2 Alignment Spaces Let an alignment be the entire structure that connects a sentence pair_comma_ and let a link be the individual word to word connections that make up an alignment.
E06-1019.txt,31,An alignment space determines the set of all possible alignments that can ex145 ist for a given sentence pair.
E06-1019.txt,32,Alignment spaces can emerge from generative stories Brown et al._comma_ 1993 _comma_ from syntactic notions Wu_comma_ 1997 _comma_ or they can be imposed to create competition between links Melamed_comma_ 2000 .
E06-1019.txt,33,They can generally be described in terms of how links interact.
E06-1019.txt,34,For the sake of describing the size of alignment spaces_comma_ we will assume that both sentences have n tokens.
E06-1019.txt,35,The largest alignment space for a sentence pair has 2n2 possible alignments.
E06-1019.txt,36,This describes the case where each of the n2 potential links can be either on or off with no restrictions.
E06-1019.txt,37,2.1 Permutation Space A straight forward way to limit the space of possible alignments is to enforce a one to one constraint Melamed_comma_ 2000 .
E06-1019.txt,38,Under such a constraint_comma_ each token in the sentence pair can participate in at most one link.
E06-1019.txt,39,Each token in the English sentence picks a token from the Foreign sentence to link to_comma_ which is then removed from competition.
E06-1019.txt,40,This allows for n possible alignments1_comma_ a substantial reduction from 2n2.
E06-1019.txt,41,Note that n is also the number of possible permutations of the n tokens in either one of the two sentences.
E06-1019.txt,42,Permutation space enforces theone to one constraint_comma_ but allows anyreordering of tokens as they are translated.
E06-1019.txt,43,Permutation space methods include weighted maximum matching Taskar et al._comma_ 2005 _comma_ and approximations to maximum matching like competitive linking Melamed_comma_ 2000 .
E06-1019.txt,44,The IBM models Brown et al._comma_ 1993 search a version of permutation space with a one to many constraint.
E06-1019.txt,45,2.2 ITG Space Inversion Transduction Grammars_comma_ or ITGs Wu_comma_ 1997 provide an efficient formalism to synchronously parse bitext.
E06-1019.txt,46,Thisproduces aparse tree that decomposes both sentences and also implies a word alignment.
E06-1019.txt,47,ITGs are transduction grammars because their terminal symbols can produce tokens in both the English and Foreign sentences.
E06-1019.txt,48,Inversions occur when the order of constituents is reversed in one of the two sentences.
E06-1019.txt,49,In this paper_comma_ we consider the alignment space induced by parsing with a binary bracketing ITG_comma_ such as A AA AA e f 1 1This is a simplification that ignores null links.
E06-1019.txt,50,The actual number of possible alignments lies between n and n 1 n.
E06-1019.txt,51,The terminal symbol e f represents tokens output to the English and Foreign sentences respectively.
E06-1019.txt,52,Square brackets indicate a straight combination of non terminals_comma_ while angle brackets indicate an inverted combination A1A2 means that A1A2 appears in the English sentence_comma_ whileA2A1 appears in the Foreign sentence.
E06-1019.txt,53,Used as a word aligner_comma_ an ITG parser searches a subspace of permutation space the ITG requires that any movement that occurs during translation be explained by a binary tree with inversions.
E06-1019.txt,54,Alignments that allow no phrases to be formed in bitext are not attempted.
E06-1019.txt,55,This results in two forbidden alignment structures_comma_ shown in Figure 1_comma_ called inside out transpositions in Wu_comma_ 1997 .
E06-1019.txt,56,Note that no pair of contiguous tokens in the top a0 a1 a2 a3 a4 a5 a6 a7 a4 a5 a6 a7 a0 a1 a2 a3 Figure 1 Forbidden alignments in ITG sentence remain contiguous when projected onto the bottom sentence.
E06-1019.txt,57,Zens and Ney 2003 explore the re orderings allowed by ITGs_comma_ and provide a formulation for the number of structures that can be built for a sentence pair of size n.
E06-1019.txt,58,ITGs explore almost all of permutation space when n is small_comma_ but their coverage of permutation space falls off quickly for n 5 Wu_comma_ 1997 .
E06-1019.txt,59,2.3 Dependency Space Dependency space defines the set of all alignments that maintain phrasal cohesion with respect to a dependency tree provided for the English sentence.
E06-1019.txt,60,The space is constrained so that the phrases in the dependency tree always move together.
E06-1019.txt,61,Fox 2002 introduced the notion of headmodifier and modifier modifier crossings.
E06-1019.txt,62,These occur when a phrase s image in the Foreign sentence overlaps with theimage of itshead_comma_ or one of its siblings.
E06-1019.txt,63,An alignment with no crossings maintains phrasal cohesion.
E06-1019.txt,64,Figure 2 shows a headmodifiercrossing theimagecofahead 2overlaps with the image b_comma_d of 2 s modifier_comma_ 3_comma_4 .
E06-1019.txt,65,Lin a8 a9 a10 a11 a12 a13 a14 a15 Figure 2 A phrasal cohesion violation.
E06-1019.txt,66,andCherry 2003 used thenotion ofphrasal cohe146 sion to constrain a beam search aligner_comma_ conducting a heuristic search of the dependency space.
E06-1019.txt,67,The number of alignments in dependency space depends largely on the provided dependency tree.
E06-1019.txt,68,Because all permutations of a head and its modifiers are possible_comma_ a tree that has a single head with n 1 modifiers provides no guidance the alignment space is the same as permutation space.
E06-1019.txt,69,If the tree is a chain where every head has exactly one modifier _comma_ alignment space has only 2n permutations_comma_ which is by far the smallest space we have seen.
E06-1019.txt,70,In general_comma_ there are producttext m 1 permutations for a given tree_comma_ where stands for a head node in thetree_comma_ andm counts smodifiers.
E06-1019.txt,71,Dependency space is not a subspace of ITG space_comma_ as it can create both the forbidden alignments in Figure 1 when given a single headed tree.
E06-1019.txt,72,3 Dependency constrained ITG In this section_comma_ we introduce a new alignment space defined by a dependency constrained ITG_comma_ or D ITG.
E06-1019.txt,73,The set of possible alignments in this space is the intersection of the dependency space for a given dependency tree and ITG space.
E06-1019.txt,74,Our goal is an alignment search that respects the phrases specified by the dependency tree_comma_ but attempts all ITG orderings of those phrases_comma_ rather than all permutations.
E06-1019.txt,75,The intuition is that most ordering decisions involve only a small number of phrases_comma_ so the search should still cover a large portion of dependency space.
E06-1019.txt,76,This new space has several attractive computational properties.
E06-1019.txt,77,Since it is a subspace of ITG space_comma_ we will be able to search the space completely using a polynomial time ITG parser.
E06-1019.txt,78,This places an upper bound on the search complexity equal to ITG complexity.
E06-1019.txt,79,This upper bound is very loose_comma_ as the ITG will often be drastically constrained by the phrasal structure of the dependency tree.
E06-1019.txt,80,Also_comma_ by working in the ITG framework_comma_ we will be able to take advantage of advances in ITG parsing_comma_ and we will have access to the forward backward algorithm to implicitly count events over all alignments.
E06-1019.txt,81,3.1 A simple solution Wu 1997 suggests that in order to have an ITG take advantage of a known partial structure_comma_ one can simply stop the parser from using any spans that would violate the structure.
E06-1019.txt,82,In a chart parsing framework_comma_ this can be accomplished by assigning the invalid spans a value of before parsing begins.
E06-1019.txt,83,Our English dependency tree qualifies as a partial structure_comma_ as it does not specify a complete binary decomposition of the English sentence.
E06-1019.txt,84,In this case_comma_ any ITG span that would contain part_comma_ but not all_comma_ of two adjacent dependency phrases can be invalidated.
E06-1019.txt,85,The sentence pair can then be parsed normally_comma_ automatically respecting phrases specified by the dependency tree.
E06-1019.txt,86,For example_comma_ Figure 3a shows an alignment for the sentence pair_comma_ His house in Canada_comma_ Sa maison au Canada and the dependency tree provided for the English sentence.
E06-1019.txt,87,The spans disallowed by the tree are shown using underlines.
E06-1019.txt,88,Note that the illegal spans are those that would break up the in Canada subtree.
E06-1019.txt,89,After invalidating these spans in the chart_comma_ parsing the sentence pair with the bracketing ITG in 1 will produce the two structures shown in Figure 3b_comma_ both of which correspond to the correct alignment.
E06-1019.txt,90,This solution is sufficient to create a D ITG that obeys the phrase structure specified by a dependency tree.
E06-1019.txt,91,This allows us to conduct a complete search of a well defined subspace of the dependency space described in Section 2.3.
E06-1019.txt,92,3.2 Avoiding redundant derivations with a recursive ITG The above solution can derive two structures for the same alignment.
E06-1019.txt,93,It is often desirable to eliminate redundant structures when working with ITGs.
E06-1019.txt,94,Having a single_comma_ canonical tree structure for each possible alignment can help when flattening binary trees_comma_ as it indicates arbitrary binarization decisions Wu_comma_ 1997 .
E06-1019.txt,95,Canonical structures also eliminate double counting when performing tasks like EM Zhang and Gildea_comma_ 2004 .
E06-1019.txt,96,The nature of null link handling in ITGs makes eliminating all redundancies difficult_comma_ but we can at least eliminate them in the absence of nulls.
E06-1019.txt,97,Normally_comma_ one would eliminate the redundant structures produced by the grammar in 1 by replacing it with the canonical form grammar Wu_comma_ 1997 _comma_ which has the following form S A B C A AB  BB  CB  AC  BC  CC B AA BA CA AC BC CC C e f 2 By design_comma_ this grammar allows only one struc147 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a2 a8 a10 a8 a1 a2 a3 a6 a8 a4 a7 a8 a6 a8 a9 a8 a8 a11 a12 a11 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a13 a11 Figure 3 An example of how dependency trees interact with ITGs. a shows the input_comma_ dependency tree_comma_ and alignment.
E06-1019.txt,98,Invalidated spans are underlined. b shows valid binary structures.
E06-1019.txt,99,c shows the canonical ITG structure for this alignment.
E06-1019.txt,100,a14 a15 a16 a14 a17 a18 a16 a19 a15 a20 a15 a20 a21 a22 a20 a22 a23 a22 a24 a25 a26 a24 a25 a26 Figure 4 A recursive ITG. ture per alignment.
E06-1019.txt,101,It works by restricting rightrecursion to specific inversion combinations.
E06-1019.txt,102,The canonical structure for a given alignment is fixed by this grammar_comma_ without awareness of the dependency tree.
E06-1019.txt,103,When the dependency tree invalidates spans that are used in canonical structures_comma_ the parser will miss the corresponding alignments.
E06-1019.txt,104,The canonical structure corresponding to the correct alignment in our running example is shown in Figure 3c.
E06-1019.txt,105,This structure requires the underlined invalid span_comma_ so the canonical grammar fails to produce the correct alignment.
E06-1019.txt,106,Our task requires a new canonical grammar that isawareofthe dependency tree_comma_ and will choose among valid structures deterministically.
E06-1019.txt,107,Our ultimate goal is to fall back to ITG reordering when the dependency tree provides no guidance.
E06-1019.txt,108,We can implement this notion directly with a recursive ITG.
E06-1019.txt,109,Let a local tree be the tree formed by a head node and its immediate modifiers.
E06-1019.txt,110,We begin our recursive process by considering the local tree at the root of our dependency tree_comma_ and marking each phrasal modifier with a labeled placeholder.
E06-1019.txt,111,We then create a string by flattening the local tree.
E06-1019.txt,112,The top oval of Figure 4 shows the result of this operation on our running example.
E06-1019.txt,113,Because all phrases have been collapsed to placeholders_comma_ an ITG built over this string will naturally respect the dependency tree s phrasal boundaries.
E06-1019.txt,114,Since we do not need to invalidate any spans_comma_ we can parse this string using the canonical ITG in 2 .
E06-1019.txt,115,The phrasal modifiers can in turn be processed by applying the same algorithm recursively to their root nodes_comma_ as shown in the lower oval of Figure 4.
E06-1019.txt,116,This algorithm will explore the exact same alignment space as the solution presented in Section 3.1_comma_ but because it uses acanonical ITGateveryordering decision point_comma_ it will produce exactly one structure for each alignment.
E06-1019.txt,117,Returning to our running example_comma_ the algorithm will produce the left structure of Figure 3b.
E06-1019.txt,118,Thisrecursive approach can be implemented inside a traditional ITG framework using grammar templates.
E06-1019.txt,119,The templates take the form of whatever grammar will be used to permute the local trees.
E06-1019.txt,120,They are instantiated over each local tree before ITG parsing begins.
E06-1019.txt,121,Each instantiation has its non terminals marked with its corresponding span_comma_ and its pre terminal productions are customized to match the modifiers of the local tree.
E06-1019.txt,122,Phrasal modifiers point to another instantiation of thetemplate.
E06-1019.txt,123,Inourcase_comma_ thetemplatecorresponds to the canonical form grammar in 2 .
E06-1019.txt,124,The result of applying the templates to our running example is S0_comma_4 A0_comma_4 B0_comma_4 C0_comma_4 A0_comma_4 A0_comma_4B0_comma_4  B0_comma_4B0_comma_4  C0_comma_4B0_comma_4  A0_comma_4C0_comma_4  B0_comma_4C0_comma_4  C0_comma_4C0_comma_4 B0_comma_4 A0_comma_4A0_comma_4  B0_comma_4A0_comma_4  C0_comma_4A0_comma_4  A0_comma_4C0_comma_4  B0_comma_4C0_comma_4  C0_comma_4C0_comma_4 C0_comma_4 his f house f S2_comma_4 S2_comma_4 A2_comma_4 B2_comma_4 C2_comma_4 A2_comma_4 A2_comma_4B2_comma_4  B2_comma_4B2_comma_4  C2_comma_4B2_comma_4  A2_comma_4C2_comma_4  B2_comma_4C2_comma_4  C2_comma_4C2_comma_4 B2_comma_4 A2_comma_4A2_comma_4  B2_comma_4A2_comma_4  C2_comma_4A2_comma_4  A2_comma_4C2_comma_4  B2_comma_4C2_comma_4  C2_comma_4C2_comma_4 C2_comma_4 in f Canada f Recursive ITGsand grammar templates provide a conceptual framework to easily transfer grammars for flat sentence pairs to situations with fixed phrasal structure.
E06-1019.txt,125,We have used the framework here to ensure only one structure is constructed for each possible alignment.
E06-1019.txt,126,We feel that this recursive view of the solution also makes it easier to visualize the space that the D ITG is searching.
E06-1019.txt,127,It is trying all ITG orderings of each head and its modifiers.
E06-1019.txt,128,148 a0 a1 a2 a3 a4 a0 a1 a2 a1 a5 a6 a7 a8 a9 a10 a11 Figure 5 A counter intuitive ITG structure.
E06-1019.txt,129,3.3 Head constrained ITG D ITGs can construct ITG structures that do not completely agree with the provided dependency tree.
E06-1019.txt,130,If a head in the dependency tree has more than one modifier on one of its sides_comma_ then those modifiers may form a phrase in the ITG that should not exist according to the dependency tree.
E06-1019.txt,131,For example_comma_ the ITG structure shown in Figure 5 will be considered by our D ITG as it searches alignment space.
E06-1019.txt,132,The resulting here quickly subtree disagrees with our provided dependency tree_comma_ which specifies that ran ismodified byeach word individually_comma_ and not by a phrasal concept that includes both.
E06-1019.txt,133,This is allowed by the parser because we have made the ITG aware of the dependency tree s phrasal structure_comma_ but it still has no notion of heads or modifiers.
E06-1019.txt,134,It is possible that by constraining our ITG according to this additional syntactic information_comma_ we can provide further guidance to our alignment search.
E06-1019.txt,135,The simplest way to eliminate these modifier combinations is to parse with the redundant bracketing grammar in 1 _comma_ and to add another set of invalid spans to the set described in Section 3.1.
E06-1019.txt,136,These new invalidated chart entries eliminate all spans that include two or more modifiers without their head.
E06-1019.txt,137,With this solution_comma_ the structure in Figure 5 is no longer possible.
E06-1019.txt,138,Unfortunately_comma_ the grammar allows multiple structures for each alignment to represent an alignment with no inversions_comma_ this grammar will produce all three structures shown in Figure 6.
E06-1019.txt,139,If we can develop a grammar that will produce canonical head aware structures for local trees_comma_ we can easily extend it to complete dependency trees using the concept of recursive ITGs.
E06-1019.txt,140,Such a grammar requires a notion of head_comma_ so we can ensure that every binary production involves the head or a phrase containing the head.
E06-1019.txt,141,A redundant_comma_ headaware grammar is shown here A MA MA AM AM H M he f here f quickly f H ran f 3 Note that two modifiers can never be combined without also including the A symbol_comma_ which always contains the head.
E06-1019.txt,142,This grammar still considers all the structures shown in Figure 6_comma_ but it requires no chart preprocessing.
E06-1019.txt,143,We can create a redundancy free grammar by expanding 3 .
E06-1019.txt,144,Inspired by Wu s canonical form grammar_comma_ we will restrict the productions so that certain structures are formed only when needed for specific inversion combinations.
E06-1019.txt,145,Tospecify the necessary inversion combinations_comma_ our ITG will need more expressive non terminals.
E06-1019.txt,146,Split A into two non terminals_comma_ L and R_comma_ to represent generators for left modifiers and right modifiers respectively.
E06-1019.txt,147,Then split L into L and L_comma_ for generators that produce straight and inverted left modifiers.
E06-1019.txt,148,We now have a rich enough non terminal set to design a grammar with a default behavior it will generate all right modifiers deeper in the bracketing structure than all left modifiers.
E06-1019.txt,149,This rule is broken only to create a re ordering that is not possible with the default structure_comma_ such as MH M .
E06-1019.txt,150,A grammar that accomplishes this goal is shown here S L L R R bracketleftBig LM bracketrightBig angbracketleftbig LMangbracketrightbig RM RM H L bracketleftbigM Lbracketrightbig bracketleftBigM LbracketrightBig MR L angbracketleftbigM Langbracketrightbig angbracketleftBigM LangbracketrightBig MR M he f here f quickly f H ran f 4 This grammar will generate one structure for each alignment.
E06-1019.txt,151,In the case of an alignment with no inversions_comma_ it will produce the tree shown in Figure6c.
E06-1019.txt,152,Thegrammarcanbeexpanded intoarecursive ITG by following a process similar to the one explained in Section 3.2_comma_ using 4 as a template.
E06-1019.txt,153,3.3.1 The head constrained alignment space Because we have limited the ITG s ability to combine them_comma_ modifiers of the same head can no longer occur at the same level of any ITG tree.
E06-1019.txt,154,In Figure 6_comma_ we see that in all three valid structures_comma_ quickly is attached higher in the tree than here .
E06-1019.txt,155,As a result of this_comma_ no combination of inversions can bring quickly between here and ran .
E06-1019.txt,156,In general_comma_ the alignment space searched by this ITG is constrained so that_comma_ among modifiers_comma_ relative distance from head is maintained.
E06-1019.txt,157,More formally_comma_ let Mi and Mo be modifiers of H such that Mi appears between Mo and H in the dependency tree.
E06-1019.txt,158,No alignment will ever place the 149 a0 a1 a2 a3 a4 a0 a1 a2 a1 a5 a6 a7 a8 a9 a10 a11 a0 a1 a2 a3 a4 a0 a1 a2 a1 a5 a6 a7 a8 a9 a10 a11 a0 a1 a2 a3 a4 a0 a1 a2 a1 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a13 a15 a13 Figure 6 Structures allowed by the head constraint. outer modifier Mo between H and the inner modifier Mi.
E06-1019.txt,159,4 Experiments and Results Wecomparethealignment spaces described inthis paper under two criteria.
E06-1019.txt,160,First we test the guidance provided by a space_comma_ or its capacity to stop an aligner from selecting bad alignments.
E06-1019.txt,161,We also test expressiveness_comma_ or how often a space allows an aligner to select the best alignment.
E06-1019.txt,162,In all cases_comma_ we report our results in terms of alignment quality_comma_ using the standard word alignment error metrics precision_comma_ recall_comma_ F measure andalignment error rate OchandNey_comma_ 2003 .
E06-1019.txt,163,Our test set is the 500 manually aligned sentence pairs created by Franz Och and Hermann Ney 2003 .
E06-1019.txt,164,These English French pairs are drawn from the Canadian Hansards.
E06-1019.txt,165,English dependency trees are supplied by Minipar Lin_comma_ 1994 .
E06-1019.txt,166,4.1 Objective Function In our experiments_comma_ we hold all variables constant except for the alignment space being searched_comma_ and in the case of imperfect searches_comma_ the search method.
E06-1019.txt,167,In particular_comma_ all of the methods we test will use the same objective function to select the best alignment from their space.
E06-1019.txt,168,Let A be an alignment for an English_comma_ Foreign sentence pair_comma_ E_comma_F .
E06-1019.txt,169,A is represented as a set of links_comma_ where each link is a pair of English and Foreign positions_comma_ i_comma_j _comma_ that are connected by the alignment.
E06-1019.txt,170,The score of a proposed alignment is falign A_comma_E_comma_F summationdisplay a A flink a_comma_E_comma_F 5 Note that this objective function evaluates each link independently_comma_ unaware of the other links selected.
E06-1019.txt,171,Taskar et al 2005 have shown that with a strong flink_comma_ one can achieve state of the art results using this objective function and the maximum matching algorithm.
E06-1019.txt,172,Our two experiments will vary the definition of flink to test different aspects of alignment spaces.
E06-1019.txt,173,All of the methods will create only one to one alignments.
E06-1019.txt,174,Phrasal alignment would introduce unnecessary complications that could mask some of the differences in the re orderings defined by these spaces.
E06-1019.txt,175,4.2 Search methods tested We test seven methods_comma_ one for each of the four syntactic spaces described in this paper_comma_ and three variations of search in permutation space Greedy A greedy search of permutation space.
E06-1019.txt,176,Links are added in the order of their link scores.
E06-1019.txt,177,This corresponds to the competitive linking algorithm Melamed_comma_ 2000 .
E06-1019.txt,178,Beam A beam search of permutation space_comma_ where links are added to a growing alignment_comma_ biased bytheirlinkscores.
E06-1019.txt,179,Beamwidth is 2 and agenda size is 40.
E06-1019.txt,180,Match The weighted maximum matching algorithm West_comma_ 2001 .
E06-1019.txt,181,This is a perfect search of permutation space.
E06-1019.txt,182,ITG The alignment resulting from ITG parsing with the canonical grammar in 2 .
E06-1019.txt,183,This is a perfect search of ITG space.
E06-1019.txt,184,Dep A beam search of the dependency space.
E06-1019.txt,185,Thisisequivalent toBeamplusadependency constraint.
E06-1019.txt,186,D ITG The result of ITG parsing as described in Section 3.2.
E06-1019.txt,187,This is a perfect search of the intersection of the ITG and dependency spaces.
E06-1019.txt,188,HD ITG TheD ITG method with an added head constraint_comma_ as described in Section 3.3.
E06-1019.txt,189,4.3 Learned objective function Thelinkscoreflink isusually imperfect_comma_ because it is learned from data.
E06-1019.txt,190,Appropriately defined alignment spaces may rule out bad links even if they areassigned highflink values_comma_ based onother links in the alignment.
E06-1019.txt,191,We define the following simple link score to test the guidance provided by different alignment spaces flink a_comma_E_comma_F  2 ei_comma_fj C i j 6 Here_comma_ a i_comma_j is a link and 2 ei_comma_fj returns the 2 correlation metric Gale and Church_comma_ 1991 150 Table 1 Results with the learned link score.
E06-1019.txt,192,Method Prec Rec F AER Greedy 78.1 81.4 79.5 20.47 Beam 79.1 82.7 80.7 19.32 Match 79.3 82.7 80.8 19.24 ITG 81.8 83.7 82.6 17.36 Dep 88.8 84.0 86.6 13.40 D ITG 88.8 84.2 86.7 13.32 HD ITG 89.2 84.0 86.9 13.15 between the English token at i and the Foreign token at j.
E06-1019.txt,193,The 2 scores were obtained using co occurrence counts from 50k sentence pairs of Hansard data.
E06-1019.txt,194,The second term is an absolute position penalty.
E06-1019.txt,195,C is a small constant selected to be just large enough to break ties in favor of similar positions.
E06-1019.txt,196,Links to null are given a flat score of 0_comma_ while token pairs with no value in our 2 table are assigned 1.
E06-1019.txt,197,The results of maximizing falign on our test set are shown in Table 1.
E06-1019.txt,198,The first thing to note is that our flink is not artificially weak.
E06-1019.txt,199,Our function takes into account token pairs and position_comma_ making it roughly equivalent to IBM Model 2.
E06-1019.txt,200,Our weakest method outperforms Model 2_comma_ which scores an AERof 22.0 on this test set whentrained with roughly twice as many sentence pairs Och and Ney_comma_ 2003 .
E06-1019.txt,201,The various search methods fall into three categories in terms of alignment accuracy.
E06-1019.txt,202,The searches through permutation space allhave AERs of roughly 20_comma_ with the more complete searches scoring better.
E06-1019.txt,203,The ITG method scores an AER of 17.4_comma_ a 10 reduction in error rate from maximum matching.
E06-1019.txt,204,This indicates that the constraints established by ITG space are beneficial_comma_ even before adding an outside parse.
E06-1019.txt,205,The three dependency tree guided methods all have AERs of around 13.3.
E06-1019.txt,206,This is a 31 improvement over maximum matching.
E06-1019.txt,207,One should also note that_comma_ with the exception of the HD ITG_comma_ recall goes up as smaller spaces are searched.
E06-1019.txt,208,In a one to one alignment_comma_ enhancing precision canalso enhance recall_comma_ asevery error of commission avoided presents two new opportunities to avoid an error of omission.
E06-1019.txt,209,The small gap between the beam search and maximum matching indicates that for this flink_comma_ the beam search is a good approximation to complete enumeration of a space.
E06-1019.txt,210,This is important_comma_ as the only method we have available to search dependency space is also a beam search.
